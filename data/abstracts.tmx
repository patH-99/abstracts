<?xml version="1.0" encoding="UTF-8"?>
<tmx version="1.4">
  <header creationtool="Python Script" creationtoolversion="1.0" datatype="PlainText" segtype="sentence" o-tmf="XML" adminlang="en" srclang="cs"/>
  <body>

        <tu>
            <tuv xml:lang="cs">
                <seg>Studie zkoumá kvalitu překladů, zejména styl včetně přítomnosti rušivých vlivů zdrojového jazyka, a to na základě analýzy kvality českých překladů dvou anglických novinových článků. Překlady vyhotovili tři překladatelé najatí překladatelskou agenturou. Další oblastí, kterou studie zkoumá, je tvorba nadlidských překladů, tj. překladů považovaných za nejlepší možné za daných podmínek. Hodnocení kvality se zaměřuje především na styl stávajících překladů a zjištěné problémy spadají do široké škály jazykových kategorií, jako je pravopis, morfologie, gramatika, slovní zásoba a tvorba slov. Zvláštní důraz je kladen na interference a návrh typologie sestavený autory lze rozšířit o několik dalších typů interference, které se v analyzovaných překladech opakují. Nadlidské překlady, vyplývající z týmové práce dvou překladatelů a teoretiků, kteří syntetizovali to nejlepší ze tří existujících překladů a přitom využívali značnou část vlastní kreativity, mohou být v budoucnu použity při hodnocení vynikajících strojových překladů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The study explores translation quality, particularly style including the presence of source-
language interference, by analysing the quality of Czech translations of two English newspaper
articles, done by three translators hired by a translation company; the other area explored by the study
is the creation of superhuman translations, i.e. translations thought to be the best possible under the
given conditions. The primary focus of the quality assessment is on the style of the existing
translations, and the problems identified fall into a wide range of linguistic categories such as spelling,
morphosyntax, grammar, lexicon and word formation. Special emphasis is placed on interference, and
the draft typology compiled by the authors can be expanded to include several other types of
interference recurrent in the translations analysed. The superhuman translations, resulting from the
teamwork of two translators-cum-theoreticians who synthesised the best of the three existing
translations while employing a considerable amount of their own creativity, may be used in future
assessments of excellent machine translations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem systémů účastnících se v morfosegmentační úloze při workshopu SIGMORPHON 2022 bylo rozdělit slovo na sekvenci morfémů a pokrýt přitom většinu morfologických procesů (inflexi, derivaci a skládání). Podúloha 1 byla zaměřená na rozdělování 5 miliónů slov v 9 jazycích (čeština, angličtina, španělština, maďarština, francouzština, italština, ruština, latina, mongolština). Zúčastnilo se 13 systémů od 7 týmů, přičemž nejlepší systém dosáhl průměrné úspěšnosti 97.29% F1, s rozsahem hodnot od 93.84% pro angličtinu po 99.38% pro latinu.
Druhý podúkol byl zaměřený na segmentaci slov ve větném kontextu, celkam 18.735 vět pro tři jazyky (čeština, angličtina, mongolština). Zúčastnilo se 10 systémů od 3 týmů. Nejlepší systémy překročily dosavadní nejlepší metody od 30.71 % absolutně. Pro zjednodušení chybové analýzy a přípravu budoucích studií jsme všechny predikce zveřejnili.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The SIGMORPHON 2022 shared task on morpheme segmentation challenged systems to decompose a word into a sequence of morphemes and covered most types of morphology: compounds, derivations, and inflections. Subtask 1, word-level morpheme segmentation, covered 5 million words in 9 languages (Czech, English, Spanish, Hungarian, French, Italian, Russian, Latin, Mongolian) and received 13 system submissions from 7 teams and the best system averaged 97.29% F1 score across all languages, ranging English (93.84%) to Latin (99.38%). Subtask 2, sentence-level morpheme segmentation, covered 18,735 sentences in 3 languages (Czech, English, Mongolian), received 10 system submissions from 3 teams, and the best systems outperformed all three state-of-the-art subword tokenization methods (BPE, ULM, Morfessor2) by 30.71% absolute. To facilitate error analysis and support any type of future studies, we released all system predictions, the evaluation script, and all gold standard datasets.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace o projektech na ÚFALu a PhD projektu Multi-Source Speech Translation pro mgr. studenty na LCT Summer School 30.8.2022, délka 15 minut.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Prezentation about ÚFAL projects and PhD project.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Statické a kontextové vícejazyčné embeddingy mají komplementární přednosti. Statické embeddingy, i když jsou méně expresivní než kontextové jazykové modely, se dají lépe párovat mezi jazyky. V tomto článku kombinujeme přednosti statických a kontextových, čímž docílíme vyšší kvality vícejazyčných kontextových embedingů. Z modelu XLM-R extrahujeme statické embeddingy pro 40 jazyků, validujeme jejich kvalitu pomocí indukce dvojjazyčných slovníků a pak je zarovnáváme pomocí nástroje VecMap. Výsledkem jsou vysoce kvalitní, vysoce vícejazyčné statické embeddingy. Poté aplikujeme nový přístup pokračujícího pre-training modelu XLM-R, kde využíváme tyto statické embeddingy pro lepší zarovnání reprezentačního prostoru XLM-R. Náš postup dosazuje pozitivních výsledků pro sémanticky náročných úloh. Statické embeddingy a kód pokračujícího pre-training jsou veřejně dostupné. Na rozdíl od většiny předchozí práce náš přístup pokračujícího pre-training nevyžaduje paralelní text.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Static and contextual multilingual embeddings have complementary strengths. Static embeddings, while less expressive than contextual language models, can be more straightforwardly aligned across multiple languages. We combine the strengths of static and contextual models to improve multilingual representations. We extract static embeddings for 40 languages from XLM-R, validate those embeddings with cross-lingual word retrieval, and then align them using VecMap. This results in high-quality, highly multilingual static embeddings. Then we apply a novel continued pre-training approach to XLM-R, leveraging the high quality alignment of our static embeddings to better align the representation space of XLM-R. We show positive results for multiple complex semantic tasks. We release the static embeddings and the continued pre-training code. Unlike most previous work, our continued pre-training approach does not require parallel text.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Hodnoticí kampaň 19. Mezinárodní konference o překladu z mluveného jazyka (IWSLT 2021) letos představila osm sdílených úkolů: (i) Simultánní překlad mluvené řeči, (ii) Offline překlad mluvené řeči, (iii) Překlad řeči na řeč, (iv) Překlad řeči s malým množstvím zdrojů, (v) Vícejazyčný překlad řeči, (vi) Překlad řeči v dialektu, (vii) Kontrola formality pro překlad řeči, (viii) Izometrický překlad řeči. Celkem se zúčastnilo 27 týmů alespoň na jedné z úloh. Tento dokument u každého sdíleného úkolu podrobně popisuje účel úkolu, zveřejněná data, použité metriky hodnocení, obdržené podání a dosažené výsledky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The evaluation campaign of the 19th International Conference on Spoken Language Translation featured eight shared tasks: (i) Simultaneous speech translation, (ii) Offline speech translation, (iii) Speech to speech translation, (iv) Low-resource speech translation, (v) Multilingual speech translation, (vi) Dialect speech translation, (vii) Formality control for speech translation, (viii) Isometric speech translation. A total of 27 teams participated in at least one of the shared tasks. This paper details, for each shared task, the purpose of the task, the data that were released, the evaluation metrics that were applied, the submissions that were received and the results that were achieved.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Technické termíny mohou vyžadovat zvláštní zacházení, pokud je cílová skupina dvojjazyčná, v závislosti na kulturních a vzdělávacích
normách dané společnosti. Zejména některé překladové situace mohou vyžadovat „zachování termínů“, tj. ponechání technických termínů zdrojového jazyka v cílovém jazyce výstupu, aby vznikla plynulá a srozumitelná věta.
Ukazujeme, že standardní model strojového překladu založený na Transformeru lze snadno přizpůsobit k dosažení tohoto cíle, aniž by přitom trpěla kvalita jeho výstupu obecně.
Představujeme anglicko-hindský model, který je natrénovat k uposlechnutí signálu „zachování“, tj. za běhu zajistí, že vybrané termíny nebudou přeloženy. Navrženou metodu vyhodnocujeme automatickými metrikami (BLEU pro překlad obecně, F1 pro zachování termínů), i ručně (celková kvalita výstupních vět).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Technical terms may require special handling
when the target audience is bilingual,
depending on the cultural and educational
norms of the society in question. In particular,
certain translation scenarios may require
“term retention” i.e.
preserving of the
source language technical terms in the target
language output to produce a fluent and
comprehensible code­switched sentence.
We show that a standard transformer­based
machine translation model can be adapted
easily to perform this task with little or no
damage to the general quality of its output.
We present an English­to­Hindi model that is
trained to obey a “retain” signal, i.e. it can
perform the required code­switching on a list
of terms, possibly unseen, provided at runtime.
We perform automatic evaluation using BLEU
as well as F1 metrics on the list of retained
terms; we also collect manual judgments on
the quality of the output sentences.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slovní embeddingy jsou ve zpracování přirozeného jazyka čím dál podstatnější komponentou. Tento článek představuje novou metodu pro přenos statických podslovných embeddings z jazyka s relativním dostatkem zdrojů do jazyka s nedostatkem zdrojů. Primárně pracujeme s jazykovým párem hindština-márátština, přičemž pro márátštinu je nedostatek zdrojů pouze simulovaný a výsledky dále potvrzujme na nepálštině. Náš přístup výrazně překonává baseline fastText pro oba jazyky na úlohách podobnost slov a testování synonymie. Na první úloze je úspěšnost na máráštině dokonce srovnatelná s úspěšností standardní metody při využití o tři řády většího množství dat.


 We primarily work with Hindi-Marathi, simulating a low-resource scenario for Marathi, and confirm observed trends on Nepali. We demonstrate the consistent benefits of unsupervised morphemic segmentation on both source and target sides over the treatment performed by fastText. Our best-performing approach uses an EM-style approach to learning bilingual subword embeddings; we also show, for the first time, that a trivial “copy-and-paste” embeddings transfer based on even
perfect bilingual lexicons is inadequate in cap-
turing language-specific relationships. We find that our approach substantially outperforms the fastText baselines for both Marathi and Nepali on the Word Similarity task as well as WordNet-Based Synonymy Tests; on the former task, its  performance for Marathi is close to that of pretrained fastText embeddings that use three orders of magnitude more Marathi data.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Word embeddings are growing to be a crucial resource in the field of NLP for any language. This work introduces a novel technique for static subword embeddings transfer for Indic languages from a relatively higher resource language to a genealogically related low resource language. We primarily work with Hindi-Marathi, simulating a low-resource scenario for Marathi, and confirm observed trends on Nepali. We demonstrate the consistent benefits of unsupervised morphemic segmentation on both source and target sides over the treatment performed by fastText. Our best-performing approach uses an EM-style approach to learning bilingual subword embeddings; we also
show, for the first time, that a trivial “copy-
and-paste” embeddings transfer based on even perfect bilingual lexicons is inadequate in capturing language-specific relationships. We find that our approach substantially outperforms the fastText baselines for both Marathi and Nepali on the Word Similarity task as well as WordNet-Based Synonymy Tests; on the former task, its performance for Marathi is close to that of pretrained fastText embeddings that use three orders of magnitude more Marathi data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nástroj vytvořený na bázi hlubokého učení nazvaný Word Formation Analyzer for Czech, který na základě vstupního lexému vrátí lemma nebo lemmata, jež tvoří jeho základ. Této úloze říkáme rozpoznávání základových slov. Na základě počtu slov ve výstupní sekvenci a jejím srovnání se vstupem lze pak vstup klasifikovat do jedné ze tří kategorií: kompozitum, derivát, nebo nemotivované. Této úloze říkáme slovotvorná klasifikace. V rozpoznávání základových slov dosáhl Word Formation Analyzer for Czech accuracy 71%, v klasifikaci slovotvorných procesů pak 87%.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a deep-learning tool called Word Formation Analyzer for Czech, which, given an
input lexeme, automatically retrieves the lemma or lemmas from which the input lexeme was
formed. We call this task parent retrieval. Furthermore, based on the number of words in the
output sequence and its comparison to the input, the input word is classified into one of three
categories: compound, derivative or unmotivated. We call this task word formation classification.
In the task of parent retrieval, Word Formation Analyzer for Czech achieved an accuracy of 71%.
In word formation classification, the tool achieved an accuracy of 87%.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nedávný pokrok ve standardizaci anotovaných jazykových zdrojů vedl k úspěšným velkým projektům jako Universal Dependencies (UD), kde se syntakticky anotují data pro mnoho jazyků. Anotace koreference, která spojuje opakované zmínky téže entity v textu a je pro porozumění jazyku velmi důležitá, je zatím standardizačním úsilím relativně nepoznamenaná. V tomto článku prezentujeme CorefUD, mnohojazyčnou sbírku korpusů a standardizovaný formát pro anotaci koreference, kompatibilní s morfosyntaktickou anotací v UD a rozšiřitelný na příbuzné úlohy, jako je rozpoznávání pojmenovaných entit. Jde o první krok směrem ke konvergenci koreferenčních zdrojů napříč jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Recent advances in standardization for annotated language resources have led to successful large scale efforts, such as the Universal Dependencies (UD) project for multilingual syntactically annotated data. By comparison, the important task of coreference resolution, which clusters multiple mentions of entities in a text, has yet to be standardized in terms of data formats or annotation guidelines. In this paper we present CorefUD, a multilingual collection of corpora and a standardized format for coreference resolution, compatible with morphosyntactic annotations in the UD framework and including facilities for related tasks such as named entity recognition, which forms a first step in the direction of convergence for coreference resolution across languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CorefUD je kolekce existujících korpusů s anotací koreference, které jsme převedli na jednotné anotační schéma. Ve své současné verzi 1.0 CorefUD celkem obsahuje 17 korpusů pro 11 jazyků a v porovnání s verzí 0.2 obsahuje přepracovaný formát souborů a opravy řady chyb.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CorefUD is a collection of previously existing datasets annotated with coreference, which we converted into a common annotation scheme. In total, CorefUD in its current version 0.2 consists of 17 datasets for 11 languages, and compared to the version 0.2, the file format has been reworked and a number of annotation errors have been fixed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládáme přehled o stavu výzkumu v oblasti strojového překladu s nízkými zdroji. V současnosti se na světě mluví asi 7 000 jazyky a téměř všem jazykovým párům chybí významné zdroje pro trénování modelů strojového překladu. Vzrůstá zájem o výzkum zabývající se problémem vytváření užitečných překladatelských modelů, když je k dispozici velmi málo přeložených trénovacích dat. Předkládáme shrnutí této aktuální oblasti na vysoké úrovni a poskytujeme přehled osvědčených postupů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a survey covering the state of the art in low-resource machine translation. There
are currently around 7000 languages spoken in the world and almost all language pairs lack
significant resources for training machine translation models. There has been increasing interest
in research addressing the challenge of producing useful translation models when very little
translated training data is available. We present a high level summary of this topical field and
provide an overview of best practices.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Presentation of language tools for Česká spořitelna.
Speech Recognition, Immediate Response Machine Translation, Cross-Lingual Information Retrieval, Question Answering, Automatic Summarization of Meetings.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Prezentace jazykových nástrojů pro Českou spořitelnu.
Rozpoznávání mluvené řeči, strojový překlad v rámci okamžité odpovědi, mezijazyčné vyhledávání informací, odpovídání na otázky, automatické shrnování schůzek.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této kapitole uvádíme hlavní úspěchy české velké výzkumné infrastruktury LINDAT/CLARIAH-CZ. Poskytujeme krátký popis infrastruktury a její historii a stručný popis jejího vědeckého, technologického a infrastrukturního rozsahu. Zaměřujeme se na technologické inovace již implementované v úložišti a v nabídkách služeb a nastiňujeme některé budoucí plány.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this chapter we present the main achievements of the Czech large research infrastructure LINDAT/CLARIAH-CZ. We provide a short description of the infrastructure and its history, and a brief account of its scientific, technological and infrastructural scope. We  focus on the technological innovations already implemented in the repository and in the service offerings, and outline some future plans.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku jsou uvedeny příklady synonymie a homonymie v češtině. Zdrojem homonymie jsou morfologické, syntaktické a lexikální prostředky a jejich kombinace ve větě vedoucí k jejich povrchové identitě. Synonymii demonstrují věty s různými syntaktickými a morfemickými formami vyjadřujícími stejný význam. Místní příslovce s významem Kde? (LOC) jsou zde analyzovány podrobněji. Pozornost je zaměřena na analýzu předložkových konstrukcí s předložkou v(e) + Loc [in] a na + Loc [on], u + Gen [at/by]. Jsou navržena kritéria pro určování synonymie a gramatických požadavků na formu lokálních určení. Analýza se omezuje na úzkou část lokálních významů s cílem představit obě strany jazykového znaku z hlediska jeho asymetrie.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper examples of synonymy and homonymy in Czech are presented. The sources of   homonymy are exemplified by the morphological, syntactic and lexical means and their combinations in the sentence leading to their surface identity. Synonymy is demonstrated by sentences with different syntactic and morphemic forms expressing the same meaning. Local adverbials with the meaning Where? (LOC) are analyzed here in a more detail. Attention is focused on the analysis of the prepositional constructions with the preposition v(e) + Loc [in] and na + Loc [on] and the differences between the construction including them as well as examples of their interchangeability  are demonstrated. Prepositional constructions with the preposition u + Gen [at/by] are added to the central domain of the local adverbials connected with the answer to the question Where? [LOC]. Criteria for the description of grammatical requirements for their form as well as for their lexicalized forms applied in this domain are proposed as the sources of the relation of homonymy and synonymy. The analysis is, however, limited to the narrow part of local meanings with the aim to present both sides of the language signs from the point of view of their asymmetry demonstrated by means of the transparent part of grammatical description.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Náš článek se zaměřuje na vývoj vícejazyčného zdroje dat pro morfologickou segmentaci. Představujeme přehled 17 existujících
datových zdrojů relevantních pro segmentaci ve 32 jazycích a analyzujeme rozmanitost způsobů, jakými jsou v nich jednotlivé jazykové jevy zachyceny. Nechali jsme se inspirovat úspěchem Universal Dependencies a navrhujeme harmonizované schéma pro reprezentaci segmentaci a převádíme data z těchto zdrojů do jednotného schématu. Harmonizované verze zdrojů dostupné pod bezplatnými licencemi jsou publikovány jako kolekce s názvem UniSegments 1.0.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Our work aims at developing a multilingual data resource for morphological segmentation. We present a survey of 17 existing
data resources relevant for segmentation in 32 languages, and analyze diversity of how individual linguistic phenomena are
captured across them. Inspired by the success of Universal Dependencies, we propose a harmonized scheme for segmentation
representation, and convert the data from the studied resources into this common scheme. Harmonized versions of resources
available under free licenses are published as a collection called UniSegments 1.0.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Segmentations (UniSegments) je kolekce lexikálních zdrojů zachycujících morfologické segmentace mnoha jazyků harmonizované do lingvisticky konzistentního anotačního schématu. Anotační schéma je uloženo jednoduchém sloupcovém formátu, přičemž jednotlivé sloupce jsou odděleny tabulátory. K jednotlivým slovům se ukládá jejich morfologická segmentace, včetně různých informací o slovech a segmentovaných jednotkách, např. slovní druhy, typy morfů/morfémů atd. Současná veřejná verze kolekce obsahuje 38 harmonizovaných datových souborů pokrývajících 30 různých jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Segmentations (UniSegments) is a collection of lexical resources capturing morphological segmentations harmonised into a cross-linguistically consistent annotation scheme for many languages. The annotation scheme consists of simple tab-separated columns that stores a word and its morphological segmentations, including pieces of information about the word and the segmented units, e.g., part-of-speech categories, type of morphs/morphemes etc. The current public version of the collection contains 38 harmonised segmentation datasets covering 30 different languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje přehled výsledků soutěže (společné úlohy) ve vícejazyčné automatické analýze koreference, která byla přidružená k workshopu CRAC 2022. Účastníci soutěže měli vyvinout trénovatelné systémy schopné identifikovat zmínky o entitách a shlukovat tyto zmínky na základě identické koreference. Jako zdroj trénovacích a vyhodnocovacích dat byla použita veřejná část CorefUD 1.0, která obsahuje 13 korpusů pro 10 jazyků. Jako hlavní vyhodnocovací metrika bylo použito skóre CoNLL, které se používalo v dřívějších soutěžích zaměřených na koreferenci. 5 účastnických týmů vyvinulo celkem 8 systémů na predikci koreference; kromě toho jsou k dispozici výsledky baseline systému, který je založen na transformerech a byl poskytnut organizátory na začátku soutěže. Vítězný systém překonal baseline o 12 procentních bodů (průměr CoNLL skóre přes korpusy jednotlivých jazyků).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents an overview of the shared task on multilingual coreference resolution associated with the CRAC 2022 workshop. Shared task participants were supposed to develop trainable systems capable of identifying mentions and of clustering the mentions according to identity coreference. The public edition of CorefUD 1.0, which contains 13 datasets for 10 languages was used as the source of training and evaluation data. The CoNLL score used in previous coreference-oriented shared tasks was used as the main evaluation metric. There were 8 coreference prediction systems submitted by 5 participating teams; in addition, there was a competitive transformer-based baseline system provided by the organizers at the beginning of the shared task. The winner system outperformed the baseline by 12 percent points (in terms of the CoNLL scores averaged across all datasets for individual languages).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dobře teoreticky podložená všestranná anotace  je velmi důležitou součástí jazykových korpusů. Jako příklad ve stati uvádíme Pražský závislostní korpus, který obsahuje anotace na několika jazykových rovinách, a tento přístup ilustrujeme na několika případkových studiích z oblasti aktuálního členění. Zároveň na těchto příkladech dokládáme, že práce s anotovaným korpusem může vést k upřesnění výchozí teorie.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A theoretically well-founded annotation of language corpora is a very importatnt component part of such corpora. In our paper, we support this point by the example of the Prague Dependency Treebank  and its annotation scheme. We bring some case studies concerning the information structure of the sentence. At the same time, we also demonstrate that the work with a consistently annotated corpus may lead to some modifications of the original theory.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zamyšlení nad snahou vysvětlit některé pravidelnosti a zdánlivé nepravidelnosti v češtině a jiných slovanských jazycích (především v oblasti deklinační i slovotvorné morfologie) pomocí pravidel, tedy tyto jevy "vypočítat".</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A glimpse at an effort to explain certain regularities as well as (seemingly) irregularities in Czech and other Slavonic languages (first of all in the domain of inflectional and derivational morphology) by means of rules, i.e. to "calculate" these phenomena.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem této kapitoly je ukázat, že dobře navržená a teoreticky podložená korpusová anotace významně přispívá k využití korpusu pro testování lingvistické teorie a pro její další rozvoj. Data
našich analýz pocházejí z korpusové rodiny Prague Dependency Treebank; jde o  jednojazyčná česká data a o paralelní anglicko-česká data, která 
se týkají základní syntaktické úrovně popisu jazyka a anotace struktury diskurzu. Konkrétní případové studie se týkají tří výzkumných otázek, a to (i) sémantické relevance informační struktury věty, (ii) vztahu mezi fokalizátory a diskurzními konektory vzhledem k sémantice diskurzních vztahů a (iii) vztahu mezi primárními a sekundárními spojovacími výrazy. V příloze jsou uvedeny a rozebrány údaje o měření mezianotátorské shody.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of the present chapter is to demonstrate that a well-designed
and theoretically founded corpus annotation contributes significantly to the use of
the corpus for testing a linguistic theory and its further development. The data for
our analyses come from the Prague Dependency Treebank family both monolingual
Czech and parallel English-Czech and concern the underlying syntactic level of
language description and the annotation of discourse structure. In particular, the
case studies concern three research questions, namely (i) the semantic relevance
of information structure of the sentence, (ii) the relation between focus sensitive
particles and discourse connectives with respect to the semantics of discourse
relations, and (iii) the relation between primary and secondary connectives. In the
Appendix, some data on measuring inter-annotator agreement are presented and
discussed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek je zaměřen na komplexní popis zacházení s tzv. funkčními slovy v rámci závislostního funkčního generativního popisu a jeho odraz v Pražském závislostním korpusu. Teoretický rámec i treebank jsou založeny na stratifikačním modelu jazyka, jehož součástí jsou dvě úrovně syntaktické struktury založené na závislosti, jedna orientovaná na povrchovou syntaktickou strukturu věty (analytická rovina) a druhá orientovaná na hloubkovou větnou strukturu (tektogramatická). Závislostní stromová struktura věty na analytické úrovni obsahuje všechna slova přítomná ve větě jako samostatné uzly, zatímco závislostní reprezentace věty na tektogramatické úrovni koncipované jako lingvisticky strukturovaný význam věty obsahuje pouze obsahová slova jako její uzly. Na analytické úrovni se rozlišují různé třídy funkčních slov, přičemž hlavní hranice je mezi funkčními slovy fungujícími v rámci verbálních komplexů a obsahujícími informace o morfosyntaktických vlastnostech sloves a těch, které jsou součástí nominálních skupin (předložky) nebo spojují věty (případně části vět) v jeden celek (spojky). V příspěvku se tvrdí, že pomocná slovesa se považují za závislé na slovesu, ke kterému „patří“, a předložky a spojky se naopak považují za hlavy podstatných jmen, jejichž formu „řídí“ nebo „ovládají“. Na tektogramatické rovině se sémantický přínos funkčních slov k významu věty odráží v informacích připojených k uzlům tektogramatického stromu ve formě komplexních atributů. Pomocná slovesa, předložky a spojky jsou nejzřetelnějšími třídami funkčních slov, existují skupiny také slov, jako jsou částice, které stojí na pomezí mezi funkčními slovy a obsahovými slovy, kterým je ve studii také věnována pozornost.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present contribution is aimed at a complex description of the treatment of the so-called function words within the framework of a dependency-based Functional Generative Description as proposed in Prague by Petr Sgall and his team and its reflection in the Prague Dependency Treebank, an original annotated corpus of Czech. Both the framework and the treebank are based on a stratificational model of language, a part of which are two levels of dependency-based syntactic structure, one oriented towards the syntactic structure of the sentence on the surface layer called analytical and the other oriented towards the underlying, deep sentence structure called tectogrammatical. The dependency tree structure of the sentence on the analytical level contains all the words present in the sentence as separate nodes, while the dependency representation of a sentence on the tectogrammatical level conceived of as a linguistically structured meaning of the sentence contains only content words as its nodes. On the analytical level, a distinction is made between different classes of function words, the main boundary being between the function words functioning within verbal complexes and containing information about the morpho-syntactic properties of verbs (i.e. auxiliaries) and those being parts of nominal groups (prepositions) or connecting clauses (or, as the case may be, parts of clauses) into one whole (conjunctions). It is argued in the paper that auxiliaries should be considered to be dependents on the verb that is their governor and to which they „belong“, and the prepositions and conjunctions, on the contrary, should be considered to be the heads of the nouns or clauses the form of which they “govern” or “control”. On the tectogrammatical level, the semantic contribution of the function words to the meaning of the sentence is reflected by information attached to the nodes of the tectogrammatical tree in the form of complex labels.  Auxiliaries, prepositions and conjunctions are the most evident classes of function words, though there are some groups of words such as particles that stand on the borderline between function words and content words, to which we also pay attention in our study.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V poslední době bylo vyvinuto mnoho korpusů, které obsahují více anotací různých jazykových jevů, od morfologických kategorií slov přes syntaktickou stavbu vět až po diskurz a koreferenční vztahy v textech. Probíhají diskuse o vhodném anotačním schématu pro velké množství různorodých informací. V našem příspěvku vyjadřujeme přesvědčení, že vícevrstvé anotační schéma nabízí pohled na jazykový systém v jeho komplexnosti a v interakci jednotlivých jevů a že existují minimálně dva aspekty, které vícevrstvé anotační schéma podporují: (i) Vícevrstvé anotační schéma umožňuje použít anotaci jedné vrstvy k návrhu anotace další vrstvy (vrstev) jak koncepčně, tak ve formě předanotačního postupu nebo pravidel kontroly anotace. (ii) Vícevrstvé anotační schéma představuje spolehlivý základ pro korpusové studie založené na vlastnostech napříč vrstvami. Tyto aspekty jsou demonstrovány na případu Pražského závislostního korpusu. Jeho více-rovinné anotační schéma obstálo ve zkoušce času a dobře poslouží i pro složité textové anotace, ve kterých se s výhodou používají dřívější morfosyntaktické anotace. Kromě odkazu na předchozí projekty, které využívají toto anotační schéma, uvádíme několik aktuálních výzkumů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Recently, many corpora have been developed that contain multiple annotations of various linguistic phenomena, from morphological categories of words through the syntactic structure of sentences to discourse and coreference relations in texts. Discussions are ongoing on an appropriate annotation scheme for a large amount of diverse information. In our contribution we express our conviction that a multilayer annotation scheme offers to view the language system in its complexity and in the interaction of individual phenomena and there are at least two aspects that support a multilayer annotation scheme:
(i) A multilayer annotation scheme makes it possible to use the annotation of one layer to design the annotation of another layer(s) both conceptually and in a form of a pre-annotation procedure or annotation checking rules.
(ii) A multilayer annotation scheme presents a reliable ground for corpus studies based on features across the layers.
These aspects are demonstrated on the case of the Prague Dependency Treebank. Its multilayer annotation scheme withstood the test of time and serves well also for complex textual annotations, in which earlier morpho-syntactic annotations are advantageously used. In addition to a reference to the previous projects that utilise its annotation scheme, we present several current investigations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>SynSemClass lexicon 4.0 zkoumá s ohledem na kontextově založenou slovesnou synonymii, sémantickou „ekvivalenci“ českých, anglických a německých sloves a jejich valenční chování v paralelních česko-anglických a německo-anglických jazykových zdrojích. SynSemClass 4.0 je vícejazyčná ontologie typu události založená na třídách synonymních významů sloves, doplněná sémantickými rolemi a odkazy na existující sémantické lexikony. Slovník je nejen obohacen o další množství tříd, ale v rámci hierarchizace obsahu byly některé třídy sloučeny. V porovnání se starou verzí obsahuje slovník nově  definice tříd a definice rolí. Kromě již použitých odkazů na položky PDT-Vallex, EngVallex, CzEngVallex, FrameNet, VerbNet, PropBank, Ontonotes a English WordNet pro české a anglické záznamy jsou nové odkazy na německé jazykové lexikální zdroje, jako je Woxikon, E-VALBU a GUP, využívány pro německé slovesné záznamy. Německá část lexikonu byla vytvořena v rámci projektu Multilingual Event-Type-Anchored Ontology for Natural Language Understanding (META-O-NLU) dvěma spolupracujícími týmy - týmem Univerzity Karlovy, Matematicko-fyzikální fakulty, Ústavu formální a aplikované lingvistiky, Praha (ÚFAL), Česká republika a týmem Německého výzkumného centra pro umělou inteligenci (DFKI) Speech and Language Technology, Berlín, Německo.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The SynSemClass synonym verb lexicon version 3.5 investigates, with respect to contextually-based verb synonymy, semantic ‘equivalence’ of Czech, English, and German verb senses and their valency behavior in parallel Czech-English and German-English language resources. SynSemClass4.0 is a multilingual event-type ontology based on classes of synonymous verb senses, complemented with semantic roles and links to existing semantic lexicons. The SynSemClass is not only enriched by an additional number of classes but in the context of content hierarchy, some classes have been merged. Compared to the older version of the lexicon, the novelty is the definitions of classes and the definitions of roles.
Apart from the already used links to PDT-Vallex, EngVallex, CzEngVallex, FrameNet, VerbNet, PropBank, Ontonotes, and English WordNet for Czech and English entries the new links to German language lexical resources are exploited for German verb entries, such as Woxikon, E-VALBU, and GUP. The German part of the lexicon has been created within the project Multilingual Event-Type-Anchored Ontology for Natural Language Understanding (META-O-NLU) by two cooperating teams - the team of the Charles University, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics, Prague (ÚFAL), Czech Republic and the team of the German Research Center for Artificial Intelligence (DFKI) Speech and Language Technology, Berlin, Germany.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme rozšíření ontologie typu události SynSemClass, původně koncipované jako dvojjazyčný česko-anglický zdroj. Do tříd představujících koncepty ontologie jsme přidali německé záznamy. Vzhledem k tomu, že jsme měli jiný výchozí bod než
původní práce (neoznačený paralelní korpus bez odkazů na valenční lexikon a samozřejmě různé existující lexikální zdroje), bylo náročné přizpůsobit pokyny pro anotaci, datový model a nástroje použité pro původní verzi. Popisujeme proces a výsledky práce v takovém nastavení. Dále ukazujeme další kroky k úpravě procesu anotace,
datové struktury a formáty a nástroje nezbytné k tomu, aby přidání nového jazyka v budoucnu bylo plynulejší a efektivnější a případně aby různé týmy mohly pracovat na rozšíření SynSemClass do mnoha jazyků současně.
představujeme nejnovější verzi, která obsahuje výsledky přidání němčiny, volně dostupné ke stažení i pro online přístup.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an extension of the SynSemClass event-type ontology, originally conceived as a bilingual Czech-English resource. We added German entries to the classes representing the concepts of the ontology. Having a different starting point than the original work (unannotated parallel corpus without links to a valency lexicon and, of course, different existing lexical resources), it was a challenge to adapt the annotation guidelines, the data model and the tools used for the original version. We describe the process and results of working in such a setup. We also show the next steps to adapt the annotation process, data structures and formats and tools necessary to make the addition of a new language in the future more smooth and efficient, and possibly to allow for various teams to work on SynSemClass extensions to many languages concurrently. We also present the latest release which contains the results of adding German, freely available for download as well as for online access.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme NomVallex, ručně anotovaný valenční slovník českých substantiv a adjektiv. Slovník je vytvořený v teoretickém rámci Funkčního generativního popisu a je založený na korpusových datech. Obsahuje celkem 1027 lexikálních jednotek v celkovém počtu 570 lexémů, přičemž zahrnuje následující slovnědruhové a derivační kategorie: deverbální a deadjektivní substantiva, a deverbální, desubstantivní, deadjektivní a primární adjektiva. Valenční vlastnosti lexikálních jednotek zachycuje v podobě valenčního rámce, který pro každé valenční doplnění uvádí funktor a množinu možných morfematických vyjádření, a dokládá je pomocí příkladů, které se vyskytly v použitých korpusech. NomVallex je koncipován jako lexikografický zdroj umožňující výzkum valence derivačně příbuzných lexikálních jednotek, proto v relevantních případech poskytuje odkaz od určité lexikální jednotky k odpovídající lexikální jednotce jejího základového slova, obsaženého buď v NomVallexu, nebo – v případě sloves – ve slovníku VALLEX, čímž propojuje až tři slovní druhy, konkrétně substantivum – sloveso, adjektivum – sloveso, substantivum – adjektivum nebo substantivum – adjektivum – sloveso. NomVallex umožňuje srovnání valenčního chování velkého počtu českých substantiv a adjektiv s valencí jejich základových slov, což je jeden
z předpokladů ke zkoumání teoretické otázky dědičnosti valence a k popisu systémového a nesystémového valenčního chování.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present NomVallex, a manually annotated valency lexicon of Czech nouns and adjectives. The lexicon is created in the theoretical
framework of the Functional Generative Description and based on corpus data. In total, NomVallex 2.0 is comprised of 1027 lexical
units contained in 570 lexemes, covering the following part-of-speech and derivational categories: deverbal and deadjectival nouns,
and deverbal, denominal, deadjectival and primary adjectives. Valency properties of a lexical unit are captured in a valency frame
which is modeled as a sequence of valency slots, supplemented with a list of morphemic forms. In order to make it possible to study
the relationship between valency behavior of base words and their derivatives, lexical units of nouns and adjectives in NomVallex are
linked to their respective base words, contained either in NomVallex itself or, in case of verbs, in a valency lexicon of Czech verbs
called VALLEX. NomVallex enables a comparison of valency properties of a significant number of Czech nominals with their base
words, both manually and in an automatic way; as such, we can address the theoretical question of argument inheritance, concentrating
on systemic and non-systemic valency behavior.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>NomVallex 2.0 je ručně anotovaný valenční slovník českých substantiv a adjektiv, vytvořený v teoretickém rámci Funkčního generativního popisu a založený na korpusových datech (korpusy řady SYN Českého národního korpusu a korpus Araneum Bohemicum Maximum). Slovník obsahuje celkem 1027 lexikálních jednotek v celkovém počtu 570 lexémů, přičemž zahrnuje následující slovnědruhové a derivační kategorie: (i) deverbální a deadjektivní substantiva, (ii) deverbální, desubstantivní, deadjektivní a primární adjektiva. Valenční vlastnosti lexikálních jednotek zachycuje v podobě valenčního rámce, který pro každé valenční doplnění uvádí funktor a množinu možných morfematických vyjádření, a dokládá je pomocí příkladů, které se vyskytly v použitých korpusech. NomVallex je koncipován jako lexikografický zdroj umožňující výzkum valence derivačně příbuzných lexikálních jednotek, proto v relevantních případech poskytuje odkaz od určité lexikální jednotky k odpovídající lexikální jednotce jejího základového slova, obsaženého buď v NomVallexu, nebo – v případě sloves – ve slovníku VALLEX, čímž propojuje až tři slovní druhy, konkrétně substantivum – sloveso, adjektivum – sloveso, substantivum – adjektivum nebo substantivum – adjektivum – sloveso. Za účelem umožnit srovnání obsahuje toto vydání rovněž zkrácená hesla základových sloves daných substantiv a adjektiv ze slovníku VALLEX a zjednodušená hesla příslušných substantiv a adjektiv ze slovníku PDT-Vallex.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>NomVallex 2.0 is a manually annotated valency lexicon of Czech nouns and adjectives, created in the theoretical framework of the Functional Generative Description and based on corpus data (the SYN series of corpora from the Czech National Corpus and the Araneum Bohemicum Maximum corpus). In total, NomVallex is comprised of 1027 lexical units contained in 570 lexemes, covering the following parts-of-speech and derivational categories: deverbal or deadjectival nouns, and deverbal, denominal, deadjectival or primary adjectives. Valency properties of a lexical unit are captured in a valency frame (modeled as a sequence of valency slots, each supplemented with a list of morphemic forms) and documented by corpus examples. In order to make it possible to study the relationship between valency behavior of base words and their derivatives, lexical units of nouns and adjectives in NomVallex are linked to their respective base lexical units (contained either in NomVallex itself or, in case of verbs, in the VALLEX lexicon), linking up to three parts-of-speech (i.e., noun – verb, adjective – verb, noun – adjective, and noun – adjective – verb).
In order to facilitate comparison, this submission also contains abbreviated entries of the base verbs of these nouns and adjectives from the VALLEX lexicon and simplified entries of the covered nouns and adjectives from the PDT-Vallex lexicon.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V první části přednášky představíme projekt Universal Dependencies. Ve druhé se zaměříme na parsery natrénované na českých korpusech v UD a na jejich využití při budování staročeského treebanku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the first part of the talk we will present the Universal Dependencies project. In the second part we will focus on parsers trained on the Czech corpora in UD and their usage in building an Old Czech treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je mezinárodní komunitní projekt a sbírka morfosyntakticky anotovaných dat (treebanků) pro více než 100 jazyků. Tato sbírka je neocenitelným zdrojem pro různé jazykovědné studie od gramatických konstrukcí v jednom jazyku po jazykovou typologii, dokumentaci ohrožených jazyků a historického vývoje jazyka.
V tomto tutoriálu nejdříve rychle představím hlavní principy UD, potom ukážu vlastní data a různé nástroje, které jsou k dispozici pro práci s nimi: parsery, dávkové procesory, vyhledávače a prohlížeče.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is an international community project and a collection of morphosyntactically annotated data sets (“treebanks”) for more than 100 languages. The collection is an invaluable resource for various linguistic studies, ranging from grammatical constructions within one language to language typology, documentation of endangered languages, and historical evolution of language.
In the tutorial, I will first quickly show the main principles of UD, then I will present the actual data and various tools that are available to work with it: parsers, batch processors, search engines and viewers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je šestnácté vydání treebanků UD, verze 2.10.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the sixteenth release of UD Treebanks, Version 2.10.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce se zaměřuje na detekci zdrojů v českých článcích publikovaných na zpravodajském serveru Českého veřejnoprávního rozhlasu. Hledáme zejména přiřazení ve větách a rozeznáváme přiřazené zdroje a jejich větný kontext(signály). Zorganizovali jsme crowdsourcingovou anotační úlohu, jejímž výsledkem byl datový soubor 2 167 článků s ručně rozpoznanými signály a zdroji. Zdroje byly navíc zařazeny do kategorií jmenovaných a nejmenovaných zdrojů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper focuses on detection of sources in the Czech articles published on a news server of Czech public radio. In particular, we search for attribution in sentences and we recognize attributed sources and their sentence context (signals). We organized a crowdsourcing annotation task that resulted in a data set of 2,167 stories with manually recognized signals and sources. In addition, the sources were classified into the classes of named and unnamed sources.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vstupní data, jednotlivé experimentální anotace a úplný a podrobný přehled naměřených výsledků souvisejících s experimentem zabývajícím se vlivem automatické před-anotace na kvalitu a efektivitu anotačního úsilí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Input data, individual experimental annotations, and a complete and detailed overview of the measured results related to the experiment dealing with the influence of automatic pre-annotation on the quality and efficiency of manual annotation efforts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje analýzu anotace pomocí automatické před-anotace pro úkol anotace závislostní syntaxe. Porovnává úsilí anotátorů, kteří používají před-anotovanou verzi (s vysoce přesným parserem), a úsilí vytvořené plně ruční anotací. Cílem experimentu je posoudit výslednou kvalitu anotace při použití před-anotace. Kromě toho hodnotí vliv automatických lingvisticky založených (pravidlově formulovaných) kontrol a jiné anotace na stejných datech, kterou mají anotátoři k dispozici, a jejich vliv na kvalitu a efektivitu anotace. Experiment potvrdil, že pre-anotace je účinným nástrojem pro rychlejší manuální syntaktickou anotaci, která zvyšuje konzistenci výsledné anotace bez snížení její kvality.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents an analysis of annotation using an automatic pre-annotation for a mid-level annotation complexity task - dependency syntax annotation. It compares the annotation efforts made by annotators using a pre-annotated version (with a high-accuracy parser) and those made by fully manual annotation. The aim of the experiment is to judge the final annotation quality when pre-annotation is used. In addition, it evaluates the effect of automatic linguistically-based (rule-formulated) checks and another annotation on the same data available to the annotators, and their influence on annotation quality and efficiency. The experiment confirmed that the pre-annotation is an efficient tool for faster manual syntactic annotation which increases the consistency of the resulting annotation without reducing its quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Naše studie vznikla jako první pokus zmapovat podhoubí české pedagogiky digitálních humanitních věd v roce 2018. Máme za to, že české paměťové instituce jsou už dobře integrovány do evropských infrastrukturních projektů (například Knihovna Akademie věd ČR, Národní knihovna a Národní muzeum v projektu Europeana ) a čeští badatelé se již také úspěšně připojili k mezinárodní DH komunitě (o čemž svědčí také jejich příspěvky v tomto svazku). Zatím však málo víme o DH paradigmatu ve výuce humanitních oborů na českých vysokých školách.Proto jsme během roku 2018 ručně prohledali aktuální studijní katalogy většiny českých veřejných vysokých škol s cílem shromáždit kontakty na vyučující, kteří by mohli stát u zrodu digitálně humanitní pedagogické komunity v Česku, a udělat si představu o jejich specializacích. K agregaci informací jsme použili Latentně sémantickou analýzu na názvech kurzů a institucí, kde se vyučují.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Our study was created as the first attempt to map the Czech pedagogy of Digital Humanities in 2018. We believe that Czech memory institutions are already well integrated into European infrastructure projects (eg Library of the Academy of Sciences, National Library and National Museum in Europeana) and Czech researchers have also successfully joined the international DH community (as evidenced by their contributions in this volume). So far, however, we know little about the DH paradigm in the teaching of humanities at Czech universities. To aggregate the information, we used Latent semantic analysis on the names of the courses and institutions where they are taught.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Studie se zabývá českými konverzními dvojicemi, v nichž podstatné jméno i sloveso mají dějový význam (test 'zkoušet.n' - testovat 'zkoušet'). V návaznosti na předchozí výzkum prototypických případů deverbální a denominální konverze v češtině je směr v těchto dvojicích určován na základě toho, zda sloveso tvoří svůj vidový protějšek změnou kmenotvorné přípony (což je charakteristické pro deverbální směr), nebo zda sufixální protějšek není k dispozici (typické pro denominální slovesa). Analýza provedená na korpusovém vzorku 1 300 podstatných jmen s dějovým významem a přímo souvisejících sloves ukazuje, že dvojice založené na domácích kořenech mají většinou rysy deverbálního tvoření, zatímco denominální směr se uplatňuje v menší skupině domácích dvojic a v datech s cizími kořeny jasně převažuje. Denominální směr přisuzovaný dvojicím s cizími kořeny je v souladu s typologickou hypotézou, že slovesa jsou přejímána do češtiny spíše jako podstatná jména a následně se v cílovém jazyce mění na slovesa.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The study deals with Czech conversion pairs of a noun and a verb, both of which denote actions (test ‘test.n’ – testovat ‘to test’). Elaborating on previous research on prototypical verb-to-noun and noun-to-verb conversion in Czech, the direction in these pairs is determined based on whether the verb forms its aspectual counterpart by changing the theme (which is characteristic of the deverbal direction), or whether the suffixed counterpart is not available (typical of denominal verbs). The analysis, carried out on a corpus sample of 1,300 action nouns and directly related verbs, demonstrates that pairs with native roots mostly conform to the deverbal pattern, whereas the denominal direction applies to a smaller subset of the native sample but clearly prevails in the data with foreign roots. The denominal direction ascribed to foreign pairs is consistent with the typological hypothesis that verbs are borrowed rather as nouns and subsequently turned into verbs in the target language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přestože dějový význam je prototypicky přisuzován slovesu, existují slovesa, u nichž se předpokládá, že jsou odvozena nebo konvertována od podstatných jmen s dějovým významem. Předkládaný příspěvek se zabývá českými konverzními dvojicemi, kde jak podstatné jméno, tak sloveso vyjadřují děj (např. řez - řezat, test - testovat). V návaznosti na náš předchozí výzkum tvoření slov bez derivačních afixů v češtině jsou pro určení směru konverze v jednotlivých dvojicích využity fonologické a morfologické vlastnosti podstatných jmen a sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Although actions are prototypical verbal concepts, there are verbs that are assumed to be derived or converted from nouns with action meanings, cf. apology > to apologize or attack.n > to attack. Conversion of action nouns to verbs, discussed as the PERFORMATIVE category by Plag (1999) or Bauer et al. (2013), has been attested since the earliest period documented by the Oxford English Dictionary and, remarkably, have become predominant among verbs converted from nouns in English during the 20th century (Gottfurcht 2008).
The present paper deals with Czech conversion pairs, where both the noun and the verb denote an action (e.g. řez ‘cut.n’ – řezat ‘to cut’, test ‘test.n’ – testovat ‘to test’). Elaborating on our previous research on word formation without derivational affixes in Czech, phonological and morphological features of nouns and verbs are employed to determine the direction of conversion in individual pairs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Studie se zabývá anglickými konverzními dvojicemi podstatných a sloves, které mají formálně i významově blízké protějšky v češtině. Cílem studie je prozkoumat, jak se tato substantiva a slovesa, která jsou v angličtině a češtině spojena podobnými sémantickými vztahy, chovají v jazycích, které mají odlišnou morfologickou strukturu a v jejichž slovotvorných systémech hraje konverze odlišnou roli. Dvojice substantiv a sloves, extrahované z Britského národního korpusu a z korpusu SYN2000, jsou popisována jako dvoučlenná paradigmata a zkoumány spolu s vybranými deriváty. Data ukazují, že v českých dvojicích jsou pro vyjadřování konkrétních významů preferována substantiva před slovesy a většina sloves je používána jako denominální formace, často odlišně od anglických protějšků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The study deals with English noun/verb conversion pairs that have both formally and semantically close counterpart pairs in Czech. The study’s aim is to examine how these nouns and verbs, linked with similar semantic relations in English and Czech, are accommodated in the two languages with different morphological structures and conversion playing a different role. The noun/verb pairs, extracted from the British National Corpus and from the SYN2000 corpus, are analysed as two-cell paradigms and examined along with selected derivatives. The data suggest that in the Czech sample, nominals are preferred over verbs in expressing the particular meanings and most verbs appear as denominal formations, often differently from their English counterparts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce představuje korpus ParlaMint obsahující přepisy ze zasedání 17 evropských národních parlamentů s půl miliardou slov. Korpusy jsou jednotně kódovány, obsahují bohatá metadata o 11 tisících mluvčích a jsou lingvisticky anotovány podle Universal Dependencies a s pojmenovanými entitami. Vzorky korpusů a konverzních skriptů jsou k dispozici v úložišti projektu na GitHub a kompletní korpusy jsou otevřeně k dispozici prostřednictvím úložiště CLARIN.SI ke stažení, stejně jako prostřednictvím NoSketch Engine a KonText a rozhraní Parlameter pro on-line průzkum a analýzu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the ParlaMint corpora containing transcriptions of the sessions of the 17 European national parliaments with half a billion words. The corpora are uniformly encoded, contain rich meta-data about 11 thousand speakers, and are linguistically annotated following the Universal Dependencies formalism and with named entities. Samples of the corpora and conversion scripts are available from the project’s GitHub repository, and the complete corpora are openly available via the CLARIN.SI repository for download, as well as through the NoSketch Engine and KonText concordancers and the Parlameter interface for on-line exploration and analysis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek je zasazen do kontextu projektu SSHOC a jeho cílem je prozkoumat, jak mohou jazykové technologie pomoci při podpoře a usnadnění vícejazyčnosti v sociálních a humanitních vědách (SSH). Přestože většina výzkumných pracovníků v oblasti SSH vytváří kulturně a společensky relevantní práce ve svých místních jazycích, metadata a slovníky používané v oblasti SSH k popisu a indexování výzkumných dat jsou v současné době většinou v angličtině. Zkoumáme proto přístupy zpracování přirozeného jazyka a strojového překladu s cílem poskytnout zdroje a nástroje na podporu vícejazyčného přístupu k obsahu SSH a jeho vyhledávání v různých jazycích. Jako případové studie vytváříme a poskytujeme jako volně dostupná data soubor vícejazyčných metadatových konceptů a automaticky extrahovanou vícejazyčnou terminologii Data Stewardship. Tyto dvě případové studie umožňují také vyhodnotit výkonnost nejmodernějších nástrojů a odvodit soubor doporučení, jak je nejlépe použít. Přestože nejsou přizpůsobeny konkrétní doméně, použité nástroje se ukázaly být platným přínosem pro překladatelské úlohy. Nicméně ověření výsledků experty na danou doménu, kteří ovládají daný jazyk, je nevyhnutelnou fází celého pracovního postupu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper is framed in the context of the SSHOC project and aims at exploring how Language Technologies can help in promoting and facilitating multilingualism in the Social Sciences and Humanities (SSH). Although most SSH researchers produce culturally and societally relevant work in their local languages, metadata and vocabularies used in the SSH domain to describe and index research data are currently mostly in English. We thus investigate Natural Language Processing and Machine Translation approaches in view of providing resources and tools to foster multilingual access and discovery to SSH content across different languages. As case studies, we create and deliver as freely, openly available data a set of multilingual metadata concepts and an automatically extracted multilingual Data Stewardship terminology. The two case studies allow as well to evaluate performances of state-of-the-art tools and to derive a set of recommendations as to how best apply them. Although not adapted to the specific domain, the employed tools prove to be a valid asset to translation tasks. Nonetheless, validation of results by domain experts proficient in the language is an unavoidable phase of the whole workflow.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace jazykových nástrojů systému Elitr a generování scénářů THEaiTRE pro účastníky EUROSAI kongresu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presentation of Elitr language tools and THEaiTRE script generation for EUROSAI congress attendees.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Hackování jazykového modelu GPT-2
Sestavení webové aplikace
Vytváří se scénář divadelní hry
Představení hry na jevišti
Demo nástroje</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Hacking the GPT-2 language model
Building a web application
Generating a theatre play script
Performing the play on stage
Demo of the tool</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace interaktivního generování scnářů divadelních her pro zaměstnance společnosti Novartis.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presentation of interactive theatre script generation for employees of the Novartis company.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Experimentujeme s adaptací generativních jazykových modelů pro generování dlouhých souvislých vyprávění v podobě divadelních her. Protože plně automatické generování celých her není v současné době proveditelné, vytvořili jsme interaktivní nástroj, který umožňuje lidskému uživateli poněkud nasměrovat generování a zároveň minimalizovat zásahy. Pro generování dlouhých textů sledujeme dva přístupy: ploché generování se sumarizací kontextu a hierarchický dvoufázový přístup text-to-text, kdy je nejprve vygenerována synopse a poté použita k podmiňování generování konečného scénáře. Naše předběžné výsledky a diskuse s divadelními profesionály ukazují zlepšení oproti základnímu generování, ale také identifikují důležitá omezení našeho přístupu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We experiment with adapting generative language models for the generation of long coherent narratives in the form of theatre plays. Since fully automatic generation of whole plays is not currently feasible, we created an interactive tool that allows a human user to steer the generation somewhat while minimizing intervention. We pursue two approaches to long-text generation: a flat generation with summarization of context, and a hierarchical text-to-text two-stage approach, where a synopsis is generated first and then used to condition generation of the final script. Our preliminary results and discussions with theatre professionals show improvements over vanilla language model generation, but also identify important limitations of our approach.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme volně dostupné online demo THEaiTRobot, dvojjazyčný nástroj s otevřeným zdrojovým kódem pro interaktivní generování scénářů divadelních her, ve dvou verzích. THEaiTRobot 1.0 používá jazykový model GPT-2 s minimálními úpravami. THEaiTRobot 2.0 používá dva modely vytvořené doladěním GPT-2 na cíleně shromážděných a zpracovaných datových souborech a několika dalších komponentách, které hierarchicky generují scénáře divadelních her (název → synopse → skript). Podkladový nástroj se používá v projektu THEaiTRE pro generování scénářů divadelních her, které pak na jevišti hraje profesionální divadlo.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a free online demo of THEaiTRobot, an open-source bilingual tool for interactively generating theatre play scripts, in two versions. THEaiTRobot 1.0 uses the GPT-2 language model with minimal adjustments. THEaiTRobot 2.0 uses two models created by fine-tuning GPT-2 on purposefully collected and processed datasets and several other components, generating play scripts in a hierarchical fashion (title → synopsis → script). The underlying tool is used in the THEaiTRE project to generate scripts for plays, which are then performed on stage by a professional theatre.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme velký a různorodý český korpus pro opravu gramatických chyb s cílem přispět ke stále nedostatkovým datovým zdrojům v této doméně pro jiné jazyky než angličtinu. Korpus pro gramatickou opravu chyb pro češtinu (GECCC) nabízí čtyři domény, které pokrývají distribuci chyb od esejů s vysokou hustotou chyb napsaných nerodilými mluvčími až po texty webových stránek, kde jsou chyby mnohem méně časté. Porovnáváme několik českých GEC systémů, včetně několika na bázi architektury Transformer, a nastavujeme tak silnou baseline pro budoucí výzkum. V neposlední řadě také provádíme meta-evaluaci běžných GEC metrik pomocí ručního hodnocení na našich datech. Nový český GEC korpus zveřejňujeme pod licencí CC BY-SA 4.0 na adrese http://hdl.handle.net/11234/1-4639.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a large and diverse Czech corpus annotated for grammatical error correction (GEC) with the aim to contribute to the still scarce data resources in this domain for languages other than English. The Grammar Error Correction Corpus for Czech (GECCC) offers a variety of four domains, covering error distributions ranging from high error density essays written by non-native speakers, to website texts, where errors are expected to be much less common. We compare several Czech GEC systems, including several Transformer-based ones, setting a strong baseline to future research. Finally, we meta-evaluate common GEC metrics against human judgements on our data. We make the new Czech GEC corpus publicly available under the CC BY-SA 4.0 license at http://hdl.handle.net/11234/1-4639.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V ParlaMint I, projektu podporovaném CLARIN-ERIC v době pandemie, byla v roce 2021 vyvinuta a vydána sada srovnatelných a jednotně anotovaných vícejazyčných korpusů pro 17 národních parlamentů. Pro roky 2022 a 2023 byl projekt rozšířen na ParlaMint II, opět s finanční podporou CLARIN-ERIC, s cílem rozšířit stávající korpusy o nová data a metadata; aktualizovat schéma XML; přidat korpusy pro 10 nových parlamentů; poskytnout více scénářů aplikace a provést další experimenty. Dokument informuje o těchto plánovaných krocích, včetně některých, které již byly podniknuty, a nastiňuje budoucí plány.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In ParlaMint I, a CLARIN-ERIC supported project in pandemic times, a set of comparable and uniformly annotated multilingual corpora for 17 national parliaments were developed and released in 2021. For 2022 and 2023, the project has been extended to ParlaMint II, again with the CLARIN ERIC financial support, in order to enhance the existing corpora with new data and metadata; upgrade the XML schema; add corpora for 10 new parliaments; provide more application scenarios and carry out additional experiments. The paper reports on these planned steps, including some that have already been taken, and outlines future plans.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve většině modelů kombinujících jazyk a vizuální informaci (Vision-Language, VL) je porozumění struktuře obrazu umožněno přidáním informací o poloze objektů v obraze. V naší případové studii se věmnujeme modelu VL modelu LXMERT a zkoumáme použití jakým způsobem poziční informaci používá a studujeme její vliv na úspěšnost v úloze odpovídání otázek o obrázcích. Ukazujeme, že model není schopen poziční informaci využít pro přiřazování textů k obrázkům, pokud se texty liší polohou objektů. A to i přesto, že další experimenty ale ukazují, že PI je v modelech skutečně přítomna. Představujeme dvě strategie, jak se s tímto problémem vypořádat: (i) předtrénování s přidanou informací o poloze a (ii) kontrastní učení s porovnáváním napříč modalitami. Tímto způsobem může model správně klasifikovat, zda se obrázky s podrobnými výroky PI shodují. Kromě 2D informací o objektech na obrázku, přidáváme hloubku objektu pro lepší lokalizaci v prostoru. Přestože se nám podařilo zlepšit vlastnosti modelu, na kvalitu odpovídání otázek to má jen zanedbatelný vliv. Naše výsledky tak poukazují na důležitý problém multimodálního modelování: pouhá přítomnost informace detekovatelné klasifikátorem není zárukou, že tato informace je k dispozici napříč modalitami.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In most Vision-Language models (VL), the understanding of the image structure is enabled by injecting the position information (PI) about objects in the image. In our case study of LXMERT, a state-of-the-art VL model, we probe the use of the PI in the representation and study its effect on Visual Question Answering. We show that the model is not capable of leveraging the PI for the image-text matching task on a challenge set where only position differs. Yet, our experiments with probing confirm that the PI is indeed present in the representation. We introduce two strategies to tackle this: (i) Positional Information Pre-training and (ii) Contrastive Learning on PI using Cross-Modality Matching. Doing so, the model can correctly classify if images with detailed PI statements match. Additionally to the 2D information from bounding boxes, we introduce the object’s depth as new feature for a better object localization in the space. Even though we were able to improve the model properties as defined by our probes, it only has a negligible effect on the downstream performance. Our results thus highlight an important issue of multimodal modeling: the mere presence of information detectable by a probing classifier is not a guarantee that the information is available in a cross-modal setup.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku popisujeme naše předložení k Simultaneous Speech Translation na
IWSLT 2022. Zkoumáme strategie využití offline modelu v simultánním prostředí bez nutnosti upravovat původní model. V našich experimentech ukazujeme, že náš online algoritmus je téměř na stejné úrovni jako offline nastavení, přičemž je 3× rychlejší než offline, pokud jde o latenci na testovací sadě. Náš systém zpřístupňujeme veřejnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we describe our submission to the Simultaneous Speech Translation at IWSLT 2022. We explore strategies to utilize an offline model in a simultaneous setting without the need to modify the  riginal model. In our experiments, we show that our onlinization algorithm is almost on par with the offline setting while being 3×  faster than offline in terms of latency on the test set. We make our system publicly available.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Sumarizace schůzek se primárně zaměřuje spíše na aktuální téma než na plynulost nebo koherenci. Je to náročný a zdlouhavý úkol, i když se shrnutí schůzek vytváří ručně. Výsledná shrnutí se liší v cílech, stylu a jsou nevyhnutelně velmi subjektivní kvůli člověku, který je ve smyčce. Pro vytvoření adekvátních a informativních shrnutí je rovněž nezbytné uvědomění si kontextu schůzky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Meeting summarization is primarily focused on topi cal coverage rather than on fluency or coherence. It is a challenging and tedious task, even when meeting summaries are created manually. The resulting sum maries vary in the goals, style, and they are inevitably very subjective due to the human in the loop. Also, the awareness of the context of the meeting is essential to create adequate and informative summaries.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Práce se zabývá paradigmatickým systémem české flexe z perspektivy distribuční sémantiky. Využíváme rozsáhlé morfologické zdroje a korpusy k získání modelů distribuční sémantiky češtiny a zkoumáme chování vybraných morfosémantických vlastností českých podstatných a přídavných jmen.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The work deals with the paradigmatic system of Czech inflexion from the perspective of distributional semantics. We use extensive morphological and corpus resources available for Czech to obtain models of the Czech distributional vector space and examine the behaviour of selected morphosyntactic features of Czech nouns and adjectives.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace shrnuje dosavadní práci našeho grantového týmu START na tématu kompetice ve slovotvorbě. Prezentují se jak jazykové zdroje, tak metodologie používané v rámci dané výzkumné oblasti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presentation summarises the work of our START grant team on the topic of competition in word formation. It presents language resources as well as the methodology used in the mentioned research area.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Práce se zabývá hranicí mezi flektivní a derivační morfologií v češtině, zvláště pak z perspektivy distribuční sémantiky. Využíváme rozsáhlé morfologické zdroje a korpusy k získání modelů distribuční sémantiky češtiny a zkoumáme kolekci 24 typů morfologických kontrastů reprezentujících jak kanonickou flexi a kanonickou derivaci, tak různé typy mezních případů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The work deals with the borderline between inflexion and derivation in Czech, especially from the perspective of distributional semantics. We use extensive morphological and corpus resources available for Czech to obtain models of the Czech distributional vector space and examine a collection of 24 types of morphological contrasts exemplifying canonical inflexion, canonical derivation, and different types of intermediate cases.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slova jsou v jazycích provázána slovotvornými vztahy, např. sloveso examplify a substantivum examples obě souvisí s example, přičemž uvedené sloveso z něj vzniklo odvozením a substantivní tvar inflexí. Mezi jazykovými zdroji pro ruštinu je inflexe pokrytá dostatečně, nicméně derivace je pokryta datovými zdroji daleko omezeněji. Tento článek je věnován vylepšení metody konstrukce derivačních sítí a aplikaci tohoto postupu na ruštinu, vedoucí k vytvoření dosud největšího datového zdroje ruských derivačních relací. Výsledná databáze DeriNet.RU obsahuje víc než 300 tisíc lemmat spojených s více než 164 tisíci slovotvornými relacemi. Pro vytvoření takových dat jsme použili metody strojového učení. Databáze je zveřejněna pod otevřenou licencí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Words of any language are to some extent related thought the ways they are formed. For instance, the verb exempl-ify and the noun example-s are both based on the word example, but the verb is derived from it, while the noun is inflected. In Natural Language Processing of Russian, the inflection is satisfactorily processed; however, there are only a few machine-tractable resources that capture derivations even though Russian has both of these morphological processes very rich. Therefore, we
devote this paper to improving one of the methods of constructing such resources and to the application of the method to a Russian lexicon, which results in the creation of the largest lexical resource of Russian derivational relations. The resulting database dubbed DeriNet.RU includes more than 300 thousand lexemes connected with more than 164 thousand binary
derivational relations. To create such data, we combined the existing machine-learning methods that we improved to manage this goal. The whole approach is evaluated on our newly created data set of manual, parallel annotation. The resulting
DeriNet.RU is freely available under an open license agreement.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme soubor dat Eyetracked Multi-Modal Translation (EMMT), který obsahuje záznamy monokulárních očních pohybů, zvuková data a data 4elektrodového nositelného elektroencefalogramu (EEG) 43 účastníků, kteří se věnovali překladu z angličtiny do češtiny, a to na základě psaného textu a doprovodných obrázků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present Eyetracked Multi-Modal Translation (EMMT), a dataset containing monocular eye movement recordings, audio data and 4-electrode wearable electroencephalogram (EEG) data of 43 participants while engaged in sight translation task supported by an image.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem této experimentální studie je prozkoumat proces překladu z angličtiny do češtiny v multimodálním scénáři s využitím eye trackeru. Zkoumáme specifické aspekty překladu nejednoznačných a jednoznačných vět a současně se zaměřujeme na možný vliv vizuálních informací na proces překladu. Ukazujeme tak, jak lze na základě různých údajů o pohybu očí zkoumat mechanismy vizuálního vyhledávání, jakož i mechanismy přítomnosti a pozornosti zapojené do těchto překladatelských procesů, tj. kognitivní mechanismy zapojené do čtení originálních vět a vytváření odpovídajícího překladu studujeme pomocí několika metrik specifických pro sledování očí. Článek mimo jiné ukazuje, jak se v experimentálním uspořádání projevuje Stroopův efekt.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This experimental study aims to investigate the translation process from English to Czech in a multimodal scenario by using an eye tracker.
We investigate specific aspects of translating ambiguous and unambiguous sentences, and simultaneously, we focus on the possible impact of visual information on the translation process. Thus, we show how mechanisms of visual search, as well as the presence and attention mechanisms involved in such translation processes, can be explored based on various eye-movement data, i.e., cognitive mechanisms involved in reading original sentences and producing the corresponding translation are studied using a plethora of eye-tracking-specific metrics.
Among other things, the paper demonstrates how the Stroop effect is visible in the experimental setup.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Data sledování očí jsou velmi užitečným zdrojem informací pro studium kognice a zejména porozumění jazyku u lidí. V tomto článku popisujeme naše systémy pro sdílenou úlohu CMCL 2022 o předpovídání informací o sledování očí. Popisujeme naše experimenty s předem připravenými modely, jako jsou BERT a XLM, a různé způsoby, jak jsme tyto reprezentace použili k predikci čtyř funkcí sledování očí. Spolu s analýzou účinku použití dvou různých druhů předtrénovaných vícejazyčných jazykových modelů a různých způsobů sdružování reprezentací na úrovni tokenů také zkoumáme, jak kontextové informace ovlivňují výkon systémů. Nakonec také zkoumáme, zda faktory, jako je rozšíření jazykových informací, ovlivňují předpovědi. Naše příspěvky dosáhly průměrného MAE 5,72 a umístily se na 5. místě ve sdíleném úkolu. Průměrný MAE ukázal další snížení na 5,25 při hodnocení po úkolu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Eye-Tracking data is a very useful source of information to study cognition and especially language comprehension in humans. In this paper, we describe our systems for the CMCL 2022 shared task on predicting eye-tracking information. We describe our experiments with pretrained models like BERT and XLM and the different ways in which we used those representations to predict four eye-tracking features. Along with analysing the effect of using two different kinds of pretrained multilingual language models and different ways of pooling the tokenlevel representations, we also explore how contextual information affects the performance of the systems. Finally, we also explore if factors like augmenting linguistic information affect the predictions. Our submissions achieved an average MAE of 5.72 and ranked 5th in the shared task. The average MAE showed further reduction to 5.25 in post task evaluation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve hře Shannon je cílem uhodnout další písmeno ve větě na základě předchozího kontextu.
Od té doby se stal široce známým myšlenkovým experimentem, na kterém jsou založeny koncepty v psycholingvistice, počítačové lingvistice a zpracování přirozeného jazyka.
Tuto hru rozšiřujeme o volitelnou extra modalitu ve formě obrázků a provádíme experiment na lidských účastnících.

Zjistili jsme, že přítomnost obrázku výrazně zvyšuje důvěru a přesnost uživatelů ve všech POS.
To zahrnuje determinanty (a, an, the), které by jinak měly být predikovány výhradně z předchozího (levého) kontextu věty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the Shannon game, the goal is to guess the next letter in a sentence based on the previous context.
It has since become a widely known thought experiment on which concepts in psycholinguistics, computational linguistics and natural language processing are based.
We extend this game by including an optional extra modality in the form of images and run an experiment on human participants.

We find that the presence of an image greatly improves users' confidence and accuracy accross all POS.
This includes determiners (a, an, the), which should otherwise be predicted solely from the previous (left) context of the sentence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Není jasné, zda, jak a kde velké předem trénované jazykové modely zachycují jemné lingvistické rysy, jako je nejednoznačnost, gramatika a složitost vět.
Prezentujeme výsledky automatické klasifikace těchto znaků a porovnáváme jejich životaschopnost a vzorce napříč typy reprezentace.
Ukazujeme, že datové sady založené na šablonách s artefakty na úrovni povrchu by neměly být používány pro sondování,
měla by být provedena pečlivá srovnání se základními hodnotami
a že grafy t-SNE by se neměly používat k určení přítomnosti rysu mezi reprezentacemi hustých vektorů. Také ukazujeme, jak mohou být prvky vysoce lokalizovány ve vrstvách těchto modelů a ztratit se v horních vrstvách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>It is unclear whether, how and where large pre-trained language models capture subtle linguistic traits like ambiguity, grammaticality and sentence complexity.
We present results of automatic classification of these traits and compare their viability and patterns across representation types.
We demonstrate that template-based datasets with surface-level artifacts should not be used for probing,
careful comparisons with baselines should be done
and that t-SNE plots should not be used to determine the presence of a feature among dense vectors representations. We also demonstrate how features might be highly localized in the layers for these models and get lost in the upper layers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Neurální reprezentace v multimodálním a vícejazyčném modelování (NEUREM3) je projekt financovaný Grantovou agenturou ČR (GAČR) program Výzkum, experimentální vývoj a inovace pro podporu grantových projektů základního výzkumu EXPRO 2019 od ledna 2019 do prosince 2023. Tato zpráva se týká prvních tří let 2019--2021. Vědecká práce se soustředila na 5 širokých oblastí: Základy, Interpretovatelnost a závislost na úkolu, Těsná integrace, Robustnost a Vztah neurálních reprezentací k vícejazyčným konceptům. Jeho popis je seskupený podle 5 úkolů definovaných v návrhu projektu a několika technických témat. V několika z nich jsme dosáhli nad rámec nejmodernějších výsledků. Zahraniční spolupráce na projektu byla intenzivní od najímání zahraničních specialistů, přes studentské stáže v respektovaných zahraničních laboratořích až po synergie s projekty EU a USA. Tým projektu je konsolidovaný a má dobrou rovnováhu mezi špičkovými PI a co-PI, výzkumníky/postdoktorandy a českými i zahraničními doktorandy. Projekt je konkurenceschopný na mezinárodní úrovni, hodnocení probíhá prostřednictvím mezinárodních technologických hodnocení (výzev), bibliografických metrik a pořádání špičkových mezinárodních vědeckých akcí. Tým NEUREM3 efektivně spolupracuje a je jádrem budování české i mezinárodní řečové/NLP/MT komunity. Dosud vedl projekt k celkem 96 publikacím, z nichž 12 bylo v recenzovaných časopisech, 49 na špičkových konferencích a 35 na místních workshopech, challenge a evaluačních workshopech atd. Pravidelně také zveřejňujeme výzkumná data a kód v otevřená úložiště.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Neural Representations in Multi-modal and Multi-lingual Modeling (NEUREM3) is a
project funded by the Czech Science Foundation (GAČR) program “Research, Experimental Development and Innovation for the Support of Basic Research Grant Projects”
– EXPRO 2019 from January 2019 till December 2023. This report covers its first three
years, 2019–2021. The scientific work was articulated around 5 broad areas: Foundations, Interpretability and task-dependence, Tight integration, Robustness, and Relation
of neural representations to multi-lingual concepts. Its description is clustered according
to 5 tasks defined in the project proposal and several technical topics. In several of these,
we have reached beyond state-of-the-art results. Foreign cooperation in the project was
intensive ranging from hiring foreign specialists, through student interns in respected foreign laboratories, to synergies with EU and US projects. The team of the project is consolidated and has a good balance of top-class PI and co-PI, researchers/post-docs, and
both Czech and international PhD students. The project is competitive on the international level, the assessment is done via international technology evaluations (challenges),
bibliographic metrics, and organization of top international scientific events. NEUREM3
team efficiently cooperates and is at the core of building both Czech and international
speech/NLP/MT communities.
So far, the project led to a total of 96 publications of which 12 were in peer-reviewed
journals, 49 at top conferences, and 35 at local workshops, challenge and evaluation workshops, etc. We are also regularly releasing research data and code in open repositories.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se věnuje textovým konektorům, jejichž pozice v textu se liší od obvyklých pozic spojek a textově-propojovacích částic a adverbií. Jsou to takové konektory, které se nevyskytují ani v jednom z textových segmentů, které propojují - a jako takové představují výzvu pro automatickou analýzu diskurzu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper investigates discourse connectives whose position in a text deviates from the usual setting - namely connectives that occur in neither of the two arguments - and as such present a challenge for discourse parsers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Při trénování systémů pro generování textu z dat na konkrétní doméně dochází k nadměrnému přizpůsobování modelů reprezentaci dat a opakování chyb v trénovacích datech na výstupu. Zkoumáme, jak se obejít bez dotrénovávání jazykových modelů na datasetech pro tuto úlohu a zároveň přitom využít schopností těchto modelů pro povrchovou realizaci. Inspirováni sekvenčními přístupy navrhujeme generovat text transformací krátkých textů pro jednotlivé položky pomocí posloupnosti modulů natrénovaných na obecných textových operacích: řazení, agregaci a kompresi odstavců. Modely pro provádění těchto operací trénujeme na syntetickém korpusu WikiFluent, který pro tento účel vytváříme z anglické Wikipedie. Naše experimenty na dvou významných datasetech pro převod RDF trojic na text — WebNLG a E2E — ukazují, že náš přístup umožňuje generování textu z RDF trojic i při absenci trénovacích dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In data-to-text (D2T) generation, training on in-domain data leads to overfitting to the data representation and repeating training data noise. We examine how to avoid finetuning pretrained language models (PLMs) on D2T generation datasets while still taking advantage of surface realization capabilities of PLMs. Inspired by pipeline approaches, we propose to generate text by transforming single-item descriptions with a sequence of modules trained on general-domain text-based operations: ordering, aggregation, and paragraph compression. We train PLMs for performing these operations on a synthetic corpus WikiFluent which we build from English Wikipedia. Our experiments on two major triple-to-text datasets — WebNLG and E2E — show that our approach enables D2T generation from RDF triples in zero-shot settings.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Efektivní modely strojového překladu jsou komerčně důležité, protože mohou zvýšit rychlost překladu a snížit náklady a emise uhlíku. V poslední době je velký zájem o neautoregresivní (NAR) modely, které slibují rychlejší překlad. Paralelně s výzkumem modelů NAR proběhly úspěšné pokusy o vytvoření optimalizovaných autoregresních modelů v rámci sdíleného úkolu WMT o efektivním překladu. V tomto článku poukazujeme na nedostatky v metodice hodnocení v literatuře o modelech NAR a poskytujeme spravedlivé srovnání mezi nejmodernějším modelem NAR a autoregresivními příspěvky ke sdílenému úkolu. Zastáváme důsledné hodnocení modelů NAR a také důležitost porovnávání modelů NAR s jinými široce používanými metodami pro zlepšení efektivity. Provádíme experimenty s modelem NAR založeným na konekcionisticko-temporální klasifikaci (CTC) implementovaným v C++ a porovnáváme jejich čas s autoregresivními modely AR. Naše výsledky ukazují, že ačkoli jsou modely NAR rychlejší na GPU, s malými velikostmi dávek, jsou téměř vždy pomalejší za reálnějších podmínek použití. V budoucí práci požadujeme realističtější a rozsáhlejší hodnocení modelů NAR.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Efficient machine translation models are commercially important as they can increase inference speeds, and reduce costs and carbon emissions. Recently, there has been much interest in non-autoregressive (NAR) models, which promise faster translation. In parallel to the research on NAR models, there have been successful attempts to create optimized autoregressive models as part of the WMT shared task on efficient translation. In this paper, we point out flaws in the evaluation methodology present in the literature on NAR models and we provide a fair comparison between a state-of-the-art NAR model and the autoregressive submissions to the shared task. We make the case for consistent evaluation of NAR models, and also for the importance of comparing NAR models with other widely used methods for improving efficiency. We run experiments with a connectionist-temporal-classification-based (CTC) NAR model implemented in C++ and compare it with AR models using wall clock times. Our results show that, although NAR models are faster on  GPUs, with small batch sizes, they are almost always slower under more realistic usage conditions. We call for more realistic and extensive evaluation of NAR models in future work.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Úspěch hlubokého učení NLP je často popisován tak, že o jazyku nic nepředpokládáme a necháváme data mluvit sama za sebe. Ačkoli je to na mnoha úrovních diskutabilní, jedna věc je neobyčejně podezřelá: většina nejmodernějších NLP modelů předpokládá existenci diskrétních tokenů a používá segmentaci na podslova, která kombinuje pravidla s jednoduchými statistickými heuristikami. Vyhnout se explicitní segmentaci vstupi je obtížnější, než se zdá.

První část přednášky představí neurální neuronovou edistační vzdálenost, novou interpretovatelnou architekturu založenou na dobře známé Levenshteinově vzdálenosti, kterou lze použít pro čistě znakové úlohy, jako je transliterace nebo detekce kognátů. Ve druhé části přednášky si přiblížíme neuronový strojový překlad na úrovni znaků. Představíme, jak inovace v oblasti trénování a návrhu architektur mohou zlepšit kvalitu překladu. I přes tento pokrok se ukazuje, že metody na úrovni znaků ve strojovém překladu stále zaostávají za modely na bázi podslov téměř ve všech ohledech, které lze měřit.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The success of deep learning NLP is often narrated as not assuming anything about the language and letting the data speak for itself. Although this is debatable on many levels, one thing is outstandingly suspicious: most state-of-the-art NLP models assume the existence of discrete tokens and use subword segmentation which combines rules with simple statistical heuristics. Avoiding explicit input segmentations is more difficult than it seems.

The first part of the talk will present neural edit distance, a novel interpretable architecture based on well-known Levenshtein distance that can be used for purely character-level tasks such as transliteration or cognate detection. In the second part of the talk, we will zoom out and have a look at character-level methods for neural machine translation. We will present how innovations in training and architectures design can improve translation quality. Despite this progress, we will show that character-level methods in machine translation still lack behind the subword-based models nearly in all respect that can be measured.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Navrhujeme model neuronová editační vzdálenost pro párování řetězců a převod řetězců na základě naučené editační vzdálenosti. Upravili jsme původní MT algoritmus tak, aby využíval diferencovalnou ztrátovou funkci, což nám umožňuje integrovat ji do neuronové sítě poskytující kontextovou reprezentaci vstupu. Hodnotíme detekci kognatů, transliteraci a konverzi grafémů na fonémy a ukazujeme, že v jednom teoretickém rámci připravovat modely, kde jde proti sobě intepretovatelnost a přesnost. Pomocí kontextových reprezentací, které jsou ale hůře interpretovatelné, dosahuje stejné přesnosti jako nejlepší metody pro párování řetězců. Pomocí statických embedingů a mírně odlišné ztrátové funkce dokážeme vynutit interpretabilitu na úkor poklesu přesnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We propose the neural string edit distance model for string-pair matching and string transduction based on learnable string edit distance. We modify the original expectation-maximization learned edit distance algorithm into a differentiable loss function, allowing us to integrate it into a neural network providing a contextual representation of the input. We evaluate on cognate detection, transliteration, and grapheme-to-phoneme conversion, and show that we can trade off between performance and interpretability in a single framework. Using contextual representations, which are difficult to interpret, we match the performance of state-of-the-art string-pair matching models. Using static embeddings and a slightly different loss function, we force interpretability, at the expense of an accuracy drop.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku prezentuje přehled literatury a empirický průzkum, který kriticky hodnotí předchozí práci v oblasti strojového překladu na úrovni znaků. Navzdory tvrzením v literatuře, že systémy na úrovni znaků jsou srovnatelné se systémy, které pracují na úrovni podslov, prakticky nikdy se nepoužívají v soutěžních systémech WMT. Empiricky ukazujeme, že i s nedávnými inovacemi v modelování zpracování přirozeného jazyka na úrovni znaků se systémy strojového překladu na úrovni znaků stále obtížně vyrovnávají svým protějškům na bázi podslov. Strojový překlad na úrovni znaků nevykazuje ani lepší doménovou robustnost, ani lepší morfologické zobecnění, přestože to bývá často hlavní motivace pro jejich vývoj. Systémy zpracovávající vstup po znacích naopak vykazují velkou robustnost vůči šumu a že kvalita překladu neklesá ani s klesající mírou ořezávání během dekódování.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a literature and empirical survey that critically assesses the state of the art in character-level modeling for machine translation (MT). Despite evidence in the literature that character-level systems are comparable with subword systems, they are virtually never used in competitive setups in WMT competitions. We empirically show that even with recent modeling innovations in character-level natural language processing, character-level MT systems still struggle to match their subword-based counterparts. Character-level MT systems show neither better domain robustness, nor better morphological generalization, despite being often so motivated. However, we are able to show robustness towards source side noise and that translation quality does not degrade with increasing beam size at decoding time.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>jen anglický abstrakt k dispozici</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Semantics is a central concept to natural language understanding.
A qualitative measure to perceive language semantics is to measure the similarity between texts. Semantic Textual Similarity finds applications in almost
all major areas of Natural Language Processing (NLP) including textual entailment, summarization, machine translation, etc. Despite major progress in
the recent years, especially with the intrusion of deep learning in NLP, mea-
suring semantic textual similarity, especially for longer texts, is still a chal-
lenging and open research problem. The ambiguous nature of language makes
semantic similarity more challenging and difficult to model. It is trivial to
produce examples of sentence pairs which are superficially similar but bear a
very different meaning and vice-versa. To address this problem, several type
of methods have been proposed over the years which span from traditional
knowledge-based methods to recent ones relying on deep neural networks and
large volumes of that data. Here in this work, we present a comprehensive sys-
tematic review of the various semantic similarity techniques. We emphasize the
various paradigms of semantic similarity measurement and resources available
to aid further research on this crucial topic. Our systematic literature review
would aid the reader to know about the evolution of semantic similarity techniques, the latest state-of-the-art methods, datasets to work-upon, challenges
and hence identify the research gaps.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nalezení rodokmenu výzkumného tématu je klíčové pro pochopení předchozího stavu umění a postupujícího vědeckého posunu. Záplava odborných článků ztěžuje nalezení nejvhodnější předchozí práce. Výzkumníci tak tráví značné množství času sestavováním seznamu literatury. Citace hrají zásadní roli při objevování relevantní literatury. Nicméně ne všechny citace jsou vytvořeny stejně. Většina citací, které noviny obdrží, poskytuje podkladové a kontextové informace citujícím dokumentům. V těchto případech není citovaný dokument ústředním tématem citujících listin. Některé dokumenty však vycházejí z daného papíru, čímž se dále posouvá hranice výzkumu. V těchto případech hraje dotčený citovaný dokument v citujícím dokumentu stěžejní roli. Podstata citace, kterou prvně jmenovaný obdrží od druhého, je tedy významná. V této práci probíráme naše výzkumy směřující k objevení významných citací daného dokumentu. Dále ukazujeme, jak můžeme využít významné citace k sestavení výzkumné linie prostřednictvím významného citačního grafu. Účinnost naší myšlenky demonstrujeme dvěma případovými studiemi v reálném životě. Naše experimenty přinášejí slibné výsledky, pokud jde o současný stav klasifikace významných citací, předčí ty předchozí s relativním náskokem 20 bodů, pokud jde o přesnost. Předpokládáme, že takový automatizovaný systém může usnadnit vyhledávání příslušné literatury a pomoci identifikovat tok znalostí pro určitou kategorii papírů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Finding the lineage of a research topic is crucial for understanding the prior state of the art and advancing scientific displacement. The deluge of scholarly articles makes it difficult to locate the most relevant previous work. It causes researchers to spend a considerable amount of time building up their literature list. Citations play a crucial role in discovering relevant literature. However, not all citations are created equal. The majority of the citations that a paper receives provide contextual and background information to the citing papers. In those cases, the cited paper is not central to the theme of citing papers. However, some papers build upon a given paper, further the research frontier. In those cases, the concerned cited paper plays a pivotal role in the citing paper. Hence, the nature of citation the former receives from the latter is significant. In this work, we discuss our investigations towards discovering significant citations of a given paper. We further show how we can leverage significant citations to build a research lineage via a significant citation graph. We demonstrate the efficacy of our idea with two real-life case studies. Our experiments yield promising results with respect to the current state-of-the-art in classifying significant citations, outperforming the earlier ones by a relative margin of 20 points in terms of precision. We hypothesize that such an automated system can facilitate relevant literature discovery and help identify knowledge flow for a particular category of papers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nalezení rodokmenu výzkumného tématu je klíčové pro pochopení předchozího stavu umění a postupujícího vědeckého posunu. Záplava odborných článků ztěžuje vyhledávání nejvýznamnějších předchozích prací a způsobuje, že výzkumníci tráví značné množství času sestavováním seznamu literatury. Citace hrají významnou roli při objevování relevantní literatury. Nicméně ne všechny citace jsou vytvořeny stejně. Většina citací, které redakce obdrží, slouží k poskytování kontextuálních a podkladových informací citujícím listinám a nejsou ústředním tématem těchto listin. Některé práce jsou však pro citující noviny stěžejní a inspirují nebo brzdí výzkum v citujících novinách. Z toho vyplývá, že podstata citace, kterou prvně jmenovaný obdrží od druhého, je významná. V tomto rozpracovaném dokumentu diskutujeme o naší předběžné myšlence vytvořit rodokmen pro daný výzkum prostřednictvím identifikace významných citací. Předpokládáme, že takový automatizovaný systém může usnadnit vyhledávání příslušné literatury a pomoci identifikovat tok znalostí alespoň pro určitou kategorii papírů. Distálním cílem této práce je zjistit skutečný dopad výzkumné práce nebo zařízení mimo přímá citační čísla.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Finding the lineage of a research topic is crucial for understanding the prior state of the art and advancing scientific displacement. The deluge of scholarly articles makes it difficult to locate the most relevant prior work and causes researchers to spend a considerable amount of time building up their literature list. Citations play a significant role in discovering relevant literature. However, not all citations are created equal. A majority of the citations that a paper receives are for providing contextual, and background information to the citing papers and are not central to the theme of those papers. However, some papers are pivotal to the citing paper and inspire or stem up the research in the citing paper. Hence the nature of citation the former receives from the latter is significant. In this work in progress paper, we discuss our preliminary idea towards establishing a lineage for a given research via identifying significant citations. We hypothesize that such an automated system can facilitate relevant literature discovery and help identify knowledge flow for at least a certain category of papers. The distal goal of this work is to identify the real impact of research work or a facility beyond direct citation counts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Speciální zasedání SummDial o sumarizaci dialogů a setkání více stran se konalo prakticky v rámci konference SIGDial 2021 dne 29. července 2021. SummDial @ SIGDial 2021 si kladl za cíl spojit komunity zabývající se řečí, dialogem a sumarizací, aby se podpořilo vzájemné opylování myšlenek a podpořily diskuse/spolupráce při pokusu o tento zásadní a aktuální problém. Když pandemie omezila většinu našich osobních interakcí, současný scénář donutil lidi přejít na virtuální, což vyústilo v zahlcení informacemi z častých dialogů a setkání ve virtuálním prostředí. Shrnutí by mohlo pomoci snížit kognitivní zátěž účastníků, nicméně sumarizace projevů více stran přináší vlastní soubor výzev. Speciální zasedání SummDial si kladlo za cíl využít komunitní zpravodajství k nalezení efektivních řešení a zároveň brainstorming o budoucnosti intervencí umělé inteligence na zasedáních a dialozích. O výsledcích zvláštního zasedání informujeme v tomto článku. Speciální sekci SummDial jsme uspořádali pod záštitou projektu H2020 European Live Translator (ELITR) financovaného EU.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The SummDial special session on summarization of dialogues and multi-party meetings was held virtually within the SIGDial 2021 conference on July 29, 2021. SummDial @ SIGDial 2021 aimed to bring together the speech, dialogue, and summarization communities to foster cross-pollination of ideas and fuel the discussions/collaborations to attempt this crucial and timely problem. When the pandemic has restricted most of our in-person interactions, the current scenario has forced people to go virtual, resulting in an information overload from frequent dialogues and meetings in the virtual environment. Summarization could help reduce the cognitive burden on the participants; however, multi-party speech summarization comes with its own set of challenges. The SummDial special session aimed to leverage the community intelligence to find effective solutions while also brainstorming the future of AI interventions in meetings and dialogues. We report the findings of the special session in this article. We organized the SummDial special session under the aegis of the EU-funded H2020 European Live Translator (ELITR) project.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento výstup představuje tři podrobné případové studie pro každou z hlavních tematických oblastí úkolu SSHOC 3.1 "Vícejazyčné terminologie", jejichž cílem je prozkoumat přístupy NLP a MT s ohledem na poskytování zdrojů a nástrojů pro podporu vícejazyčného přístupu k obsahu SSH v různých jazycích a zlepšení vyhledávání pro nerodilé mluvčí.  Soubor vícejazyčných metadatových konceptů, vícejazyčných slovníků a automaticky extrahovaných vícejazyčných terminologií byl dodán jako volně dostupná data, plně odpovídající zásadám FAIR prosazovaným v rámci EOSC, která lze nalézt prostřednictvím VLO a dalších služeb CLARIN a SSHOC.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The deliverable presents three detailed case studies for each of the main topical areas of SSHOC Task 3.1 “Multilingual Terminologies'' aiming to investigate NLP and MT approaches  in  view  of  providing resources  and  tools  to  foster  multilingual  access  to  SSH  content  across  different  languages  and improve  discovery  by  non-native  speakers.  A  set  of  multilingual  metadata  concepts,  multilingual vocabularies  and  automatically  extracted  multilingual  terminologies  has  been  delivered  as  freely, openly available data, fully corresponding to the FAIR  principles promoted within the EOSC, findable through the VLO and other CLARIN and SSHOC services.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozpoznávání ručně psané hudby je náročná úloha, které může vést ke zlepšení dostupnosti archivních rukopisů nebo usnadnění hudební kompozice. Moderních metody strojového učení však nelze na tuto úlohu snadno aplikovat kvůli omezené dostupnosti kvalitních trénovacích dat. Ruční anotace takových dat je drahá, a proto není v potřebném měřítku proveditelná. Tento problém již byl v jiných oblastech vyřešen trénováním  na automaticky generovaných syntetických datech. V tomto článku používáme stejný přístup k rozpoznávání ručně psané hudby a představujeme metodu generování syntetických snímků ručně psaných hudebních zápisů a ukazujeme, že trénování na těchto datech vede k výborným výsledkům.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Handwritten music recognition is a challenging task that could be of great use if mastered, e.g., to improve the accessibility of archival manuscripts or to ease music composition. Many modern machine learning techniques, however, cannot be easily applied to this task because of the limited availability of high-quality training data. Annotating such data manually is expensive and thus not feasible at the necessary scale. This problem has already been tackled in other fields by training on automatically generated synthetic data. We bring this approach to handwritten music recognition and present a method to generate synthetic handwritten music images (limited to monophonic scores) and
show that training on such data leads to state-of-the-art results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nedávný výbuch falešných informací na sociálních
média vedla k intenzivnímu výzkumu automatických modelů fake news detection a ověřovačů faktů. Falešné zprávy a dezinformace,
vzhledem ke své zvláštnosti a rychlému šíření, představovaly mnoho
zajímavé výzvy v oblasti zpracování přirozeného jazyka (NLP)
a komunity Machine Learning (ML). Přípustná literatura
ukazuje, že neotřelé informace zahrnují moment překvapení,
což je hlavní charakteristika pro zesílení a
viralita dezinformací. Román a emocionální informace
přitahuje okamžitou pozornost čtenáře. Emoce jsou
prezentace určitého pocitu nebo sentimentu. Sentiment pomáhá
jedince, který by vyjadřoval své emoce prostřednictvím výrazu a
proto spolu tyto dvě věci souvisejí. Tedy novinka v novince
a následně zjištění emočního stavu a pocitu
čtečka vypadá jako tři klíčové ingredience, těsně spojené
s dezinformacemi. V tomto dokumentu navrhujeme hluboký multiúkol
učící se model, který společně provádí detekci novot, emocí
rozpoznávání, předpovídání nálad a odhalování dezinformací.
Náš navrhovaný model dosahuje nejmodernějších parametrů (SOTA) pro detekci falešných zpráv na třech srovnávacích datasetech,
viz. ByteDance, Fake News Challenge(FNC), a Covid-Stance
s 11,55%, 1,58% a 21,76% zlepšením přesnosti,
respektive. Navrhovaný přístup také ukazuje na účinnost
rámec jednoho úkolu se ziskem přesnosti 11,53, 28,62,
a 14,31 procentního bodu u výše uvedených tří souborů údajů. Zdrojový kód je dostupný na https://github.com/Nish-19/MultitaskFake-News-NES</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The recent explosion in false information on social
media has led to intensive research on automatic fake news detection models and fact-checkers. Fake news and misinformation,
due to its peculiarity and rapid dissemination, have posed many
interesting challenges to the Natural Language Processing (NLP)
and Machine Learning (ML) community. Admissible literature
shows that novel information includes the element of surprise,
which is the principal characteristic for the amplification and
virality of misinformation. Novel and emotional information
attracts immediate attention in the reader. Emotion is the
presentation of a certain feeling or sentiment. Sentiment helps
an individual to convey his emotion through expression and
hence the two are co-related. Thus, Novelty of the news item
and thereafter detecting the Emotional state and Sentiment of
the reader appear to be three key ingredients, tightly coupled
with misinformation. In this paper we propose a deep multitask
learning model that jointly performs novelty detection, emotion
recognition, sentiment prediction, and misinformation detection.
Our proposed model achieves the state-of-the-art(SOTA) performance for fake news detection on three benchmark datasets,
viz. ByteDance, Fake News Challenge(FNC), and Covid-Stance
with 11.55%, 1.58%, and 21.76% improvement in accuracy,
respectively. The proposed approach also shows the efficacy over
the single-task framework with an accuracy gain of 11.53, 28.62,
and 14.31 percentage points for the above three datasets. The source code is available at https://github.com/Nish-19/MultitaskFake-News-NES</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Falešné zprávy nebo dezinformace jsou informace nebo příběhy záměrně vytvořené s cílem oklamat nebo uvést čtenáře v omyl. V dnešní době se platformy sociálních médií staly zralým důvodem k dezinformacím a během několika minut je rozšířily, což vedlo k chaosu, panice a potenciálním zdravotním rizikům mezi lidmi. Rychlé šíření a plodný nárůst šíření falešných zpráv a dezinformací vytváří časově nejkritičtější výzvy pro komunitu zpracování přirozeného jazyka (NLP). Z příslušné literatury vyplývá, že přítomnost momentu překvapení v příběhu je silnou hnací silou rychlého šíření dezinformací, které přitahuje okamžitou pozornost a vyvolává ve čtenáři silné emocionální podněty. Falešné příběhy nebo falešné informace jsou psány, aby vzbudily zájem a aktivovaly emoce lidí, aby je šířili. Falešné příběhy mají tedy vyšší úroveň novosti a emočního obsahu než pravdivé příběhy. Z toho vyplývá, že novost zpravodajského příspěvku a rozpoznání emočního stavu čtenáře po přečtení příspěvku jsou dva klíčové úkoly, které úzce souvisejí s odhalováním dezinformací. Předchozí literatura nezkoumala detekci dezinformací vzájemným učením pro detekci novot a rozpoznávání emocí podle našeho nejlepšího vědomí. Naše současná práce tvrdí, že společné učení novosti a emocí z cílového textu je pádným argumentem pro odhalování dezinformací. V tomto dokumentu navrhujeme hluboký víceúčelový vzdělávací rámec, který společně provádí detekci novot, rozpoznávání emocí a detekci dezinformací. Náš hluboký multitask model dosahuje nejmodernějších výkonů (SOTA) pro detekci falešných zpráv na čtyřech srovnávacích datasetech, viz. ByteDance, FNC, Covid-Stance a FNID s přesností 7,73%, 3,69%, 7,95% a 13,38%. Z hodnocení vyplývá, že náš víceúkolový vzdělávací rámec zlepšuje výkon oproti jednoúkolovému rámci pro čtyři datové soubory o 7,8 %, 28,62 %, 11,46 % a 15,66 % celkového zisku přesnosti. Tvrdíme, že textová novinka a emoce jsou dva klíčové aspekty, které je třeba zvážit při vývoji automatického mechanismu detekce falešných zpráv. Zdrojový kód je dostupný na adrese https://github.com/Nish-19/Misinformation-Multitask-Attention-NE.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Fake news or misinformation is the information or stories intentionally created to deceive or mislead the readers. Nowadays, social media platforms have become the ripe grounds for misinformation, spreading them in a few minutes, which led to chaos, panic, and potential health hazards among people. The rapid dissemination and a prolific rise in the spread of fake news and misinformation create the most time-critical challenges for the Natural Language Processing (NLP) community. Relevant literature reveals that the presence of an element of surprise in the story is a strong driving force for the rapid dissemination of misinformation, which attracts immediate attention and invokes strong emotional stimulus in the reader. False stories or fake information are written to arouse interest and activate the emotions of people to spread it. Thus, false stories have a higher level of novelty and emotional content than true stories. Hence, Novelty of the news item and recognizing the Emotional state of the reader after reading the item seems two key tasks to tightly couple with misinformation Detection. Previous literature did not explore misinformation detection with mutual learning for novelty detection and emotion recognition to the best of our knowledge. Our current work argues that joint learning of novelty and emotion from the target text makes a strong case for misinformation detection. In this paper, we propose a deep multitask learning framework that jointly performs novelty detection, emotion recognition, and misinformation detection. Our deep multitask model achieves state-of-the-art (SOTA) performance for fake news detection on four benchmark datasets, viz. ByteDance, FNC, Covid-Stance and FNID with 7.73%, 3.69%, 7.95% and 13.38% accuracy gain, respectively. The evaluation shows that our multitask learning framework improves the performance over the single-task framework for four datasets with 7.8%, 28.62%, 11.46%, and 15.66% overall accuracy gain. We claim that textual novelty and emotion are the two key aspects to consider while developing an automatic fake news detection mechanism. The source code is available at https://github.com/Nish-19/Misinformation-Multitask-Attention-NE.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jednou z časově nejkritičtějších výzev pro komunitu zpracování přirozeného jazyka (NLP) je boj proti šíření falešných zpráv a dezinformací. Stávající přístupy k odhalování dezinformací využívají modely neurální sítě, statistické metody, jazykové znaky, strategie ověřování faktů atd. Zdá se však, že hrozba falešných zpráv s příchodem humorných a neobvykle kreativních jazykových modelů sílí. Z příslušné literatury vyplývá, že jedním z hlavních rysů virality falešných zpráv je přítomnost momentu překvapení v příběhu, který přitahuje okamžitou pozornost a vyvolává ve čtenáři silné emocionální podněty. Při této práci tuto myšlenku zužitkujeme a navrhujeme jako dva úkoly související s automatickou detekcí dezinformací detekci textových novinek a předvídání emocí. Pro detekci novot používáme znovu účelové textové obnosy a k třídění falešných informací využíváme modely trénované na rozsáhlých datových souborech obnosů a emocí. Naše výsledky korelují s myšlenkou, protože dosahujeme nejmodernějších výsledků (SOTA) ve čtyřech rozsáhlých souborech dezinformačních dat (7,92%, 1,54%, 17,31% a 8,13% zlepšení z hlediska přesnosti). Doufáme, že naše současná sonda bude motivovat komunitu k dalšímu výzkumu odhalování dezinformací v tomto směru. Zdrojový kód je dostupný na GitHub.2</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>One of the most time-critical challenges for the Natural Language Processing (NLP) community is to combat the spread of fake news and misinformation. Existing approaches for misinformation detection use neural network models, statistical methods, linguistic traits, fact-checking strategies, etc. However, the menace of fake news seems to grow more vigorous with the advent of humongous and unusually creative language models. Relevant literature reveals that one major characteristic of the virality of fake news is the presence of an element of surprise in the story, which attracts immediate attention and invokes strong emotional stimulus in the reader. In this work, we leverage this idea and propose textual novelty detection and emotion prediction as the two tasks relating to automatic misinformation detection. We re-purpose textual entailment for novelty detection and use the models trained on large-scale datasets of entailment and emotion to classify fake information. Our results correlate with the idea as we achieve state-of-the-art (SOTA) performance (7.92%, 1.54%, 17.31% and 8.13% improvement in terms of accuracy) on four large-scale misinformation datasets. We hope that our current probe will motivate the community to explore further research on misinformation detection along this line. The source code is available at the GitHub.2</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Počítačová lingvistika vytváří modely, které jsou užitečné při zpracování a generování jazyka a které mohou zlepšit naše porozumění jazykovým jevům. Z pohledu počítačového zpracování představují jazyková data výzvu hlavně kvůli jejich různému stupni idiosynkrazie (nečekané vlastnosti, které má jen několik podobných objektů) a kvůli všudypřítomným nekompozicionálním jevům, jako jsou víceslovné výrazy (jejichž význam nelze jednoduše odvodit z významů jednotlivých částí, např. angl. red tape, by and large, to pay a visit a to pull one’s leg).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Computational linguistics builds models that can usefully process and produce language and that can increase our understanding of linguistic phenomena. From the computational perspective, language data are particularly challenging notably due to their variable degree of idiosyncrasy (unexpected properties shared by few peer objects), and the pervasiveness of non-compositional phenomena such as multiword expressions (whose meaning cannot be straightforwardly deduced from the meanings of their components, e.g. red tape, by and large, to pay a visit and to pull one’s leg) and constructions (conventional associations of forms and meanings). Additionally, if models and methods are to be consistent and valid across languages, they have to face specificities inherent either to particular languages, or to various linguistic traditions.
These challenges were addressed by the Dagstuhl Seminar 21351 entitled "Universals of Linguistic Idiosyncrasy in Multilingual Computational Linguistics", which took place on 30-31 August 2021. Its main goal was to create synergies between three distinct though partly overlapping communities: experts in typology, in cross-lingual morphosyntactic annotation and in multiword expressions. This report documents the program and the outcomes of the seminar. We present the executive summary of the event, reports from the 3 Working Groups and abstracts of individual talks and open problems presented by the participants.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jedním z nejobtížnějších aspektů současné sumarizace zpráv z jednoho dokumentu je to, že souhrn často obsahuje "extrinsické halucinace", tj. fakta, která nejsou obsažena ve zdrojovém dokumentu a která jsou často odvozena prostřednictvím znalosti světa. To způsobuje, že se sumarizační systémy chovají spíše jako otevřené jazykové modely se sklonem k halucinacím faktů, které jsou chybné. V tomto článku tento problém zmírňujeme pomocí doplňkových zdrojových dokumentů, které pomáhají při řešení úlohy. Představujeme novou datovou sadu MiRANews a srovnáváme stávající sumarizační modely.  Na rozdíl od vícedokumentové sumarizace, která se zabývá více událostmi z několika zdrojových dokumentů, se stále zaměřujeme na generování souhrnu pro jeden dokument. Pomocí analýzy dat ukazujeme, že na vině nejsou jen modely: více než 27 % faktů uvedených ve zlatých souhrnech MiRANews je lépe podloženo v pomocných dokumentech než v hlavních zdrojových článcích. Analýza chyb vygenerovaných souhrnů z předtrénovaných modelů dotrénovaných na MiRANews ukazuje, že to má na modely ještě větší vliv: asistovaná sumarizace snižuje počet halucinací o 55 % ve srovnání s modely pro sumarizaci jednotlivých dokumentů trénovanými pouze na hlavním článku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>One of the most challenging aspects of current single-document news summarization is that the summary often contains `extrinsic hallucinations', i.e., facts that are not present in the source document, which are often derived via world knowledge. This causes summarisation systems to act more like open-ended language models tending to hallucinate facts that are erroneous. In this paper, we mitigate this problem with the help of multiple supplementary resource documents assisting the task. We present a new dataset MiRANews and benchmark existing summarisation models.  In contrast to multi-document summarization, which addresses multiple events from several source documents, we still aim at generating a summary for a single document. We show via data analysis that it's not only the models which are to blame: more than 27% of facts mentioned in the gold summaries of MiRANews are better grounded on assisting documents than in the main source articles. An error analysis of generated summaries from pretrained models fine-tuned on MiRANews reveals that this has an even bigger effects on models: assisted summarisation reduces 55% of  hallucinations when compared to single-document summarisation models trained on the main article only.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme model AggGen (vyslovuje se "again"), který do neuronových systémů pro převod dat na text znovu zavádí dvě explicitní fáze plánování vět: řazení a agregaci vstupů. Na rozdíl od předchozích prací využívajících plánování vět je náš model stále end-to-end: AggGen provádí plánování vět současně s generováním textu tím, že se učí latentní zarovnání (prostřednictvím sémantických faktů) mezi vstupní reprezentací a cílovým textem. Experimenty na datech ze soutěží WebNLG a E2E ukazují, že díky použití zarovnání na základě faktů je náš přístup interpretovatelnější, expresivnější, odolnější vůči šumu a snadněji kontrolovatelný, přičemž si zachovává výhody end-to-end systémů z hlediska plynulosti. Náš kód je k dispozici na adrese https://github.com/XinnuoXu/AggGen.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present AggGen (pronounced ‘again’) a data-to-text model which re-introduces two explicit sentence planning stages into neural data-to-text systems: input ordering and input aggregation. In contrast to previous work using sentence planning, our model is still end-to-end: AggGen performs sentence planning at the same time as generating text by learning latent alignments (via semantic facts) between input representation and target text. Experiments on the WebNLG and E2E challenge data show that by using fact-based alignments our approach is more interpretable, expressive, robust to noise, and easier to control, while retaining the advantages of end-to-end systems in terms of fluency. Our code is available at https://github.com/XinnuoXu/AggGen.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci popisujeme naše systémové předkládání úkolu SemEval 2021 11: NLP Contribution Graph Challenge. Vyzkoušíme všechny tři dílčí úkoly výzvy a podáme zprávu o výsledcích. Cílem dílčího úkolu 1 je identifikovat přispívající věty v dané publikaci. Dílčí úkol č. 2 vyplývá z dílčího úkolu č. 1, jehož cílem je vyjmout vědecký termín a predikovat věty z označených přispívajících vět. Závěrečný dílčí úkol 3 spočívá ve vyjmutí trojic (předmět, predikát, objekt) z frází a jejich kategorizaci do jedné nebo více definovaných informačních jednotek. Sdíleným úkolem NLPContributionGraph organizátoři formalizovali vytvoření odborně zaměřeného grafu nad odbornými články NLP jako automatizovaný úkol. Mezi naše přístupy patří klasifikační model založený na BERT pro identifikaci přispívajících vět ve výzkumné publikaci, analýza závislosti založená na pravidlech pro extrakci frází, následovaná modelem podle CNN pro klasifikaci informačních jednotek a soubor pravidel pro extrakci trojice. Kvantitativní výsledky ukazují, že pátou, pátou a sedmou příčku získáváme ve třech fázích hodnocení. Naše kódy jsou dostupné na https://github.com/HardikArora17/SemEval-2021-INNOVATORS.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this work, we describe our system submission to the SemEval 2021 Task 11: NLP Contribution Graph Challenge. We attempt all the three sub-tasks in the challenge and report our results. Subtask 1 aims to identify the contributing sentences in a given publication. Subtask 2 follows from Subtask 1 to extract the scientific term and predicate phrases from the identified contributing sentences. The final Subtask 3 entails extracting triples (subject, predicate, object) from the phrases and categorizing them under one or more defined information units. With the NLPContributionGraph Shared Task, the organizers formalized the building of a scholarly contributions-focused graph over NLP scholarly articles as an automated task. Our approaches include a BERT-based classification model for identifying the contributing sentences in a research publication, a rule-based dependency parsing for phrase extraction, followed by a CNN-based model for information units classification, and a set of rules for triples extraction. The quantitative results show that we obtain the 5th, 5th, and 7th rank respectively in three evaluation phases. We make our codes available at https://github.com/HardikArora17/SemEval-2021-INNOVATORS.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek obsahuje popis sdílených úloh na WAT 2021 podle našeho tým "NLPHut". Zúčastnili jsme se úlohy multimodálního překladu angličtina→hindština, úlohy multimodálního překladu angličtina→malajština a úlohy vícejazyčného překladu z indických jazyků. Použili jsme nejmodernější model Transformeru
s jazykovými značkami v různých nastaveních
pro překladatelskou úlohu a navrhli jsme
nové "regionálně specifické" generování titulků využívající kombinaci obrazových
CNN a LSTM pro hindštinu a malajalamštinu. Náš příspěvek v angličtině→malajálamštině Multimodální
překladatelské úlohy (překlad pouze textu a titulků) a na druhém místě v úloze multimodálního překladu angličtina→hindština (překlad pouze textu a titulků). Naše příspěvky si vedly dobře také v indických vícejazyčných překladových úlohách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper provides the description of
shared tasks to the WAT 2021 by our
team “NLPHut”. We have participated
in the English→Hindi Multimodal translation task, English→Malayalam Multimodal translation task, and Indic Multilingual translation task. We have used
the state-of-the-art Transformer model
with language tags in different settings
for the translation task and proposed a
novel “region-specific” caption generation
approach using a combination of image
CNN and LSTM for the Hindi and Malayalam image captioning. Our submission
tops in English→Malayalam Multimodal
translation task (text-only translation, and
Malayalam caption), and ranks secondbest in English→Hindi Multimodal translation task (text-only translation, and Hindi
caption). Our submissions have also performed well in the Indic Multilingual translation tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení technologických možností překladatele ve 21. století a projektů věnujících se výzkumu a hodnocení strojového překladu na ÚFAL.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presentation of the technological possibilities of the translator in the 21st century and projects dedicated to research and evaluation of machine translation at ÚFAL.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška se zaměřovala na rozdíly mezi strojovým a lidským překladem – poznáme na první pohled, co přeložil člověk a co stroj? Existuje jen jeden správný překlad? Jak se hodnotí kvalita překladu?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The lecture focused on the differences between machine and human translation - can we tell at a glance what is translated by a human and what by a machine? Is there only one correct translation? How is the quality of a translation assessed?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek poskytuje stručný přehled možných metod, jak zjistit, že referenční překlady byly ve skutečnosti vytvořeny posteditací strojového překladu. Využity byly dvě metody založené na automatických metrikách: 1. rozdíl BLEU mezi podezřelým MT a některým jiným
dobrým MT a 2. rozdíl BLEU s využitím dalších referencí. Tyto dvě metody odhalily
podezření, že oficiální reference z WMT 2020 je ve skutečnosti posteditovaný strojový překlad. Toto podezření bylo potvrzeno při manuální analýze zjištěním konkretních dokladů o posteditačním postupu. Také byla vytvořena typologie
posteditačních změn, kde jsou uvedeny některé chyby nebo změny provedené posteditorem, nebo také chyby převzaté ze strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper provides a quick overview of possible methods how to detect that reference translations were actually created by post-editing
an MT system. Two methods based on automatic metrics are presented: BLEU difference
between the suspected MT and some other
good MT and BLEU difference using additional references. These two methods revealed
a suspicion that the WMT 2020 Czech reference is based on MT. The suspicion was confirmed in a manual analysis by finding concrete proofs of the post-editing procedure in
particular sentences. Finally, a typology of
post-editing changes is presented where typical errors or changes made by the post-editor
or errors adopted from the MT are classified.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme vítězný systém z Multilingual Lexical Normalization (MultiLexNorm) Shared Task na W-NUT 2021 (van der Goot et al., 2021a), který vyhodnocuje lexikálně-normalizační systémy na 12 datasetech sociálních médií v 11 jazycích. Naše řešení zakládáme na předtrénovaném jazykovém modelu ByT5 (Xue et al., 2021a), který dále trénujeme na syntetických datech a poté dotrénováváme na autentických normalizačních datech. Náš systém dosahuje nejlepších výsledků s velkým náskokem v intrinsic hodnocení a také nejlepších výsledků v extrinsic vyhodnocení prostřednictvím syntaktické analýzy. Zdrojový kód je uvolněn na https://github.com/ufal/multilexnorm2021 a natrénované modely na https://huggingface.co/ufal.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the winning entry to the Multilingual Lexical Normalization (MultiLexNorm) shared task at W-NUT 2021 (van der Goot et al., 2021a), which evaluates lexical-normalization systems on 12 social media datasets in 11 languages. We base our solution on a pre-trained byte-level language model, ByT5 (Xue et al., 2021a), which we further pre-train on synthetic data and then fine-tune on authentic normalization data. Our system achieves the best performance by a wide margin in intrinsic evaluation, and also the best performance in extrinsic evaluation through dependency parsing. The source code is released at https://github.com/ufal/multilexnorm2021 and the fine-tuned models at https://huggingface.co/ufal.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Evropa je mnohojazyčná společnost, v níž se hovoří desítkami jazyků. Jedinou možností, jak mnohojazyčnost umožnit a využít, jsou jazykové technologie (LT), tj. technologie zpracování přirozeného jazyka a řeči. Popisujeme evropskou jazykovou síť (European Language Grid, ELG), která se má vyvinout v primární platformu a tržiště LT v Evropě tím, že poskytne jednu zastřešující platformu pro evropský LT prostor, včetně výzkumu a průmyslu, která umožní všem zúčastněným stranám nahrávat, sdílet a distribuovat své služby, produkty a zdroje. Na konci projektu, který v roce 2022 vytvoří právní subjekt, poskytne ELG přístup k cca. 1300 služeb pro všechny evropské jazyky a tisíce datových souborů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Europe is a multilingual society, in which dozens of languages are spoken. The only option to enable and to benefit from multilingualism is through Language Technologies (LT), i.e., Natural Language Processing and Speech Technologies. We describe the European Language Grid (ELG), which is targeted to evolve into the primary platform and marketplace for LT in Europe by providing one umbrella platform for the European LT landscape, including research and industry, enabling all stakeholders to upload, share and distribute their services, products and resources. At the end of our EU project, which will establish a legal entity in 2022, the ELG will provide access to approx. 1300 services for all European languages as well as thousands of data sets.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Abstrakt je pouze v angličtině.

Interpreters facilitate multi-lingual meetings but the affordable set of languages is often smaller than what is needed. Automatic simultaneous speech translation can extend the set of provided languages. We investigate if such an automatic system should rather follow the original speaker, or an interpreter to achieve better translation quality at the cost of increased delay. To answer the question, we release Europarl Simultaneous Interpreting Corpus (ESIC), 10 hours of recordings and transcripts of European Parliament speeches in English, with simultaneous interpreting into Czech and German. We evaluate quality and latency of speaker-based and interpreter-based spoken translation systems from English to Czech. We study the differences in implicit simplification and summarization of the human interpreter compared to a machine translation system trained to shorten the output to some extent. Finally, we perform human evaluation to measure information loss of each of these approaches.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Interpreters facilitate multi-lingual meetings but the affordable set of languages is often smaller than what is needed. Automatic simultaneous speech translation can extend the set of provided languages. We investigate if such an automatic system should rather follow the original speaker, or an interpreter to achieve better translation quality at the cost of increased delay.
To answer the question, we release Europarl Simultaneous Interpreting Corpus (ESIC), 10 hours of recordings and transcripts of European Parliament speeches in English, with simultaneous interpreting into Czech and German. We evaluate quality and latency of speaker-based and interpreter-based spoken translation systems from English to Czech. We study the differences in implicit simplification and summarization of the human interpreter compared to a machine translation system trained to shorten the output to some extent. Finally, we perform human evaluation to measure information loss of each of these approaches.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Klasifikace předmětných článků je důležitým problémem
Schosimilar Document Processing to address the huge information overload in the scholarly space. Tento dokument popisuje
přístup našeho týmu CUNI-NU pro Biocreative VII-Track 5 challenge: Litcovid multilabel topic classification for
COVID-19 literatura [1]. Cílem dotčeného úkolu je automatizovat
manuální nakládání s biomedicínskými předměty do sedmi různých
značek, konkrétně pro datové úložiště LitCovid. Naše nejlepší
model používá dokument SPECTER [2]
zápatí pro reprezentaci abstraktních, a tituly vědeckých
články následované mechanismem Dual-Attention [3] do perform multilabel kategorizace. Dosáhli jsme významného lepší výkon než základní metody. Děláme
náš kód dostupný na https://github.com/Nid989/
CUNI-NU-Biocreative-Track5</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Subject-Article classification is an important problem in
Scholarly Document Processing to address the huge information overload in the scholarly space. This paper describes the
approach of our team CUNI-NU for the Biocreative VII-Track
5 challenge: Litcovid multi-label topic classification for
COVID-19 literature [1]. The concerned task aims to automate
the manual curation of biomedical articles into seven distinct
labels, specifically for the LitCovid data repository. Our best
performing model makes use of the SPECTER [2] document
embeddings for representing abstract, and titles of scientific
articles followed by a Dual-Attention [3] mechanism to perform the multi-label categorization. We achieve significantly
better performance than the baseline methods. We make our code available at https://github.com/Nid989/
CUNI-NU-Biocreative-Track5</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje znalostního vícejazyčného konverzačního agenta "MyWelcome Agent", který funguje jako osobní asistent migrantů v kontextu jejich přijímání a integrace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a knowledge-driven multilingual conversational agent (referred to as ``MyWelcome Agent'') that acts as personal assistant for migrants in the contexts of their reception and integration. In order to also account for tasks that go beyond communication and require advanced service coordination skills, the architecture of the proposed platform separates the dialogue management service from the agent behavior including the service selection and planning. The involvement of genuine agent planning strategies in the design of personal assistants, which tend to be limited to dialogue management tasks, makes the proposed agent significantly more versatile and intelligent. To ensure high quality verbal interaction, we draw upon state-of-the-art multilingual spoken language understanding and generation technologies.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Metriky srozumitelnosti textu hodnotí, kolik úsilí musí čtenář vynaložit na porozumění danému textu. Používají se např. K výběru vhodných materiálů pro četbu pro různé úrovně znalostí studentů nebo k zajištění efektivního přenosu důležitých informací (např. V případě nouze). Flesch Reading Ease je natolik globálně používaný vzorec, že je dokonce integrován do textového procesoru MS. Jeho konstanty jsou však závislé na jazyce. Původní vzorec byl vytvořen pro angličtinu. Doposud byl přizpůsoben několika evropským jazykům, bengálštině a hindštině. Tento článek popisuje českou adaptaci, přičemž jazykově závislé konstanty jsou optimalizovány algoritmem strojového učení pracujícím na paralelních korpusech češtiny s angličtinou, ruštinou, italštinou a francouzštinou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Text readability metrics assess how much effort a reader must put into comprehending a given text. They are e.g., used to choose appropriate readings for different student proficiency levels, or to make sure that crucial information is efficiently conveyed (e.g., in an emergency). Flesch Reading Ease is such a globally used formula that it is even integrated into the MS Word Processor. However, its constants are language dependent. The original formula was created for English. So far it has been adapted to several European languages, Bangla, and Hindi. This paper describes the Czech adaptation, with the language-dependent constants optimized by a machine-learning algorithm working on parallel corpora of Czech with English, Russian, Italian, and French.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Adaptovali jsme na češtinu čtyři klasické metriky srozumitelnosti na základě dat InterCorp (paralelní korpus s ručním zarovnáním vět)a CzEng 2.0 (velký paralelní korpus procházených webových textů) a algoritmu optimize.curve z knihovny SciPy. Adaptované metriky jsou: Flesch Reading Ease, Flesch-Kincaid Grade Level, Coleman-Liau Index a Automated Readability Index. Popisujeme podrobnosti postupu a předkládáme uspokojivé výsledky. Kromě toho diskutujeme citlivost těchto metrik na textové parafráze a korelaci skóre srozumitelnosti s empiricky pozorovaným porozuměním čtenému a také adaptaci Flesch Reading Ease na češtinu z ruštiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We have fitted four classic readability metrics to Czech, using InterCorp (a parallel corpus with manual sentence alignment), CzEng 2.0 (a large parallel corpus of crawled web texts),  and the optimize.curve fit algorithm from the SciPy library. The adapted metrics are: Flesch Reading Ease, Flesch-Kincaid Grade Level, Coleman-Liau Index, and Automated Readability Index. We describe the details of the procedure and present satisfactory results. Besides, we discuss the sensitivity of these metrics to text paraphrases and correlation of readability scores with empirically observed reading comprehension, as well as the adaptation of Flesch Reading Ease to Czech from Russian.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přestože přístupy založené na neuronových sítích výrazně zvýšily plynulost strojově generovaného textu, jsou náchylné k chybám v přesnosti, jako je vynechávání částí vstupu (opomenutí) nebo generování výstupu, který nemá oporu v žádném vstupu (halucinace). Tento problém se projevuje zejména při menším množství trénovacích dat, generování delších textů nebo šumu v trénovacích datech. Všechny tyto podmínky jsou pro úlohy generování přirozeného jazyka (NLG) velmi časté.

V této přednášce ukážu několik příkladů přístupů zaměřených na vytváření přesnějších výstupů v rámci systému NLG nebo na odhalování chyb ve výstupech systému NLG. Konkrétně se zaměřím na neuronové architektury sequence-to-sequence s předtrénovanými jazykovými modely a jejich rozšíření. Budu se zabývat především generováním textu ze strukturovaných dat, ale uvedu i srovnání se situací při sumarizaci nebo generování dialogových odpovědí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>While neural-network-based approaches have significantly increased the fluency of machine-generated text, they are prone to accuracy errors, such as leaving out parts of the input (omissions) or generating output that is not grounded in any input (hallucination). This problem becomes especially apparent with lower amounts of training data, generation of longer texts, and noise in the training data. All of these conditions are very common for natural language generation (NLG) tasks.

In this talk, I will show several example approaches aiming to produce more accurate outputs within an NLG system, or to detect errors in an NLG system output. I will specifically focus on sequence-to-sequence neural architectures with pretrained language models and their
extensions. I will mostly discuss generation of text from structured data, but I will also make comparisons to the situation in summarization or dialogue response generation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kdo se bojí umělé inteligence.
Přijďte do tematického prostoru Goethe-Institutu a zjistěte, jak s pomocí robotů vzniká literatura!

https://www.goethe.de/ins/cz/cs/ver.cfm?event_id=22345514</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Who's afraid of artificial intelligence?
Come to the Goethe-Institut thematic space and find out how literature is made with the help of robots!

https://www.goethe.de/ins/cz/en/ver.cfm?event_id=22345514</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Hodnotící kampaň Mezinárodní konference o překladu z mluveného jazyka
(IWSLT 2021) letos představila čtyři společné 
úkoly: (i) Simultánní překlad mluvené řeči, (ii) Offline překlad mluvené řeči, (iii) Vícejazyčný
překlad řeči, (iv) překlad řeči s malým množstvím zdrojů. Celkem se zúčastnilo 22 týmů
alespoň na jedné z úloh. Tento článek popisuje každou společnou úlohu, data a hodnocení.
metriky a uvádí výsledky obdržených podání.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The evaluation campaign of the International
Conference on Spoken Language Translation
(IWSLT 2021) featured this year four shared
tasks: (i) Simultaneous speech translation, (ii)
Offline speech translation, (iii) Multilingual
speech translation, (iv) Low-resource speech
translation. A total of 22 teams participated
in at least one of the tasks. This paper de-
scribes each shared task, data and evaluation
metrics, and reports results of the received submissions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá konstrukcemi s kategoriálním slovesem v češtině. Částěčně reviduje pravidla pro formování syntaktické struktury těchto konstrukcí navržená v rámci funkčního generativního popisu, a to zejména s ohledem na roli fakultativních volných doplnění kategoriálních sloves v těchto konstrukcích. Provedená analýza ukazuje, že v případech, kdy kategoriální slovesa neposkytují dostatečný počet valenčních doplnění pro povrchové vyjádření sémantických participantů jména, využívají participanty pro své povrchové vyjádření i fakultativní volná doplnění slovesa. Distribuce slovesných a jmenných doplnění analyzovaná v 1 600 konstrukcích s kategoriálním slovesem ukázala, že povrchové vyjádření participantů jména skrze fakultativní volná doplnění kategoriálního slovesa je v češtině silně preferováno (88% fakultativních volných doplnění slovesa oproti 12% valenčních doplnění jména).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper addresses Czech light verb constructions, partly revising principles of their syntactic structure formation formulated within the Functional Generative Description. It argues that obligatoriness of valency complementations should be reflected in these principles. Namely, the role of optional valency complementations of light verbs played in this process has been analyzed. This analysis has shown that in the cases where light verbs do not provide a sufficient number of valency complementations for the surface expression of semantic participants of predicative nouns, semantic participants of nouns make use of optional verbal complementations. In such cases, semantic participants can be expressed on the surface, either as optional verbal complementation or as nominal complementation. The distribution of verbal and nominal complementations have been observed in 1,600 light verb constructions extracted from the Czech National Corpus, with the result that the surface expression of these participants through the optional verbal complementations is strongly preferred (88% of verbal complementations and 12% of nominal ones).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Reflexiva představují velkou výzvu pro teoretický i lexikografický popis jazyka. Vzhledem ke změnám morfo-syntaktických vlastností sloves, která s sebou užití reflexiv nese, je jejich zachycení a popis vysoce relevantní pro slovesnou valenci. V češtině fungují reflexiva buď jako reflexivní osobní zájmeno, nebo jako morfém tvořící slovesné lemma či slovesný tvar. V tomto příspěvku se zabýváme těmi jazykovými jevy, které jsou kódovány reflexivním osobním zájmenem, tj. reflexivitou a reciprocitou.
Lexikografické znázornění těchto dvou jazykových jevů zpracováváme v rámci Valenčního slovníku českých sloves VALLEX. Reprezentace ve VALLEXu využívá rozdělení lexikonu na datovou a gramatickou komponentu; bere v úvahu, že reflexivita i reciprocita jsou podmíněny sémantickými vlastnostmi sloves, ovšem morfologické změny vyvolané těmito jevy jsou systémové a lze je popsat pomocí pravidel.
Zhruba třetina lexikálních jednotek obsažených v datové komponentě lexikonu má přiřazenu informaci o možném užití v reflexivních a/nebo recipročních konstrukcí, a to v podobě párů dotčených valenčních komplementů (2 039 u reflexivity a 2 744 u reciprocity). Dále je v gramatické komponentě formulován soubor pravidel pro odvození valenčních rámců pro syntakticky reflexní a reciproční konstrukce, která se aplikují na základové valenční rámce popisující nereflexní a nereferenční konstrukce (3 pravidla pro reflexivitu a 18 pravidel pro reciprocitu). V příspěvku se dále diskutují situace, kdy lexikální jednotky sloves vytvářejí nejednoznačné konstrukce, které mohou být vykládány buď jako reflexivní, nebo jako reciproční.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Reflexives, encoding a variety of meanings, pose a great challenge for both theoretical and lexicographic description. As they are associated with changes in morphosyntactic properties of verbs, their description is highly relevant for verb valency. In Czech, reflexives function as the reflexive personal pronoun and as verbal affixes. In this paper, we address those language phenomena that are encoded by the reflexive personal pronoun, i.e., reflexivity and reciprocity.
We introduce the lexicographic representation of these two language phenomena in the VALLEX lexicon, a valency lexicon of Czech verbs, accounting for the role of the reflexives with respect to the valency structure of verbs. This representation makes use of the division of the lexicon into a data component and a grammar component. It takes into account that reflexivity and reciprocity are conditioned by the semantic properties of verbs on the one hand and that morphosyntactic changes brought about by these phenomena are systemic on the other. 
About one third of the lexical units contained in the data component of the lexicon are assigned the information on reflexivity and/or reciprocity in the form of pairs of the affected valency complementations (2,039 on reflexivity and 2,744 on reciprocity). A set of rules is formulated in the grammar component (3 rules for reflexivity and 18 rules for reciprocity). These rules derive the valency frames underlying syntactically reflexive and reciprocal constructions from the valency frames describing non-reflexive and non-reciprocal constructions. Finally, the proposed representation makes it possible to determine which lexical units of verbs create ambiguous constructions that can be interpreted either as reflexive or as reciprocal.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V posledních desetiletích stále větší počet vystupujících umělců a technologů přivedl roboty na středové pódium při jejich vystoupeních. AI si přisvojila role komediálních improvizátorů, vypravěčů, herců, tanečníků a choreografů, čímž narušila tradiční obousměrnou lidskou herecko-lidskou diváckou interakci. Předložením této přednášky se snažíme zpochybnit myšlenku, že živé představení je specificky lidskou činností, a prozkoumat různé formy interakce mezi člověkem a umělou inteligencí v divadle. Jaký příběh Al vypráví? Jaké emoce to může vyvolat? Myslíte si, že umělá inteligence je schopna vytvořit příjemný divadelní scénář? Může se robot stát dramatikem? Existuje dokonalý divadelní večer, který vyprodukuje autonomní stroj?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Over the past decades an increasing number of performing artists and technologists have brought robots to the center stage in their performances. AI has taken roles such as comedy improvisers, storytellers, actors, dancers, and choreographers, disrupting the traditional two-way human actor-human audience interaction. By presenting this talk we try to challenge the idea that live performance is a specifically human activity and explore different forms of human-AI interactions in the theatre. What story does AI tell? What emotions can it generate? Do you think artificial intelligence is able to create an enjoyable theatre script? Can a robot become a playwright? Does the perfect theater evening exist, produced by an autonomous machine?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Šestý ročník workshopu Search-Oriented Conversational AI (SCAI 2021) byl uspořádán jako diskusní platforma o konverzační umělé inteligenci pro inteligentní přístup k informacím. Workshop byl koncipován jako multidisciplinární a spojil výzkumné pracovníky a odborníky z praxe napříč obory zpracování přirozeného jazyka (NLP), vyhledávání informací (IR), strojového učení (ML) a interakce člověk-počítač (HCI). Workshop zahrnoval čtyři sekce, na nichž zazněly pozvané přednášky, samostatnou posterovou sekci a sekci, na níž se diskutovalo o výsledcích soutěže v konverzačním zodpovídání otázek (SCAI-QReCC).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The 6th edition of the Search-Oriented Conversational AI workshop (SCAI 2021) was organised as a discussion platform on conversational AI for intelligent information access. The workshop was designed to be multidisciplinary, bringing together researchers and practitioners across the fields of natural language processing (NLP), information retrieval (IR), machine learning (ML) and human-computer interaction (HCI). The workshop included four sessions featuring invited talks, a separate poster session, and a session discussing the results of a shared task on conversational question answering (SCAI-QReCC).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme experimenty s automatickým odhalováním nekonzistentního chování na základě kontextu u dialogových systémů orientovaných na úkoly. Obohacujeme data bAbI/DSTC2 (Bordes et al., 2017) o automatickou anotaci nekonzistencí v dialogu a ukazujeme, že nekonzistence korelují s neúspěšnými dialogy. Předpokládáme, že použití omezené historie dialogů a předvídání dalšího tahu uživatele může zlepšit klasifikaci nekonzistencí. Zatímco obě hypotézy se potvrzují pro dialogový model založený na memory networks, neplatí pro trénování jazykového modelu GPT-2, který nejvíce těží z použití úplné historie dialogu a dosahuje 99% přesnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present experiments on automatically detecting inconsistent behavior of task-oriented dialogue systems from the context. We enrich the bAbI/DSTC2 data (Bordes et al., 2017) with automatic annotation of dialogue inconsistencies, and we demonstrate that inconsistencies correlate with failed dialogues. We hypothesize that using a limited dialogue history and predicting the next user turn can improve inconsistency classiﬁcation. While both hypotheses are conﬁrmed for a memory-networks-based dialogue model, it does not hold for a training based on the GPT-2 language model, which benefits most from using full dialogue history and achieves a 0.99 accuracy score.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předtrénované jazykové modely založené na attention, jako je GPT-2, přinesly značný pokrok v modelování dialogů "end-to-end". Pro dialog zaměřený na úkoly však představují také značná rizika, jako je nedostatečná podloženost fakty nebo rozmanitost odpovědí. Abychom tyto problémy vyřešili, zavádíme modifikované trénovací cíle pro dotrénování jazykového modelu a využíváme masivní augmentaci dat pomocí zpětného překladu, abychom zvýšili rozmanitost trénovacích dat. Dále zkoumáme možnosti kombinace dat z více zdrojů s cílem zlepšit výkonnost na cílové datové sadě. Naše příspěvky pečlivě vyhodnocujeme pomocí lidských i automatických metod. Náš model podstatně překonává základní model na datech MultiWOZ a vykazuje výkon konkurenceschopný se současným stavem techniky při automatickém i lidském hodnocení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Attention-based pre-trained language models such as GPT-2 brought considerable progress to end-to-end dialogue modelling. However, they also present considerable risks for task-oriented dialogue, such as lack of knowledge grounding or diversity. To address these issues, we introduce modified training objectives for language model finetuning, and we employ massive data augmentation via back-translation to increase the diversity of the training data. We further examine the possibilities of combining data from multiples sources to improve performance on the target dataset. We carefully evaluate our contributions with both human and automatic methods. Our model substantially outperforms the baseline on the MultiWOZ data and shows competitive performance with state of the art in both automatic and human evaluation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předtrénované jazykové modely založené na attention, jako je GPT-2, přinesly značný pokrok v end-to-end modelování dialogů. Pro dialog zaměřený na úkoly však představují také značná rizika, jako je nedostatečná korespondence s databází nebo nedostatek rozmanitosti odpovědí. Abychom tyto problémy vyřešili, zavádíme pro doladění jazykového modelu modifikované trénovací cíle a využíváme masivní rozšíření trénovacích dat pomocí zpětného překladu, čímž zvyšujeme jejich rozmanitost. Dále zkoumáme možnosti kombinace dat z více zdrojů s cílem zlepšit výkonnost na cílové datové sadě. Naše příspěvky pečlivě vyhodnocujeme pomocí ručních i automatických metod. Náš model dosahuje nejlepších výsledků na datové sadě MultiWOZ a vykazuje konkurenceschopný výkon při lidském hodnocení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Attention-based pre-trained language models such as GPT-2 brought considerable progress to end-to-end dialogue modelling. However, they also present considerable risks for task-oriented dialogue, such as lack of knowledge grounding or diversity. To address these issues, we introduce modified training objectives for language model finetuning, and we employ massive data augmentation via back-translation to increase the diversity of the training data. We further examine the possibilities of combining data from multiples sources to improve performance on the target dataset. We carefully evaluate our contributions with both human and automatic methods. Our model achieves state-of-the-art performance on the MultiWOZ data and shows competitive performance in human evaluation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Naše kniha „The Reality of Multi-Lingual Machine Translation“ pojednává o výhodách a nebezpečích používání více než dvou jazyků v systémech strojového překladu. I když se kniha zaměřuje na konkrétní úkol zpracování sekvencí a víceúkolového učení (multi-task learning), cílí i poněkud mimo oblast zpracování přirozeného jazyka. Strojový překlad je pro nás ukázkovým příkladem aplikací hlubokého učení, kde jsou lidské dovednosti a schopnosti učení brány jako laťka, kterou se mnozí snaží dosáhnout a překonat. Dokumentujeme, že některé z výdobytků pozorovaných v mnohojazyčném překladu mohou vyplývat z jednodušších důvodů, než je předpokládaný přenos znalostí napříč jazyky.

V první, spíše obecné části vás kniha provede motivací pro mnohojazyčnost, univerzálností hlubokých neuronových sítí zejména v úlohách typu sequence-to-sequence až ke komplikacím tohoto učení. Obecnou část uzavíráme varováním před příliš optimistickým a neopodstatněným vysvětlením zlepšení, které neuronové sítě demonstrují.

Ve druhé části se plně ponoříme do mnohojazyčných modelů, se zvlášť pečlivým zkoumáním učení přenosem (transfer learning) jako jednoho z přímočařejších přístupů využívajících další jazyky. Zkoumány jsou současné vícejazyčné techniky, včetně masivních modelů, a diskutovány jsou praktické aspekty nasazení mnohojazyčných systémů. Závěr knihy zdůrazňuje otevřený problém strojového porozumění a připomíná dva etické aspekty budování rozsáhlých modelů: inkluzivnost výzkumu a jeho ekologický dopad.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Our book "The Reality of Multi-Lingual Machine Translation" discusses the benefits and perils of using more than two languages in machine translation systems. While focused on the particular task of sequence-to-sequence processing and multi-task learning, the book targets somewhat beyond the area of natural language processing. Machine translation is for us a prime example of deep learning applications where human skills and learning capabilities are taken as a benchmark that many try to match and surpass. We document that some of the gains observed in multi-lingual translation may result from simpler effects than the assumed cross-lingual transfer of knowledge.

In the first, rather general part, the book will lead you through the motivation for multi-linguality, the versatility of deep neural networks especially in sequence-to-sequence tasks to complications of this learning. We conclude the general part with warnings against too optimistic and unjustified explanations of the gains that neural networks demonstrate.

In the second part, we fully delve into multi-lingual models, with a particularly careful examination of transfer learning as one of the more straightforward approaches utilizing additional languages. The recent multi-lingual techniques, including massive models, are surveyed and practical aspects of deploying systems for many languages are discussed. The conclusion highlights the open problem of machine understanding and reminds of two ethical aspects of building large-scale models: the inclusivity of research and its ecological trace.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V mediálním průmyslu se zaměření globálního zpravodajství může přes noc změnit. Existuje přesvědčivá
potřeba být schopni vyvinout nové systémy strojového překladu v krátkém časovém období, aby bylo možné efektivněji pokrýt rychle se vyvíjející příběhy. Jako součást stroje s nízkými zdroji
překladatelského projektu GOURMET jsme náhodně vybrali jazyk, pro který musel být systém
postaveno a vyhodnoceno za dva měsíce (únor a březen 2021). Vybraný jazyk byl
Paštština, indoíránský jazyk používaný v Afghánistánu, Pákistánu a Indii. V tomto období jsme
dokončili celý proces vývoje systému neuronového strojového překladu: procházení dat, čištění, zarovnání, vytváření testovacích sad, vývoj a testování modelů a jejich poskytování
uživatelským partnerům. V tomto článku popisujeme rychlý proces vytváření dat a experimenty
s transferovým učením a přípravou na paštskou angličtinu. Zjišťujeme, že začínáme od existujícího
velký model předem proškolený na 50 jazycích vede k mnohem lepším výsledkům BLEU než předtrénování na
jeden vysoce zdrojový jazykový pár s menším modelem. Uvádíme také lidské hodnocení
naše systémy, což naznačuje, že výsledné systémy fungují lépe než volně dostupné
komerční systém při překladu z angličtiny do paštštiny a podobně při
překlad z paštštiny do angličtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the media industry, the focus of global reporting can shift overnight. There is a compelling
need to be able to develop new machine translation systems in a short period of time, in order to more efficiently cover quickly developing stories. As part of the low-resource machine
translation project GoURMET, we selected a surprise language for which a system had to be
built and evaluated in two months (February and March 2021). The language selected was
Pashto, an Indo-Iranian language spoken in Afghanistan, Pakistan and India. In this period we
completed the full pipeline of development of a neural machine translation system: data crawling, cleaning, aligning, creating test sets, developing and testing models, and delivering them
to the user partners. In this paper we describe the rapid data creation process, and experiments
with transfer learning and pretraining for Pashto-English. We find that starting from an existing
large model pre-trained on 50 languages leads to far better BLEU scores than pretraining on
one high-resource language pair with a smaller model. We also present human evaluation of
our systems, which indicates that the resulting systems perform better than a freely available
commercial system when translating from English into Pashto direction, and similarly when
translating from Pashto into English.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku popisujeme, jak byla korpusová platforma TEITOK integrována s korpusovými platformami KonText a PML-TQ v LINDAT, aby poskytla vizualizaci dokumentů pro stávající i budoucí zdroje v LINDAT. TEITOK je online platforma pro vyhledávání, prohlížení a úpravy korpusů, kde jsou soubory korpusu ukládány jako anotované soubory TEI / XML. Integrace TEITOK také znamená, že zdroje LINDAT budou k dispozici ve formátu TEI / XML a lze je vyhledávat v CWB nad rámec stávajících nástrojů ústavu. Ačkoli integrace popsaná v tomto článku je specifická pro LINDAT, tato metoda by měla být použitelná pro integraci TEITOK nebo podobných nástrojů do existující korpusové architektury.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we describe how the TEITOK corpus platform was integrated with the KonText and PML-TQ corpus platforms at LINDAT to provide document visualization for both existing and future resources at LINDAT. TEITOK is an online platform for searching, viewing, and editing corpora, where corpus files are stored as annotated TEI/XML files. The TEITOK integration also means LINDAT resources will become available in TEI/XML format, and searchable in CWB on top of existing tools at the institute. Although the integration described in this paper is specific for LINDAT, the method should be applicable to the integration of TEITOK or similar tools into an existing corpus architecture.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Deep Universal Dependencies (hluboké univerzální závislosti) je sbírka treebanků poloautomaticky odvozených z Universal Dependencies. Obsahuje přídavné hloubkově-syntaktické a sémantické anotace. Verze Deep UD odpovídá verzi UD, na které je založena. Mějte však na paměti, že některé treebanky UD nebyly do Deep UD zahrnuty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Deep Universal Dependencies is a collection of treebanks derived semi-automatically from Universal Dependencies. It contains additional deep-syntactic and semantic annotations. Version of Deep UD corresponds to the version of UD it is based on. Note however that some UD treebanks have been omitted from Deep UD.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme GEM, živý benchmark pro generování přirozeného jazyka (NLG), jeho evaluaci a metriky. Měření pokroku v oblasti NLG se opírá o neustále se vyvíjející ekosystém automatizovaných metrik, datových sad a standardů lidské evaluace. Vzhledem k tomuto pohyblivému cíli se nové modely často stále vyhodnocují na odlišných anglocentrických korpusech s dobře zavedenými, ale chybnými metrikami. Tato nesouvislost ztěžuje identifikaci omezení současných modelů a příležitostí k pokroku. GEM toto omezení řeší a poskytuje prostředí, v němž lze modely snadno aplikovat na široký soubor úloh a v němž lze testovat evaluační strategie. Pravidelné aktualizace benchmarku pomohou výzkumu NLG stát se více mnohojazyčným a rozvíjet úlohu spolu s modely. Tento článek slouží jako popis dat pro sdílenou úlohu 2021 na souvisejícím workshopu GEM.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent anglo-centric corpora with well-established, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of tasks and in which evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for the 2021 shared task at the associated GEM Workshop.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek vysvětluje pojem „praktický soulad“, zahrnující rutinní neproblematický vztah hmotných artefaktů a textových předmětů, který poskytuje nezbytný základ pro práci ve třídě. Analýza videonahrávky interakce se zaměřuje na konkrétní případ tří studentů, kteří společně pracují se sdíleným notebookem a papírovým pracovním listem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article explicates the notion of ‘practical accord’, encompassing the routine nonproblematic relationship of material artifacts and textual objects that provides the necessary grounds for further classroom work. Practical accord consists of courses of action that are temporally aligned with structured objects such as series of pages, slides or questions, ordered as a sequence of steps. Grounded in the video-based analysis of a single case, the article argues that such practical accord can be a necessary requirement for an educative activity to take place in an orderly way in the classroom. The analysis focuses on a particular instance of three students working with a shared laptop and a paper worksheet, and losing their grasp of the relationship between the screen and the sheet. The identified practices used to get back ‘on the page’ include the verbal and gestural constitution of the screen and the sheet as two separate objects that are related through instructions provided on the screen, which serves as a link between two independent but interrelated numbering systems used to organise the on-screen material and the questions on the worksheet. The article concludes with a discussion of the notion of practical accord with regard to instructed action and gestalt contextures.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zaměřuje na sekvence opětovného sledování ve školní práci s digitální orální historií, tj. na účastníky provádějící druhé sledování videoklipu, který již jednou viděli. Podrobně zkoumám, jak účastníci zahajují přehrávání pomocí řeči a ztělesněného chování. K videoklipu přistupují jako ke strukturovanému objektu obsahujícímu již známý sled promluv vypravěče. Sekvence opakovaného sledování se uzavřou, když účastníci v rámci videa naleznou konkrétní promluvu, který mohou považovat za řešení dříve zjištěné nejednoznačnosti. K tomu vyhledávají relevantní promluvy manipulací s časovou značkou na obrazovce a stabilizují je zastavením klipu. Účastníci tak vykazují místní dočasnou kompetenci opětovně sledovat videoklipy a manipulovat s nimi jako s relevantními tématy a zdroji při řešení úkolů ve školní třídě.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper focuses on rewatching sequences in classroom work with digital oral history, i.e., participants conducting a second watching of a video clip that they have already seen. I provide a detailed examination of how the participants initiate rewatching using speech and embodied conduct. They treat the video clip as a structured object containing an already familiar succession of utterances by the narrator. The rewatching sequences are closed when the participants have identified a particular utterance within the video that they can treat as a solution to a previously encountered ambiguity. To do this, they search for the relevant utterances by manipulating the time marker on the screen and stabilize them by pausing the clip. The participants thus exhibit a local temporary competence in rewatching and manipulating video clips as topics and resources in classroom tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>O čem robot přemýšlí, co cítí a dokáže napsat divadelní hru? Střípky ze života umělé inteligence jejími vlastními slovy. Futuristický Malý princ z doby postpandemické.

Inscenace, po jejíž každé repríze následuje debata s odborníky na umělou inteligenci i divadelními tvůrci, je nejen svědectvím o současných schopnostech počítačových technologií, ale i poutavou vizí budoucího světa inspirující se v klasikách žánru sci-fi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>What does the robot think about, how does it feel and can it write a play? Snippets from the life of artificial intelligence in its own words. Futuristic Little Prince of the postpandemic era. 

Every reprise will be followed by a debate with experts in artificial intelligence and theater makers. The show is not only a testament to the current capabilities of computer technology, but also an engaging vision of the future world inspired by the sci-fi classics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Švandovo divadlo při příležitosti stoletého výročí premiéry hry R.U.R  Karla Čapka uvádí prezentaci unikátního projektu THEAITRE, který zkoumá, zda umělá inteligence dokáže napsat divadelní hru. Během několika měsíců počítač generoval obrazy ze života robota, který musí čelit radostem a strastem každodenního života. A odkryl nám, jak vnímá základní lidské otázky jako je narození, umírání, touha po lásce, hledání pracovních příležitostí, či stárnutí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>On the occasion of the centenary of Karel Čapek's play R.U.R, the Švanda Theatre prepared a presentation of a unique project THEAITRE that examines whether artificial intelligence can write a play. Within a few months, the computer generated images from the life of a robot that has to face the joys and sorrows of everyday life. And it revealed to us how it perceives basic human issues such as birth, dying, the desire for love, the search for jobs, or aging.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku představujeme metodu pro vytváření slovotvorných sítí pomocí přenosu informací z jiného jazyka. Navrhovaný algoritmus využívá existující slovotvornou síť a paralelní texty a vytváří síť s nízkou precision a středním recallem v jazyce, pro který nemusí být k dispozici manuální anotace. Recall výsledné sítě pak rozšířujeme tím, že ji využijeme k natrénování metody strojového učení a výsledný model aplikujeme na větší slovník, čímž získáme výsledek s obdobnou precision, ale vyšším recallem. Přístup je vyhodnocován proti existujícím slovotvorným sítím ve francouzštině, němčině a češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this article, we present a proof-of-concept method for creating word-formation networks by transferring information from another language. The proposed algorithm utilizes an existing word-formation network and parallel texts and creates a low-precision and moderate-recall network in a language, for which no manual annotations need to be available. We then extend the coverage of the resulting network by using it to train a machine-learning method and applying the resulting model to a larger lexicon, obtaining a moderate-precision and high-recall result. The approach is evaluated on French, German and Czech against existing word-formation networks in those languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>DeriNet je lexikální síť, která modeluje derivační vztahy ve slovníku češtiny. Uzly sítě odpovídají českým lexémům, zatímco hrany představují slovotvorné vztahy mezi odvozeným slovem a jeho základním slovem / slovy. Současná verze, DeriNet 2.1, obsahuje 1 039 012 lexemes (extrahovány ze slovníku MorfFlex CZ 2.0) spojených 782 814 derivačními relacemi, 50 533 ortografickými variantami, 1 952 vztahů skládání, 295 univerbizačními vztahy a 144 konverzními vztahy.
Ve srovnání s předchozí verzí obsahuje verze 2.1 anotace ortografických variant, plně automaticky generovanou anotaci hranic afixů (kromě kořenů anotovaných v 2.0), 202 affixoidů sloužících jako základ pro skládání, anotaci četností lexémů z korpusů, anotaci slovesných tříd a pilotní anotaci univerbace. Sada tagů pro slovní druhy byla převedena na Universal POS z projektu Universal Dependencies.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>DeriNet is a lexical network which models derivational relations in the lexicon of Czech. Nodes of the network correspond to Czech lexemes, while edges represent word-formational relations between a derived word and its base word / words. The present version, DeriNet 2.1, contains 1,039,012 lexemes (sampled from the MorfFlex CZ 2.0 ​dictionary) connected by 782,814 derivational, 50,533 orthographic variant, 1,952 compounding, 295 univerbation and 144 conversion relations.
Compared to the previous version, version 2.1 contains annotations of orthographic variants, full automatically generated annotation of affix morpheme boundaries (in addition to the roots annotated in 2.0), 202 affixoid lexemes serving as bases for compounding, annotation of corpus frequency of lexemes, annotation of verbal conjugation classes and a pilot annotation of univerbation. The set of part-of-speech tags was converted to Universal POS from the Universal Dependencies project.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme pilotní experimenty na dělení a identifikaci českých složených slov. Vytvořili jsme algoritmus měřící jazykovou podobnost dvou slov založený na nalezení nejkratšího
cesta skrze matici vzájemných odhadovaných korespondencí mezi dvěma fonologicky přepsanými řetězci.
Dále jsme vytvořili nástroj pro splitting neboli dělení složených slov (Czech Compound Splitter) pomocí frameworku Marian Neural Machine Translator, který byl vytrénován na datové sadě obsahující 1 164 ručně anotovaných sloučenin a zhruba 280 000 synteticky vytvořených kompozit. Ve splittingu kompozit dosáhlo první řešení přesnosti 28 % a druhé řešení 54 % na validačním datové sadě. V úloze identifikace kompozit dosáhl Czech Compound Splitter přesnosti 91%.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present pilot experiments on splitting and identifying Czech compound words. We created
an algorithm measuring the linguistic similarity of two words based on finding the shortest
path through a matrix of mutual estimated correspondences between two phonemic strings.
Additionally, a neural compound-splitting tool (Czech Compound Splitter) was implemented
by using the Marian Neural Machine Translator framework, which was trained on a data set
containing 1,164 hand-annotated compounds and about 280,000 synthetically created compounds.
In compound splitting, the first solution achieved an accuracy of 28% and the second solution
achieved 54% on a separate validation data set. In compound identification, the Czech Compound
Splitter achieved an accuracy of 91%.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jazykové domény vyžadující velmi pečlivé používání terminologie jsou hojné a odrážejí významnou část překladatelského průmyslu. V této práci představujeme metriku pro hodnocení kvality a konzistentnosti překladu terminologie se zaměřením na lékařskou (a konkrétně COVID-19) doménu pro pět jazykových párů: angličtinu do francouzštiny, čínštiny, ruštiny a korejštiny, a dále češtinu do němčiny. Uvádíme popisy a výsledky zúčastněných systémů, vyjadřujeme nutnost dalšího výzkumu jak pro adekvátnější zacházení s terminologií, tak pro správnou formulaci a vyhodnocení úlohy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Language  domains  that  require  very  carefuluse of terminology are abundant and reflect asignificant part of the translation industry.  Inthis work we introduce a benchmark for eval-uating the quality and consistency of terminol-ogy translation, focusing on the medical (andCOVID-19  specifically)  domain  for  five  lan-guage pairs:  English to French, Chinese, Rus-sian,  and  Korean,  as  well  as  Czech  to  Ger-man.   We  report  the  descriptions  and  resultsof  the  participating  systems,  commenting  onthe  need  for  further  research  efforts  towardsboth more adequate handling of terminologiesas  well  as  towards  a  proper  formulation  andevaluation of the task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme pilotní experiment zaměřený na harmonizaci různorodých datových zdrojů, které obsahují anotace související s koreferencí. Převedli jsme 17 existujících korpusů 11 jazyků do společného anotačního schématu, založeného na Universal Dependencies, a zveřejnili jsme podmnožinu této kolekce pod názvem CorefUD 0.1 v repozitáři LINDAT-CLARIAH-CZ (http://hdl.handle.net/11234/1-3510).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe a pilot experiment aimed at harmonizing diverse data resources that contain coreference-related annotations. We converted 17 existing datasets for 11 languages into a common annotation scheme based on Universal Dependencies, and released a subset of the resulting collection publicly under the name CorefUD 0.1 via the LINDAT-CLARIAH-CZ repository (http://hdl.handle.net/11234/1-3510).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CorefUD je kolekce existujících korpusů s anotací koreference, které jsme převedli na jednotné anotační schéma. Ve své současné verzi 0.1 CorefUD celkem obsahuje 17 korpusů pro 11 jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CorefUD is a collection of previously existing datasets annotated with coreference, which we converted into a common annotation scheme. In total, CorefUD in its current version 0.1 consists of 17 datasets for 11 languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CorefUD je kolekce existujících korpusů s anotací koreference, které jsme převedli na jednotné anotační schéma. Ve své současné verzi 0.2 CorefUD celkem obsahuje 17 korpusů pro 11 jazyků a v porovnání s verzí 0.1 obsahuje kvalitnejší automatickou morfo-syntaktickou anotaci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CorefUD is a collection of previously existing datasets annotated with coreference, which we converted into a common annotation scheme. In total, CorefUD in its current version 0.2 consists of 17 datasets for 11 languages, and compared to the version 0.1, the automatic morpho-syntactic annotation has improved.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládáme empirickou studii, která srovnává hlavy zmínek anotovaných ručně ve čtyřech koreferenčních datových sadách (pro nizozemštinu, angličtinu, polštinu a ruštinu) na jedné straně a hlavy vyplývající z automaticky predikovaných závislostních syntaktických stromů na straně druhé.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an empirical study that compares mention heads as annotated manually in four coreference datasets (for Dutch, English, Polish, and Russian) on one hand, with heads induced from dependency trees parsed automatically, on the other hand.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento dokument popisuje náš systém účasti na argumentačním textu chápajícím sdílený úkol pro AI Debater na NLPCC 2021 (http://www.fudan-disc.com/sharedtask/AIDebater21/tracks.html). Úkoly jsou motivovány k rozvoji autonomního diskusního systému. Počáteční pokus provedeme s trackem-3, konkrétně extrakcí argumentačních dvojic z recenzního posudku a vyvrácením, kdy extrahujeme argumenty z recenzního posudku a jejich odpovídající vyvrácení z autorových odpovědí. Ve srovnání s víceúkolovou základnou organizátorů zavádíme dvě významné změny: i) používáme vkládání tokenu ERNIE 2.0, které dokáže lépe zachytit lexikální, syntaktické a sémantické aspekty informací v tréninkových datech, ii) provádíme dvojí učení pozornosti, abychom zachytili dlouhodobé závislosti. Náš navrhovaný model dosahuje nejmodernějších výsledků s relativním zlepšením skóre F1 o 8,81% oproti základnímu modelu. Náš kód zveřejníme na adrese https://github.com/guneetsk99/ArgumentMining_SharedTask. Náš tým ARGUABLY je jedním z třetích oceněných týmů v Tracku 3 společného úkolu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes our participating system run to the argumentative text understanding shared task for AI Debater at NLPCC 2021 (http://www.fudan-disc.com/sharedtask/AIDebater21/tracks.html). The tasks are motivated towards developing an autonomous debating system. We make an initial attempt with Track-3, namely, argument pair extraction from peer review and rebuttal where we extract arguments from peer reviews and their corresponding rebuttals from author responses. Compared to the multi-task baseline by the organizers, we introduce two significant changes: (i) we use ERNIE 2.0 token embedding, which can better capture lexical, syntactic, and semantic aspects of information in the training data, (ii) we perform double attention learning to capture long-term dependencies. Our proposed model achieves the state-of-the-art results with a relative improvement of 8.81% in terms of F1 score over the baseline model. We make our code available publicly at https://github.com/guneetsk99/ArgumentMining_SharedTask. Our team ARGUABLY is one of the third prize-winning teams in Track 3 of the shared task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Překladový model z katalánštiny do okcitánštiny pro systém Marian.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Marian NMT model for Catalan to Occitan translation. The model was submitted to WMT'21 Shared Task on Multilingual Low-Resource Translation for Indo-European Languages as a CUNI-Primary system for Catalan to Occitan.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vícejazyčný překladový model z katalánštiny do rumunštiny, italštiny a okcitánštiny pro systém Marian.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Marian multilingual translation model from Catalan into Romanian, Italian and Occitan. This model is an updated version (trained for 2.1M updates) of the model submitted to WMT'21 Shared Task on Multilingual Low-Resource Translation for Indo-European Languages as a CUNI-Primary system for Catalan to Romanian and Italian (trained for 430k updates).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Lexikálně omezený strojový překlad umožňuje uživateli manipulovat s výstupní větou vynucením přítomnosti nebo nepřítomnosti určitých slov a frází. Přestože současné přístupy dokáží vynutit, aby se v překladu objevily specifikované termíny, často se snaží, aby povrchová forma omezeného slova souhlasila se zbytkem vygenerovaného výstupu. Ruční analýza ukazuje, že 46% chyb ve výstupu základního omezeného modelu překladu z angličtiny do češtiny souvisí s gramatickou shodou. Zkoumáme mechanismy, které umožňují neuronových strojový překlad k určení správné inflexe omezujících slov specifikovaných pomocí lemmat. Zaměřujeme se zejména na metody založené na tréninku modelu s omezeními, které jsou součástí vstupu. Naše experimenty na anglicko-českém jazykovém páru ukazují, že tento přístup zlepšuje překlad s omezením pomocí termínů a to jak v automatickém i ručním hodnocení, snížením počtu chyb v gramatické shodě. Náš přístup
tak odstraňuje inflexní chyby, aniž by zaváděl nové chyby nebo snižoval celkovou
kvalitu překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Lexically constrained machine translation allows the user to manipulate the output sentence by enforcing the presence or absence of certain words and phrases. Although current approaches can enforce terms to appear in the translation, they often struggle to make the constraint word form agree with the rest of the generated output. Our manual analysis shows that 46% of the errors in the output of a baseline constrained model for English to Czech translation are related to agreement. We investigate mechanisms to allow neural machine translation to infer the correct word inflection given lemmatized constraints. In particular, we focus on methods based on training
the model with constraints provided as part of the input sequence. Our experiments on the English-Czech language pair show that this approach improves the translation of constrained
terms in both automatic and manual evaluation
by reducing errors in agreement. Our approach
thus eliminates inflection errors, without introducing new errors or decreasing the overall
quality of the translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje příspěvek Univerzity Karlovy do soutěže ve vícejazyčném překladu indoeurópskych jazyků s nedostatečnými zdroji na konferenci WMT21.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes Charles University submission for  Multilingual Low-Resource Translation for Indo-European Languages shared task at WMT21. We competed in translation from Catalan into Romanian, Italian and Occitan. Our systems are based on shared multilingual model. We show that using joint model for multiple similar language pairs improves upon translation quality in each pair. We also demonstrate that chararacter-level bilingual models are competitive for very similar language pairs (Catalan-Occitan) but less so for more distant pairs. We also describe our experiments with multi-task learning, where aside from a textual translation, the models are also trained to perform grapheme-to-phoneme conversion.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje příspěvek Univerzity Karlovy do soutěže v překladu terminologie na konferenci WMT21.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This  paper  describes  Charles  University  submission for Terminology translation Shared Task at WMT21.  The objective of this task is to design a system which translates certain terms based on a provided terminology database, while preserving high overall translation quality. We competed in English-French language pair. Our approach is based on providing the desired translations alongside the input sentence and  training the model to use these provided terms. We lemmatize the terms both during the training and inference, to allow the model to learn how to produce correct surface forms of the words, when they differ from the forms provided in the terminology database. Our submission ranked second in Exact Match metric which evaluates the ability of the model to produce desired terms in the translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje účast týmu CUNI-MTIR v soutěži COVID-19 MLIA @ Eval Task 3.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the participation of our team (CUNIMTIR) in the COVID-19 MILA Machine Translation (MT) task. We present our implementation of four systems (English into French, German, Swedish and Spanish) in both constrained and unconstrained settings. We employ the Marian implementation of the Transformer model to train the constrained systems in the given training data (MLIA MT parallel data), while in the unconstrained systems, we use external medicaldomain training data for the base models and then fine-tune those models using MLIA MT data</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje účast týmu CUNI-MTIR v soutěži COVID-19 MLIA @ Eval Task 2.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The multi-lingual search task in MLIA Community Effort aims at improving
COVID-19 related information access for searchers in multi-lingual settings [3].
We choose in our participation to build a monolingual system where we index
the provided English documents and use the English queries for retrieval (monolingual system) then we design five runs in the monolingual settings.
As for the bilingual task, we design five runs where the documents are in English, and the queries are translated into English following the query-translation
approach.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dokáže AI tvořit umělecká díla? Hudbu, malířství, literaturu, film, divadlo? Nebo něco úplně nového? Dokáže neuronová síť vzbudit emoce? Zapojte stroje do tvorby spolu s garantem výzvy THEaiTRE.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Can AI create works of art? Music, painting, literature, film, theatre? Or something completely new? Can a neural net stir emotions? Engage the machines in the creation along with the THEaiTRE challenge guarantor.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Neuronové sítě jsou nejmodernější metodou strojového učení pro mnoho problémů v NLP. Jejich úspěch ve strojovém překladu a dalších úlohách NLP je fenomenální, ale jejich interpretovatelnost je náročná. Chceme zjistit, jak neuronové sítě reprezentují význam. Za tímto účelem navrhujeme prozkoumat rozložení významu ve vektorovém prostoru reprezentace slov v neuronových sítích trénovaných pro úlohy NLP. Dále navrhujeme zvážit různé teorie významu ve filosofii jazyka a najít metodologii, která by nám umožnila tyto oblasti propojit.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Neural networks are the state-of-the-art method of machine learning for many problems in NLP. Their success in machine translation and other NLP tasks is phenomenal, but their interpretability is challenging. We want to find out how neural networks represent meaning. In order to do this, we propose to examine the distribution of meaning in the vector space representation of words in neural networks trained for NLP tasks. Furthermore, we propose to consider various theories of meaning in the philosophy of language and to find a methodology that would enable us to connect these areas.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dataset MultiWOZ (Budzianowski et al.,2018) je často užíván na poměřování schopností generovat odpověď z kontextu v případě dialogových systému zaměřených na úkoly. V této práci identifikujeme nekonzistence v předzpracování dat a reportování tří metrik založených na evaluačním korpusu, tj., BLEU skóre, míry Inform a míry Success, v kontextu tohoto datasetu. Poukazujeme na několik problémů benchmarku MultiWOZ jako je neuspokojivé předzpracování dat, nedostatečné nebo nedostatečně specifikované evaluační metriky, nebo neohebná databáze. Ve spravedlivých podmínkách jsme znovu vyhodnotili 7 end-to-end a 6 policy optimization modelů a ukázali jsme, že jejich původně reportovaná skóre nemohou být přímo srovnávána. Abychom ulehčili porovnávání budoucích systémů, zveřejňujeme naše soběstačné standardizované evaluační skripty. Rovněž dáváme základní doporučení pro budoucí vyhodnocování založená na evaluačním korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The MultiWOZ dataset (Budzianowski et al.,2018) is frequently used for benchmarking context-to-response abilities of task-oriented dialogue systems. In this work, we identify inconsistencies in data preprocessing and reporting of three corpus-based metrics used on this dataset, i.e., BLEU score and Inform &amp; Success rates. We point out a few problems of the MultiWOZ benchmark such as unsatisfactory preprocessing, insufficient or under-specified evaluation metrics, or rigid database. We re-evaluate 7 end-to-end and 6 policy optimization models in as-fair-as-possible setups, and we show that their reported scores cannot be directly compared. To facilitate comparison of future systems, we release our stand-alone standardized evaluation scripts. We also give basic recommendations for corpus-based benchmarking in future works.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Se stále rostoucím tempem výzkumu
a velký objem vědecké komunikace,
stipendisty čeká nelehký úkol. Nejen musí
udržují krok s rostoucí literaturou v
svých vlastních a souvisejících oborů, musí vědci stále častěji také vyvracet pseudovědu a
dezinformace. Tyto potřeby motivovaly
rostoucí zaměření na výpočetní metody pro zlepšení vyhledávání, sumarizace a
analýzy odborných dokumentů. Nicméně
různé oblasti výzkumu vědeckého zpracování dokumentů zůstávají roztříštěné. K dosažení
širší komunitě NLP a AI/ML, společné úsilí v této oblasti a
umožnit sdílený přístup k publikovanému výzkumu, my
uspořádal 2. seminář o zpracovávání dokumentů (SDP) v rámci programu NAACL 2021 jako
virtuální událost (https://sdproc.org/2021/). The
SDP workshop sestával z výzkumné dráhy,
tři pozvané rozhovory a tři sdílené úkoly
(LongSumm 2021, SCIVER a 3C). Program byl zaměřen na NLP, získávání informací a dolování dat pro vědecké dokumenty s důrazem na identifikaci a
řešení otevřených výzev.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>With the ever-increasing pace of research
and high volume of scholarly communication,
scholars face a daunting task. Not only must
they keep up with the growing literature in
their own and related fields, scholars increasingly also need to rebut pseudo-science and
disinformation. These needs have motivated
an increasing focus on computational methods for enhancing search, summarization, and
analysis of scholarly documents. However, the
various strands of research on scholarly document processing remain fragmented. To reach
out to the broader NLP and AI/ML community, pool distributed efforts in this area, and
enable shared access to published research, we
held the 2nd Workshop on Scholarly Document Processing (SDP) at NAACL 2021 as a
virtual event (https://sdproc.org/2021/). The
SDP workshop consisted of a research track,
three invited talks and three Shared Tasks
(LongSumm 2021, SCIVER and 3C). The program was geared towards NLP, information retrieval, and data mining for scholarly documents, with an emphasis on identifying and
providing solutions to open challenges.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Švandovo divadlo chystá premiéru první české hry, jejíž dialogy „napsala“ umělá inteligence. Inscenace AI: Když robot píše hru vznikla ve spolupráci s Matematicko-fyzikální fakultou Univerzity Karlovy a připomíná sto let od uvedení hry R.U.R. od Karla Čapka, v níž slovo „robot“ poprvé zaznělo.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Švandovo Theatre is about to premiere the first Czech play, the dialogue of which was "written" by an artificial intelligence. "AI: When a robot writes a play" was created in collaboration with the Faculty of Mathematics and Physics of Charles University and commemorates a century since the release of the play R.U.R. by Karel Čapek, in which the word "robot" was first mentioned.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Seznámení uchazečů o různé projekty našimi zkušenostmi s koordinací projektu ELITR.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Describing our experience from the coordination of the EU project ELITR for grant applicants.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představil jsem výzkumný projekt ELITR a současný stav poznání v oblasti strojového překladu a překladu mluvené řeči.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I presented the research project ELITR and the state of the art in machine translation and speech translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení dvou aktuálních aplikovaných projektů ELITR a Bergamot a projektu základního výzkumu NEUREM3, realizovaných na ÚFALu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A brief description of two applied projects, ELITR and Bergamot, and one basic research project NEUREM3, run at UFAL</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představil jsem obor počítačové lingvistiky s důrazem na změnu paradigmatu zavedením neuronových sítí, zejména v oblasti strojového překladu a tlumočení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I presented the area of computational linguitics with a focus on the paradigm change brought by neural networks, in articular in the application area of machine translation and interpreting.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek na semináři pro studenty překladatelství a zejména tlumočnictví.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A presentation for students of translation studies and interpretring.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>542 / 5000
Translation results
Tento článek představuje systém automatického překladu řeči zaměřený na živé titulkování konferenčních prezentací. Popisujeme celkovou architekturu a klíčové komponenty zpracování. Důležitější je, že vysvětlujeme naši strategii budování komplexního systému pro koncové uživatele z mnoha jednotlivých komponent, z nichž každá byla testována pouze v laboratorních podmínkách. Systém je funkčním prototypem, který je rutinně testován v rozpoznávání anglické, české a německé řeči a současně je prezentován přeložený do 42 cílových jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents an automatic speech translation system aimed at live subtitling of conference presentations. We describe the overall architecture and key processing components. More importantly, we explain our strategy for building a complex system for end-users from numerous individual components, each of which has been tested only in laboratory conditions. The system is a working prototype that is routinely tested in recognizing English, Czech, and German speech and presenting it translated simultaneously into 42 target languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje praktické zkušenosti z testování systému pro simultánní překlad mluvené řeči z projevu řečníků a tlumočníků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes the practical experience from testing our complex system translating from the speech of speakers and interpreters.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato testovací sada strojového překladu obsahuje 2223 českých vět shromážděných v rámci projektu FAUST (https://ufal.mff.cuni.cz/grants/faust, http://hdl.handle.net/11234/1-3308). Každá původní věta obsahující šum byla normalizována (clean1 a clean2) a nezávisle na sobě přeložena do angličtiny dvěma překladateli.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This machine translation test set contains 2223 Czech sentences collected within the FAUST project (https://ufal.mff.cuni.cz/grants/faust, http://hdl.handle.net/11234/1-3308). Each original (noisy) sentence was normalized (clean1 and clean2) and translated to English independently by two translators.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve stati se analyzují na základě korpusových dat vybraná adjektiva tvořená od sloves příponou -lý a porovnávají se s dalšími typy slovesných adjektiv adjektiv z hlediska vzájemné zaměnitelnosti a významových rozdílů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Selected Czech adjectives derived from the verbs by the suffix -lý are analyzed and compared with the other types of deverbal adjectives and their mutual interchanges are presented in comparison with their independent meanings. The Russian system of deverbal adjectives is briefly demonstrated as to their position different from Czech system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Studie se zabývá tématem z české slovotvorby, konkrétně stanovenám směru tvoření ve dvojicích bezpříponového substantiva a příbuzného slovesa (např. skok - skočit, sůl - solit). Hledá kritéria, která by směr tvoření v těchto dvojicích pomohla určit. Zatímco korpusová frekvence je z důvodu formálně-významových asymetrií a významových posunů kritériem nespolehlivým, užitečným vodítkem jsou alternace v předponových i kořenných morfémech zkoumaných substantiv a sloves a také způsob tvoření vidových protějšků – toto kritérium se dá aplikovat na slovesa domácí i cizí, včetně neologismů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The study deals with a topic in Czech word formation, namely with the determination of the direction of formation in pairs of a suffixless noun and a related verb (e.g. skok "jump" - skočit "to jump", sůl "salt" - solit "to salt"). While corpus frequency is an unreliable criterion due to formal-meaning asymmetries and meaning shifts, alternations in the prefix and root morphemes of the studied nouns and verbs, as well as the way of formation of the aspectual counterparts, are useful clues - the latter criterion can be applied to both domestic and foreign verbs, including neologisms.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku sledujeme vztah slovosledu a aktuálního členění ve srovnání angličtiny s češtinou, a to ve dvou oblastech: umístění příslovečných určení času a místa v  ohniskové části věty a sémantického dosahu částic zvaných fokalizátory. Materiálem nám byl anotovaný paralelní korpus anglicko–český (PCEDT).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we study the relationship between word order and topic-focus articulation in comparison of English and Czech, namely in two areas: the position of proverbial determinations of time and place in the focal part of a sentence and the semantic range of particles called focalizers. We used the material of an annotated parallel English-Czech corpus (PCEDT).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení funkcionality repozitáře LINDAT/CLARIAH-CZ, tedy hlavně CLARIN/Dspace, která podporuje principy Open Science, v první řadě tzv. FAIR principy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Introducing the functionality of the LINDAT/CLARIAH-CZ repository, that is, mainly CLARIN/Dspace, which supports the principles of Open Science, primarily the so-called FAIR principles.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Proč a jakým způsobem integruje datový repozitář LINDATu služby podporující a využívající přímé citování dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Why and how the LINDAT data repository integrates services supporting and using direct data citation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Informace o centrálním repozitáři LINDAT/CLARIAH-cz, obzvláště z hlediska FAIR aspektů</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Information about the central repository of LINDAT/CLARIAH-cz, especially in terms of FAIR aspects</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přinosy zveřejňování dat, obzvláště pomocí kvalitních certrifikovaných repozitářů dobře integrovaných s další infrastrukturou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The benefits of data publishing, especially with high-quality certrified repositories well integrated with other infrastructure.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představíme repozitář pro jazyková data a jiná data primárně z humanitních oborů. Repozitář je provozován velkou výzkumnou infrastrukturou (VVI) LINDAT/CLARIAH-CZ a umožňuje snadné, bezpečné a dlouhodobé uchování, sdílení a přímé citování vědeckých dat. Repozitář jsme vlastním vývojem rozšířili o modul, který umožňuje datům přiřadit libovolnou licenci a tuto případně i online podepsat. Stručně vysvětlíme, proč je takové řešení nutné. Věnujeme se také FAIR aspektům repozitáře a tomu, proč je mnohem lepší svá výzkumná data uložit k nám, než je sdílet pomocí své vlastní webové stránky a jinými ad hoc metodami.

Součástí repozitáře je také řešení pro výběr co nejotevřenější možné licence pro daná data: Public License Selector. uživatel při ukládání dat odpovídá na otázky, které jej navigují bludištěm veřejných licencí k nejvhodnější možnosti pro daná data nebo software, jež chce sdílet.

Obě propojená řešení, která představíme, jsou open source, vyvíjená veřejně v rámci VVI LINDAT/CLARIAH-CZ (https://github.com/ufal/clarin-dspace, https://github.com/ufal/public-license-selector) a jsou používána v řadě instalací v celé Evropě.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We will present a repository for language data and other data primarily from the humanities. The repository is operated by the large research infrastructure (LRI) LINDAT/CLARIAH-CZ and allows for easy, safe and long-term preservation, sharing and direct quoting of scientific data. We have expanded the repository by our own development to include a module that allows data to assign any license and sign this, if necessary, online. We will briefly explain why such a solution is necessary. We also address FAIR aspects of the repository, and why it is far better to store our research data with us than to share it with our own website and other ad hoc methods.  

The repository also includes a solution for selecting the most open possible license for the data: Public License Selector. The user answers questions when saving data, navigating it through a maze of public licenses to the most appropriate option for the data or software they want to share.  

The two connected solutions we will present are open source, developed publicly under LRI LINDAT/CLARIAH-CZ (https://github.com/ufal/clarin-dspace, https://github.com/ufal/public-license-selector) and used in a variety of installations across Europe.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení funkcionality repozitáře LINDAT/CLARIAH-CZ, tedy hlavně CLARIN/Dspace, která podporuje principy Open Science, v první řadě tzv. FAIR principy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Introducing the functionality of the LINDAT/CLARIAH-CZ repository, that is, mainly CLARIN/Dspace, which supports the principles of Open Science, primarily the so-called FAIR principles.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Technická zprava shrnuje pravidla pro zařazení německých synonym do synonymního slovníku SynSemClass.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This report presents a guideline for including German synonyms into the multilingual SynSemClass lexicon.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>SynSemClass lexicon 3.5 zkoumá s ohledem na kontextově založenou slovesnou synonymii, sémantickou „ekvivalenci“ českých, anglických a německých sloves a jejich valenční chování v paralelních česko-anglických a německo-anglických jazykových zdrojích.
SynSemClass 3.5 je vícejazyčná ontologie typu události založená na třídách synonymních významů sloves, doplněná sémantickými rolemi a odkazy na existující sémantické lexikony.
Kromě již použitých odkazů na položky PDT-Vallex, EngVallex, CzEngVallex, FrameNet, VerbNet, PropBank, Ontonotes a English WordNet pro české a anglické záznamy jsou nové odkazy na německé jazykové lexikální zdroje, jako je Woxikon, E-VALBU a GUP, využívány pro německé slovesné záznamy.
Německá část lexikonu byla vytvořena v rámci projektu Multilingual Event-Type-Anchored Ontology for Natural Language Understanding (META-O-NLU) dvěma spolupracujícími týmy - týmem Univerzity Karlovy, Matematicko-fyzikální fakulty, Ústavu formální a aplikované lingvistiky, Praha (ÚFAL), Česká republika a týmem Německého výzkumného centra pro umělou inteligenci (DFKI) Speech and Language Technology, Berlín, Německo.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The SynSemClass synonym verb lexicon version 3.5 investigates, with respect to contextually-based verb synonymy, semantic ‘equivalence’ of Czech, English and German verb senses and their valency behavior in parallel Czech-English and German-English language resources. 
SynSemClass3.5 is a multilingual event-type ontology based on classes of synonymous verb senses, complemented with semantic roles and links to existing semantic lexicons.
Apart of the already used links to PDT-Vallex, EngVallex, CzEngVallex, FrameNet, VerbNet, PropBank, Ontonotes, and English WordNet for Czech and English entries the new links to German language lexical resources are exploited for German verb entries, such as Woxikon, E-VALBU, and GUP. 
The German part of the lexicon has been created within the project Multilingual Event-Type-Anchored Ontology for Natural Language Understanding (META-O-NLU) by two cooperating teams - by the team of the Charles University, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics, Prague (ÚFAL), Czech Republic and the team of the German Research Center for Artificial Intelligence (DFKI) Speech and Language Technology, Berlin, Germany.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>PDT-Vallex je český valenční lexikon propojený s reálnými texty v několika českých korpusech a je také součástí PDT-C 1.0. PDT-Vallex 4.0 obsahuje 14528 valenčních rámců pro 8498 sloves, která se vyskytla v PDT-C 1.0. PDT-Vallex 4.0 je rozšířenou verzí původního PDT-Vallexu, který obsahoval 7121 slovesných záznamů s 11933 rámci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>PDT-Vallex is a Czech valence lexicon linked to real texts in several Czech corpora and is also part of PDT-C 1.0. PDT-Vallex 4.0 contains 14528 valence frames for 8498 verbs that occurred in PDT-C 1.0. PDT-Vallex 4.0 is an expanded version of the original PDT-Vallex, which contained 7121 verb entries with 11933 frames.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku prezentujeme výsledky automatického srovnání valenčních rámců vzájemně propojených adjektivních a slovesných lexikálních jednotek, obsažených ve valenčních slovnících NomVallex a VALLEX. Rozlišujeme devět derivačních typů deverbálních adjektiv a zkoumáme, zda vykazují systémové valenční chování, nebo spíše nesystémové valenční chování. K projevům nesystémového valenčního chování patří změny v počtu valenčních doplnění a především nesystémové formy aktantů, zvláště nesystémová předložková skupina.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present results of an automatic comparison of valency frames of interlinked adjectival and verbal lexical units based on the valency lexicons NomVallex and VALLEX. We distinguish nine derivational types of deverbal adjectives and examine whether they tend to display systemic valency behavior, or rather the non-systemic one. The non-systemic valency behavior includes changes in the number of valency complementations and, more dominantly, non-systemic forms of actants, especially a prepositional group.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme metodu pro rozšíření pokrytí Slovníku českých diskurzních konektorů (CzeDLex) pomocí anotační projekce. Využíváme dva jazykové zdroje: (i) Penn Discourse Treebank 3.0 jako zdroj ručně anotovaných diskurzních vztahů v angličtině a (ii) Pražský česko -anglický závislostí korpus 2.0 jako překlad anglických textů do češtiny a propojení mezi tokeny v obou jazycích. Přestože byl CzeDLex původně extrahován z velkého českého korpusu, vedla prezentovaná metoda k přidání řady nových konektorů a nových užití (diskurzních typů) pro již přítomné položky ve slovníku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a method for extending coverage of the Lexicon of Czech Discourse Connectives – CzeDLex – using annotation projection. We take advantage of two language resources: (i) the Penn Discourse Treebank 3.0 as a source of manually annotated discourse relations in English, and (ii) the Prague Czech–English Dependency Treebank 2.0 as a translation of the English texts to Czech and a link between tokens on the two language sides. Although CzeDLex was originally extracted from a large Czech corpus, the presented method resulted in an addition of a number of new connectives and new types of usages (discourse types) for already present entries in the lexicon.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CzeDLex 1.0 je první produkční verze slovníku českých diskurzních konektorů, navazující na 3 předchozí verze vývojové. Slovník obsahuje konektory částečně automaticky extrahované z Pražského diskurzního korpusu 2.0 (PDiT 2.0) a z dalších zdrojů. Všechna slovníková hesla byla ručně zkontrolována, přeložena do angličtiny a doplněna dalšími lingvistickými informacemi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CzeDLex 1.0 is the first production version of a lexicon of Czech discourse connectives, following three previous  development versions. The lexicon contains connectives partially automatically extracted from the Prague Discourse Treebank 2.0 (PDiT 2.0) and other resources. All entries in the lexicon have been manually checked, translated to English and supplemented with additional linguistic information.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládáme experimenty zaměřené na predikci významu explicitních mezivětných diskurzních vztahů v češtině a angličtině, s využitím hlubokého učení (BERT) pro predikci významů a anotační projekce z angličtiny do češtiny pro zvětšení množství trénovacích dat pro češtinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present experiments in predicting a discourse sense for explicit inter-sentential discourse relations in Czech and English, using embedding and deep learning (fine-tuned BERT) to predict the senses, and annotation projection from English to Czech to increase the size of training data for Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pokoušíme se osvětlit rozličné způsoby, kterými jazyky udávají datum a čas, a možnosti, které máme, když se příslušné konstrukce snažíme jednotně zachytit ve formalismu Universal Dependencies. Probíráme příklady z několika jazykových rodin a navrhujeme jejich anotaci. Doufáme, že tento (nebo podobný) návrh by se mohl v budoucnosti stát součástí anotačních pravidel UD, což by přispělo k větší konzistenci treebanků UD. Současné anotace mají ke konzistenci daleko, jak přehledně ukazujeme v dodatcích k této studii.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We attempt to shed some light on the various ways how languages specify date and time, and on the options we have when trying to annotate them uniformly across Universal Dependencies. Examples from several language families are discussed, and their annotation is proposed. Our hope is to eventually make this (or similar) proposal an integral part of the UD annotation guidelines, which would help improve consistency of he UD treebanks. The current annotations are far from consistent, as can be seen from the survey we provide in appendices to this paper.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies (UD, univerzální závislosti) je mnohojazyčná kolekce korpusů, morfologicky a syntakticky anotovaných v jednotném stylu. Představujeme volitelnou rovinu hloubkově-syntaktické anotace v UD, zvanou Enhanced Universal Dependencies (rozšířené univerzální závislosti). Podáváme přehled rozšířených anotací ve verzi 2.8 a zvažujeme dvě možná budoucí pokračování: poloautomatické přidávání známých typů rozšíření do nových jazyků a přidávání nových typů rozšíření.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies (UD) is a multilingual collection of corpora featuring morphological and syntactic annotation in a unified style. We discuss an optional layer of deep-syntactic annotation in UD, called Enhanced Universal Dependencies. We survey the existing enhanced representation as of release 2.8 and consider two possible future expansions: semi-automatic addition of existing enhancement types to new languages, and addition of new enhancement types.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představuji Universal Dependencies, mezinárodní komunitní projekt, v jehož rámci vznikla morfologická a syntaktická anotační pravidla aplikovatelná na všechny přirozené jazyky světa. Probírám modely pro automatickou syntaktickou analýzu a jejich využití v digitálních humanitních studiích: v lingivistice, výuce jazyků, dokumentaci ohrožených jazyků, jazykové typologii a historickém vývoji jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I present Universal Dependencies, a worldwide community project to design morphological and syntactic annotation guidelines applicable to all the languages of the world. I discuss automatic parsing models and their use in digital humanities: linguistics, language teaching, documentation of endangered languages, linguistic typology and language change.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představím Universal Dependencies, mezinárodní komunitní projekt a kolekci morfosyntakticky anotovaných datových sad (treebanků) pro více než 100 jazyků. Tato kolekce je neocenitelným zdrojem pro různé lingvistické studie od gramatických konstrukcí v jednom jazyce k jazykové typologii, dokumentaci ohrožených jazyků a historickému vývoji jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I will present Universal Dependencies, an international community project and a collection of morphosyntactically annotated data sets (“treebanks”) for more than 100 languages. The collection is an invaluable resource for various linguistic studies, ranging from grammatical constructions within one language to language typology, documentation of endangered languages, and historical evolution of language.

From the engineering perspective, UD treebanks serve as training data for automatic parsers that can be subsequently used to analyze previously unseen text. The parsed output is an intermediate representation between the input text and its underlying meaning. It is helpful in foreign language learning as well as for automatic extraction of semantic relations (answers to “who did what to whom”). I will thus discuss these semantic aspects in the last part of my talk; in particular, I will look at extensions of UD that have been proposed and that focus more on deep-syntactic and semantic relations expressed in natural language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Deep Universal Dependencies (hluboké univerzální závislosti) je sbírka treebanků poloautomaticky odvozených z Universal Dependencies. Obsahuje přídavné hloubkově-syntaktické a sémantické anotace. Verze Deep UD odpovídá verzi UD, na které je založena. Mějte však na paměti, že některé treebanky UD nebyly do Deep UD zahrnuty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Deep Universal Dependencies is a collection of treebanks derived semi-automatically from Universal Dependencies. It contains additional deep-syntactic and semantic annotations. Version of Deep UD corresponds to the version of UD it is based on. Note however that some UD treebanks have been omitted from Deep UD.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento balíček obsahuje data použitá v soutěži IWPT 2021. Obsahuje trénovací, vývojová a testovací (vyhodnocovací) datové množiny. Data jsou založena na podmnožině vydání 2.7 Universal Dependencies (http://hdl.handle.net/11234/1-3424), ale některé treebanky obsahují další obohacené anotace nad rámec UD 2.7.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This package contains data used in the IWPT 2021 shared task. It contains training, development and test (evaluation) datasets. The data is based on a subset of Universal Dependencies release 2.7 (http://hdl.handle.net/11234/1-3424) but some treebanks contain additional enhanced annotations. Moreover, not all of these additions became part of Universal Dependencies release 2.8 (http://hdl.handle.net/11234/1-3687), which makes the shared task data unique and worth a separate release to enable later comparison with new parsing algorithms. The package also contains a number of Perl and Python scripts that have been used to process the data during preparation and during the shared task. Finally, the package includes the official primary submission of each team participating in the shared task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je čtrnácté vydání treebanků UD, verze 2.8.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the fourteenth release of UD Treebanks, Version 2.8.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je patnácté vydání treebanků UD, verze 2.9.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the fifteenth release of UD Treebanks, Version 2.9.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V monografii předkládáme celistvý popis forem a funkcí vybraných okolnostních určení založený na detailní analýze rozsáhlých korpusových dat. Usilujeme o komplexnost popisu co do šíře i co do hloubky: analyzujeme všechny v korpusech dostupné příklady a pro každý stanovujeme jeho funkci a formu. Výsledkem je podrobný seznam okolnostních významů a jejich jemnějších významových podtypů včetně seznamu formálních realizací, doložený reálnými příklady. Analýza se soustředí na
popis určení prostorových a časových jakožto jádra okolnostních významů.
Kniha se skládá ze čtyř částí.
- Část I: Teoretický úvod do analýzy okolnostních určení (repertoár funktorů, hranice mezi funktory a subfunktory, principy determinace subfunktoru, synonymie a nejednoznačnost ve vztahu forma-funkce, stanovení třídy sekundárních předložek).
- Část II: Podrobný a komplexní popis prostorových a časových určení
- Část III: Dvě srovnávací studie.
- Část IV: Přehled navržených funktorů a subfunktorů</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The necessity of a subdivision of the functors as bearers of the main semantic features in the Functional Generative Description as well as in the annotation scenario for the Prague Dependency Treebanks into more grained units called there subfunctors is presented here. The discussion is focused on the spatial and temporal functors as the most frequently occurring modification types.  The book consists of the four parts.
- Part I: Theoretical introduction to the description of adverbials analysis (repertory of functors, boundary between functors and subfunctors, principles of subfunctor determination, synonymy and ambiguity in form-function relation, determination of the class of secondary prepositions).
- Part II: Corpus-based, detailed and comprehensive description of spatial and temporal modification
- Part III: Two comparative studies.
- Part IV: Overview of proposed functors and subfunctors</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládaný Valenční slovník českých sloves (též VALLEX 4.0) poskytuje informace o valenční struktuře českých sloves v jejich jednotlivých významech, které charakterizuje pomocí glos a příkladů; tyto údaje jsou doplněny o další syntaktické a sémantické charakteristiky. VALLEX 4.0 zachycuje 4 659 českých sloves, která odpovídají 11 030 lexikálním jednotkám, tedy vždy „danému slovesu v daném významu“ (což odpovídá 6 829 lexikálním jednotkám, sdruží-li se vidové protějšky do lexémů).

VALLEX 4.0 poskytované informace obohacuje o charakteristiku sloves vyjadřujících reflexivní a reciproční významy – jde jednak o slovesa, která tyto rysy obsahují ve svém lexikálním významu, jednak o slovesa vyjadřující reciprocitu nebo reflexivitu pomocí specifických syntaktických konstrukcí. VALLEX 4.0 takto charakterizuje 2 909 sloves ve 4 453 lexikálních jednotkách, která vyjadřují či mohou vyjadřovat reciproční vztahy (což odpovídá 2 744 lexikálním jednotkám, sdruží-li se vidové protějšky do lexémů), a 2 291 sloves ve 3 287 lexikálních jednotkách s možností vyjadřovat reflexivitu (jde o 2 039 lexikálních jednotek pro vidové protějšky sdružené do lexémů).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Valency Dictionary of Czech Verbs (also VALLEX 4.0) provides information on the valency structure of Czech verbs in their individual meanings, characterized by glosses and examples; these data are supplemented by other syntactic and semantic characteristics. VALLEX 4.0 captures 4,659 Czech verbs, which correspond to 11,030 lexical units, "a given verb in a given meaning" (which corresponds to 6,829 lexical units if the mode counterparts are grouped into lexemes).

VALLEX 4.0 enriches the information with the characteristics of verbs expressing reflexive and reciprocal meanings - these are both verbs that contain these features in their lexical meaning, and verbs expressing reciprocity or reflexivity using specific syntactic constructions. VALLEX 4.0 thus characterizes 2,909 verbs in 4,453 lexical units, which express or can express reciprocal relations (corresponding to 2,744 lexical units if the visual counterparts are grouped into lexemes), and 2,291 verbs in 3,287 lexical units with the possibility of expressing reflexivity (these are 2,039 lexical units for mode counterparts grouped into lexemes).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto textu na vybraných příkladech představujeme fungování slovníku VALLEX v obou jeho složkách: datové i pravidlové. Předpokládáme, že metodologie zde užitá je užitečná pro další lexikografickou práci. Navrhly jsme také způsob, jak na příkladech z korpusů ověřit vhodnost distinkcí užitých ve VALLEXu. S ohledem na frekvenci jednotlivých typů delimitace významů v textech jsme na příkladu slovesa projet / projíždět navrhly i jistá zjednodušení na škále významů jakožto proces potřebný pro případné budoucí úpravy hesel ve slovníku VALLEX.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this text we present the VALLEX dictionary in both its components: data and rule. We assume that the methodology used here is useful for further lexicographic work. We also proposed a way to verify the suitability of the distinctions used in VALLEX on examples from corpora. With regard to the frequency of individual types of delimitation of meanings in texts, we also proposed certain simplifications on the scale of meanings on the example of the verb projet / projíždět as a process necessary for possible future modifications of passwords in the VALLEX dictionary.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme druhou soutěž IWPT v automatické analýze prostého textu do struktur Enhanced Universal Dependencies. Uvádíme podrobnosti o mírách použitých při vyhodnocení, jakož i o datových sadách použitých pro učení a vyhodnocení. Srovnáváme přístupy jednotlivých týmů, které se soutěže zúčastnily, a rozebíráme výsledky, mimo jiné i ve srovnání s výsledky prvního ročníku této soutěže.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe the second IWPT task on end-to-end parsing from raw text to Enhanced Universal Dependencies. We provide details about the evaluation metrics and the datasets used for training and evaluation. We compare the approaches taken by participating teams and discuss the results of the shared task, also in comparison with the first edition of this task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek přináší diskusi o homonymii českých podstatných jmen s různým nebo kolísavým rodem. Lemata s tímto typem homonymie jsou v novém vydání slovníku MorfFlex považována za různá. Ukazujeme, že rozdělení paradigmat podle rodu je nejen zbytečné, ale také nepraktické. Proto tomuto druhu hononymie říkáme „umělé“.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents a discussion about homonymy of Czech nouns with different or varying genders. The lemmas with this type of homonymy are treated in the new release of the dictionary MorfFlex as separated. We show that the separation of paradigms according to the gender is not only superfluous, but also clumsy, because it forces to make a choice when it is not necessary. That’s why we call this type of homonymy “artificial”.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Koncept textové srozumitelnosti: jako jazyková kompetence vyjádřená v úrovních CEFR, jako čtenářská gramotnost ve studiích OECD PIAAC, jako souhrn textových vlastností.  
 Výhody, které přináší: úspora času, úspora peněz, společenská soudržnost, důvěra ve veřejné instituce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Readability as (1) language competence expressed by CEFR levels, (2) literacy/reading comprehension level in OECD PIAAC studies, (3) a set of textual features. Benefits of clarity: time and cost savings, social cohesion, trust in public administration</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příručka snadných jazyků v Evropě popisuje historické pozadí, principy a postupy Easy Language ve 21 evropských zemích. Pojem Easy Language odkazuje na upravené formy standardních jazyků, jejichž cílem je usnadnit čtení a porozumění.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Handbook of Easy Languages in Europe describes the historical background, the principles and the practices of Easy Language in 21 European countries. The notion of Easy Language refers to modified forms of standard languages that aim to facilitate reading and comprehension.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>EngVallex 2.0 je aktualizovaná verze slovníku EngVallex. Jedná se o anglický protějšek valenčního slovníku PDT-Vallex, který používá stejný pohled na valenci, valenční rámečky a popis povrchové formy slovních argumentů. EngVallex obsahuje také odkazy na PropBank (anglický predicate-argument lexicon). Slovník EngVallex je plně propojen s anglickou stranou PCEDT paralelního treebanku, což je ve reanotovaný PTB korpus ve stylu anotace Prague Dependency Treebank. EngVallex je k dispozici ve formátu XML a také ve formě k online vyhledávání s příklady z PCEDT. EngVallex 2.0 je stejný datový soubor jako EngVallex ve vydání PCEDT 3.0, ale vydává se samostatně na základě benevolentnější licence, čímž se předejde nutnosti pro uživatele získat LDC licenci, která je vázána na PCEDT 3.0 jako celek.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>EngVallex 2.0 as a slightly updated version of EngVallex. It is the English counterpart of the PDT-Vallex valency lexicon, using the same view of valency, valency frames and the description of a surface form of verbal arguments. EngVallex contains links also to PropBank (English predicate-argument lexicon). The EngVallex lexicon is fully linked to the English side of the PCEDT parallel treebank(s), which is in fact the PTB re-annotated using the Prague Dependency Treebank style of annotation. The EngVallex is available in an XML format in our repository, and also in a searchable form with examples from the PCEDT. EngVallex 2.0 is the same dataset as the EngVallex lexicon packaged with the PCEDT 3.0 corpus, but published separately under a more permissive licence, avoiding the need for LDC licence which is tied to PCEDT 3.0 as a whole.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Funkce na generování stop-slov v 110 jazycích, na základě morfologického značkování Universal Dependencies</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Functions to generate stop-word lists in 110 languages, in a way consistent across all the languages supported. The generated lists are based on the morphological tagset from the Universal Dependencies.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení úlohy Argumentation Mining a náčrt anotačního schématu k trénovacím datům z pohledu právních expertů - potenciálních přispěvatelů textů a anotátorů. Širší kontext úlohy, současné možnosti NLP.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presentation of the Argumentation Mining task and an outline of the annotation scheme to train it, explained to law experts - potential text contributors and annotators. A broader context of the task, state of the art in NLP.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Studie se zabývá českými substantivy s nulovou příponou. Poukazuje na to, že pokud se substantivum vztahuje ke dvěma slovesům s různou tematickou příponou, jedná se o substantivum od těchto sloves odvozené. Oproti tomu substantiva, pro která je v korpusových datech k dispozici sloveso s jedinou tematickou příponou a toto sloveso mění vid připojením předpony, slouží jako slova základová pro odvození (těchto denominálních) sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Suffixless action nouns are mostly analysed as deverbal derivatives (e.g., výběr ‘choice’ < vybírat ‘to choose.IPFV’), but dictionaries ascribe the reverse direction to some noun–verb pairs (útok ‘attack’ > útočit ‘to attack.IPFV’) despite being both formally and semantically close to the former type. The question is addressed in the present study of whether any linguistic features can be identified in pairs of suffixless nouns and directly corresponding verbs that would speak in favour of one or the other direction. The analysis of 250 Czech suffixless nouns reveals a correlation between the number of directly related verbs derived by suffixes and the direction as recorded in the dictionaries: While deverbal nouns correspond mostly to a pair of verbs with different (aspect-changing) suffixes (cf. výběr ‘choice’ : vybrat/vybírat ‘to choose.PFV/IPFV’), nouns that are bases for verbs tend to share the root with a single (imperfective) verb (útok ‘attack’ : útočit ‘to attack.IPFV’). This correlation is elaborated into two different paradigms, one being based on verbal roots while the other on nominal roots, which might be applicable in hypothesizing the direction also with nouns that are not covered by the dictionaries.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato studie se zabývá českými podstatnými jmény bez slovotvorné přípony. Na dvou vzorcích dat je analyzováno, že bezpříponová substantiva s dějovým významem většinou odpovídají dvojici sloves s různými kmenotvornými příponami (vyjadřujícími různé vidové hodnoty; např. skok < skočit : skákat), zatímco bezpříponový substantiva s jiným než dějovým významem zpravidla korespondují s jediným slovesem, které nemění vid změnou kmentovorné přípony, ale připojením předpony (noc  > nocovat > přenocovat). Způsob tvoření vidového protějšku je zkoumán ve třetím vzorku dat s cílem určit v příslušných dvojicích dvojic bezpříponového substantiva a slovesa směr motivace. Rozdíl mezi deverbálním a denominálním směrem je spojován se slovnědruhovou kategorií kořenového morfému, který je společný pro bezpříponové substantivum a příslušné sloveso (slovesa). Pozorované vztahy jsou modelovány jako paradigmata opakující se ve slovotvorném systému češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present study deals with suffixless nouns in Czech. Two data samples are analysed to demonstrate that suffixless nouns with an action meaning mostly correspond to a pair of verbs with different themes (conveying different grammatical aspects; e.g., skok ‘jump’ < skočit ‘to jump.PFV’ : skákat ‘to jump.IPFV’), whereas non-actional suffixless nouns tend to form a single corresponding verb which uses a prefix to change the aspect (noc ‘night’ > nocovat ‘to stay.IPFV overnight’ > přenocovat ‘to stay.PFV overnight’). This distinction is applied to a third data sample to determine direction of motivation in pairs of suffixless nouns and verbs. The difference between the deverbal and denominal direction is explained by the part-of-speech category of the root morpheme which is shared by the suffixless noun and the corresponding verb(s). The relations observed are modelled as paradigms recurring across the word-formation system of Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce shrneme hlavní vlastnosti slovotvorné sítě DeriNet vyvíjené pro češtinu v Ústavu formální a aplikované lingvistiky MFF UK. Nastíníme rozmanitost podobně zaměřených databází vznikajících pro jiné jazyky, jakož i neskromné vyhlídky na jejich syntézu ve stopách Universal Dependencies. Dále se budeme věnovat dvěma tématům, kterými se v souvislosti s DeriNetem zabýváme v poslední době. První souvisí s aplikovatelností tzv. paradigmatického přístupu k odvozování v češtině, kontrastovaného se stromovým přístupem zvoleným v DeriNetu. Druhým tématem je segmentace českých lemmat na morfémy; jakkoli je morfém jedním z “nejklasičtějších” lingvistických pojmů, jeho rigorózní datové uchopení se zdá být překvapivě nesnadnou výzvou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this talk, we will summarize the main features of the word formation network DeriNet developed for Czech at the Institute of Formal and Applied Linguistics, MFF UK. We will also discuss two topics that have been recently addressed in connection with DeriNet. The first is related to the applicability of the so-called paradigmatic approach to Czech word formation, in contrast with the tree approach taken in DeriNet. The second topic is the segmentation of Czech lemmas into morphemes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Práce se zabývá tvorbou agentních jmen v češtině, zejména kompeticí mezi osmi nejfrekventovanějšími agentními příponami. Na základě metod strojového učení je v práci vyčíslen vliv různých formálně-lingvistických vlastností na výslednou volbu přípony při tvorbě agentního substantiva.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The work deals with the formation of agent nouns in Czech, especially the rivalry among the eight most frequent agent suffixes. We use machine learning methods to calculate the influence of several different formal-linguistic properties on the resulting suffix selected when an agent noun is coined.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje projekt ParlaMint z hlediska jeho cílů, úkolů, účastníků, výsledků a aplikačního potenciálu. Projekt vytvořil jazykové korpusy ze zasedání národních parlamentů 17 zemí, celkem téměř půl miliardy slov. Korpusy jsou rozděleny na subkorpusy související s COVID (od listopadu 2019) a referenční korpusy (do října 2019). Korpusy jsou jednotně kódovány podle schématu ParlaMint se stejnými lingvistickými anotacemi podle Universal Dependencies. Ukázky korpusů a konverzních skriptů jsou dostupné z GitHub úložiště projektu. Kompletní korpusy je volně dostupné ke stažení přes repozitář CLARIN.SI a přes concordancery NoSketch Engine a KonText i přes rozhraní Parlameter pro procházení a analýzu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper outlines the ParlaMint project from the perspective of its goals, tasks, participants, results and applications potential. The project produced language corpora from the sessions of the national parliaments of 17 countries, almost half a billion words in total. The corpora are split into COVID-related subcorpora (from November 2019) and reference corpora (to October 2019). The corpora are uniformly encoded according to the ParlaMint schema with the same Universal Dependencies linguistic annotations. Samples of the corpora and conversion scripts are available from the project’s GitHub repository. The complete corpora are openly available via the CLARIN.SI repository for download, and through the NoSketch Engine and KonText concordancers as well as through the Parlameter interface for exploration and analysis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>ParlaMint je vícejazyčný soubor srovnatelných korpusů parlamntních debat. Obsahuje parlamentní data těchto zemí: Belgie, Bulharsko, Česko, Dánsko, Francie, Chorvatsko, Island, Itálie, Litva, Lotyšsko, Maďarsko, Nizozemí, Polsko, Slovinsko, Španělsko, Turecko, UK.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>ParlaMint is a multilingual set of comparable corpora containing parliamentary debates. It contains parliament debates from these countries: Belgium, Bulgaria, Czech Republic, Denmark, Croatia, France, Iceland, Italy, Lithuania, Latvia, Hungary, Netherlands, Poland, Slovenia, Spain, Turkey, UK</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>ParlaMint je vícejazyčný soubor srovnatelných
 korpusů parlamntních debat.
Obsahuje parlamentní data těchto zemí: Belgie, Bulharsko, Česko, Dánsko, Chorvatsko, Island, Itálie, Litva, Lotyšsko, Maďarsko, Nizozemí, Polsko, Slovinsko, Španělsko, Turecko, UK.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>ParlaMint is a multilingual set of comparable
 corpora containing parliamentary debates.
It contains parliament debates from these countries: Belgium, Bulgaria, Czech Republic, Denmark, Croatia, Iceland, Italy, Lithuania, Latvia, Hungary, Netherlands, Poland, Slovenia, Spain, Turkey, UK</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>ParlaMint.ana je vícejazyčný soubor srovnatelných lingvisticky anotovaných korpusů parlamntních debat.

Obsahuje parlamentní data těchto zemí: Belgie, Bulharsko, Česko, Dánsko, Chorvatsko, Island, Itálie, Litva, Lotyšsko, Maďarsko, Nizozemí, Polsko, Slovinsko, Španělsko, Turecko, UK.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>ParlaMint.ana is a multilingual set of comparable linguistically annotated corpora containing parliamentary debates.

It contains parliament debates from these countries: Belgium, Bulgaria, Czech Republic, Denmark, Croatia, Iceland, Italy, Lithuania, Latvia, Hungary, Netherlands, Poland, Slovenia, Spain, Turkey, UK</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>ParlaMint.ana je vícejazyčný soubor srovnatelných lingvisticky anotovaných korpusů parlamntních debat.
Obsahuje parlamentní data těchto zemí: Belgie, Bulharsko, Česko, Dánsko, Francie, Chorvatsko, Island, Itálie, Litva, Lotyšsko, Maďarsko, Nizozemí, Polsko, Slovinsko, Španělsko, Turecko, UK.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>ParlaMint.ana is a multilingual set of comparable linguistically annotated corpora containing parliamentary debates.
It contains parliament debates from these countries: Belgium, Bulgaria, Czech Republic, Denmark, France, Croatia, Iceland, Italy, Lithuania, Latvia, Hungary, Netherlands, Poland, Slovenia, Spain, Turkey, UK</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek zkoumá nedávné pokroky v analýze Index Thomisticus Treebank, který zahrnuje středověké latinské texty Tomáše Akvinského. Výzkum se zaměřuje na dva typy proměnných. Na jedné straně zkoumá, jaký vliv má větší soubor dat na výsledky parsování, na druhé straně jsou analyzovány výkony nových parserů s ohledem na méně aktuální nástroje. Termínem srovnání pro určení efektivního pokroku v parsování jsou výsledky při parsování Index Thomisticus Treebank popsané v předchozí práci. Nejprve je nejvýkonnější parser z těch, kterých se týkala tato studie, testován na větším souboru dat, než byl ten původně použitý. Poté jsou vyhodnoceny i některé kombinace parserů, které byly vyvinuty v téže studii, přičemž je posouzeno, že více trénovacích dat vede k přesnějším výkonům. Nakonec, abychom prozkoumali, jaký vliv mají nově dostupné nástroje na výsledky parsování, trénujeme, testujeme a vyhodnocujeme dva neuronové parsery vybrané mezi těmi, které dosáhly nejlepších výsledků ve sdílené úloze CoNLL 2018. Naše experimenty dosahují dosud nejvyšší dosažené míry přesnosti v automatickém syntaktickém rozboru Index Thomisticus Treebank a latiny celkově.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper investigates the recent advances in parsing the Index Thomisticus Treebank, which encompasses Medieval Latin texts by Thomas Aquinas. The research focuses on two types of variables. On the one hand, it examines the impact that a larger dataset has on the results of parsing; on the other hand, performances of new parsers are analysed with respect to less recent tools. Term of comparison to determine the effective parsing advances are the results in parsing the Index Thomisticus Treebank described in a previous work. First, the best performing parser among those concerned in that study is tested on a larger dataset than the one originally used. Then, some parser combinations that were developed in the same study are evaluated as well, assessing that more training data result in more accurate performances. Finally, to examine the impact that newly available tools have on parsing results, we train, test, and evaluate two neural parsers chosen among those best performing in the CoNLL 2018 Shared Task. Our experiments reach the highest accuracy rates achieved so far in automatic syntactic parsing of the Index Thomisticus Treebank and of Latin overall.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>O umělé inteligenci slyšíme v poslední době víc a víc. Hraje šachy, překládá texty, řídí auta... Ale zvládne umělá inteligence tvořit umění, například napsat divadelní hru? Co od ní v budoucnosti můžeme očekávat? Do jakých oblastí lidského života zasáhne? A co to vlastně je, ta umělá inteligence?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We've been hearing more and more about artificial intelligence lately. It plays chess, translates texts, drives cars... But can artificial intelligence create art, such as writing a play? What can we expect from it in the future? What areas of human life will it interfere with? And what exactly is this artificial intelligence?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>O umělé inteligenci slyšíme v poslední době víc a víc. Hraje šachy, překládá texty, řídí auta... Ale zvládne umělá inteligence tvořit umění, například napsat divadelní hru? Co od ní v budoucnosti můžeme očekávat? Do jakých oblastí lidského života zasáhne? A co to vlastně je, ta umělá inteligence?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We've been hearing more and more about artificial intelligence lately. It plays chess, translates texts, drives cars... But can artificial intelligence create art, such as writing a play? What can we expect from it in the future? What areas of human life will it interfere with? And what exactly is this artificial intelligence?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Měkké aspekty projektu THEaiTRE (humanitní vědy, PR, diskuse, diváci...)</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Soft aspects of THEaiTRE (humanities, PR, discussions, spectators...)</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento projekt kombinující počítačovou lingvistiku a divadelnictví má za cíl přímým způsobem reflektovat nástup a zkoumat roli umělé inteligence ve společnosti a v umění a možnost její spolupráce s člověkem. Navrhujeme spojit teatrologický výzkum s komputačně lingvistickým výzkumem, kde aplikace poznatků a výsledků z těchto oborů povede k sestavení a natrénování systému pro automatické generování scénářů divadelních her. Tento systém následně bude naším týmem interaktivně využit pro vytvoření hry, která bude realizována hereckým souborem ke 100. výročí premiéry dramatu R.U.R. Vzniklé dílo a jeho přijetí diváky budeme zpětně zkoumat v rámci obou vědních oborů a získané poznatky využijeme pro další zdokonalení systému, který zpřístupníme na webu, a pro realizaci dalších her.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This project, combining computational linguistics and theatre, aims to directly reflect the onset and explore the role of artificial intelligence in society and art and the possibility of its cooperation with humans. We propose to combine theatrological research with computational linguistic research: the application of results from these fields will lead to the creation and training of a system for automatic generation of theatre play scripts. This system will be interactively used by our team to create a play that will be put on by a theatre on the occasion of the 100th anniversary of the R.U.R drama premiere. The resulting work of art and its acceptance by viewers will be examined, and the knowledge gained will be used to improve the system and to put on another play.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Hackování jazykového modelu GPT-2
Sestavení webové aplikace
Vytváří se scénář divadelní hry
Představení hry na jevišti</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Hacking the GPT-2 language model
Building a web application
Generating a theatre play script
Performing the play on stage</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme dosavadní stav řešení projektu:
1. Nástroj THEaiTRobot
2. Tvorba scénáře
3. Realizace divadelní hry</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the status of the project solution to date:
1. THEaiTRobot tool
2. Script making
3. Realization of the play</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>O umělé inteligenci slyšíme v poslední době víc a víc. Hraje šachy, překládá texty, řídí auta... Ale zvládne umělá inteligence tvořit umění, například napsat divadelní hru? Co od ní v budoucnosti můžeme očekávat? Do jakých oblastí lidského života zasáhne? A co to vlastně je, ta umělá inteligence?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We've been hearing more and more about artificial intelligence lately. It plays chess, translates texts, drives cars... But can artificial intelligence create art, such as writing a play? What can we expect from it in the future? What areas of human life will it interfere with? And what exactly is this artificial intelligence?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>O umělé inteligenci slyšíme v poslední době víc a víc. Hraje šachy, překládá texty, řídí auta... Ale zvládne umělá inteligence tvořit umění, například napsat divadelní hru? Co od ní v budoucnosti můžeme očekávat? Do jakých oblastí lidského života zasáhne? A co to vlastně je, ta umělá inteligence?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We've been hearing more and more about artificial intelligence lately. It plays chess, translates texts, drives cars... But can artificial intelligence create art, such as writing a play? What can we expect from it in the future? What areas of human life will it interfere with? And what exactly is this artificial intelligence?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nástroj THEaiTRobot 1.0 umožňuje uživateli interaktivně generovat scénáře pro jednotlivé divadelní scény.
Nástroj je založen na jazykovém modelu GPT-2 XL.
Při vytváření skriptu tímto způsobem jsme narazili na řadu problémů. Některé problémy se nám podařilo vyřešit různými úpravami, ale některé z nich je třeba vyřešit v budoucí verzi.

THEaiTRobot 1.0 byl použit k vytvoření první hry THEaiTRE, "AI: Když robot píše hru".</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The THEaiTRobot 1.0 tool allows the user to interactively generate scripts for individual theatre play scenes.
The tool is based on GPT-2 XL generative language model.
We encountered numerous problems when generating the script in this way. We managed to tackle some of the problems with various adjustments, but some of them remain to be solved in a future version.

THEaiTRobot 1.0 was used to generate the first THEaiTRE play, "AI: When a robot writes a play".</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme první verzi systému pro interaktivní tvorbu divadelních scénářů. Systém je založen na základním modelu GPT-2 s několika úpravami, se zaměřením na konkrétní problémy, se kterými jsme se setkali v praxi. Popisujeme i další problémy, se kterými jsme se setkali, ale plánujeme je řešit až v budoucí verzi systému. Předložený systém byl použit k vytvoření scénáře divadelní hry, která měla premiéru v únoru 2021.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the first version of a system for interactive generation of theatre play scripts. The system is based on a vanilla GPT-2 model with several adjustments, targeting specific issues we encountered in practice. We also list other issues we encountered but plan to only solve in a future version of the system. The presented system was used to generate a theatre play script premiered in February 2021.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Informujeme o AI: Když robot píše hru, divadelní hře s převážně uměle vytvořeným scénářem. Popisujeme nástroj THEaiTRobot 1.0, který byl použit ke generování scénáře. Diskutujeme o různých problémech, se kterými se v procesu setkáváme, včetně těch, které jsme do určité míry vyřešili, i těch, které plánujeme vyřešit v budoucí verzi systému.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We inform about AI: When a Robot Writes a Play, a theatre play with a mostly artificially generated script. We describe the THEaiTRobot 1.0 tool, which was used to generate the script. We discuss various issues encountered in the process, including those that we solved to some extent as well as those which we plan to solve in a future version of the system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V únoru 2021 jsme zinscenovali první divadelní hru, pro kterou bylo 90% scénáře automaticky generováno systémem umělé inteligence.

Systém THEaiTRobot je založen na jazykovém modelu GPT-2, který vytvořilo konsorcium OpenAI a který je doplněn o automatický překlad. Model jsme museli různě upravovat, zejména abychom se vyhnuli opakování a zapomínání kontextu, a abychom se drželi omezeného souboru postav. Jako vstup do systému jsme použili krátké úvodní výzvy (scénické nastavení a prvních pár řádků dialogu), připravené dramaturgem, které THEaiTRobot rozšířil do celých scén. Scénář byl následně posteditován a uveden na scénu. Recenze většinou zaznamenaly, že AI neumí napsat dobrou hru (zatím), ale uznaly, že představení bylo hlavně zajímavé a zábavné.

Se svým přístupem jsme čelili mnoha omezením. Mohli jsme generovat pouze jednotlivé scény nezávisle, s omezeným počtem postav a s charaktery, které se často náhodně zaměňují a slučují. Systém také nevidí za text scénáře, chybí mu porozumění vztahu scénáře k tomu, co se děje na jevišti. V současné době pracujeme na nové verzi systému, která by měla některé problémy zlepšit a zároveň dále minimalizovat množství lidského vlivu. Měl by také do procesu začlenit koncept dramatických situací.

Projekt THEaiTRE souvisí s dalšími podobnými pokusy, jako je hra Životní styl Richarda a rodiny, muzikál Za plotem, krátký film Sluneční jaro nebo představení divadelní skupiny Improbotics, které do jisté míry využívají automaticky generovaný obsah. Naše hra mezi těmito projekty vyniká tím, že je poměrně dlouhá (60 minut) a zároveň má velmi vysoký podíl automaticky generovaného obsahu (90%).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In February 2021, we staged the first theatre play for which 90% of the script was automatically generated by an artificial intelligence system.

The THEaiTRobot system is based on the GPT-2 language model, created by the OpenAI consortium, complemented with automated translation. We had to adapt the model in various ways, especially to avoid repetitiveness and forgetting of context, and to stick to a limited set of characters. As input for the system, we used short starting prompts (scene setting and first few lines of dialogue), prepared by a dramaturge, which were expanded into full scenes by THEaiTRobot. The script was then post-edited and put on stage. Reviews mostly noted that AI cannot really write a good play (yet), but acknowledged that the performance was mostly interesting and entertaining to watch.

We faced numerous limitations with our approach. We could only generate individual scenes independently, with a limited number of characters, and with the character personalities often randomly switching and merging. Also, the system does not see beyond the text of the script, lacking the understanding of the relation of the script to what is happening on stage. We are currently working on a new version of the system, which should improve on some of the issues, while also further minimizing the amount of human influence. It should also incorporate the concept of dramatic situations into the generation process.

The THEaiTRE project is related to other similar attempts, such as the play Lifestyle of the Richard and Family, the musical Beyond the Fence, the short movie Sunspring, or the performances of the Improbotics theatre group, all of which use automatically generated content to some extent. Our play stands out among these projects by being rather long (60 minutes) while having a very high proportion of automatically generated content (90%).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Panel sleduje české divadelní představení „AI: When a Robot Writes a Play“, které bylo napsáno s pomocí systému AI a – namluveno robotem – pojednává o hledání sounáležitosti v odcizeném světě. S tvůrci hry a dalšími umělci pracujícími s umělou inteligencí prozkoumáme, do jaké míry mohou systémy umělé inteligence imitovat lidskou tvořivost a co tato imitace vypovídá o našem chápání umění a společnosti. Kromě strachu z toho, že počítače nahradí člověka, chceme diskutovat o tom, jaké nové možnosti může umělec nabídnout umění a jak umělci mohou kriticky a originálně přemýšlet o nových technologiích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The panel follows the Czech theater performance “AI: When a Robot Writes a Play”, which was written by the help of an AI system and – narrated by a robot – deals with the quest for belonging in an alienated world. With the makers of the play and other artists working with AI, we will explore to what extent AI systems can imitate human creativity and what this imitation says about our understanding of art and society. Beyond the fear of computers replacing humans, we want to discuss what new possibilities AI can offer to art and how artists can think critically and originally about new technologies.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>O čem robot přemýšlí, co cítí a dokáže napsat divadelní hru? Střípky ze života umělé inteligence jejími vlastními slovy. Futuristický Malý princ z doby postpandemické.

Inscenace, po jejíž každé repríze následuje debata s odborníky na umělou inteligenci i divadelními tvůrci, je nejen svědectvím o současných schopnostech počítačových technologií, ale i poutavou vizí budoucího světa inspirující se v klasikách žánru sci-fi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>What does the robot think about, how does it feel and can it write a play? Snippets from the life of artificial intelligence in its own words. Futuristic Little Prince of the postpandemic era. 

Every reprise will be followed by a debate with experts in artificial intelligence and theater makers. The show is not only a testament to the current capabilities of computer technology, but also an engaging vision of the future world inspired by the sci-fi classics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>R.U.R. Karla Čapka byla první divadelní hra napsaná člověkem o robotech (a lidech). Premiéru měl 25. ledna 1921. O sto let později, se všemi současnými pokroky ve zpracování přirozeného jazyka a umělých neuronových sítích, jsme v našem projektu THEaiTRE tuto myšlenku otočili. 25. ledna 2021 budeme mít premiéru "AI: When a robot writes a play" ("AI: robot píše hru"), divadelní hru o lidech (a robotech), kterou napsala naše umělá inteligence THEaiTRobot. Umožňují to počítačoví lingvisté, kteří pro tento unikátní výzkumný projekt spojují své síly s divadelními odborníky. O co v tomto projektu jde a jak byl vyvinut? Jakým výzvám jsme čelili? Jak ten umělou inteligencí vytvořený scénář vypadá? ...a opravdu chceme, aby umělá inteligence vytvářela umění?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Karel Čapek's R.U.R. was the first theatrical play written by a human about robots (and humans). It premiered on 25 January 1921. A hundred years later, with all the current advances in natural language processing and artificial neural networks, we have turned the idea around in our THEaiTRE project. On 25 January 2021, we will premiere "AI: When a robot writes a play" ("AI: Když robot píše hru"), a theatre play about humans (and robots) written by our artificial intelligence called THEaiTRobot. It is made possible by computational linguists joining forces with theatre experts for this unique research project. What is this project about and how has it been developed? What challenges have we faced? What does the AI-generated script look like? ...and do we actually want AI to create art?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Invertování R.U.R. s umělou inteligencí.
Čapkovo výročí.
Vytváření divadelních her.
Příklady výstupů.
Pohled divadelního experta.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Inverting R.U.R. with Artificial Intelligence.
Čapek anniversary.
Generating Theatre Plays.
Output Examples.
View of a Theatre Expert.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Překladový model z katalánštiny do okcitánštiny pro systém Marian. Je to multi-task model kromě okcitánskeho překladu produkující rovněž fonémický zápis katalánskeho zdroje.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Marian NMT model for Catalan to Occitan translation. It is a multi-task model, producing also a phonemic transcription of the Catalan source. The model was submitted to WMT'21 Shared Task on Multilingual Low-Resource Translation for Indo-European Languages as a CUNI-Contrastive system for Catalan to Occitan.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Data použity pro experiment Ptakopět s odchozím strojovým překladem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The dataset used for the Ptakopět experiment on outbound machine translation. It consists of screenshots of web forms with user queries entered. The queries are available also in a text form. The dataset comprises two language versions: English and Czech. Whereas the English version has been fully post-processed (screenshots cropped, queries within the screenshots highlighted, dataset split based on its quality etc.), the Czech version is raw as it was collected by the annotators.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce popisuje soutěžní systém pro vícejazyčný strojový překlad LMU Mnichov do WMT 2021 pro úlohu č. 1, která studuje překlad mezi 6 jazyky (chorvatština, maďarština, estonština, srbština, makedonština, angličtina) ve 30 směrech. Zkoumáme, do jaké míry mohou dvojjazyčné překladové systémy ovlivňovat mnohojazyčné překladové systémy. Konkrétně jsme natrénovali 30 dvojjazyčných překladových systémů, které pokrývají všechny jazykové dvojice, a použili jsme techniky pro rozšíření dat, jako je zpětný překlad a destilace znalostí.. Náš nejlepší překladový systém má o 5 až 6 BLEU vyšší skóre než silný základní systém poskytovaný organizátory (Goyal et al.,2021). Jak je vidět v žebříčku Dynalabu, náš systém je jediným systémem, který využívá pouze korpus poskytnutý organizátory a nepoužívá žádné předtrénované modely</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the submission of LMU Munich to the  WMT  2021  multilingual machine translation task for small track #1, which studies translation between 6 languages (Croatian,  Hungarian,  Estonian,  Serbian,  Macedonian, English) in 30 directions. We investigate the extent to which bilingual translation systems can influence multilingual translation systems.   More specifically,  we trained 30 bilingual translation systems, covering all language pairs, and used data augmentation techniques such as back-translation and knowledge distillation to improve the multilingual translation systems.  Our best translation system scores 5 to 6 BLEU higher than a strong baseline system provided by the organizers (Goyal et al.,2021).   As  seen  in  the  Dynalab  leaderboard, our  submission  is  the  only  fully  constrained submission that uses only the corpus provided by  the  organizers  and  does  not  use  any  pre-trained models</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Argument těžby se zaměřuje na struktury v přirozeném jazyce související s tlumočením a přesvědčováním, které jsou pro vědeckou komunikaci stěžejní. Většina vědecké rozpravy zahrnuje interpretaci experimentálních důkazů a snahu přesvědčit ostatní vědce, aby přijali stejné závěry. Různé studie o těžbě argumentů se sice zabývaly studentskými eseji a zpravodajskými články, ale těch, které se zaměřují na vědecký diskurz, je stále málo. Tento dokument zkoumá stávající práci v oblasti argumentační těžby odborného diskurzu a poskytuje přehled o aktuálních modelech, datech, úkolech a aplikacích. Určujeme řadu klíčových výzev, před nimiž stojí argumentační těžba ve vědecké oblasti, a navrhujeme některá možná řešení a budoucí směry.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Argument mining targets structures in natural language related to interpretation and persuasion which are central to scientific communication. Most scholarly discourse involves interpreting experimental evidence and attempting to persuade other scientists to adopt the same conclusions. While various argument mining studies have addressed student essays and news articles, those that target scientific discourse are still scarce. This paper surveys existing work in argument mining of scholarly discourse, and provides an overview of current models, data, tasks, and applications. We identify a number of key challenges confronting argument mining in the scientific domain, and suggest some possible solutions and future directions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku představujeme novou architekturu pro obnovu diakritiky založenou na kontextualizovaných vektorových reprezentacích, konkrétně BERT, a vyhodnocujeme ji ve dvanácti jazycích s diakritikou. Dále jsme provedli detailní chybovou analýzu v češtině, jazyce s bohatou morfologií a vysokou úrovní diakritizace. Zejména jsme ručně anotovali všechny chybné predikce a ukázali jsme, že zhruba 44% z chybně určené diakritizace nepředstavují skutečné chyby, nýbrž z 19% paralelní přijatelné varianty nebo dokonce systémové opravy diakritizace indukované různými chybami v datech (25%). Nakonec jsme také detailně kategorizovali skutečné chyby systému. Zdrojový kód jsme vydali zde: https://github.com/ufal/bert-diacritics-restoration.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We propose a new architecture for diacritics restoration based on contextualized embeddings, namely BERT, and we evaluate it on 12 languages with diacritics. Furthermore, we conduct a detailed error analysis on Czech, a morphologically rich language with a high level of diacritization. Notably, we manually annotate all mispredictions, showing that roughly 44% of
them are actually not errors, but either plausible variants (19%), or the system corrections of erroneous data (25%). Finally, we categorize the real errors in detail. We release the code at https://github.com/ufal/bert-diacritics-restoration.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Citlivost modelů hlubokého neuronového učení k šumu na vstupu je známý a výrazný problém. Při strojovém zpracování přirozeného jazyka se výkon modelu často zhoršuje při přirozeně se vyskytujícím šumu, například při překlepech a pravopisných chybách. Aby se tomuto problém zabránilo, modely často využívají data s uměle vytvořenými chybami. Ovšem množství a typ takto generovaného šumu bylo dosud určováno libovolně. My proto navrhujeme modelovat chyby statisticky z korpusů pro opravy gramatiky. Předkládáme pečlivou evaluaci několika současných nástrojů strojového zpracování textu co do robustnosti v několika jazycích a úlohách, včetně morfo-syntaktické analýzy, rozpoznávání pojmenovaných entit, neuronového strojového překladu, podmnožiny úloh v GLUE a porozumění textu. Dále srovnáváme dva přístupy pro zamezení zhoršení výkonu: a) trénování modelů za použití dat se šumem zavedeným pomocí našeho modelu; a b) redukci vstupního šumu pomocí externího nástroje pro kontrolu gramatiky. Zdrojový kód je vydán na adrese https://github.com/ufal/kazitext.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Sensitivity of deep-neural models to input noise is known to be a challenging problem. In NLP, model performance often deteriorates with naturally occurring noise, such as spelling errors. To mitigate this issue, models may leverage artificially noised data. However, the amount and type of generated noise has so far been determined arbitrarily. We therefore propose to model the errors statistically from grammatical-error-correction corpora. We present a thorough evaluation of several state-of-the-art NLP systems' robustness in multiple languages, with tasks including morpho-syntactic analysis, named entity recognition, neural machine translation, a subset of the GLUE benchmark and reading comprehension. We also compare two approaches to address the performance drop: a) training the NLP models with noised data generated by our framework; and b) reducing the input noise with external system for natural language correction. The code is released at https://github.com/ufal/kazitext.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje výsledky soutěže strojových překladačů pro indoevropské jazyky a dalších úloh automatického překladu
a post-editační úlohy, které byly uspořádány v rámci konference o strojovém překladu (WMT) 2021.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the news
translation task, the multilingual low-resource
translation for Indo-European languages, the
triangular translation task, and the automatic
post-editing task organised as part of the Conference on Machine Translation (WMT) 2021.
In the news task, participants were asked to
build machine translation systems for any of
10 language pairs, to be evaluated on test
sets consisting mainly of news stories. The
task was also opened up to additional test
suites to probe specific aspects of translation. In the Similar Language Translation
(SLT) task, participants were asked to develop systems to translate between pairs of
similar languages from the Dravidian and Romance family as well as French to two sim-
ilar low-resource Manding languages (Bambara and Maninka). In the Triangular MT
translation task, participants were asked to
build a Russian to Chinese translator, given
parallel data in Russian-Chinese, Russian-
English and English-Chinese. In the mul-
tilingual low-resource translation for Indo-
European languages task, participants built
multilingual systems to translate among Romance and North-Germanic languages. The task was designed to deal with the translation of documents in the cultural heritage domain for relatively low-resourced languages.
In the automatic post-editing (APE) task, participants were asked to develop systems capable to correct the errors made by an unknown
machine translation systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento dokument představuje výsledky sdílených úkolů z 8. workshopu o překladech do asijských jazyků (WAT2021). WAT2021 se účastnilo 28 týmů a 24 týmů předložilo výsledky překladů pro lidské hodnocení. Obdrželi jsme také 5 článků. Zhruba 2100 výsledků překladů bylo odevzdáno na automatickém hodnotícím serveru a vybraná podání byla vyhodnocena ručně.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the shared
tasks from the 8th workshop on Asian translation (WAT2021). For the WAT2021, 28 teams
participated in the shared tasks and 24 teams
submitted their translation results for the human evaluation. We also accepted 5 research
papers. About 2,100 translation results were
submitted to the automatic evaluation server,
and selected submissions were manually evaluated.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V krátkém vstupu představíme projekt THEaiTRE a řekneme jak roboti píšou divadelní hry.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In a short sketch, we present the THEaiTRE project and say how robots write plays.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Diskuze o inscenaci a projektu Švandova divadla AI: Když robot píše hru, uskutečněná po on-line premiéře 18. února 2021.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Discussion about the production and project of the Švanda Theater AI: When a Robot Writes a Play held after the on-line premiere on February 18, 2021.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V debatě navazující na inscenaci „AI: Když robot píše hru‟ se probírá otázka, která stála u zrodu celého projektu THEaiTRE, zda AI dokáže napsat divadelní hru. Jak to vidí experti z nejrůznějších oborů IT? Jak to vidí divadelní tvůrci, kteří se s textem, jež AI z 90% vytvořila, museli potýkat? Jak to má AI s kreativitou? A k jakým horizontům se AI ubírá nejen v oblasti umění?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the debate following the production of AI: When a Robot Writes a Play, the question that was present from the very beginning of the entire THEaiTRE project is discussed, and that is whether AI can write a play. How do experts from various fields of IT see it? How do theatre creators who had to deal with the text that AI created from 90% see it? How does AI work with creativity? And what other horizons does AI go to besides the field of art?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V debatě navazující na inscenaci „AI: Když robot píše hru‟ se probírá otázka, která stála u zrodu celého projektu THEaiTRE, zda AI dokáže napsat divadelní hru. Jak to vidí experti z nejrůznějších oborů IT? Jak to vidí divadelní tvůrci, kteří se s textem, jež AI z 90% vytvořila, museli potýkat? Jak to má AI s kreativitou? A k jakým horizontům se AI ubírá nejen v oblasti umění?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the debate following the production of AI: When a Robot Writes a Play, the question that was present from the very beginning of the entire THEaiTRE project is discussed, and that is whether AI can write a play. How do experts from various fields of IT see it? How do theatre creators who had to deal with the text that AI created from 90% see it? How does AI work with creativity? And what other horizons does AI go to besides the field of art?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Při příležitosti oslavy sto let od prvního uvedení Čapkovy hry R. U. R. (která přinesla světu mj. slovo „robot“) navrhl Tomáš Studeník  v roce 2019 divadelní kus, jehož autorem by nebyl člověk, ale chytrý software – tzv. umělá inteligence (Artificial Intelligence – AI). Na základě této myšlenky vznikl tým složený z informatiků Matematicko-fyzikální fakulty Univerzity Karlovy (v čele s počítačovým lingvistou Rudolfem Rosou), divadelníků ze Švandova divadla a studentů DAMU (vedený ředitelem Švandova divadla, režisérem a pedagogem Danielem Hrbkem). Ve vzájemné kooperaci a debatě se rozhodli odpovědět na otázku, zda je umělá inteligence schopna napsat divadelní hru a jak se při zpracování takového úkolu bude chovat. Projekci této neobvyklé divadelní hry uvedeme v sále Mozartea Arcidiecézního muzea Olomouc.

Inscenace, po jejíž každé repríze následuje debata s odborníky na umělou inteligenci i divadelními tvůrci, je nejen svědectvím o současných schopnostech počítačových technologií, ale i poutavou vizí budoucího světa inspirující se v klasikách žánru sci-fi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In 2019, the innovator Tomáš Studeník came up with an idea to celebrate one hundred years since the first performance of Karel Čapeks play RUR (which brought the word "robot" into the world) in an unconventional way – by a theatre play written not by a human, but by artificial intelligence. Based on this idea, a team was formed consisting of computer scientists from Charles University Faculty of Mathematics and Physics (led by computational linguist Rudolf Rosa) and theatre-makers from the Švanda Theater and students of The Academy of Performing Arts in Prague (led by the head of the Švanda Theater, director and pedagogue Daniel Hrbek). By mutual cooperation and debate, they decided to answer the question of whether artificial intelligence is able to write a play and how it will behave when processing such a task.

Every reprise will be followed by a debate with experts in artificial intelligence and theater makers. The show is not only a testament to the current capabilities of computer technology, but also an engaging vision of the future world inspired by the sci-fi classics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Automatické hodnocení kvality strojového překladu (MT) bylo zkoumáno po několik desetiletí. Strojový překlad mluveného jazyka (SLT), zejména simultánní, musí zvážit další kritéria a nemá standardní postup hodnocení a široce využívanou sadu nástrojů. Abychom zaplnili tuto mezeru, představujeme SLTev, open-source nástroj pro komplexní hodnocení SLT. SLTev informuje o kvalitě, latenci a stabilitě výstupu kandidáta SLT na základě časově vyznačeného přepisu a překladu odkazu do cílového jazyka. Pokud jde o kvalitu, spoléháme na SacreBLEU, která poskytuje MT hodnotící opatření, jako je chrF nebo BLEU. Pro latenci navrhujeme dvě nové bodovací techniky. V zájmu stability rozšiřujeme dříve definovaná opatření normalizovaným flickerem v naší práci. Navrhujeme také nové zprůměrování starších metod. V projektu IWSLT 2020 SHARED TASK byla použita předběžná verze programu SLTev. Navíc se rozšiřuje sbírka testovacích datových souborů, které jsou přímo přístupné přes SLTev, pro hodnocení systémů srovnatelných napříč papíry.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Automatic evaluation of Machine Translation(MT)  quality  has  been  investigated  over  several  decades.   Spoken  Language  Translation (SLT), especially when simultaneous, needs to consider additional criteria and does not have a standard evaluation procedure and a widely used  toolkit.    To  fill  the  gap,  we  introduce SLTev, an open-source tool for assessing SLT in a comprehensive way.   SLTev reports the quality,  latency,  and  stability  of  an  SLT  candidate output based on the time-stamped transcript  and  reference  translation  into  a  target language.  For quality, we rely on sacreBLEU which provides MT evaluation measures such as chrF or BLEU. For latency, we propose two new scoring techniques.  For stability, we extend  the  previously  defined  measures  with  a normalized Flicker in our work.  We also propose a new averaging of older measures. A preliminary version of SLTev was used in the IWSLT  2020 SHARED  TASK.  Moreover, a  growing  collection  of  test  datasets  directly accessible by SLTev are provided for system evaluation comparable across papers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme koncept rozšíření vícejazyčného slovesného slovníku, který bude zahrnovat také němčinu. V tomto slovníku jsou zatím zahrnuta česká a anglická slovesa a seskupena podle významu a sémantických vlastností. Položky jsou dále propojeny s externími lexikálními zdroji jako VerbNet a PropBank. V tomto článku představujeme náš plán zahrnout také německá slovesa jako kandidáty propojené s existujícími anglickými a českými slovesy.
Také identifikujeme specifické německé lexikální zdroje, se kterými bude slovník provázán.
Cílem této pilotní studie malého rozsahu je poskytnout návrh na rozšíření již existujícího
lexikálního zdroje o nový jazyk.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the concept of extending a multilingual verb lexicon
also to include German. In this lexicon, verbs are grouped by meaning
and by semantic properties (following frame semantics) to form multilingual
classes, linking Czech and English verbs. Entries are further linked
to external lexical resources like VerbNet and PropBank. In this paper,
we present our plan also to include German verbs, by experimenting
with word alignments to obtain candidates linked to existing English
entries, and identify possible approaches to obtain semantic role information.
We further identify German-specific lexical resources to link to.
This small-scale pilot study aims to provide a blueprint for extending a
lexical resource with a new language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pod širým nebem promítneme záznam online premiéry první české divadelní hry, jejíž dialogy vygenerovala umělá inteligence. Následuje beseda s tvůrci unikátního díla o radostech a strastech každodenního života z pohledu robota. Netradiční představení Švandova divadla v Praze vzniklo ve spolupráci s DAMU a Matematicko-fyzikální fakultou Univerzity Karlovy a na Strži bude k vidění ještě před uvedením hry před diváky smíchovské scény. Účast přislíbili: Daniel Hrbek (hudba a režie), Martina Kinská (dramaturgyně), Rudolf Rosa (počítačový lingvista)... a zástupce hereckého souboru Švandova divadla.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the open air, we will show a recording of the online premiere of the first Czech play the dialogue of which was generated by artificial intelligence. Next is a sit-down with the creators of a unique work on the joys and tribulations of everyday life from the robot's point of view. The unconventional performance of the Švandova Theatre in Prague was created in collaboration with DAMU and the Mathematical Physics Faculty of Charles University, and will be on view at Strž before the play is released in front of the audience of the Smíchov scene. Daniel Hrbek (music and direction), Martina Kinská (dramaturgy), Rudolf Rosa (computer linguist)... and a representative of the acting ensemble of the Švanda Theatre.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>O čem robot přemýšlí, co cítí a dokáže napsat divadelní hru? Střípky ze života umělé inteligence jejími vlastními slovy. Futuristický Malý princ z doby postpandemické.

Inscenace, po jejíž každé repríze následuje debata s odborníky na umělou inteligenci i divadelními tvůrci, je nejen svědectvím o současných schopnostech počítačových technologií, ale i poutavou vizí budoucího světa inspirující se v klasikách žánru sci-fi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>What does the robot think about, how does it feel and can it write a play? Snippets from the life of artificial intelligence in its own words. Futuristic Little Prince of the postpandemic era. 

Every reprise will be followed by a debate with experts in artificial intelligence and theater makers. The show is not only a testament to the current capabilities of computer technology, but also an engaging vision of the future world inspired by the sci-fi classics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>THEaiTRE
Je výzkumný projekt oslavující sté výročí premiéry divadelní hry R.U.R od autora Karla Čapka, ve které bylo slovo „robot“ poprvé použito v roce 1921. Na oslavu tohoto 100 let starého jubilea se spojil Institut formální a aplikované lingvistiky na Matematicko-fyzikální fakultě Univerzity Karlovy se Švandovým divadlem na Smíchově s organizátory Hackathons CEE Hacks, aby čelili svým robotům s novou výzvou, dosud nevídanou v plném rozsahu - aby umělá inteligence napsala scénář.
První fáze projektu vyvrcholila premiérou divadelní inscenace AI: Když robot píše hru s cílem vytvořit scénář složený z dialogů vytvořených umělou inteligencí. V současné době se připravuje druhá fáze, která bude uspořádána na rok 2022, kdy bude představen další premiér s propracovanějším jazykovým modelem.
Tento projekt financovala Technologická agentura České republiky ve spolupráci s prg.ai a Divadelní fakultou Akademie múzických umění v Praze.
Rudolf Rosa promluví o technickém zázemí projektu (jak umělá inteligence vytváří scénář), David Košťák promluví o vytvoření tohoto konkrétního scénáře a Daniel Hrbek shrne proces tvorby hry.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>THEaiTRE  
Is a research project celebrating the 100th year anniversary of the premiere of the theater play R.U.R by author Karel Čapek, in which, the word “robot” was used for the first time in 1921. To celebrate this 100 years old jubilee Institute of Formal and Applied Linguistics  at the Faculty of Mathematics and Physics of the Charles University joined together with Švanda Theater in Smíchov and the organizers of Hackathons CEE Hacks to face their robots with a new challenge, not previously seen in its full extent - to make artificial intelligence write a screenplay.
The first phase of the project culminated with the premiere of a theater production AI: When a robot writes a play with the purpose to create a screenplay made up of dialogues generated by artificial intelligence. The second phase is currently in the making, staged for 2022 when another premier with a more sophisticated linguistic model will be introduced.
This project was funded by the Technology Agency of the Czech Republic in cooperation with prg.ai and the Theater Faculty of the Academy of Performing Arts in Prague. 
Rudolf Rosa will talk about the technical background of the project (how artificial intelligence generates a screenplay), David Košťák will talk about the creation of this particular screenplay and Daniel Hrbek will summarize the process of the play´s production.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>O čem robot přemýšlí, co cítí a dokáže napsat divadelní hru? Střípky ze života umělé inteligence jejími vlastními slovy. Futuristický Malý princ z doby postpandemické.

Příběh robota, který po smrti svého mistra zůstal vydán napospas nejrůznějším vzorkům z lidské společnosti​, balancuje na hranici absurdní černé komedie a existenciálního dramatu. 

„Autobiografická“ hra umělé inteligence o hledání blízkosti ve světě, který již nějakou dobu nezná či neumí prostý kontakt, a v němž je tak tou nejhůře zdolatelnou vzdáleností cesta jednoho člověka k druhému.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>What does the robot think about, how does it feel and can it write a play? Snippets from the life of artificial intelligence in its own words. Futuristic Little Prince of the postpandemic era. 

The story of the robot, which after the death of its master was left at the mercy of various individuals of human society, balances on a thin line between absurd black comedy with existential drama.  

An "autobiographical" play written by artificial intelligence that talks about the search for closeness of someone in a world where people have not known or are not able to make simple contact with each other for some time, and in which the path of one person to another is the hardest to cover.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Citace jsou pro vědecký diskurz klíčové. Vedle poskytování dodatečných kontextů výzkumným dokumentům působí citace jako sledovatelé směru výzkumu v určité oblasti a jako důležité měřítko pro pochopení dopadu výzkumné publikace. S rychlým růstem publikací ve výzkumu začínají být velmi důležitá automatizovaná řešení pro identifikaci účelu a vlivu citací. Úloha 3C Citation Context Classification Task organizovaná v rámci druhého semináře o zpracovávání dokumentů Schopodobně @ NAACL 2021 je sdíleným úkolem pro řešení výše uvedených problémů. V tomto příspěvku představujeme náš tým, IITP-CUNI@3C, který se hlásí ke sdíleným úkolům 3C. Pro úkol A, citační kontextovou klasifikaci účelu, navrhujeme neurální víceúčelový vzdělávací rámec, který využívá strukturální informace výzkumných prací a vztah mezi citačním kontextem a citovaným dokumentem pro citační klasifikaci. Pro úkol B, citační kontext ovlivňuje klasifikaci, používáme sadu jednoduchých funkcí pro klasifikaci citací na základě jejich vnímaného významu. Dosahujeme srovnatelného výkonu s ohledem na systémy s nejlepšími výsledky v úkolu A a nahradili jsme většinovou základní linii v úkolu B velmi jednoduchými funkcemi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Citations are crucial to a scientific discourse. Besides providing additional contexts to research papers, citations act as trackers of the direction of research in a field and as an important measure in understanding the impact of a research publication. With the rapid growth in research publications, automated solutions for identifying the purpose and influence of citations are becoming very important. The 3C Citation Context Classification Task organized as part of the Second Workshop on Scholarly Document Processing @ NAACL 2021 is a shared task to address the aforementioned problems. In this paper, we present our team, IITP-CUNI@3C’s submission to the 3C shared tasks. For Task A, citation context purpose classification, we propose a neural multi-task learning framework that harnesses the structural information of the research papers and the relation between the citation context and the cited paper for citation classification. For Task B, citation context influence classification, we use a set of simple features to classify citations based on their perceived significance. We achieve comparable performance with respect to the best performing systems in Task A and superseded the majority baseline in Task B with very simple features.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>S rychlým růstem publikací ve výzkumu nabývají na významu automatizovaná řešení, která mají řešit přetížení vědeckých informací. Správné určení záměru citací je jedním z takových úkolů, který vyhledává aplikace od předpovídání vědeckého dopadu, hledání šíření myšlenek, přes sumarizaci textu až po vytvoření informativnějších citačních indexů. V této probíhající práci využíváme informace z citovaného dokumentu a prokazujeme, že to pomáhá při efektivní klasifikaci citačních záměrů. Navrhujeme neurální víceúčelový vzdělávací rámec, který využívá strukturální informace z výzkumných prací a vztah mezi citačním kontextem a citovaným dokumentem pro citační klasifikaci. Naše počáteční experimenty se třemi datovými soubory pro klasifikaci referenčních citací ukazují, že se začleněním citovaných papírových informací (názvu) náš neurální model dosahuje na datovém souboru ACL-ARC nového stavu s absolutním nárůstem F1 skóre o 5,3% oproti předchozímu nejlepšímu modelu. Náš přístup rovněž předčí podání v rámci 3C Shared task: Citation Context Classification s nárůstem o 8 % resp. 3,6 % oproti předchozímu nejlepšímu skóre Public F1-macro a Private F1-macro.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>With the rapid growth in research publications, automated solutions to tackle scholarly information overload is growing  more  relevant.  Correctly  identifying  the  intent  of  the  citations  is  one  such  task  that  finds  applications  ranging from predicting scholarly impact, finding idea propagation, to text summarization to establishing more informative citation indexers. In this in-progress work, we leverage the cited paper’s information and demonstrate that this helps in the effective classification of citation intents. We propose a neural multi-task learning framework that harnesses the structural information of the research papers and the relation between the citation context and the  cited  paper  for  citation  classification.  Our  initial  experiments  on  three  benchmark  citation  classification  datasets show that with incorporating cited paper information (title), our neural model achieves a new state of the art on the ACL-ARC dataset with an absolute increase of 5.3% in the F1 score over the previous best model. Our approach also outperforms the submissions made in the 3C Shared task: Citation Context Classification with an increase of 8% and 3.6%over the previous best Public F1-macro and Private F1-macro scores respectively.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Korpus ParCzech 3.0 je třetí verze ParCzech, který obsahuje stenografické protokoly sedmého volebního období (2013-2017) a současného 8. období (2017-březen 2021). Protokoly jsou v jejich původním HTML formátu, v Parla-CLARIN  TEI formátu a ve formátu vhodném pro automatické rozpoznávání řeči. Korpus je automaticky obohacen o morfologii, syntax a jmenné entity programy UDPipe 2 a NameTag 2. Audio soubory jsou zarovnané s texty v anotovaných TEI souborech</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The ParCzech 3.0 corpus is the third version of ParCzech consisting of stenographic protocols that record the Chamber of Deputies’ meetings held in the 7th term (2013-2017) and the current 8th term (2017-Mar 2021). The protocols are provided in their original HTML format, Parla-CLARIN TEI format, and the format suitable for Automatic Speech Recognition. The corpus is automatically enriched with the morphological, syntactic, and named-entity annotations using the procedures UDPipe 2 and NameTag 2. The audio files are aligned with the texts in the annotated TEI files.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Uvádíme ParCzech 3.0, mluvený korpus záznamů jednání Poslanecké sněmovny Parlamentu České republiky z období od 25. listopadu 2013 do 1. dubna 2021.

Na rozdíl od předchozích mluvených korpusů češtiny zachováváme nejen ortografii, ale také všechna dostupná metadata (identitu mluvčích, pohlaví, hypertextové odkazy, příslušnosti, politické strany atd.) a doplňujeme je automatickou morfologickou a syntaktickou anotací a rozpoznáním pojmenovaných entit. Korpus je kódován ve formátu TEI, který umožňuje přímočaré a mnohostranné využití.

Díky bohatým metadatům a anotaci je korpus relevantní pro široké spektrum výzkumníků od inženýrů v oblasti rozpoznávání řeči až po teoretické lingvisty zkoumající rétorické vzorce z rozsáhlých materiálů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present ParCzech 3.0, a speech corpus of the Czech parliamentary speeches from The Czech Chamber of Deputies which took place from 25th November 2013 to 1st April 2021.

Different from previous speech corpora of Czech, we preserve not just orthography but also all the available metadata (speaker identities, gender, web pages links, affiliations committees, political groups, etc.) and complement this with automatic morphological and syntactic annotation, and named entities recognition. The corpus is encoded in the TEI format which allows for a straightforward and versatile exploitation.

The rather rich metadata and annotation make the corpus relevant for a~wide audience of researchers ranging from engineers in the speech community to theoretical linguists studying rhetorical patterns at scale.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>AI: Když robot píše hru.
Oslava výročí Karla Čapka.
Jak je to uděláno.
Příklady výstupů.
Pohled dramaturga.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>AI: When a robot writes a play.
Celebrating the anniversary of Karel Čapek.
How it's done.
Examples of outputs.
View of a dramaturge.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>O čem robot přemýšlí, co cítí a dokáže napsat divadelní hru? Střípky ze života umělé inteligence jejími vlastními slovy. Futuristický Malý princ z doby postpandemické.

Inscenace, po jejíž každé repríze následuje debata s odborníky na umělou inteligenci i divadelními tvůrci, je nejen svědectvím o současných schopnostech počítačových technologií, ale i poutavou vizí budoucího světa inspirující se v klasikách žánru sci-fi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>What does the robot think about, how does it feel and can it write a play? Snippets from the life of artificial intelligence in its own words. Futuristic Little Prince of the postpandemic era. 

Every reprise will be followed by a debate with experts in artificial intelligence and theater makers. The show is not only a testament to the current capabilities of computer technology, but also an engaging vision of the future world inspired by the sci-fi classics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje anglicko-německé a anglicko-hauské systémy z Edinburské univerzity pro sdílenou úlohu na WMT 2021 týkající se překladu zpráv.
En-De systémy budujeme ve třech fázích: korpusové filtrování, zpětný překlad a jemné ladění.
Pro En-Ha používáme iterativní zpětný překlad
přístup na vrchol předtrénovaných modelů En-De
a zkoumat mapování vkládání slovní zásoby.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the University of Edinburgh’s constrained submissions of English-German and English-Hausa systems to the
WMT 2021 shared task on news translation.
We build En-De systems in three stages: corpus filtering, back-translation, and fine-tuning.
For En-Ha we use an iterative back-translation
approach on top of pre-trained En-De models
and investigate vocabulary embedding mapping.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies (UD) je formalismus pro morfologickou a syntaktickou anotaci, který byl dosud použit při tvorbě treebanků pro více než 100 přirozených jazyků. V tomto článku nastiňujeme lingvistickou teorii UD, která staví na dlouhé tradici typologicky orientovaných gramatických teorií. V centru stojí gramatické vztahy mezi slovy, které objasňují, jaké morfosyntaktické strategie používají různé jazyky k zakódování struktury predikátů a jejich argumentů. Morfologické rysy a slovní druhy popisují vlastnosti slov. Tvrdíme, že tato teorie poskytuje dobrý základ pro mezijazykově konzistentní anotaci typologicky rozmanitých jazyků způsobem, který podporuje jak počítačové porozumění přirozenému jazyku, tak širší jazykovědné studie.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies (UD) is a framework for morphosyntactic annotation of human language, which to date has been used to create treebanks for more than 100 languages. In this article, we outline the linguistic theory of the UD framework, which draws on a long tradition of typologically oriented grammatical theories. Grammatical relations between words are centrally used to explain how predicate-argument structures are encoded morphosyntactically in different languages while morphological features and part-of-speech classes give the properties of words. We argue that this theory is a good basis for cross-linguistically consistent annotation of typologically diverse languages in a way that supports computational natural language understanding as well as broader linguistic studies.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dialogové systémy orientované na úkoly obvykle vyžadují manuální anotaci dialogových slotů v trénovacích datech, jejichž získání je nákladné.
Navrhujeme metodu, která tento požadavek eliminuje:
K identifikaci potenciálních kandidátů na sloty využíváme slabou supervizi z existujících modelů pro lingvistickou anotaci a poté automaticky identifikujeme doménově relevantní sloty pomocí clusterovacích algoritmů.
Dále používáme výslednou anotaci slotů k natrénování taggeru založeného na neuronové síti, který je schopen provádět tagování slotů bez lidského zásahu. 
Tento tagger je trénován výhradně na výstupech naší metody, a není tedy závislý na žádných označených datech.

Náš model vykazuje špičkový výkon v označování slotů bez anotovaných trénovacích dat na čtyřech různých dialogových doménách. Kromě toho jsme zjistili, že anotace slotů zjištěné naším modelem výrazně zlepšují výkonnost end-to-end modelu pro generování odpovědi v dialogu v porovnání s modelem, který anotace slotů vůbec nepoužívá.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Task-oriented dialogue systems typically require manual annotation of dialogue slots in training data, which is costly to obtain.
We propose a method that eliminates this requirement:
We use weak supervision from existing linguistic annotation models to identify potential slot candidates, then automatically identify domain-relevant slots by using clustering algorithms.
Furthermore, we use the resulting slot annotation to train a neural-network-based tagger that is able to perform slot tagging with no human intervention. 
This tagger is trained solely on the outputs of our method and thus does not rely on any labeled data.

Our model demonstrates state-of-the-art performance in slot tagging without labeled training data on four different dialogue domains. Moreover, we find that slot annotations discovered by our model significantly improve the performance of an end-to-end dialogue response generation model, compared to using no slot annotation at all.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Porozumění textu je jednou z klíčových dovedností, které se učíme během školních let. Jednak je  základem úspěšného porozumění čtenářská gramotnost, jednak textové vlastnosti, tj. jeho srozumitelnost. Srozumitelnosttak představuje
důležitý problém - jak recepce (lidé nemusí být schopni plně porozumět textu kvůli)
k jeho nízké čitelnosti), tak produkce (lidé nemusí být schopni vytvořit dobře srozumitelný
text). Současná studie představuje experimentální metodu měření srozumitelnosti českých textů
různých žánrů na základě triangulace (1) míry správných odpovědí na porozumění
otázky, (2) intratextových rysů a (3) subjektivního hodnocení čitelnosti textu. Metoda
je podrobně diskutována a jednotlivé kroky jsou ilustrovány na příkladu probíhajícího výzkumu
v rámci projektu Lingvistické faktory pochopitelnosti v českých administrativních a vzdělávacích textech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Text comprehension is one of the key skills that are learned during the school years. On the one
hand, reading literacy is fundamental for successful comprehension, on the other hand, comprehension
success is determined by textual features, i.e. its readability. Text readability thus presents an
important issue — from both the reception (people may not be able to comprehend a text fully due
to its low readability) and production perspective (people may not be able to produce a well readable
text). The current study presents an experimental method for measuring readability of Czech texts
of various genres based on the triangulation of (1) the rate of correct answers on comprehension
questions, (2) intra-textual features, and (3) subjective assessment of text readability. The method
is discussed in depth and individual steps are illustrated on an example of a research in progress
under the project Linguistic Factors of Readability in Czech Administrative and Educational Texts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Toto je rozpracovaný dokument o hodnotící
analýze různých přístupů k nakládání s reprezentacemi faktů a postojů v lingvistice. Cílem
této srovnávací analýzy je najít nejvhodnější přístup pro vývoj schématu anotací za účelem vybudování slovníku postojových výrazů v dalších fázích projektu. Mezi analyzovanými přístupy
jsou Appraisal Theory, schéma vyvinuté
Bednárkovou a také analýza sentimentu a techniky opinion miningu a argument miningu. Výsledky tohoto článku by měly být považovány za první
krok ve výzkumu rozlišování faktů a postojů v
diplomatických projevech Rady bezpečnosti OSN.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This is a work-in-progress paper on evaluative
analysis of different approaches to managing representations of facts and attitudes in linguistics. The aim of
this comparative analysis is to find the most suitable approach for developing an annotation scheme in order to
build a dictionary of attitudinal expressions during the further stages of the project. Among the approaches analyzed
are the Appraisal Theory, the scheme developed by
Bednarek, as well as sentiment analysis and opinion
mining techniques, and argument mining. The
results of the current paper should be considered as the first
step in the research of distinguishing facts and attitudes in
diplomatic speeches of the United Nations Security Council.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Strojový překlad je úkol automatického překládání textu z jednoho jazyka do druhého. V posledních letech tomuto oboru dominovala řešení založená na neuronových sítích.
V této prezentaci uvedu krátký úvod k tématu a zmíním několik postupů, které lze použít ke zlepšení konečné kvality překladu. Podělím se také o své zkušenosti z účasti na jedné z nejdéle probíhajících mezinárodních strojově překladatelských soutěží.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Machine Translation is a task of automatically translating text from one language
into another. In recent years, solutions based on neural networks have dominated
the field.
In this talk I will give a short introduction to the topic and mention a few tricks
that can be used to improve the final quality of translation. I will also share my
experience from participating in one of the longest-running international Machine
Translation competitions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci ukazujeme, že automaticky generované otázky a odpovědi mohou být použity k hodnocení kvality strojových překladatelských systémů. V návaznosti na nedávnou práci na hodnocení abstraktní sumarizace textu navrhujeme novou metriku pro systémové hodnocení strojového překladu, porovnáme ji s ostatními nejmodernějšími řešeními a prokážeme její robustnost provedením experimentů pro různé překladatelské směry.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we show that automatically-generated questions and answers can be used to evaluate the quality of Machine Translation systems. Building on recent work on the evaluation of abstractive text summarization, we propose a new metric for system-level Machine Translation evaluation, compare it with other state-of-the-art solutions, and show its robustness by conducting experiments for various translation directions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto dokumentu popisujeme naši účast ve WMT 2021 Metrics Shared Task. Automaticky generované otázky a odpovědi používáme k hodnocení kvality systémů strojového překladu. Naše experimenty jsou založeny na nedávno navržené metodě MTEQA. Pokusy s vyhodnocovacími datovými soubory WMT20 ukazují, že na systémové úrovni dosahuje MTEQA výkonu srovnatelného s jinými nejmodernějšími řešeními, přičemž zohledňuje je velmi omezené informace z celého překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we describe our submission to the WMT 2021 Metrics Shared Task. We use the automatically-generated questions and answers to evaluate the quality of Machine Translation (MT) systems. Our submission builds upon the recently proposed MTEQA framework. Experiments on WMT20 evaluation datasets show that at the system-level the MTEQA metric achieves performance comparable with other state-of-the-art solutions, while considering only a certain amount of information from the whole translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Řešení úloh z benchmarku SCAN, který testuje schopnost neuronových sítí využít kompozicionalitu. Návrh nové úlohy, kde prezentované řešení selhává.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We address the compositionality challenge presented by the SCAN benchmark. Using data augmentation and a modification of the standard seq2seq architecture with attention, we achieve SOTA results on all the relevant tasks from the benchmark, showing the models can generalize to words used in unseen contexts. We propose an extension of the benchmark by a harder task, which cannot be solved by the
proposed method.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>In most of neural machine translation distillation or stealing scenarios, the highest-scoring hypothesis of the target model (teacher) is used to train a new model (student). If reference translations are also available, then better hypotheses (with respect to the references) can be oversampled and poor hypotheses either removed or undersampled. This paper explores the sampling method landscape (pruning, hypothesis oversampling and undersampling, deduplication and their combination) with English to Czech and English to German MT models using standard MT evaluation metrics. We show that careful oversampling and combination with the original data leads to better performance when compared to training only on the original or synthesized data or their direct combination.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Ve většině scénářů destilace nebo krádeže neuronových strojových překladů se k výcviku nového modelu (studenta) používá hypotéza s nejvyšším bodovým ohodnocením cílového modelu (učitele). Jsou-li k dispozici i referenční překlady, pak lze lepší hypotézy (s ohledem na odkazy) přetížit a špatné hypotézy buď odstranit, nebo podtrhnout. Tento dokument zkoumá prostředí metody odběru vzorků (prořezávání, hypotetické nadměrné vzorkování a nedostatečné vzorkování, deduplikace a jejich kombinace) s anglickými až českými a anglickými až německými modely MT pomocí standardních metrik hodnocení MT. Ukazujeme, že pečlivé nadměrné vzorkování a kombinace s původními údaji vede k lepším výsledkům ve srovnání se školením pouze o původních nebo syntetizovaných údajích nebo jejich přímé kombinaci.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek zkoumá vlyv odhadování kvality překladu na slovní úrovni, zpětného překladu a automatického parafrázování na kvalitu odchozího překladu do jazyka, kterému jeho uživatelé nerozumí, a subjektivní důvěru uživatelů v kvalitu tohoto překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Translating text into a language unknown to the text’s author, dubbed outbound translation, is a modern need for which the user experience has significant room for improvement, beyond the basic machine translation facility. We demonstrate this by showing three ways in which user confidence in the outbound translation, as well as its overall final quality, can be affected: backward translation, quality estimation (with alignment) and source paraphrasing. In this paper, we describe an experiment on outbound translation from English to Czech and Estonian. We examine the effects of each proposed feedback module and further focus on how the quality of machine translation systems influence these findings and the user perception of success. We show that backward translation feedback has a mixed effect on the whole process: it increases user confidence in the produced translation, but not the objective quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Testujeme přirozené očekávání, že použití strojového překladu (MT) v profesionálním překladu ušetří lidem čas. Poslední takovou studii provedli Sanchez-Torron a Koehn (2016) s frázovým MT, čímž uměle snížili kvalitu překladu. My se oproti tomu zaměřujeme na vysoce kvalitní neuronový strojový překlad (NMT), který převyšuje kvalitu frázového MT a také ho přijala většina překladatelských společností.
Prostřednictvím experimentální studie, do níž se zapojilo přes 30 profesionálních překladatelů pro anglicko-český překlad, zkoumáme vztah mezi výkonem NMT a časem a kvalitou post-editace. Ve všech modelech jsme zjistili, že lepší systémy MT skutečně vedou k menšímu počtu změn vět v tomto průmyslovém prostředí. Vztah mezi systémovou kvalitou a dobou post-editace však není jednoznačný a na rozdíl od výsledků frázového MT není BLEU stabilním prediktorem času post-editace či konečné výstupní kvality.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We test the natural expectation that using MT in professional translation saves human processing time. The last such study was carried out by Sanchez-Torron and Koehn (2016) with phrase-based MT, artificially reducing the translation quality. In contrast, we focus on neural MT (NMT) of high quality, which has become the state-of-the-art approach since then and also got adopted by most translation companies.
Through an experimental study involving over 30 professional translators for English -> Czech translation, we examine the relationship between NMT performance and post-editing time and quality. Across all models, we found that better MT systems indeed lead to fewer changes in the sentences in this industry setting. The relation between system quality and post-editing time is however not straightforward and, contrary to the results on phrase-based MT, BLEU is definitely not a stable predictor of the time or final output quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>End-to-end neurální systémy automatického rozpoznávání řeči dosáhly v poslední době nejmodernějších výsledků, ale vyžadují velké datové sady a rozsáhlé výpočetní zdroje.
Přenosové učení bylo navrženo k překonání těchto obtíží i napříč jazyky, např. německý ASR trénovaný podle anglického modelu.
Experimentujeme s mnohem méně příbuznými jazyky, přičemž pro české ASR znovu používáme anglický model. Pro zjednodušení převodu navrhujeme používat přechodnou abecedu, češtinu bez přízvuků, a dokládáme, že jde o vysoce efektivní strategii. Technika je užitečná i na samotných českých datech, ve stylu tréninku „coarse-to-fine“.
Dosahujeme podstatného zkrácení doby tréninku a také word error rate (WER).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>End-to-end neural automatic speech recognition systems achieved recently  state-of-the-art results but they require large datasets and extensive computing resources.
Transfer learning has been proposed to overcome these difficulties even across languages, e.g., German ASR trained from an English model.
We experiment with much less related languages, reusing an English model for Czech ASR. To simplify the transfer, we propose to use an intermediate alphabet, Czech without accents, and we document that it is a highly effective strategy. The technique is also useful on Czech data alone, in the style of "coarse-to-fine" training.
We achieve substantial reductions in training time as well as word error rate (WER).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje náš účastnický systém ve úloze Explainable quality estimation of 2nd Workshop on Evaluation &amp; Comparison of NLP Systems. Úkolem odhadu kvality (QE, neboli bezreferenční hodnocení) je předpovídat kvalitu výstupu MT v čase odvození bez přístupu k referenčním překladům. V této navrhované práci nejprve vytvoříme model odhadu kvality na úrovni slov a poté tento model doladíme pro QE na úrovni vět. Námi navržené modely dosahují téměř nejmodernějších výsledků. V QE na úrovni slov se umístíme na 2. a 3. místě v testovaných sadách Ro-En a Et-En pod dohledem. V QE na úrovni vět dosahujeme relativního zlepšení 8,86% (Ro-En) a 10,6% (Et-En) ve smyslu Pearsonova korelačního koeficientu oproti základnímu modelu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes our participating system in the shared task Explainable quality estimation of 2nd Workshop on Evaluation &amp; Comparison of NLP Systems. The task of quality estimation (QE, a.k.a. reference-free evaluation) is to predict the quality of MT output at inference time without access to reference translations. In this proposed work, we first build a word-level quality estimation model, then we finetune this model for sentence-level QE. Our proposed models achieve near state-of-the-art results. In the word-level QE, we place 2nd and 3rd on the supervised Ro-En and Et-En test sets. In the sentence-level QE, we achieve a relative improvement of 8.86% (Ro-En) and 10.6% (Et-En) in terms of the Pearson correlation coefficient over the baseline model.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku zachycujeme výsledky pilotní studie zaměřené na intenzifikátory: absolutně, naprosto a úplně. Vycházíme ze tří korpusů současné češtiny - psaného korpusu SYN2020, webového korpusu ONLINE-ARCHIVE a mluveného korpusu ORTOFON 1. Na základě paralelní anotace náhodného vzorku všech vybraných intenzifikátorů sledujeme funkce a významy těchto výrazů v kontextu.Cílem studie je vymezit vlastnosti, které jsou relevantní pro slovnědruhové určení zkoumaných výrazů a připravit podklady pro jejich disambiguaci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present a preliminary study of three intensifiers (absolutně, naprosto, úplně) based on data from three different corpora, a written corpus SYN2020, a web corpus ONLINE-ARCHIVE, and a spoken corpus ORTOFON 1. Providing a parallel annotation of a random sample of each intensifier, we focus on their functions and meanings in context. We analyse their properties in order to define those features which are relevant to their word class assignment, and to prepare grounds for the future disambiguation tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Výzkum implicitních a explicitních diskurzních vztahů v češtině, založený na srovnání dat korpusové analýzy (korpus PDiT-EDA 1.0, vytvořený pro tento výzkum) a výsledků psycholingvistických experimentů. Studie se zaměřuje na jednolitvé sémantické typy diskurzních vztahů a na jejich mezivětnou a vnitrovětnou realizaci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Research of implicit and explicit discourse relations in Czech, based on a comparison of corpus analysis data (corpus PDiT-EDA 1.0, created for this research) and the results of psycholinguistic experiments. The study focuses on different semantic types of discourse relations and on their inter-sentential and intra-sentential realization.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek je zaměřen na analýzu diskurzních konektorů a dalších diskurzních jevů v češtině na základě korpusových dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the paper, we study discourse connectives and other discourse phenomena in Czech based on corpus data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje výsledky WMT21 Metrics Shared Task. Účastníci byli požádáni o hodnocení výstupů překladatelských systémů soutěžících v překladatelské úloze WMT21 News pomocí automatických metrik pro dvě různé oblasti: zpravodajství a TED talks. Všechny metriky byly hodnoceny podle toho, jak dobře korelují na úrovni systému a segmentu s lidským hodnocením. Na rozdíl od předchozích ročníků jsme letos získali vlastní lidská hodnocení na základě expertního lidského hodnocení prostřednictvím vícerozměrných metrik kvality (MQM). Toto nastavení mělo několik výhod:
(i) ukázalo se, že expertní hodnocení je spolehlivější, (ii) byli jsme schopni vyhodnotit všechny metriky na dvou různých doménách s použitím překladů stejných systémů MT, (iii) během vývoje systému jsme přidali 5 dalších překladů pocházejících ze stejného systému. Kromě toho jsme navrhli tři
sady výzev, které hodnotí robustnost všech automatických metrik. Předkládáme rozsáhlou analýzu toho, jak dobře metriky fungují na třech jazykových dvojicích: angličtina→němčina, angličtina→ruština a čínština→angličtina. Dále ukazujeme vliv různých referenčních překladů metriky a porovnáváme naši expertní anotaci MQM s DA skóre získanými pomocí WMT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the WMT21
Metrics Shared Task. Participants were asked
to score the outputs of the translation systems competing in the WMT21 News Translation Task with automatic metrics on two different domains: news and TED talks. All metrics were evaluated on how well they correlate at the system- and segment-level with
human ratings. Contrary to previous years’
editions, this year we acquired our own human ratings based on expert-based human evaluation via Multidimensional Quality Metrics
(MQM). This setup had several advantages:
(i) expert-based evaluation has been shown
to be more reliable, (ii) we were able to
evaluate all metrics on two different domains
using translations of the same MT systems,
(iii) we added 5 additional translations coming from the same system during system development. In addition, we designed three
challenge sets that evaluate the robustness of
all automatic metrics. We present an extensive analysis on how well metrics perform
on three language pairs: English→German,
English→Russian and Chinese→English. We
further show the impact of different reference
translations on reference-based metrics and
compare our expert-based MQM annotation
with the DA scores acquired by WMT.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dokument prezentuje nový, sjednocený morfologický popis číslovek a zájmen, který byl zkompilován pro nejnovější vydání Pražských závislostních korpusů (Prague Dependency Treebank – Consolidated 1.0) a jeho nedílnou součást je morfologický slovník MorfFlex. Na základě zkušeností s anotací skutečných dat a s užíváním morfologického slovníku byly navrženy konkrétní změny. Pro oba slovní druhy byl navržen nový soubor podtypů, založený zejména na morfologickém kritériu a jeho kombinaci se sémantickými vlastnostmi a dalšími relevantními rysy, jako je ne/určitost u číslovek a posesivita, reflexivita a klitičnost u zájmen. Každý podtyp má specifickou hodnotu na 2. pozici morfologické značky, která slouží také jako ukazatel použitelnosti dalších kategorií ve značce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents a novel and unified morphological description of numerals and pronouns, as compiled for the newest edition of the Prague Dependency Treebank (Prague Dependency Treebank – Consolidated 1.0) and its integral part the morphological dictionary MorfFlex. On the basis of considerable experience with real data annotation and the use of the morphological dictionary, particular changes were proposed. For both of the parts of speech a new set of subtypes was proposed, based mainly on the morphological criterion and its combination with semantic properties and other relevant features, such as definiteness in numerals and possessivity, reflexivity and clitichood in pronouns. Each subtype has a specific value at the 2nd position of the morphological tag which serves also as an indicator of the applicability of other tag categories.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme neautoregresivní přístup k opravě gramatiky založený na znacích s automaticky generovanými transformacemi znaků. Nedávno byla jako alternativa k současným systémům pro opravu gramatiky typu enkodér-dekodér navržena klasifikace korekčních oprav jednotlivých slov. Ukazujeme, že náhrada celých slov může být neoptimální a může vést k explozi počtu pravidel pro opravy typu překlepů, diakritizační opravy a opravy v morfologicky bohatých jazycích, a proto navrhujeme metodu pro generování transformací znaků z korpusu pro opravu gramatiky. Dále jsme natrénovali znakové transformační modely pro češtinu, němčinu a ruštinu a dosáhli jsme solidních výsledků a dramatického zrychlení ve srovnání s autoregresivními systémy. Zdrojový kód je zveřejněn zde: https://github.com/ufal/wnut2021_character_transformations_gec.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We propose a character-based non-autoregressive GEC approach, with automatically generated character transformations. Recently, per-word classification of correction edits has proven an efficient, parallelizable alternative to current encoder-decoder GEC systems. We show that word replacement edits may be suboptimal and lead to explosion of rules for spelling, diacritization and errors in morphologically rich languages, and propose a method for generating character transformations from GEC corpus. Finally, we train character transformation models for Czech, German and Russian, reaching solid results and dramatic speedup compared to autoregressive systems. The source code is released at https://github.com/ufal/wnut2021_character_transformations_gec.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme RobeCzech, jednojazyčnou RoBERTu, (jazykový model) trénovaný pouze na českých datech. RoBERTa je robustně optimalizovaný přístup pro předtrénování založený na Transformeru. V příspěvku ukazujeme, že RobeCzech výrazně překonává podobně velké vícejazyčné i české kontextualizované modely a zlepšuje současné výsledky v pěti vyhodnocovaných úlohách automatického jazykového zpracování, přičemž dosahuje nejlepších známých výsledků ve čtyřech z nich. Model RobeCzech je veřejně dostupný zde: https://hdl.handle.net/11234/1-3691 a zde: https://huggingface.co/ufal/robeczech-base.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present RobeCzech, a monolingual RoBERTa language representation model trained on Czech data. RoBERTa is a robustly optimized Transformer-based pretraining approach. We show that RobeCzech considerably outperforms equally-sized multilingual and Czech-trained contextualized language representation models, surpasses current state of the art in all five evaluated NLP tasks and reaches state-of-the-art results in four of them. The RobeCzech model is released publicly at https://hdl.handle.net/11234/1-3691 and https://huggingface.co/ufal/robeczech-base.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Derivations (UDer) je kolekce harmonizovaných lexikalních sítí zachycujících slovotvorné, zvl. derivační vztahy jednotlivých jazyků v témže anotačním schématu. Základní datovou strukturou tohoto anotačního schématu jsou zakořeněné stromy, ve kterých uzly odpovídají lexémům a hrany reprezentují derivační, příp. kompozitní vztahy. Stávající verze UDer v1.1 obsahuje 31 harmonizovaných zdrojů pokrývajících 21 různých jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Derivations (UDer) is a collection of harmonized lexical networks capturing word-formation, especially derivational relations, in a cross-linguistically consistent annotation scheme for many languages. The annotation scheme is based on a rooted tree data structure, in which nodes correspond to lexemes, while edges represent derivational relations or compounding. The current version of the UDer collection contains thirty-one harmonized resources covering twenty-one different languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>DeriNet.RU je lexikální síť inspirovaná formétem českého DeriNetu. Jako taková zachycuje základní vztahy pro tvorbu slov v ruštině. DeriNet.RU je vytvořen na podkladě slovníku z velkého korpusu Araneum Russicum Maius a na kombinaci gramatické komponenty, modelu strojového učení využívajícího metodu Random Forest a algoritmu Maximum Spanning Tree.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>DeriNet.RU is DeriNet-like lexical network of that captures core word-formation relations for Russian. It is based on a top of large corpus Araneum Russicum Maius corpus and a novel combination of a grammar-based component, supervised machine-learning model of Random Forest, and Maximum Spanning Tree algorithm.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Během našeho setkání se zaměříme na budoucnost divadla, scénických umění a rituálů ve věku algoritmů, dat a různých pokusů virtualizovat každý atom a okamžik našich žitých zkušeností. Přežijí naše autentické způsoby vyjadřování a lidská zkušenost spojená s údivem, experimentováním a vzdorem v metaverzu, v systémech řízených strojovým učením a umělou inteligencí? Anebo se stanou předvídatelnými efekty nových infrastruktur? Vzniká posthumanistická estetika, osvobozujeme novou tvořivost?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We will meet to reflect upon the future of theater, performance arts and rituals in an age of algorithms, data and various attempts to virtualize every atom and moment of our lived experiences. Will our genuine expressions and experience of human agency in wondering, experimenting and even disobedience survive in the metaverse, ML and AIs driven systems or become predictable effects of new infrastructures? Is there an emerging posthuman aesthetics and creativity that we are liberating?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V krátkém vstupu představíme projekt THEaiTRE a uvedeme ukázku z vygenerovaného scénáře.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We briefly introduce the THEaiTRE project and present a sample from the generated script.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nejmodernější kontextové vložení získáváme z velkých jazykových modelů dostupných pouze pro několik jazyků. U ostatních se musíme naučit reprezentace pomocí mnohojazyčného modelu. Probíhá diskuse o tom, zda lze vícejazyčné vložení sladit do prostoru sdíleného v mnoha jazycích. Ortogonální strukturální sonda (Limisiewicz a Mareček, 2021) nám umožňuje odpovědět na tuto otázku pro specifické jazykové rysy a naučit se projekci založenou pouze na jednojazyčných komentovaných datových souborech. Hodnotíme syntaktické (UD) a lexikální (WordNet) strukturální informace zakódované v mBERT kontextové reprezentaci pro devět různých jazyků. Pozorujeme, že u jazyků úzce spjatých s angličtinou není nutná žádná transformace. Vyhodnocená informace je zakódována ve sdíleném mezijazyčném vkládacím prostoru. Pro ostatní jazyky je výhodné použít ortogonální transformaci naučenou samostatně pro každý jazyk. Úspěšně aplikujeme naše zjištění na nulovou a málo natočenou analýzu přes jazyk.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>State-of-the-art contextual embeddings are obtained from large language models available only for a few languages. For others, we need to learn representations using a multilingual model. There is an ongoing debate on whether multilingual embeddings can be aligned in a space shared across many languages. The novel Orthogonal Structural Probe (Limisiewicz and Mareček, 2021) allows us to answer this question for specific linguistic features and learn a projection based only on mono-lingual annotated datasets. We evaluate syntactic (UD) and lexical (WordNet) structural information encoded inmBERT's contextual representations for nine diverse languages. We observe that for languages closely related to English, no transformation is needed. The evaluated information is encoded in a shared cross-lingual embedding space. For other languages, it is beneficial to apply orthogonal transformation learned separately for each language. We successfully apply our findings to zero-shot and few-shot cross-lingual parsing.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nejmodernější kontextové vložení získáváme z velkých jazykových modelů dostupných pouze pro několik jazyků. U ostatních se musíme naučit reprezentace pomocí mnohojazyčného modelu. Probíhá diskuse o tom, zda lze vícejazyčné vložení sladit do prostoru sdíleného v mnoha jazycích.
Ortogonální strukturální sonda (Limisiewicz a Mareček, 2021) nám umožňuje odpovědět na tuto otázku pro specifické jazykové rysy a naučit se projekci založenou pouze na jednojazyčných komentovaných datových souborech. Hodnotíme syntaktické (UD) a lexikální (WordNet) strukturální informace zakódované v mBERT kontextové reprezentaci pro devět různých jazyků.
Pozorujeme, že u jazyků úzce spjatých s angličtinou není nutná žádná transformace. Vyhodnocená informace je zakódována ve sdíleném mezijazyčném vkládacím prostoru. Pro ostatní jazyky je výhodné použít ortogonální transformaci naučenou samostatně pro každý jazyk. Úspěšně aplikujeme naše zjištění na nulovou a málo natočenou analýzu přes jazyk.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>State-of-the-art contextual embeddings are obtained from large language models available only for a few languages. For others, we need to learn representations using a multilingual model. There is an ongoing debate on whether multilingual embeddings can be aligned in a space shared across many languages.
The novel Orthogonal Structural Probe (Limisiewicz and Mareček, 2021) allows us to answer this question for specific linguistic features and learn a projection based only on mono-lingual annotated datasets. We evaluate syntactic (UD) and lexical (WordNet) structural information encoded inmBERT's contextual representations for nine diverse languages.
We observe that for languages closely related to English, no transformation is needed. The evaluated information is encoded in a shared cross-lingual embedding space. For other languages, it is beneficial to apply orthogonal transformation learned separately for each language. We successfully apply our findings to zero-shot and few-shot cross-lingual parsing.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vzhledem k nedávnému úspěchu předcvičených modelů v NLP byla velká pozornost věnována interpretaci jejich vyjádření. Jedním z nejvýraznějších přístupů je strukturální sondování (Hewitt a Manning, 2019), kde se provádí lineární projekce slovních vložek s cílem přiblížit topologii závislostních struktur. Při této práci zavedeme nový typ strukturálního sondování, kdy se lineární projekce rozloží na 1. izomorfní prostorovou rotaci, 2. lineární škálování, které určí a změří nejdůležitější rozměry. Kromě syntaktické závislosti hodnotíme naši metodu na dvou neotřelých úkolech (lexikální hypernymie a pozice ve větě). Společně cvičíme sondy pro více úkolů a experimentálně ukazujeme, že lexikální a syntaktické informace jsou v reprezentacích odděleny. Díky ortogonálnímu omezení jsou navíc Strukturální sondy méně náchylné k memorování.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>With the recent success of pre-trained models in NLP, a significant focus was put on interpreting their representations. One of the most prominent approaches is structural probing (Hewitt and Manning, 2019), where a linear projection of word embeddings is performed in order to approximate the topology of dependency structures. In this work, we introduce a new type of structural probing, where the linear projection is decomposed into 1. iso-morphic space rotation; 2. linear scaling that identifies and scales the most relevant dimensions. In addition to syntactic dependency, we evaluate our method on two novel tasks (lexical hypernymy and position in a sentence). We jointly train the probes for multiple tasks and experimentally show that lexical and syntactic information is separated in the representations. Moreover, the orthogonal constraint makes the Structural Probes less vulnerable to memorization.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vzhledem k nedávnému úspěchu předcvičených modelů v NLP byla velká pozornost věnována interpretaci jejich vyjádření. Jedním z nejvýraznějších přístupů je strukturální sondování (Hewitt a Manning, 2019), kde se provádí lineární projekce slovních vložek s cílem přiblížit topologii závislostních struktur. Při této práci zavedeme nový typ strukturálního sondování, kdy se lineární projekce rozloží na 1. izomorfní prostorovou rotaci, 2. lineární škálování, které určí a změří nejdůležitější rozměry. Kromě syntaktické závislosti hodnotíme naši metodu na dvou neotřelých úkolech (lexikální hypernymie a pozice ve větě). Společně cvičíme sondy pro více úkolů a experimentálně ukazujeme, že lexikální a syntaktické informace jsou v reprezentacích odděleny. Díky ortogonálnímu omezení jsou navíc Strukturální sondy méně náchylné k memorování</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>With the recent success of pre-trained models in NLP, a significant focus was put on interpreting their representations. One of the most prominent approaches is structural probing (Hewitt and Manning, 2019), where a linear projection of word embeddings is performed in order to approximate the topology of dependency structures. In this work, we introduce a new type of structural probing, where the linear projection is decomposed into 1. iso-morphic space rotation; 2. linear scaling that identifies and scales the most relevant dimensions. In addition to syntactic dependency, we evaluate our method on two novel tasks (lexical hypernymy and position in a sentence). We jointly train the probes for multiple tasks and experimentally show that lexical and syntactic information is separated in the representations. Moreover, the orthogonal constraint makes the Structural Probes less vulnerable to memorization.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ačkoli mobilita a pohyb získaly v poslední době na významu v interakcionistickém výzkumu společenského jednání, není příliš známo, jaké důsledky má pohyb pro konkrétní vývoj interakčních epizod. Prostřednictvím dvou veřejně přístupných videoklipů zachycujících situace „silniční zuřivosti“ popisujeme a analyzujeme stěžejní rysy práce rukou při eskalaci a úpadku emocionálně nabité interakce mezi účastníky silničního provozu. Vyhýbáme se apriorně kognitivistickému postoji a v detailech ukazujeme, jak může být práce rukou sama o sobě konstituentem vzteku a že může vést k otevřenému konfliktu na hranici fyzického násilí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Although mobility and movement has recently gained importance within interactionist studies of social action, not much is known about the consequentiality of being on the move for the particular unfolding of interactional episodes. Utilising two publicly accessible video clips of ‘road rage’ situations, we describe and analyse the centrality of hand-work in the escalation and decline of an emotionally charged interaction between members of traffic. Avoiding an a priori cognitivist stance, we show in detail how the work of hands can be constitutive of anger itself, and that it can lead to open conflict on the boundary of physical violence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládaná studie se zabývá kvalitativní analýzou specifické skupiny vztahů textové koherence, a to takovými vztahy, které byly v anotaci Pražského diskurzního korpusu označeny jako pragmatické či „nepravé“, na rozdíl od většinově se vyskytujících vztahů sémantických.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The study presents a qualitative analysis of pragmatic discourse relations as annotated on Czech
texts of the Prague Dependency Treebank. A detailed study of these relations (as opposed to semantic
relations) with their widest contexts reveals a considerable diversity and shows some space
for improvement in the annotation scheme.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek zkoumá možnosti a limity lokální analýzy textové koherence s ohledem na jevy globální koherence a vyšší výstavby textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present article investigates possibilities and limits of local (shallow) analysis of discourse coherence with respect to the phenomena of global coherence and higher composition of texts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pandemie Covid-19 vyvolala globální poptávku po přesných a aktuálních informacích, které často pocházejí z angličtiny a je třeba je přeložit. K natrénování systému strojového překladu pro tak úzké téma využíváme doménová trénovací data v jiných jazycích, a to jak z příbuzných, tak ze vzdálených jazykových rodin. Experimentujeme s různými rozvrhy učení pomocí metody transfer learning a pozorujeme, že přenos prostřednictvím více než jednoho pomocného jazyka přináší největší zlepšení. Porovnáváme výstupy s mnohojazyčným trénováním a nacházíme lepší výsledky při použití transfer learningu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Covid-19 pandemic has created a global demand for accurate and up-to-date information which often originates in English and needs to be translated. To train a machine translation system for such a narrow topic, we leverage in-domain training data in other languages both from related and unrelated language families. We experiment with different transfer learning schedules and observe that transferring via more than one auxiliary language brings the most improvement. We compare the performance with joint multilingual training and report superior results of the transfer learning approach.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme naše dva NMT systémy, které byly odeslány do soutěže WMT2021 v anglicko-českém překladu zpráv: CUNI-DocTransformer (document-level CUBBITT) a CUNI-Marian-Baselines. První z nich vylepšujeme lepším předzpracováním segmentace vět a následným zpracováním pro opravu chyb v číslech a jednotkách. Druhý z nich používáme při experimentech s různými variantami techniky backtranslation.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe our two NMT systems submitted to the WMT2021 shared task in English-Czech news translation: CUNI-DocTransformer (document-level CUBBITT) and CUNI-Marian-Baselines.
We improve the former with a better sentence-segmentation pre-processing
 and a post-processing for fixing errors in numbers and units.
We use the latter for experiments with various backtranslation techniques.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme příspěvek Charles-UPF pro sdílenou úlohu o hodnocení přesnosti generovaných textů na konferenci INLG 2021. Náš systém dokáže automaticky detekovat chyby pomocí kombinace systému pro generování přirozeného jazyka založeného na pravidlech a předtrénovaných jazykových modelů. Nejprve využíváme systém pro generování přirozeného jazyka založený na pravidlech, který generuje fakta odvozená ze vstupních dat. Pro každou větu, kterou vyhodnocujeme, vybereme podmnožinu faktů, které jsou relevantní na základě měření sémantické podobnosti s danou větou. Nakonec dotrénujeme předtrénovaný jazykový model na anotovaných datech spolu s relevantními fakty pro jemnou detekci chyb. Na testovací sadě dosahujeme 69% výtěžnosti (recall) a 75% přesnosti (precision) s modelem natrénovaným na mixu dat anotovaných lidmi a syntetických dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our Charles-UPF submission for the Shared Task on Evaluating Accuracy in Generated Texts at INLG 2021. Our system can detect the errors automatically using a combination of a rule-based natural language generation (NLG) system and pretrained language models (LMs). We first utilize a rule-based NLG system to generate sentences with
facts that can be derived from the input. For each sentence we evaluate, we select a subset of facts which are relevant by measuring semantic similarity to the sentence in question. Finally, we finetune a pretrained language model on annotated data along with the relevant facts for fine-grained error detection. On the test set, we achieve 69% recall and 75% precision with a model trained on a mixture of human-annotated and synthetic data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek představuje poloautomatickou metodu pro konstruování slovotvorných sítí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article presents a semi-automatic method for the construction of word-formation networks focusing particularly on derivation. The proposed approach applies a sequential pattern mining technique to construct useful morphological features in an unsupervised manner. The features take the form of regular expressions and later are used to feed a machine-learned ranking model. The network is constructed by applying the learned model to sort the lists of possible base words and selecting the most probable ones. This approach, besides relatively small training set and a lexicon, does not require any additional language resources such as a list of vowel and consonant alternations, part-of-speech tags etc. The proposed approach is evaluated on lexeme sets of four languages, namely Polish, Spanish, Czech, and French. The conducted experiments demonstrate the ability of the proposed method to construct linguistically adequate word-formation networks from small training sets. Furthermore, the performed feasibility study shows that the method can further benefit from  the interaction with a human language expert within the  active learning framework.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška představuje výsledky na konferenci WMT21 o překladu s velmi malým množstvím zdrojů a bez paralelních dat a soutěží příspěvek, se kterým se účastil tým LMU Mnichov. Dále se probíráme designová rozhodnutími, která museli účastníci soutěže udělat a spekulujeme o tom, jaké by mohly být budoucí směry ve strojovém překladu překladů s malými zdroji.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk presents overall task findings and LMU Munich submission to the very low resource and unsupervised translation at WMT21. Further, we discuss design decisions made both by LMU and other shared task participants and speculate what the future directions in machine translation of low resource translation might be.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme závěry soutěžního ve strojovém překladu bez paralelních data nebo s velmi málo paralelními daty na WMT2021. V rámci tohoto úkolu se komunita zabývala strojovým překladem s velmi málo paralelními daty  mezi němčinou a hornolužickou srbštinou, překladem bez paralelních dat mezi němčinou a dolnolužickou srbštinou a překladem s málo paralelními daty mezi ruštinou a čuvašštinou, všemi menšinovými jazyky s aktivními jazykovými komunitami pracujícími na zachování těchto jazyků. Díky tomu se nám podařilo získat většinu digitálních dat dostupných pro tyto jazyky a nabídnout je účastníkům úkolu. Soutěžního úkolu se účastnilo celkem šest týmů. Článek představuje úkoly a výsledky a pojednává o osvědčených postupech do budoucna.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the findings of the WMT2021 Shared Tasks in Unsupervised MT and Very Low Resource Supervised MT. Within the task, the community studied very low resource translation between German and Upper Sorbian, unsupervised translation between German and Lower Sorbian and low resource translation between Russian and Chuvash, all minority languages with active language communities working on preserving the languages, who are partners in the evaluation. Thanks to this, we were able to obtain most digital data available for these languages and offer them to the task participants. In total, six teams participated in the shared task. The paper discusses the background, presents the tasks and results, and discusses best practices for the future.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme naše systémy pro soutěžní úkol v strojovém překladu bez paralelní dat a s velmi málo paralelními data na WMT21: mezi němčinou a hornolužickou srbštinou, němčinou a dolnolužickou srbštinou a ruštinou a čuvašštinou. Naše nízkozdrojové systémy (němčina↔hornolužická srbština, ruština↔čuvaština) jsou předtrénovány na párech příbuzných jazyků s dostatkem data. Tyto systémy jsme dotrénovali pomocí dostupných autentických paralelních dat a dále vylepšili opakovaným zpětným překladem. Německo↔dolnolužický systém je inicializován nejlepším hornolužickosrbským systémem a vylepšen opakovaným zpětným překladem pouze za použití jednojazyčných dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our submissions to the WMT21 shared task in Unsupervised and Very Low-Resource machine translation between German and Upper Sorbian, German and Lower Sorbian, and Russian and Chuvash. Our low-resource systems (German↔Upper Sorbian, Russian↔Chuvash) are pre-trained on high-resource pairs of related languages. We fine-tune those systems using the available authentic parallel data and improve by iterated back-translation. The unsupervised German↔Lower Sorbian system is initialized by the best Upper Sorbian system and improved by iterated back-translation using monolingual data only.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pozorujeme, že různé druhy chyb, kterých se dopouštějí systémy pro generování přirozeného jazyka, jsou velmi málo reportovány v literatuře. To je problém, protože chyby jsou důležitým ukazatelem toho, kde by se systémy měly ještě zlepšit. Pokud autoři uvádějí pouze celkové metriky výkonnosti, zůstává výzkumná komunita v nevědomosti o konkrétních nedostatcích v nejmodernějších přístupech. Vedle kvantifikace rozsahu nedostatečného vykazování chyb tento článek poskytuje doporučení pro identifikaci, analýzu a vykazování chyb.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We observe a severe under-reporting of the different kinds of errors that Natural Language Generation systems make. This is a problem, because mistakes are an important indicator of where systems should still be improved. If authors only report overall performance metrics, the research community is left in the dark about the specific weaknesses that are exhibited by ‘state-of-the-art’ research. Next to quantifying the extent of error under-reporting, this position paper provides recommendations for error identification, analysis and reporting.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme scénář divadelní hry AI: When a Robot Writes a Play (AI: Když robot píše hru), kterou napsala umělá inteligence v rámci projektu THEaiTRE.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the script of the theatre play AI: When a Robot Writes a Play (AI: Když robot píše hru), which was written by artificial intelligence within the THEaiTRE project.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška úvodem krátce představí některé úlohy, kterými se zabývá počítačová lingvistika, například automatickou opravu překlepů/gramatiky či automatický větný rozbor.
Hlavní pozornost bude věnována úloze strojového překladu, zejména vývoji různých typů překladačů z angličtiny do češtiny během posledního desetiletí. Dnešní nejlepší překladače jsou založeny na technologiích umělé inteligence, konkrétně hlubokých neuronových sítí, a kvalita výsledného překladu se blíží úrovni profesionální překladatelské agentury. Některé překladové chyby jsou způsobeny překládáním jednotlivých vět nezávisle, což řeší tzv. document-level machine translation.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The lecture will briefly introduce some of the tasks dealt with in computer linguistics, such as automatic correction of misspellings/grammar or automatic sentence analysis.
The main focus will be on the role of machine translation, in particular the development of different types of English-to-Czech translators over the last decade. Today's best translators are based on artificial intelligence technologies, namely deep neural networks, and the quality of the resulting translation is close to that of a professional translation agency. Some translation errors are caused by translating sentences independently, which is being solved by so-called document-level machine translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Existují desítky datových zdrojů pro různé jazyky, ve kterých je ručně anotovaná koreference – vztah mezi dvěma nebo více výrazy, které odkazují na tutéž entitu v reálném světě. Dalo by se předpokládat, že takové výrazy obvykle tvoří syntakticky významné jednotky; avšak rozsahy koreferenčních výrazů (zmínek) byly ve většině projektů anotovány prostě vymezením intervalů tokenů, tj. nezávisle na jakékoli syntaktické reprezentaci. Tvrdíme, že by bylo z dlouhodobého hlediska výhodné, kdyby se k sobě anotace syntaxe a koreference přiblížily. Představujeme pilotní empirickou studii, která se zaměřuje na případy, kde koreferenční zmínky pasují nebo naopak nepasují na automaticky přiřazené syntaktické stromy, které odpovídají standardu Universal Dependencies. Studie zahrnuje 8 datových sad pro 7 různých jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>One can find dozens of data resources for various languages in which coreference – a relation between two or more expressions that refer to the same real-world entity – is manually annotated. One could also assume that such expressions usually constitute syntactically meaningful units; however, mention spans have been annotated simply by delimiting token intervals in most coreference projects, i.e., independently of any syntactic representation. We argue that it could be advantageous to make syntactic and coreference annotations convergent in the long term. We present a pilot empirical study focused on matches and mismatches between hand-annotated linear mention spans and automatically parsed syntactic trees that follow Universal Dependencies conventions. 8 datasets for 7 different languages are included in the study.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vysoký výkon velkých předcvičených jazykových modelů (LLM), jako je BERT (Devlin et al., 2019) na úkoly NLP, vyvolal otázky ohledně jazykových schopností BERT a v tom, jak se liší od lidských. V tomto příspěvku přistupujeme k této otázce zkoumáním znalostí BERT o lexikálních sémantických vztazích. Zaměřujeme se na hypernymii, vztah „je-a“, který spojuje slovo s nadřazenou kategorií. Jednoduše používáme metodiku nabádání
zeptejte se BERTe, co je hypernym daného slova. Zjistili jsme, že v prostředí, kde jsou všechny hypernymy uhodnutelné pomocí výzvy, BERT zná hypernymy s přesností až 57%. Navíc BERT s výzvou překonává ostatní modely bez dozoru pro hypernomické objevování i v neomezeném scénáři. Předpovědi a výkon BERT jsou však zapnuty
soubor dat obsahující neobvyklé hyponymy a
hypernymy naznačují, že jeho znalosti o hypernymii jsou stále omezené.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The high performance of large pretrained language models (LLMs) such as BERT (Devlin et al., 2019) on NLP tasks has prompted questions about BERT’s linguistic capabilities, and how they differ from humans’. In this paper, we approach this question by examining BERT’s knowledge of lexical semantic relations. We focus on hypernymy, the “is-a” relation that relates a word to a superordinate category. We use a prompting methodology to simply
ask BERT what the hypernym of a given word is. We find that, in a setting where all hypernyms are guessable via prompting, BERT knows hypernyms with up to 57% accuracy. Moreover, BERT with prompting outperforms other unsupervised models for hypernym discovery even in an unconstrained scenario. However, BERT’s predictions and performance on
a dataset containing uncommon hyponyms and
hypernyms indicate that its knowledge of hypernymy is still limited.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>BERTScore (Zhang et al., 2020), nedávno navržená automatická metrika kvality strojového překladu, používá BERT (Devlin et al., 2019), velký předškolený jazykový model pro hodnocení kandidátských překladů s ohledem na zlatý překlad. BERTScore využívá sémantických a syntaktických schopností BERT a snaží se vyhnout chybám dřívějších přístupů, jako je BLEU, místo toho hodnotí kandidátské překlady na základě jejich sémantické podobnosti se zlatou větou. BERT však není neomylný; zatímco jeho výkon v oblasti úkolů NLP obecně nastoluje nový stav, studie specifických syntaktických a sémantických jevů ukázaly, kde se výkon BERT odchyluje od výkonu lidí obecněji.
To přirozeně vyvolává otázky, kterými se v tomto dokumentu zabýváme: jaké jsou silné a slabé stránky BERTScore? Souvisejí s
známé slabiny na straně BERT? Zjistili jsme, že BERTScore sice dokáže odhalit, když se kandidát liší od odkazu v důležitých obsahových slovech, ale je méně citlivý na menší chyby, zejména pokud je kandidát lexikálně nebo stylisticky podobný odkazu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>BERTScore (Zhang et al., 2020), a recently proposed automatic metric for machine translation quality, uses BERT (Devlin et al., 2019), a large pre-trained language model to evaluate candidate translations with respect to a gold translation. Taking advantage of BERT’s semantic and syntactic abilities, BERTScore seeks to avoid the flaws of earlier approaches like BLEU, instead scoring candidate translations based on their semantic similarity to the gold sentence. However, BERT is not infallible; while its performance on NLP tasks set a new state of the art in general, studies of specific syntactic and semantic phenomena have shown where BERT’s performance deviates from that of humans more generally.
This naturally raises the questions we address in this paper: what are the strengths and weaknesses of BERTScore? Do they relate to
known weaknesses on the part of BERT? We find that while BERTScore can detect when a candidate differs from a reference in important content words, it is less sensitive to smaller errors, especially if the candidate is lexically or stylistically similar to the reference.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Architektury založené na transformaci sekvence na sekvenci sice dosahují nejmodernějších výsledků na velkém množství úloh zpracování přirozeného jazyka, přesto však mohou při trénovaní trpět problémem přeučení. V praxi se tomu obvykle předchází buď použitím regularizačních metod (např. dropout, L2-regularizace), nebo poskytnutím obrovského množství trénovacích dat. Navíc je známo, že Transformer a další architektury mají problém s generováním velmi dlouhých sekvencí. Například ve strojovém překladu dosahují neuronové systémy horších výsledků na velmi dlouhých sekvencích než předchozí překladové metody založené na frázovém překladu (Koehn and Knowles, 2017).
Předkládáme výsledky, které naznačují, že problém může být také v rozdílech mezi rozložením délek v trénovacích a validačních datech v kombinaci s výše uvedenou tendencí neuronových sítí přeučit se na trénovacích datech. Na jednoduché úloze editace řetězců a strojovém překladu demonstrujeme, že kvalita modelu Transformer výrazně klesá, když zpracovává sekvence délky odlišné od délek v trénovacích datech. Dále ukazujeme, že pozorovaný pokles kvality je způsoben spíše délkou hypotézy, která odpovídá délkám v trénovacích datech, než délkou vstupní sekvence.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Transformer-based sequence-to-sequence architectures, while achieving state-of-the-art results on a large number of NLP tasks, can still suffer from overfitting during training. In practice, this is usually countered either by applying regularization methods (e.g. dropout, L2-regularization) or by providing huge amounts of training data. Additionally, Transformer and other architectures are known to struggle when generating very long sequences. For example, in machine translation, the neural-based systems perform worse on very long sequences when compared to the preceding phrase-based translation approaches (Koehn and Knowles, 2017).
  We present results which suggest that the issue might also be in the mismatch between the length distributions of the training and validation data combined with the aforementioned tendency of the neural networks to overfit to the training data. We demonstrate on a simple string editing task and a machine translation task that the Transformer model performance drops significantly when facing sequences of length diverging from the length distribution in the training data. Additionally, we show that the observed drop in performance is due to the hypothesis length corresponding to the lengths seen by the model during training rather than the length of the input sequence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek přináší zprávu o průběhu a výsledcích 47. ročníku Olympiády v českém jazyce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article brings an overview and results of the 47th year of the Czech Language Olympics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku uvádíme Uniformní reprezentaci významu (UMR), navrženou k anotaci sémantického obsah textu. UMR je primárně založena na Abstract Meaning Representation (AMR), anotačním rámci původně určeném pro angličtinu, ale čerpá i z jiných významových reprezentací. UMR rozšiřuje AMR do dalších jazyků, obzvláště morfologicky složité jazyky s omezenými jazykovými zdroji. UMR také přidává do AMR funkce, které jsou kritické pro sémantiku a zlepšuje AMR navržením doprovodné dokumentární reprezentace, která zachycuje jazykové jevy jako je koreference a také časové a modální závislosti, které potenciálně přesahují hranice vět.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we present Uniform Meaning Representation (UMR), a meaning representation designed to annotate the semantic content of a text. UMR is primarily based on Abstract Meaning Representation (AMR), an annotation framework initially designed for English, but also draws from other meaning representations. UMR extends AMR to other languages, particularly morphologically complex, low-resource languages. UMR also adds features to AMR that are critical to semantic interpretation and enhances AMR by proposing a companion document-level representation that captures linguistic phenomena such as coreference as well as temporal and modal dependencies that potentially go beyond sentence boundaries.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zpráva o konání semináře Jazykovědného sdružení České republiky na počest Světly Čmejrkové a Františka Daneše.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Report on the seminar of the Linguistic Association of the Czech Republic in honour of Světla Čmejrková and František Daneš.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slovní asociace je důležitou součástí lidského jazyka. Existuje mnoho technik pro zachycení sémantických vztahů mezi slovy, ale jejich schopnost modelovat slovní asociace je zřídka testována v reálné aplikaci. V tomto článku hodnotíme tři modely zaměřené na různé typy slovních asociací: model pro vkládání slov pro synonymii, bodový model vzájemných informací pro slovní spojení a model závislosti pro společné vlastnosti slov. Kvalitu navrhovaných modelů testují lidé v angličtině a češtině v online verzi slovně-asociační hry „Codenames“.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Word association is an important part of human language. Many techniques for capturing semantic relations between words exist, but their ability to model word associations is rarely tested in a real application. In this paper, we evaluate three models aimed at different types of word associations: a word-embedding model for synonymy, a point-wise mutual information model for word collocations, and a dependency model for common properties of words. The quality of the proposed models is tested on English and Czech by humans in an online version of the word-association game “Codenames”.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jazyky, pro které není k dispozici dostatek zdrojů, představují mimořádné příležitosti, ale také obtíže při počítačovém zpracování. Nově vydaný korpus ručně anotovaných částí jorubské Bible připravuje cestu pro závislostní analýzu jorubštiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Low-resource languages present enormous NLP opportunities as well as varying degrees of difficulties. The newly released treebank of hand-annotated parts of the Yorùbá Bible provides an avenue for dependency analysis of the Yorùbá language; the application of a new grammar formalism to the language. In this paper, we discuss our choice of Universal Dependencies, important dependency annotation decisions considered in the creation of the first annotation guidelines for Yorùbá and results of our parsing experiments. We also lay the foundation for future incorporation of other domains with the initial test on Yorùbá Wikipedia articles and highlighted future directions for the rapid expansion of the treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Sborník příspěvků z mezinárodní konference Prague Visual History and Digital Humanities Conference (PraViDCo 2020), která proběhla v lednu 2020 k 10. výročí Centra vizuální historie Malach.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Given the occasion of celebrating the 10th year of the Malach CVH existence, we decided to substantially extend the program of our annual January anniversary conference, which we newly entitled Prague Visual History and Digital Humanities Conference (PraViDCo). By adopting this name, we desired to express in the broadest possible sense the thematic and methodological scope representative of the interdisciplinarity which, as we believe, is the very essence of the Malach CVH.

The second day of the conference (January 28, 2020) gave the floor to the contributions collected from mostly junior researchers from all over the world, who swiftly responded to our open call for papers and which you can find in a written form in this book, entitled Malach Center for Visual History on its 10th Anniversary. We hope that readers will appreciate this brief glimpse into the full palette of different topics, methodologies and approaches that Malach Center for Visual History came into contact with throughout the first ten years of its existence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Abstraktivní sumarizaci je notoricky těžké hodnotit, protože standardní metriky založené na překryvu slov jsou nedostatečné. Představujeme novou evaluační metriku, která je založena na vážení obsahu na úrovní faktů, tj. vztažení faktů z dokumentu k faktům ze shrnutí. Vycházíme z předpokladu, že dobré shrnutí bude odrážet všechna relevantní fakta, tj. ta, která jsou obsažena v referenčním shruntí vytvořeném člověkem. Na potvrzení této hypotézy ukazujeme, že naše váhy velmi dobře korelují s lidským hodnocením a jsou srovnatelné s nedávnou manuální metrikou Hardyho et al. (2019), založenou na zvýrazňování částí textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Abstractive summarisation is notoriously hard to evaluate since standard word-overlap-based metrics are insufficient. We introduce a new evaluation metric which is based on fact-level content weighting, i.e. relating the facts of the document to the facts of the summary. We follow the assumption that a good summary will reflect all relevant facts, i.e. the ones present in the ground truth (human-generated refer- ence summary). We confirm this hypothe- sis by showing that our weightings are highly correlated to human perception and compare favourably to the recent manual highlight- based metric of Hardy et al. (2019).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem naší práce je navrhnout vhodné morfologické značky pro popis indonéštiny v rámci Universal Dependencies a aplikovat tyto značky na existující indonéský závislostní korpus.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The objectives of our work are to propose the relevant Universal Dependencies morphological features for Indonesian dependency treebank and to apply the proposed features to an existing treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dávat smysl jazyku není snadný úkol. Tím spíš, že máme tak intimní zážitek -- stromy, se kterými se pravidelně setkáváme v každodenní komunikaci, nám mohou znesnadnit výhled na les. V této situaci nám počítače mohou pomoci udělat krok zpět a podívat se na velké množství textu z odstupu padesáti tisíc stop.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Making sense of language is no easy task. All the more since we have such an intimate experience of it -- the trees that we regularly encounter in everyday communication may make it hard to see the forest. In this situation, computers can help us take a step back and look at large quantities of text from a fifty-thousand-foot view.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje vyjádření týmu („ODIANLP“) k WAT 2020. Účastnili jsme se úkolu English→Hindi Multimodal a Indic. Pro úlohu překladu jsme použili nejmodernější model Transformeru a pro úlohu Hindi Image Captioning jsme použili Inception ResNetV2. Naše podání dosahuje nejlepších výsledků ve směru English→Hindi Multimodal a Odia↔English. Naše návrhy si vedly dobře i při vícejazyčných úkolech (Indic).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the team (“ODIANLP”)’s submission to WAT 2020. We have participated in the English→Hindi Multimodal task and Indic task. We have used the state-of-the-art Transformer model for the translation task and Inception ResNetV2 for the Hindi Image Captioning task. Our submission tops in English→Hindi Multimodal task in its track and Odia↔English translation tasks. Also, our submissions performed well in the Indic Multilingual tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příprava paralelních korpusů je náročným úkolem, zejména pro jazyky, které trpí nedostatečným zastoupením v digitálním světě. Ve vícejazyčné zemi, jako je Indie, je potřeba takových paralelních korpusů přísná pro několik jazyků s nízkými zdroji. V této práci poskytujeme rozšířený anglicko-odijský paralelní korpus OdiEnCorp 2.0 zaměřený zejména na systémy Neural Machine Translation (NMT), které pomohou přeložit angličtinu ↔ Odia. OdiEnCorp 2.0 zahrnuje stávající anglicko-odijské korpusy a sbírku jsme rozšířili o několik dalších metod získávání dat: paralelní škrábání dat z mnoha webů, včetně Odia Wikipedia, ale také optické rozpoznávání znaků (OCR) pro extrakci paralelních dat ze skenovaných obrázků. Náš přístup k extrakci dat založený na OCR pro vytváření paralelního korpusu je vhodný pro jiné jazyky s nízkými zdroji, které nemají online obsah. Výsledný OdiEnCorp 2.0 obsahuje 98 302 vět a 1,69 milionu anglických a 1,47 milionu žetonů Odia. Pokud je nám známo, OdiEnCorp 2.0 je největší Odia-anglický paralelní korpus pokrývající různé domény a je volně dostupný pro nekomerční a výzkumné účely.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The preparation of parallel corpora is a challenging task, particularly for languages that suffer from under-representation in the digital world. In a multi-lingual country like India, the need for such parallel corpora is stringent for several low-resource languages. In this work, we provide an extended English-Odia parallel corpus, OdiEnCorp 2.0, aiming particularly at Neural Machine Translation (NMT) systems which will help translate English↔Odia. OdiEnCorp 2.0 includes existing English-Odia corpora and we extended the collection by several other methods of data acquisition: parallel data scraping from many websites, including Odia Wikipedia, but also optical character recognition (OCR) to extract parallel data from scanned images. Our OCR-based data extraction approach for building a parallel corpus is suitable for other low resource languages that lack in online content. The resulting OdiEnCorp 2.0 contains 98,302 sentences and 1.69 million English and 1.47 million Odia tokens. To the best of our knowledge, OdiEnCorp 2.0 is the largest Odia-English parallel corpus covering different domains and available freely for non-commercial and research purposes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku představujeme první treebank Universal Dependencies (UD) pro spisovnou albánštinu, sestávající z 60 vět vybraných z albánské Wikipedie, anotovaných lematy, univerzálními značkami slovních druhů, morfologickými rysy a syntaktickými závislostmi. Kromě představení treebanku jako takového probíráme vybrané jazykové jevy v albánštině, jejichž analýza v UD není na první pohled jasná, včetně jádrových argumentů, zacházení s nepřímými předměty, zájmenných příklonek, konstrukcí s genitivem, preartikulovaných adjektiv a způsobových sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we introduce the first Universal Dependencies (UD) treebank for standard Albanian, consisting of 60 sentences collected from the Albanian Wikipedia, annotated with lemmas, universal part-of-speech tags, morphological features and syntactic dependencies. In addition to presenting the treebank itself, we discuss a selection of linguistic constructions in Albanian whose analysis in UD is not self-evident, including core arguments and the status of indirect objects, pronominal clitics, genitive constructions, prearticulated adjectives, and modal verbs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme PERIN, nový permutačně invariantní přístup k sémantickému parsingu věty na graf. PERIN je všestranná, anotačně a jazykově nezávislá architektura pro univerzální modelování sémantických struktur. Náš systém se zúčastnil shared tasku CoNLL 2020, Cross-Framework Meaning Representation Parsing (MRP 2020), kde byl hodnocen v pěti různých frameworcích (AMR, DRG, EDS, PTG a UCCA) napříč čtyřmi jazyky. PERIN byl jedním z vítězů. Zdrojový kód a předtrénované modely jsou k dispozici na adrese http://www.github.com/ufal/perin.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present PERIN, a novel permutation-invariant approach to sentence-to-graph semantic parsing. PERIN is a versatile, cross-framework and language independent architecture for universal modeling of semantic structures. Our system participated in the CoNLL 2020 shared task, Cross-Framework Meaning Representation Parsing (MRP 2020), where it was evaluated on five different frameworks (AMR, DRG, EDS, PTG and UCCA) across four languages. PERIN was one of the winners of the shared task. The source code and pretrained models are available at http://www.github.com/ufal/perin.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Mnohojazyčnost je kulturním úhelným kamenem Evropy a je pevně zakotvena v evropských smlouvách, včetně úplné jazykové rovnosti. Jazykové bariéry ovlivňující obchodní, mezijazykovou a mezikulturní komunikaci jsou však stále všudypřítomné. Jazykové technologie (LT) jsou mocným prostředkem k prolomení těchto bariér. I když v posledním desetiletí vzniklo množství různých iniciativ, které vytvořily množství přístupů a technologií přizpůsobených specifickým potřebám Evropy, stále existuje obrovská míra roztříštěnosti. Zároveň se umělá inteligence (AI) stala stále důležitějším konceptem v oblasti evropských informačních a komunikačních technologií. Už několik let AI – včetně mnoha příležitostí, synergií, ale i mylných představ – zastiňuje všechna ostatní témata. Představujeme přehled evropského prostředí jazykových technologí (LT), popisujeme programy financování, činnosti, akce a výzvy v jednotlivých zemích s ohledem na LT, včetně současného stavu v průmyslu a na trhu LT. Představujeme stručný přehled hlavních činností souvisejících s LT na úrovni EU za posledních deset let a představujeme strategické pokyny s ohledem na čtyři klíčové rozměry.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Multilingualism is a cultural cornerstone of Europe and firmly anchored in the European Treaties including full language equality. However, language barriers impacting business, cross-lingual and cross-cultural communication are still omnipresent. Language Technologies (LTs) are a powerful means to break down these barriers. While the last decade has seen various initiatives that created a multitude of approaches and technologies tailored to Europe’s specific needs, there is still an immense level of fragmentation. At the same time, AI has become an increasingly important concept in the European Information and Communication Technology area. For a few years now, AI – including many opportunities, synergies but also misconceptions – has been overshadowing every other topic. We present an overview of the European LT landscape, describing funding programmes, activities, actions and challenges in the different countries with regard to LT, including the current state of play in industry and the LT market. We present a brief overview of the main LT-related activities on the EU level in the last ten years and develop strategic guidance with regard to four key dimensions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>S 24 oficiálními jazyky EU a mnoha dalšími jazyky může být mnohojazyčnost v Evropě a inkluzivní jednotný digitální trh umožněn pouze prostřednictvím jazykových technologií (LT). Evropské LT podnikání ovládají stovky malých a středních podniků a několik velkých hráčů. Mnohé z nich jsou světové, s technologiemi, které předčí globální hráče. Evropské podnikání v LT je však také roztříštěné – v závislosti na národních státech, jazycích, vertikálech a odvětvích, což výrazně brzdí jeho dopad. Projekt evropské jazykové sítě (European Language Grid, ELG) řeší tuto roztříštěnost tím, že stanoví ELG jako primární platformu pro LT v Evropě. Skupina ELG je rozšiřitelná cloudová platforma, která umožňuje snadnou integraci přístupu ke stovkám komerčních i nekomerčních LT pro všechny evropské jazyky, včetně provozních nástrojů a služeb, jakož i datových souborů a zdrojů. Jakmile bude plně funkční, umožní komerční i nekomerční evropské komunitě LT ukládat a nahrávat své technologie a soubory dat do systému ELG, zavádět je do sítě a propojovat s dalšími zdroji. Skupina ELG podpoří vícejazyčný jednotný digitální trh směrem k prosperující evropské LT komunitě a vytvoří nová pracovní místa a příležitosti. Kromě toho projekt ELG organizuje dvě otevřené výzvy až pro 20 pilotních projektů. Zřizuje také 32 národních kompetenčních center a Evropskou radu LT pro osvětové a koordinační účely.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>With 24 official EU and many additional languages, multilingualism in Europe and an inclusive Digital Single Market can only be enabled through Language Technologies (LTs). European LT business is dominated by hundreds of SMEs and a few large players. Many are world-class, with technologies that outperform the global players. However, European LT business is also fragmented – by nation states, languages, verticals and sectors, significantly holding back its impact. The European Language Grid (ELG) project addresses this fragmentation by establishing the ELG as the primary platform for LT in Europe. The ELG is a scalable cloud platform, providing, in an easy-to-integrate way, access to hundreds of commercial and non-commercial LTs for all European languages, including running tools and services as well as data sets and resources. Once fully operational, it will enable the commercial and non-commercial European LT community to deposit and upload their technologies and data sets into the ELG, to deploy them through the grid, and to connect with other resources. The ELG will boost the Multilingual Digital Single Market towards a thriving European LT community, creating new jobs and opportunities. Furthermore, the ELG project organises two open calls for up to 20 pilot projects. It also sets up 32 national competence centres and the European LT Council for outreach and coordination purposes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Strojový překlad:

Některé metody automatického simultánního převodu řeči dlouhé formy umožňují revize výstupů,
přesnost obchodování pro nízkou latenci. Zavádění těchto systémů
uživatele čeká problém s prezentací titulků v omezeném prostoru, jako jsou dva řádky na televizní obrazovce. The
titulky musí být zobrazeny okamžitě, postupně a s
dostatečný čas na čtení. Poskytujeme algoritmus pro
titulkování. Dále navrhujeme způsob, jak odhadnout
celková využitelnost kombinace automatického překladu a titulkování měřením kvality, latence a
stabilitu na testovací sadě a navrhnout vylepšené opatření
kvůli zpoždění překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Some methods of automatic simultaneous translation of a long-form speech allow revisions of outputs,
trading accuracy for low latency. Deploying these systems
for users faces the problem of presenting subtitles in a limited space, such as two lines on a television screen. The
subtitles must be shown promptly, incrementally, and with
adequate time for reading. We provide an algorithm for
subtitling. Furthermore, we propose a way how to estimate
the overall usability of the combination of automatic translation and subtitling by measuring the quality, latency, and
stability on a test set, and propose an improved measure
for translation latency.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento abstrakt je pouze v angličtině:

This paper is an ELITR system submission for the non-native speech translation task at IWSLT 2020. We describe systems for offline ASR, real-time ASR, and our cascaded approach to offline SLT and real-time SLT. We select our primary candidates from a pool of pre-existing systems, develop a new end-to-end general ASR system, and a hybrid ASR trained on non-native speech. The provided small validation set prevents us from carrying out a complex validation, but we submit all the unselected candidates for contrastive evaluation on the test set.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper is an ELITR system submission for the non-native speech translation task at IWSLT 2020. We describe systems for offline ASR, real-time ASR, and our cascaded approach to offline SLT and real-time SLT. We select our primary candidates from a pool of pre-existing systems, develop a new end-to-end general ASR system, and a hybrid ASR trained on non-native speech. The provided small validation set prevents us from carrying out a complex validation, but we submit all the unselected candidates for contrastive evaluation on the test set.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zkoumáme vliv trénování modelů NMT na více cílových jazyků. Předpokládáme, že integrace více jazyků a zvýšení jazykové rozmanitosti povedou k silnějšímu zastoupení syntaktických a sémantických rysů zachycených modelem. Testujeme naši hypotézu na dvou různých architekturách NMT: široce používaná architektura transformátorů a architektura Attention Bridge. Trénujeme modely na datech Europarl a kvantifikujeme úroveň syntaktických a sémantických informací objevených modely pomocí tří různých metod: úkoly lingvistického průzkumu SentEval, analýza struktur pozornosti týkající se inherentních informací o frázích a závislostech a strukturální sonda na kontextových reprezentacích slov . Naše výsledky ukazují, že s rostoucím počtem cílových jazyků model Attention Bridge stále více získává určité jazykové vlastnosti, včetně některých syntaktických a sémantických aspektů věty, zatímco transformátorské modely jsou do značné míry nedotčeny. Posledně uvedené platí také pro frázovou strukturu a syntaktické závislosti, které se při zvyšování jazykové rozmanitosti při výcviku překladu nejeví ve větných vyjádřeních. To je docela překvapivé a může to naznačovat relativně malý vliv gramatické struktury na porozumění jazyku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We investigate the effect of training NMT models on multiple target languages. We hypothesize that the integration of multiple languages and the increase of linguistic diversity will lead to a stronger representation of syntactic and semantic features captured by the model. We test our hypothesis on two different NMT architectures: The widely-used Transformer architecture and the Attention Bridge architecture. We train models on Europarl data and quantify the level of syntactic and semantic information discovered by the models using three different methods: SentEval linguistic probing tasks, an analysis of the attention structures regarding the inherent phrase and dependency information and a structural probe on contextualized word representations. Our results show evidence that with growing number of target languages the Attention Bridge model increasingly picks up certain linguistic properties including some syntactic and semantic aspects of the sentence whereas Transformer models are largely unaffected. The latter also applies to phrase structure and syntactic dependencies that do not seem to be developing in sentence representations when increasing the linguistic diversity in training to translate. This is rather surprising and may hint on the relatively little influence of grammatical structure on language understanding.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V posledních letech dominovaly hluboké neuronové sítě v oblasti zpracování přirozeného jazyka (NLP). Modely trénované od konce do konce mohou dělat úkoly stejně zručně jako nikdy předtím a rozvíjet vlastní vyjádření jazyka. Působí však jako černé skříňky, které je velmi těžké interpretovat. To vyžaduje kontrolu, do jaké míry jsou jazykové koncepce v souladu s tím, co se modely učí. Používají neuronové sítě morfologii a syntaxi stejně jako lidé, když mluví o jazyce? Nebo si vyvinou vlastní způsob?
V naší přednášce pootevřeme neurální černou skříňku a zanalyzujeme vnitřní reprezentace vstupních vět s ohledem na jejich morfologické, syntaktické a sémantické vlastnosti. Zaměříme se na embedinky slov i kontextové embedinky a sebepozornost modelů Transformer (BERT, NMT). Ukážeme jak řízené, tak i neřízené postupy analýzy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In recent years, deep neural networks dominated the area of Natural Language Processing (NLP). End-to-end-trained models can do tasks as skillfully as never before and develop their own language representations. However, they act as black boxes that are very hard to interpret. This calls for an inspection to what extent the linguistic conceptualizations are consistent with what the models learn. Do neural networks use morphology and syntax the way people do when they talk about language? Or do they develop their own way?
In our talk, we will half-open the neural black-box and analyze the internal representations of input sentences with respect to their morphological, syntactic, and semantic properties. We will focus on word embeddings as well as contextual embeddings and self-attentions of the Transformer models (BERT, NMT). We will show both supervised and unsupervised analysis approaches.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této knize prozkoumáme architektury a modely neuronových sítí, které se používají pro zpracování přirozeného jazyka (NLP). Analyzujeme jejich interní reprezentace (vkládání slov, skryté stavy, mechanismus pozornosti a kontextové vkládání) a zkoumáme, jaké vlastnosti mají tyto reprezentace a jaké jazykově interpretovatelné rysy se v nich objevují. K prezentaci přehledu modelů a reprezentací a jejich jazykových vlastností používáme vlastní experimentální výsledky i výsledky publikované jinými výzkumnými týmy.

Na začátku vysvětlíme základní pojmy hlubokého učení a jeho využití v NLP a probereme podrobnosti nejvýznamnějších neurálních architektur a modelů. Poté nastíníme koncept interpretovatelnosti, různé pohledy na ni a představíme základní supervizované a nekontrolované metody, které se používají pro tlumočení
modely neuronových sítí.

Další část je věnována statickému vkládání slov. Ukážeme různé metody pro vkládání vizualizace prostoru, analýzu komponent a vkládání transformací prostoru pro interpretaci. Předem připravená vložení slov obsahují informace o morfologii i lexikální sémantice. Když jsou embedings trénovány pro konkrétní úkol, embeddings mají tendenci být organizovány podle informací, které jsou pro daný úkol důležité (např. Emoční polarita pro analýzu sentimentu).

Analyzujeme také mechanismy pozornosti, ve kterých můžeme sledovat vážené vazby mezi reprezentacemi jednotlivých tokenů. Ukazujeme, že mezijazykové pozornosti většinou spojují vzájemně odpovídající tokeny; v některých případech se však mohou velmi lišit od tradičních slovních spojení. Hlavně se zaměřujeme na sebepozornost v sítích typu Transformer. Některé hlavy spojují tokeny s určitými syntaktickými vztahy. To motivovalo vědce odvodit syntaktické stromy ze sebepozorování a porovnat je s lingvistickými anotacemi. Shrneme množství syntaxe v pozornost napříč vrstvami několika modelů NLP. Rovněž poukazujeme na skutečnost, že pozornost může být někdy velmi zavádějící a může nést velmi odlišné informace, o kterých bychom si na základě zúčastněných tokenů mysleli.

V poslední části se podíváme na kontextové vektorové representace slov a jazykové vlastnosti, které zachycují. Představují jasné vylepšení oproti statickým vektorovým reprezentacím slov, zejména pokud jde o zachycení morfologických a syntaktických znaků. Zdá se však, že některé vyšší jazykové abstrakce, jako je sémantika, se v současných kontextových vloženích odrážejí jen velmi slabě nebo vůbec.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this book, we explore neural-network architectures and models that are used for Natural Language Processing (NLP). We analyze their internal representations (word-embeddings, hidden states, attention mechanism, and contextual embeddings) and review what properties these representations have and what kinds of linguistically interpretable features emerge in them. We use our own experimental results, as well as the results published by other research teams to present an overview of models and representations and their linguistic properties.

In the beginning, we explain the basic concepts of deep learning and its usage in NLP and discuss details of the most prominent neural architectures and models. Then, we outline the concept of interpretability, different views on it, and introduce basic supervised and unsupervised methods that are used for interpreting trained
neural-network models.

The next part is devoted to static word embeddings. We show various methods for embeddings space visualization, component analysis and embedding space transformations for interpretation. Pretrained word embbedings contain information about both morphology and lexical semantics. When the embbedings are trained for a specific task, the embeddings tend to be organised by the information that is important for the given task (e.g. emotional polarity for sentiment analysis).

We also analyze attention mechanisms, in which we can observe weighted links between representations of individual tokens. We show that the cross-lingual attentions mostly connect mutually corresponding tokens; however, in some cases, they may be very different from the traditional word-alignments. We mainly focus on self-attentions in Transformers. Some heads connect tokens with certain syntactic relations. This motivated researchers to infer syntactic trees from the self-attentions and compare them to the linguistic annotations. We summarize the amount of syntax in
the attentions across the layers of several NLP models. We also point out the fact that attentions might sometimes be very misleading and may carry very different information from which we would think based on the attended tokens.

In the last part, we look at contextual word embeddings and the linguistic features they capture. They constitute a clear improvement over static word embeddings, especially in terms of capturing morphological and syntactic features. However, some higher linguistic abstractions, such as semantics, seem to be reflected in the current contextual embeddings only very weakly or not at all.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>„Roboti žít nebudou, mohou jen zemřít,“ říká stavitel Alquist, postava z divadelní hry R.U.R. Autorem těchto slov ovšem tentokrát není Karel Čapek, ale stroj, umělá inteligence, která na počest slavného díla píše drama o vztahu člověka a robota. Má být uvedena ve Švandově divadle 25. ledna, kdy od premiéry R.U.R. uplyne právě 100 let.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>"The robots will not live, they can only die," says the builder Alquist, a character in the R.U.R. play. The author of these words, however, this time is not Karel Čapek, but a machine, an artificial intelligence, writing a drama about the relationship of man and robot in honor of the famous work. It is to be shown at the Švanda Theatre on January 25th, when it will be 100 years since the premiere of R.U.R.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rychlý růst tvory škodlivého softwaru (malware) v posledním tesetiletí a počet útoků způsobených malwarem na síťová prostředí, jako je Internet, dokládá potřebu výzkumu bezpečnosti počítačových sítí a digitálně zaměřené forenzní vědy. V naší studii překládáme metodu, které identifikuje "druhy" rodin malware, které jsou pokročilé, záměrně matoucí a strukturně pestré. Navrhujeme hybridní techniku, která při klasifikaci malwarových rodin kombinuje detekci signatur s metodami založenými na strojovém učení. Metoda využívá profilové skryté markovské modely (PHMM) k behaviorálnímu popisu malwarových druhů. Tento článek vysvětluje proces modelování a trénování vícečetného zarovnání sekvencí. Vzhledem k tomu, že ne všechny části souboru jsou škodlivé, je cílem rozlišit škodlivé a neškodné části. Zaměřením na škodlivé části roste šance na detekci malwaru. Experimentální výsledky ukazují, že svou úspěšností naše metoda překonává ostatní přístupy založené na skrytých markovských řetězcích. Všechny doprovodné materiály k textu, zejména zdrojové kódy, datové sady a kompletní výsledky, jsou veřejně dostupné na Internetu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The rapid growth of malicious software (malware) production in recent decades and the increasing number of threats posed by malware to network environments, such as the Internet and intelligent environments, emphasize the need for more research on the security of computer networks in information security and digital forensics. The method presented in this study identifies “species” of malware families, which are more sophisticated, obfuscated, and structurally diverse. We propose a hybrid technique combining aspects of signature detection with machine learning-based methods to classify malware families. The method is carried out by utilizing Profile Hidden Markov Models (PHMMs) on the behavioral characteristics of malware species. This paper explains the process of modeling and training an advanced PHMM using sequences obtained from the extraction of each malware family’s paramount features, and the canonical sequences created in the process of Multiple Sequence Alignment (MSA) production. Due to the fact that not all parts of a file are malicious, the goal is to distinguish the malicious portions from the benign ones and place more emphasis on them in order to increase the likelihood of malware detection by having the least impact from the benign portions. Based on “consensus sequences”, the experimental results show that our proposed approach outperforms other HMM-based techniques even when limited training data is available. All supplementary materials including the code, datasets, and a complete list of results are available for public access on the Web.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje podrobnou analýzu první soutěže v end-to-end generování přirozeného jazyka (NLG) a na základě jejích výsledků naznačuje směr budoucího výzkumu. Cílem této soutěže úkolu bylo posoudit, zda moderní end-to-end systémy NLG mohou generovat komplexnější výstup, jsou-li natrénovány z dat lexikálně bohatších, syntakticky složitějších a zahrnujících různé diskurzní jevy. S použitím nových automatických a lidských metrik porovnáváme 62 systémů zaslaných do soutěže 17 institucemi, které zahrnují širokou škálu přístupů, včetně architektur strojového učení – kde většina implementací jsou modely typu sequence-to-sequence (seq2seq) – i systémů založených na gramatických pravidlech a šablonách. Systémy založené na architektuře seq2seq ukázaly v této souteži velký potenciál pro NLG. Zjistili jsme, že seq2seq systémy mají obecně vysoké skóre, pokud jde o metriky založené na překryvu slov a lidské hodnocení přirozenosti/plynulosti; vítězný systém Slug (Juraska et al., 2018) je založený na seq2seq. Základní modely typu seq2seq však často nedokážou správně vyjádřit vstupný reprezentaci významu, pokud postrádají silný mechanismus sémantické kontroly použitý během dekódování. Modely seq2seq mohou být navíc překonány ručně vytvořenými systémy z hlediska celkové kvality, jakož i složitosti, délky a rozmanitosti výstupů. Tento výzkum ovlivnil, inspiroval a motivoval řadu nedávných studií mimo původní soutěž, které v článku rovněž shrnujeme.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper provides a comprehensive analysis of the first shared task on End-to-End Natural Language Generation (NLG) and identifies avenues for future research based on the results. This shared task aimed to assess whether recent end-to-end NLG systems can generate more complex output by learning from datasets containing higher lexical richness, syntactic complexity and diverse discourse phenomena. Introducing novel automatic and human metrics, we compare 62 systems submitted by 17 institutions, covering a wide range of approaches, including machine learning architectures – with the majority implementing sequence-to-sequence models (seq2seq) – as well as systems based on grammatical rules and templates. Seq2seq-based systems have demonstrated a great potential for NLG in the challenge. We find that seq2seq systems generally score high in terms of word-overlap metrics and human evaluations of naturalness – with the winning Slug system (Juraska et al., 2018) being seq2seq-based. However, vanilla seq2seq models often fail to correctly express a given meaning representation if they lack a strong semantic control mechanism applied during decoding. Moreover, seq2seq models can be outperformed by hand-engineered systems in terms of overall quality, as well as complexity, length and diversity of outputs. This research has influenced, inspired and motivated a number of recent studies outwith the original competition, which we also summarise as part of this paper.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Velkým problémem v evaluaci systémů pro generování textu z dat (D2T) je jak změřit sémantickou přesnost vygenerovaného textu, tj. jeho věrnost vstupním datům. Navrhujeme novou metriku pro evaluaci sémantické přesnosti D2T generování založené na neuronovém modelu předtrénovaném na úlohu automatické jazykové inference (NLI). Pomocí modelu NLI kontrolujeme, zda generovaný text vyplývá (entailment) z dat a opačně, což nám dovoluje odhalit vynechané části dat nebo halucinované (daty nepodložené) části vygenerovaného textu. Vstupní data pro model NLI převádíme do textu pomocí triviálních šablon. Naše experimenty na dvou běžně užívaných datových sadách pro D2T ukazují, že naše metrika dokáže dosáhnout vysoké přesnosti při identifikaci chybných výstupů generátorů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A major challenge in evaluating data-to-text (D2T) generation is measuring the semantic accuracy of the generated text, i.e. its faithfulness to the input data. We propose a new metric for evaluating the semantic accuracy of D2T generation based on a neural model pretrained for natural language inference (NLI). We use the NLI model to check textual entailment between the input data and the output text in both directions, allowing us to reveal omissions or hallucinations. Input data are converted to text for NLI using trivial templates. Our experiments on two recent D2T datasets show that our metric can achieve high accuracy in identifying erroneous system outputs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce představuje výsledky překladatelských úloh týkajících se zpravodajských textů a podobného úkolu překladů jazyků, obojí organizováno v rámci Conference on Machine Translation (WMT) 2020. V úkolu týkajícím se zpravodajských textů byli účastníci požádáni o sestavení systémů strojového překladu pro kterýkoli z 11 jazykových párů, které budou hodnoceny na testovacích souborech sestávajících hlavně z reportáží. Úloha byla také otevřena pro další testovací sady, aby se daly  zkoumat specifické aspekty překladu. V další úloze účastníci sestavili systémy strojového překladu pro překládání mezi úzce příbuznými páry jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the news translation task and the similar language translation task, both organised
alongside the  Conference on Machine Translation (WMT) 2020. In the news task, participants
were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories.  The task was also opened up
to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built 
machine translation systems for translating between closely related pairs of languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Umělá inteligence zvládá leccos.
Píše burzovní zprávy, zvládá
zábavnou konverzaci, objedná za
nás stůl v restauraci… Tentokrát ale
AI zašla ještě dál a dostala se zase o kousek blíž člověku – výzkumný
projekt THEaiTRE učí svého robota
napsat divadelní hru. O tom, jak
proces probíhá, co k tomu autory
vedlo a jaká jsou úskalí AI scenáristy
jsme si povídali s Rudolfem Rosou,
Danielem Hrbkem a Tomášem
Studeníkem, kteří se na unikátním
počinu podílí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Artificial intelligence can do things. She writes stock reports, handles entertaining conversation, orders a table for us at a restaurant ... But this time AI has gone further and gotten a little bit closer to humans -- the THEaiTRE research project is teaching its robot to write a play. We talked with Rudolf Rosa, Daniel Hrbek and Tomáš Studeník, who are involved in the unique feat, about how the process is going, what led the authors to it, and what the challenges of the AI writer are.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přestože nové neuronové modely typu sequence-to-sequence neurální značně zlepšily kvalitu syntézy řeči, dosud neexistuje systém schopný rychlého trénování, rychlé inference a zároveň vysoce kvalitní syntézy. Navrhujeme dvojici sítí typu učitel-student, která je schopna vysoce kvalitní syntézy spektrogramu rychleji než v reálném čase, s nízkými požadavky na výpočetní zdroje a rychlým trénováním. Ukazujeme, že vrstvy typu self-attention nejsou pro generování vysoce kvalitní řeči nutné. Jak v učitelské, tak ve studentské síti využíváme jednoduché konvoluční bloky s reziduálním propojením; používáme pouze jednu vrstvu attention v učitelském modelu. Ve spojení s hlasovým kodérem MelGAN byla hlasová kvalita našeho modelu hodnocena signifikantně lépe než Tacotron 2. Náš model může být efektivně trénován na jednom GPU a může běžet v reálném čase i na CPU. Zdrojový kód i zvukové ukázky poskytujeme na našem úložišti na GitHubu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>While recent neural sequence-to-sequence models have greatly improved the quality of speech synthesis, there has not been a system capable of fast training, fast inference and high-quality audio synthesis at the same time. We propose a student-teacher network capable of high-quality faster-than-real-time spectrogram synthesis, with low requirements on computational resources and fast training time. We show that self-attention layers are not necessary for generation of high quality audio. We utilize simple convolutional blocks with residual connections in both student and teacher networks and use only a single attention layer in the teacher model. Coupled with a MelGAN vocoder, our model's voice quality was rated significantly higher than Tacotron 2. Our model can be efficiently trained on a single GPU and can run in real time even on a CPU. We provide both our source code and audio samples in our GitHub repository.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V češtině – podobně jako v jiných slovanských jazycích – může mít klitické reflexivum funkci slovotvorného formantu, derivujícího lexikální reciproční slovesa, tedy  slovesa obsahující rys vzájemnosti již ve svém lexikálním významu. V článku rozlišuji mezi lexikálními recipročními slovesy, u nichž rys vzájemnosti vyjadřuje klitické reflexivum (např. nenávidět se ← nenávidět, slíbit si ← slíbit), a lexikálními recipročními slovesy, u nichž má klitické reflexivum jinou funkci (např. oddělit se ← oddělit). Lexikální reciproka prvního typu vytvářejí konstrukce, u nichž se participanty ve vztahu vzájemnosti vyjadřují typicky v subjektové pozici a v pozici nepřímého objektu, vyjádřeného instrumentálem. Ukazuji však, že tato slovesa tvoří sémantické skupiny, které se do značné míry sémanticky překrývají s dalšími typy lexikálních recipročních sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In Czech (similarly as in other Slavic languages), the clitic reflexives serve – among others – as a derivational means deriving lexical reciprocal verbs, i.e., those verbs that encode mutuality directly in their lexical meaning. Here I draw a line between those lexical reciprocal verbs with which the reflexives introduce mutuality (nenávidět se ‘to hate each other’ ← nenávidět ‘to hate sb’ and slíbit si ‘to promise sth to each other’ ← slíbit ‘to promise sth to sb’) and those with which the reflexives have another function (oddělit se ‘to separate from each other’ ← oddělit ‘to separate sb/sth from sb/sth’). I show that lexical reciprocal verbs of the former type uniformly form the so-called discontinuous constructions with the nominative subject and comitative indirect object (e.g., Petr si slíbil s Marií věrnost. ‘Peter and Mary promised fidelity to each other.’) and that they fall into several semantic classes, which, however, semantically overlap to a great extent with lexical reciprocal verbs of other types.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Česká verbální jména (zakončená sufixy -ní/-tí) označující událost (např. poučení jako proces) nebo abstraktní výsledek děje (např. poučení jako předaná informace) se z hlediska valence liší především tím, jak v povrchové struktuře vyjadřují svá valenční doplnění. V nominálních konstrukcích je povrchové vyjádření valenčních doplnění verbálních jmen označujících událost odvoditelné z valenční struktury jejich základových sloves. Oproti tomu verbální substantiva označující abstraktní výsledek děje užívají k povrchovému vyjádření svých valenčních doplnění vedle systémových morfematických forem i formy nesystémové. Ve verbonominálních konstrukcích, v nichž je syntaktickým centrem kategoriální sloveso, verbální jména označující událost systematicky uplatňují na povrchu jako subjekt slovesa participant Agens (v širokém slova smyslu). Oproti tomu verbální jména označující abstraktní výsledek děje mohou často na povrchu jako rozvití slovesa vyjádřit i své další sémantické participanty a výběr různých kategoriálních sloves jim často umožňuje perspektivizovat označovanou situaci pokaždé z hlediska jiného participantu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Comparing valency behavior of two selected types of Czech verbal nouns (ending in -ní/-tí), i.e., those denoting actions, e.g., poučení ‘instructing’, and those denoting abstract results of actions, e.g., poučení ‘advice’, we can observe that they differ especially in the surface syntactic expression of their valency complementations. In nominal constructions, the surface expression of valency complementations of verbal nouns denoting actions can be typically derived from valency structure of their base verbs. In contrast, verbal nouns denoting abstract results of actions often express their valency complementations on the surface in a non-systemic way. In verbonominal constructions, the syntactic center of which is a light verb, verbal nouns denoting actions systematically employ their participant Agent (in a broad sense) on the surface as subject provided by the light verb. In contrast, verbal nouns denoting abstract results of actions can often express more semantic participants on the surface as verbal modifications and the choice of light verbs can change the perspective from which situations denoted by these verbal nouns are viewed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku představujeme soupis jazykových prostředků, které vyjadřují sémantický rys vzájemnosti v českých verbálních konstrukcích. Na základě výsledků nedávných typologických průzkumů rozlišujeme prostředky lexikální a prostředky gramatické. Lexikální prostředky pro vyjádření vzájemnosti zahrnují omezenou skupinu inherentně recipročních sloves s rysem vzájemnosti v lexikálním významu. Gramatické prostředky realizující syntaktickou operaci reciprocalizace se primárně uplatňují u velké skupiny sloves, která označujeme jako syntaktická reciproční slovesa; tyto prostředky zahrnují (i) zvratné osobní reflexivum se/sebe, si/sobě, sebou, (ii) výraz jeden - druhý a (iii) příslovce. Zatímco použití gramatických prostředků je pro vyjádření vzájemnosti u syntakticky recipročních sloves nutné, pro inherentně reciproční slovesa je typicky volitelné. V textu podrobně popisujeme různé funkce těchto jazykových prostředků ve vyjadřování vzájemnosti v češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we introduce an inventory of language means that express the semantic feature of mutuality in Czech verbal constructions. Based on the results of recent typological surveys, we distinguish between lexical and grammatical means. The lexical means include a limited group of inherently reciprocal verbs with the feature of mutuality in their lexical meaning. The grammatical means involved in the syntactic operation of reciprocalization are primarily used by a large group of verbs to which we refer as syntactic reciprocal verbs; these means involve (i) the reflexive personal pronoun se/sebe, si/sobě, sebou, (ii) the expression jeden – druhý ‘each other’, and (iii) adverbials. While the use of the grammatical means is obligatory for expressing mutuality with syntactic reciprocal verbs, it is often only optional for inherently reciprocal verbs. We thoroughly describe various functions that these language means have in encoding mutuality in Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku se věnujeme způsobům vyjadřování vzájemnost v českých konstrukcích s kategoriálními slovesy. Reciprocitu do těchto konstrukcí vnáší prediktivní substantiva, neboť ta představují jejich sémantické jádro. Zaměřujeme se na reciproční konstrukce s kategoriálními slovesy, které vznikají odvozením syntaktickou operací reciprokalizace. Ukazujeme, že komplexní mapování sémantických participantů na valenční doplnění  charakteristické pro reciprocitu se zachovává i v recipročních konstrukcích s kategoriálními slovesy. Hlavní rozdíl mezi recipročními nominálními konstrukcemi a recipročními konstrukcemi s kategoriálním slovesem spočívá v morfosyntaktickém vyjádření reciprokalizovaných participantů. Ukazujeme, že povrchové syntaktické změny v recipročních konstrukcích s kategoriálními slovesy jsou natolik pravidelné, že je lze popsat pomocí pravidel, a to  pravidel pro hlubokou a povrchovou tvorbu syntaktických struktury s kategoriálním slovesem a pravidel pro tvoření recipročních konatrukcí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we draw attention to reciprocity in Czech light verb constructions – a language phenomenon, which has not been discussed yet. Reciprocity is contributed to light verb constructions by predictive nouns, as they are the nouns that represent the semantic core of these constructions. Here we focus on reciprocal light verb constructions derived by the syntactic operation of reciprocalization. We show that the complex mapping of semantic participants onto valency complementations, characteristic of reciprocalization, is reflected in reciprocal light verb constructions in the same way as in reciprocal nominal constructions. The main difference between reciprocal nominal constructions and reciprocal light verb constructions lies in the morphosyntactic expression of reciprocalized participants. We demonstrate that surface syntactic changes in reciprocal light verb constructions are regular enough to be described on the rule basis: the rule based generation of reciprocal light verb constructions requires a cooperation of two sets of rules – rules for deep and surface syntactic structure formation of light verb constructions and rules for capturing reciprocity.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valenční slovníky obvykle popisují valenční chován sloves v jejich nereflexivním a nerecipročním užití, přestože reflexivní a reciproční konstrukce lze považovat za běžné morfosyntaktické formy sloves. Pro oba tyto typy jsou charakteristické pravidelné změny v morfosyntaktické struktuře sloves, které lze popsat gramatickými pravidly; ovšem možnost tyto konstrukce tvořit musí být vyznačena ve slovníku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Valency lexicons usually describe valency behavior of verbs in non-reflexive and non-reciprocal constructions. However, reflexive and reciprocal constructions are common morphosyntactic forms of verbs. Both of these constructions are characterized by regular changes in morphosyntactic properties of verbs, thus they can be described by grammatical rules. On the other hand, the possibility to create reflexive and/or reciprocal constructions cannot be trivially derived from the morphosyntactic structure of verbs as it is conditioned by their semantic properties as well. A large-coverage valency lexicon allowing for rule based generation of all well formed verb constructions should thus integrate the information on reflexivity and reciprocity. In this paper, we propose a semi-automatic procedure, based on grammatical constraints on reflexivity and reciprocity, detecting those verbs that form reflexive and reciprocal constructions in corpus data. However, exploitation of corpus data for this purpose is complicated due to the diverse functions of reflexive markers crossing the domain of reflexivity and reciprocity. The list of verbs identified by the previous procedure is thus further used in an automatic experiment, applying word embeddings for detecting semantically similar verbs. These candidate verbs have been manually verified and annotation of their reflexive and reciprocal constructions has been integrated into the valency lexicon of Czech verbs VALLEX.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ročník 2020 soutěže (společné úlohy) na Konferenci o počítačovém učení jazyka (CoNLL) byl věnován generování významových reprezentací (Meaning Representation Parsing, MRP) napříč formalismy a jazyky. Podmínky soutěže byly rozšířením podobné soutěže z předcházejícího roku. Pět různých přístupů k reprezentaci větného významu v podobě orientovaných grafů bylo zastoupeno v anglických trénovacích a testovacích datech pro tuto úlohu a uloženo v jednotném formátu pro abstraktní reprezentaci a serializaci grafů. U čtyř z těchto formalismů byla k dispozici další trénovací a testovací data v jednom dalším jazyce. Soutěže se zúčastnilo osm týmů, z nichž dva nejsou zahrnuty v oficiální výsledkové listině, protože jejich řešení bylo doručeno po termínu nebo zahrnovalo přídavná trénovací data. Technické informace o soutěži, včetně zaslaných řešení, oficiálních výsledků, odkazů na podpůrné zdroje a software jsou k dispozici na webu soutěže na: http://mrp.nlpl.eu</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a uniform graph abstraction and serialization; for four of these representation frameworks, additional training and evaluation data was provided for one additional language per framework. The task received submissions from eight teams, of which two do not participate in the official ranking because they arrived after the closing deadline or made use of additional training data. All technical information regarding the task, including system submissions, official results, and links to supporting resources and software are available from the task web site at: http://mrp.nlpl.eu</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>NameTag je nástroj pro rozpoznávání pojmenovaných entit. NameTag identifikuje vlastní jména v textu a klasifikuje je do definovaných kategorií, jako například jména osob, míst, organizací, apod.

NameTag 2 také rozpoznává vnořené pojmenované entity. V roce 2019 dosahuje úrovně poznání (state of the art) v češtině, angličtině, nizozemštině a španělštině; a velmi blízce také v němčině (Straková a kol., 2019).

NameTag je dostupný jako NameTag Online Demo a jako webová služba NameTag Web Service na LINDAT/CLARIN.

Modely jsou volně použitelné pro nekomerční užití a jsou distrubuovány pod licencí CC BY-NC-SA, ale některé z dat použité pro vytvoření modelů mohou vyžadovat další licenční podmínky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>NameTag is a tool for named entity recognition (NER). NameTag identifies proper names in text and classifies them into predefined categories, such as names of persons, locations, organizations, etc.

NameTag 2 also recognizes nested named entities (nested NER) and as of 2019, it achieves state of the art in Czech, English, Dutch and Spanish and nearly state of the art in German (Straková et al. 2019).

NameTag is available as an online demo NameTag Online Demo and web service NameTag Web Service hosted by LINDAT/CLARIN.

The linguistic models are free for non-commercial use and distributed under CC BY-NC-SA license, although for some models the original data used to create the model may impose additional licensing conditions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V rozhovoru pro český rozhlas popisujeme, jak probíhá generování divadelní hry v projektu THEaiTRE.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In an interview for the Czech radio Český rozhlas, we describe how we generate a theatre play in the THEaiTRE project.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nedávný pokrok v neuronovém strojovém překladu směřuje k větším sítím trénovaným na stále větším množství hardwarových zdrojů.
V důsledku toho jsou modely NMT nákladné na trénování, a to jak finančně, kvůli nákladům na elektřinu a hardware, tak ekologicky, kvůli uhlíkové stopě.
Zvláště to platí v transferu znalostí při trénování modelu "rodiče" před přenesením znalostí do požadovaného modelu "dítě".
V tomto článku navrhujeme jednoduchou metodu opakovaného použití již natrénovaného modelu pro různé jazykové páry, u nichž není nutné upravovat modelovou architekturu.
Náš přístup nepotřebuje samostatný model pro každou zkoumanou dvojici jazyků, jak je to typické v rámci přenosového učení u neuronového strojového překladu. Abychom ukázali použitelnost naší metody, recyklujeme model Transformeru, který natrénovali jiní vyzkumníci a použijeme ho pro různé jazykové páry.
Naše metoda dosahuje lepší kvality překladu a kratších časů konvergence, než když trénujeme z náhodné inicializace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Recent progress in neural machine translation is directed towards larger neural networks trained on an increasing amount of hardware resources.
As a result, NMT models are costly to train, both financially, due to the electricity and hardware cost, and environmentally, due to the carbon footprint.
It is especially true in transfer learning for its additional cost of training
the ``parent'' model before transferring knowledge and training the desired ``child'' model.
In this paper, we propose a simple method of re-using an already trained model for different language pairs where there is no need for modifications in model architecture.
Our approach does not need a separate parent model for each investigated language pair, as it is typical in NMT transfer learning. To show the applicability of our method, we recycle a Transformer model trained by different researchers and use it to seed models for different language pairs. 
We achieve better translation quality and shorter convergence times than when training from random initialization.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Genderová zaujatost ve strojovém překladu se může projevit při výběru genderových modulací na základě falešných genderových korelací. Například vždy překládat lékaře jako muže a sestry jako ženy. To může být obzvláště škodlivé, protože modely se stávají populárnějšími a jsou zaváděny v rámci komerčních systémů. Naše práce představuje největší důkaz tohoto jevu ve více než 19 systémech předložených WMT ve čtyřech různých cílových jazycích: češtině, němčině, polštině a ruštině.K dosažení tohoto cíle používáme WinoMT, nedávnou automatickou testovací sadu, která zkoumá genderovou korektnost a zkreslení při překladu z angličtiny do jazyků s gramatickým pohlavím. Bývalí pracovníci WinoMT se starají o dva nové jazyky testované ve WMT: polštinu a češtinu. Zjistili jsme, že všechny systémy důsledně používají nepravdivé korelace v datech spíše než smysluplné kontextové informace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Gender  bias  in  machine  translation  can  manifest when choosing gender inflections based on  spurious  gender  correlations.    For  example,  always  translating  doctors  as  men  and nurses  as  women.    This  can  be  particularly harmful as models become more popular and deployed  within  commercial  systems. Our work presents the largest evidence for the phenomenon  in  more  than  19  systems  submitted to the WMT over four diverse target languages:  Czech, German, Polish, and Russian.To achieve this, we use WinoMT, a recent automatic test suite which examines gender coreference and bias when translating from English to languages with grammatical gender. We ex-tend  WinoMT  to  handle  two  new  languages tested in WMT: Polish and Czech. We find that all  systems  consistently  use  spurious  correlations  in  the  data  rather  than  meaningful  contextual information.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CLARIN je evropská výzkumná infrastruktura, která poskytuje přístup k digitálním jazykovým zdrojům a nástrojům z celé Evropy i mimo ni výzkumným pracovníkům v humanitních a sociálních vědách. Tento dokument se zaměřuje na CLARIN jako platformu pro sdílení jazykových zdrojů. Přibližuje nabídku služeb pro agregaci jazykových úložišť a návrh hodnot pro řadu komunit, které těží z větší viditelnosti svých údajů a služeb v důsledku integrace do CLARIN. Zvýšená jemnost jazykových zdrojů slouží celé komunitě společenských a humanitních věd (SSH) a podporuje výzkumné komunity, které usilují o spolupráci založenou na virtuálních sbírkách pro určitou oblast. Dokument se také zabývá širším prostředím platforem služeb založených na jazykových technologiích, které mají potenciál stát se silným souborem interoperabilních zařízení pro nejrůznější využití.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CLARIN is a European Research Infrastructure providing access to digital language resources and tools from across Europe and beyond to researchers in the humanities and social sciences. This paper focuses on CLARIN as a platform for the sharing of language resources. It zooms in on the service offer for the aggregation of language repositories and the value proposition for a number of communities that benefit from the enhanced visibility of their data and services as a result of integration in CLARIN. The enhanced findability of language resources is serving the social sciences and humanities (SSH) community at large and supports research communities that aim to collaborate based on virtual collections for a specific domain. The paper also addresses the wider landscape of service platforms based on language technologies which has the potential of becoming a powerful set of interoperable facilities to a variety of communities of use.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této studii zkoumáme možné výhody využití informací z eye trackeru pro analýzu závislostní syntaxe na anglické části Dundee corpu. Abychom toho dosáhli, zavedeme parsing jako úlohu značkování sekvencí a pak rozšiřujeme neurální model pro značkování sekvencí o rysy z eye trackeru. Poté experimentujeme s různými nastaveními analyzátorů od lexikalizovaného parsingu po delexikalizovaný parser. Naše experimenty ukazují, že u lexikalizovaného parseru, i když jsou zlepšení pozitivní, nejsou statisticky významná, zatímco náš delexikalizovaný parser statisticky významně překonává baseline, kterou jsme stanovili. Analyzujeme také přínos různých rysů z eye trackeru k různým nastavením analyzátoru a zjišťujeme, že rysy z eye trackeru obsahují informace, které se svou povahou doplňují, což znamená, že rozšíření analyzátoru o různé rysy z eye trackeru seskupené dohromady poskytuje lepší výkon než jakýkoli jednotlivý prvek.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we explore the potential benefits of leveraging eye-tracking information for dependency parsing on the English part of the Dundee corpus. To achieve this, we cast dependency parsing as a sequence labelling task and then augment the neural model for sequence labelling with eye-tracking features. We then experiment with a variety of parser setups ranging from lexicalized parsing to a delexicalized parser. Our experiments show that for a lexicalized parser, although the improvements are positive they are not significant whereas our delexicalized parser significantly outperforms the baseline we established. We also analyze the contribution of various eye-tracking features towards the different parser setups and find that eye-tracking features contain information which is complementary in nature, thus implying that augmenting the parser with various gaze features grouped together provides better performance than any individual gaze feature.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme náš příspěvek do společné úlohy SIGTYP 2020 v předpovídání typologických rysů. Náš systém je patří do omezené části soutěže, neboť používá pouze databázi WALS. Zkoumáme dva přístupy. Jednodušší z nich je založen na odhadu korelace mezi hodnotami rysů u stejného jazyka pomocí podmíněných pravděpodobností a vzájemné informace. Druhý přístup je založen na neuronovém prediktoru, který využívá vektorovou reprezentaci jazyků předpočítanou na rysech z WALS. Ve výsledném systému oba přístupy kombinujeme s využitím jejich vlastního odhadu důvěryhodnosti předpovědi. Na testovacích datech dosahujeme úspěšnosti 70,7 %.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our submission to the SIGTYP 2020 Shared Task on the prediction of typological features. We submit a constrained system, predicting typological features only based on the WALS database. We investigate two approaches. The simpler of the two is a system based on estimating correlation of feature values within languages by computing conditional probabilities and mutual information. The second approach is to train a
neural predictor operating on precomputed language embeddings based on WALS features. Our submitted system combines the two approaches based on their self-estimated confidence scores. We reach the accuracy of 70.7% on the
test data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Deep Universal Dependencies (hluboké univerzální závislosti) je sbírka treebanků poloautomaticky odvozených z Universal Dependencies. Obsahuje přídavné hloubkově-syntaktické a sémantické anotace. Verze Deep UD odpovídá verzi UD, na které je založena. Mějte však na paměti, že některé treebanky UD nebyly do Deep UD zahrnuty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Deep Universal Dependencies is a collection of treebanks derived semi-automatically from Universal Dependencies. It contains additional deep-syntactic and semantic annotations. Version of Deep UD corresponds to the version of UD it is based on. Note however that some UD treebanks have been omitted from Deep UD.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Více než 50 let se výzkumníci pokouší naučit počítače číst hudební notaci, kterýžto obor se nazývá rozpoznávání notopisu (Optical Music Recognition, OMR). Do tohoto oboru je však pro začínající výzkumníky stále obtížné proniknout, obzvlášť pokud nemají nezanedbatelné hudební znalosti: je málo materiálů do problematiky uvádějících, a navíc se sám obor průběžně nedokáže shodnout na tom, jak sebe sama definovat a jak vybudovat sdílenou terminologii. V článku se těmto nedostatkům věnujeme: (1) formulujeme robustní definici OMR a vztahů k příbuzným oborům, (2) analyzujeme, jak OMR invertuje proces zapisování hudby, aby získalo z dokumentu popis hudební notace a hudební sémantiky, a (3) předkládáme taxonomii OMR, především novou taxonomii aplikací. Dále diskutujeme, jak hluboké učení ovlivňuje současný výzkum OMR v kontrastu s předchozími přístupy. Na základě tohoto článku by měl čtenář získat základní porozumění OMR: jeho cílům, jeho vnitřní struktuře, vztahům k ostatním oborům, stavu poznání a výzkumných příležitostí, které OMR poskytuje.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>For over 50 years, researchers have been trying to teach computers to read music notation, referred to as Optical Music Recognition (OMR). However, this field is still difficult to access for new researchers, especially those without a significant musical background: Few introductory materials are available, and, furthermore, the field has struggled with defining itself and building a shared terminology. In this work, we address these shortcomings by (1) providing a robust definition of OMR and its relationship to related fields, (2) analyzing how OMR inverts the music encoding process to recover the musical notation and the musical semantics from documents, and (3) proposing a taxonomy of OMR, with most notably a novel taxonomy of applications. Additionally, we discuss how deep learning affects modern OMR research, as opposed to the traditional pipeline. Based on this work, the reader should be able to attain a basic understanding of OMR: its objectives, its inherent structure, its relationship to other fields, the state of the art, and the research opportunities it affords.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek představil zkoumání „fenoménu opakování“ na příkladu opakovaného sledování videoklipu během práce studentů s digitálním učebním materiálem v malých skupinách, kdy měli k dispozici jeden počítač a vyplňovali papírový list otázkami. První sledování (W1) videoklipů bylo většinou dostačující, ale občas byla vložena sekvence opakovaného sledování (W2), která řešila problémy a nejasnosti. Příspěvek se zabývá detailní analýzou těchto interakčních sekvencí opakovaného sledování.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>My paper presents an investigation of “the phenomenon of repetition” in case of re-watching a video clip. I noticed this practice in a corpus of video data recorded in a classroom setting. Students worked with an e-learning material in small groups with one computer device to their disposal, filling a paper sheet with questions. First watching (W1) of video clips was mostly sufficient, but a sequence of re-watching (W2) was occasionally inserted to deal with troubles and ambiguities. After W1, the interactional status of video clip is changed: it becomes a structured object. The clip is (re)inspected with transparent vision acquired through W1. W2 as part of the classroom activity is oriented by the question on the paper sheet, and transparent vision emerges as a novel (and temporary) interactional competence. Practices of working with video clips point to the properties of video clip as an object in interaction, and they inform us of the work involved in constituting occasioned practical objectivities.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek se zabývá automatickou morfologickou segmentací českých lemmat obsažených v derivační síti DeriNet. Popis derivačních vztahů mezi základními a odvozenými lemmaty, a dělení lemmat na sekvence morfémů, jsou dva blízce propojené formální modely popisující vznik slov. Proto navrhujeme novou segmentační metodu, která využívá existence derivační sítě. Naše řešení překonává dosavadní metody segmentace pro češtinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper deals with automatic morphological segmentation of Czech lemmas contained in the word-formation network DeriNet. Capturing derivational relations between base and derived lemmas, and segmenting lemmas into sequences of morphemes are two closely related formal models of how words come into existence. Thus we propose a novel segmentation method that benefits from the existence of the network; our solution constitutes new state-of-the-art for the Czech language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku představujeme novou vyhlepšenou verzi vyhledávače a vizualizátoru slovotvorných sítí DeriSearch.

Slovotvorné sítě jsou datové sady zachycující derivační, kompoziční a jiné slovotvorné vztahy mezi slovy. Jsou reprezentovatelné pomocí orientovaných grafů, ve kterých uzly představují slova a orientované hrany mezi nimi vyjadřují slovotvorné vztahy. Některé sítě navíc obsahují další lingvistické anotace, například segmentaci slov na morfémy nebo identifikaci slovotvorných procesů.

Sítě pro morfologicky bohaté jazyky s produktivním odvozováním a skládáním mají velké komponenty souvislosti, které se obtížně vizualizují. Například v DeriNetu 2.0, jedné ze sítí pro češtinu, je 1/8 slovníku obsažena v komponentách souvislosti velkých přes 500 slov. V síti Word Formation Latin pro latinu je přes 10 000 slov (1/3 slovníku) v jediné komponentě.

S nedávným vydáním souboru slovotvorných sítí pro více jazyků Universal Derivations potřeba nástroje pro vyhledávání a vizualizaci takto komplexních dat dále vzrůstá.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we introduce a new and improved version of DeriSearch, a search engine and visualizer for word-formation networks.

Word-formation networks are datasets that express derivational, compounding and other word-formation relations between words. They are usually expressed as directed graphs, in which nodes correspond to words and edges to the relations between them. Some networks also add other linguistic information, such as morphological segmentation of the words or identification of the processes expressed by the relations.

Networks for morphologically rich languages with productive derivation or compounding have large connected components, which are difficult to visualize. For example, in the network for Czech, DeriNet 2.0, connected components over 500 words large contain 1/8 of the vocabulary, including its most common parts. In the network for Latin, Word Formation Latin, over 10 000 words (1/3 of the vocabulary) are in a single connected component.

With the recent release of the Universal Derivations collection of word-formation networks for several languages, there is a need for a searching and visualization tool that would allow browsing such complex data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Autoři se zabývají právními otázkami týkajícími se tvorby a používání jazykových modelů. Článek začíná vysvětlením vývoje jazykových technologií. Autoři analyzují technologický postup v rámci autorského práva, práv s ním souvisejících a práva na ochranu osobních údajů. Autoři se věnují také komerčnímu využití jazykových modelů. Hlavním argumentem autorů je, že právní omezení vztahující se na jazykové údaje obsahující materiály a osobní údaje chráněné autorským právem se obvykle nevztahují na jazykové modely. Jazykové modely nejsou obvykle považovány za odvozená díla. Vzhledem k široké škále jazykových modelů není tato pozice absolutní.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The authors address the legal issues relating to the creation and use of language models. The article begins with an explanation of the development of language technologies. The authors analyse the technological process within the framework copyright, related rights and personal data protection law. The authors also cover commercial use of language models. The authors’ main argument is that legal restrictions applicable to language data containing copyrighted material and personal data usually do not apply to language mod-els. Language models are not normally considered derivative works. Due to a wide range of language models, this position is not absolute.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve Švandově divadle se chystá nová hra. Premiéru bude mít už v lednu. Ale nikdo ještě úplně neví, o čem přesně bude. Její scénář totiž nevzniká v hlavě žádného člověka. Generuje ho umělá inteligence.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>There's a new play coming up at the Švanda Theatre. It will premiere in January. But no one quite knows what it will be about yet. Its script doesn't originate in any human's mind. It's generated by artificial intelligence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>„Large Scale Colloquial Persian Dataset“ (LSCP) je hierarchicky uspořádán do asemantické taxonomie, která se zaměřuje na víceúčelové neformální porozumění perskému jazyku jako komplexní problém. LSCP zahrnuje 120 milionů vět z 27 milionů příležitostných perských tweetů se svými závislostními vztahy ve syntaktické anotaci, tagy řeči, polaritu sentimentu a automatický překlad původních perských vět do pěti různých jazyků (EN, CS, DE, IT, HI).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Large Scale Colloquial Persian Dataset" (LSCP) is hierarchically organized in asemantic taxonomy that focuses on multi-task informal Persian language understanding as a comprehensive problem. LSCP includes 120M sentences from 27M casual Persian tweets with its dependency relations in syntactic annotation, Part-of-speech tags, sentiment polarity and automatic translation of original Persian sentences in five different languages (EN, CS, DE, IT, HI).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozpoznávání jazyků v posledních letech významně pokročilo pomocí moderních metod strojového učení, jako je deep learning a měřítka s bohatými anotacemi. Výzkum je však ve formálních jazycích s nízkými zdroji stále omezený. Skládá se z
významná mezera v popisu hovorového jazyka, zejména pro ty s nízkými zdroji, jako je perština. Aby bylo možné tuto mezeru zacílit
pro jazyky s nízkými zdroji navrhujeme „Large Scale Colloquial Persian Dataset“ (LSCP). LSCP je hierarchicky uspořádán do a
sémantická taxonomie, která se zaměřuje na víceúčelové neformální porozumění perskému jazyku jako komplexní problém. To zahrnuje uznání několika sémantických aspektů ve větách na lidské úrovni, které přirozeně zachycuje z vět z reálného světa. Věříme, že další vyšetřování a zpracování, stejně jako aplikace nových algoritmů a metod, může posílit obohacení počítačového porozumění a zpracování jazyků s nízkými zdroji. Navrhovaný korpus se skládá ze 120 milionů vět vycházejících z 27 milionů tweetů anotovaných stromem analýzy, tagy řeči, polaritou sentimentu a překladem do pěti různých jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Language recognition has been significantly advanced in recent years by means of modern machine learning methods such as deep
learning and benchmarks with rich annotations. However, research is still limited in low-resource formal languages. This consists of
a significant gap in describing the colloquial language especially for low-resourced ones such as Persian. In order to target this gap
for low resource languages, we propose a “Large Scale Colloquial Persian Dataset” (LSCP). LSCP is hierarchically organized in a
semantic taxonomy that focuses on multi-task informal Persian language understanding as a comprehensive problem. This encompasses
the recognition of multiple semantic aspects in the human-level sentences, which naturally captures from the real-world sentences. We
believe that further investigations and processing, as well as the application of novel algorithms and methods, can strengthen enriching
computerized understanding and processing of low resource languages. The proposed corpus consists of 120M sentences resulted from
27M tweets annotated with parsing tree, part-of-speech tags, sentiment polarity and translation in five different languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci navrhujeme algoritmus pro indukci morfologických sítí pro perštinu a turečtinu. Algoritmus využívá slovníky s morfematickou segmentací. Výsledná síť zachycuje jak derivační, tak flektivní relace. Algoritmus pro indukci sítě vychází buď z automaticky rozlišených afixů a kořenů, nebo z jednoduché klasifikační heuristiky. Obě varianty jsou empiricky vyhodnoceny. Pro perštinu používáme vlastní velký ručně segmentovaný slovník, pro turečtinu menší slovník publikovaný dříve. Ručně anotovaná data jsou algoritmem využita pro inicializaci sítě, která je následně rozšířena o formy pozorované v korpusech. Slovní formy, které nebyly přítomny v ručně anotovaných datech, segmentujeme řízenou i neřízenou verzí segmentačního nástroje Morfessor a nástrojem MorphSyn. Experimentální výsledky ukazují, jak inicializace ručně segmentovanými daty ovlivňuje finální kvalitu vygenerovaných sítí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this work, we propose an algorithm that induces morphological networks for Persian and Turkish. The algorithm uses morpheme-segmented lexicons for the two languages. The resulting networks capture both derivational and inflectional relations. The network induction algorithm can use either manually annotated lists of roots and affixes, or simple heuristics to distinguish roots from affixes. We evaluate both variants empirically. We use our large hand-segmented set of word forms in the experiments with Persian, which is contrasted with employing only a very limited manually segmented lexicon for Turkish that existed previously. The network induction algorithm uses gold segmentation data for initializing the networks, which are subsequently extended with additional corpus attested word forms that were unseen in the segmented data. For this purpose, we use existing morpheme-segmentation tools, namely supervised and unsupervised version of Morfessor, and (unsupervised) MorphSyn.
The experimental results show that the accuracy of segmented initial data influences derivational network quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku je představena COSTRA 1.0, dataset komplexních transformací vět. Dataset je určen ke studiu větných embeddingů nad rámec jednoduchých výměn slov nebo standardních parafrází.  COSTRA 1.0 obsahuje pouze věty v češtině, ale metoda konstrukce je univerzální a plánujeme ji použít i pro jiné jazyky.

Dataset obsahuje 4262 unikátních vět s průměrnou délkou 10 slov, ilustrujících 15 typů úprav, jako je zjednodušení, zobecnění nebo formální a neformální jazykové variace. Doufáme, že s tímto datovým souborem bychom měli být schopni otestovat sémantické vlastnosti větných embeddingů a možná dokonce najít nějaké topologicky zajímavé '' kostry '' v prostoru větných embeddingů. Předběžná analýza s využitím mnohojazyčných větných embeddingů LASER naznačuje, že nevykazuje požadované vlastnosti</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present COSTRA 1.0, a dataset of complex sentence transformations. The dataset is intended for the study of sentence-level embeddings beyond simple word alternations or standard paraphrasing. This first version of the dataset is limited to sentences in Czech but the construction method is universal and we plan to use it also for other languages.

The dataset consists of 4,262 unique sentences with an average length of 10 words, illustrating 15 types of modifications, such as simplification, generalization, or formal and informal language variation. The hope is that with this dataset, we should be able to test semantic properties of sentence embeddings and perhaps even to find some topologically interesting ''skeleton'' in the sentence embedding space. A preliminary analysis using LASER, multi-purpose multi-lingual sentence embeddings suggests that the LASER space does not exhibit the desired properties</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku představujeme nový dataset pro testování geometrických vlastností prostorů vět. Zaměřujeme se zejména na to, jak jsou v rámci větných embeddingů interpretovány komplexní jevy, jako jsou parafrázy, časy nebo zobecnění.
Dataset je přímým rozšířením Costra 1.0, kterou jsme obohatili o další vět a jejich porovnání.
Ukazujeme, že dostupným předtrénovaným větným embeddingům chybí základní předpoklad, aby synonymní věty byly zanořeny blíže k sobě než věty s výrazně odlišným významem.
Na druhou stranu se zdá, že některé embeddingy respektují lineární pořadí větných jevů jako je styl (formálnost a jednoduchost jazyka) nebo čas (minulost do budoucnosti).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present a new dataset for testing geometric properties of sentence embeddings spaces. In particular, we concentrate on examining how well sentence embeddings capture complex phenomena such paraphrases, tense or generalization. 
The dataset is a direct expansion of Costra 1.0, which we extended with more sentences and sentence comparisons.
We show that available off-the-shelf embeddings do not possess essential attributes such as having synonymous sentences embedded closer to each other than similar sentences with a significantly different meaning. 
On the other hand, some embeddings appear to capture the linear order of sentence aspects such as style (formality and simplicity of the language) or time (past to future).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Porovnáváme dva základní přístupy k vícejazyčnému vyhledávání informací: překlad dokumentů (DT) a překlad dotazů (QT). Naše experimenty jsou prováděny na datech CLEF eHealth 2013–2015, které obsahují anglické dokumenty a dotazy v několika evropských jazycích. S použitím Statistického strojového překladu (SMT) a Neurálního strojového překladu (NMT) a trénujeme několik systémů strojového překladu pro překlad neanglických dotazů do angličtiny (QT) a anglických dokumentů do jazyků dotazů (DT). Výsledky ukazují, že kvalita QT pomocí SMT je dostatečná k překonání výsledků vyhledávání s DT pro všechny jazyky. NMT pak dále zvyšuje kvalitu překladu a kvalitu vyhledávání pro QT i DT pro většinu jazyků, QT ale stále poskytuje obecně lepší výsledky vyhledávání než DT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a thorough comparison of two principal approaches to Cross-Lingual Information Retrieval: document translation (DT) and query translation (QT). Our experiments are conducted using the cross-lingual test collection produced within the CLEF eHealth information retrieval tasks in 2013–2015 containing English documents and queries in several European languages. We exploit the Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) paradigms and train several domain-specific and task-specific machine translation systems to translate the non-English queries into English (for the QT approach) and the English documents to all the query languages (for the DT approach). The results show that the quality of QT by SMT is sufficient enough to outperform the retrieval results of the DT approach for all the languages. NMT then further boosts translation quality and retrieval quality for both QT and DT for most languages, but still, QT provides generally better retrieval results than DT.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Výzkumní pracovníci Univerzity Karlovy, Švandova divadla a Akademie múzických umění v Praze v současné době pracují na zajímavém výzkumném projektu, který spojuje umělou inteligenci a robotiku s divadlem. Hlavním cílem jejich projektu je využít umělou inteligenci k vytvoření inovativního divadelního představení, které by mělo mít premiéru v lednu 2021.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Researchers at Charles University, Švanda Theater and the Academy of Performing Arts in Prague are currently working on an intriguing research project that merges artificial intelligence and robotics with theater. Their project's main objective is to use artificial intelligence to create an innovative theatrical performance, which is expected to premiere in January 2021.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme přístup k vícejazyčné syntéze řeči, který využívá koncepce meta-učení  – generování parametrů na základě kontextu – a produkuje přirozeně znějící vícejazyčnou řeč s využitím více jazyků a méně trénovacích dat než předchozí přístupy. Náš model je založen na Tacotronu 2 s plně konvolučním enkodérem vstupního textu, jehož váhy jsou predikovány samostatnou sítí – generátorem parametrů. Pro zlepšení klonování hlasu napříč jazyky náš model používá adversariální klasifikaci mluvčího s vrstvou obracející gradienty, která z enkodéru odstraňuje informace specifické pro daného mluvčího. Provedli jsme dva experimenty, abychom náš model porovnali s baseliny používajícími různé úrovně sdílení parametrů napříč jazyky a přitom vyhodnotili: 1) stabilitu a výkonnost při trénování na malém množství dat, 2) přesnost výslovnosti a kvalitu hlasu při code-switchingu (změně jazyka uprostřed věty). Pro trénování jsme použili dataset CSS10 a náš nový malý dataset založený na nahrávkách Common Voice v pěti jazycích. Ukazujeme, že náš model efektivně sdílí informace napříč jazyky a podle subjektivní evaluace vytváří přirozenější a přesnější vícejazyčnou řeč než baseliny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce an approach to multilingual speech synthesis which uses the meta-learning concept of contextual parameter generation and produces natural-sounding multilingual speech using more languages and less training data than previous approaches. Our model is based on Tacotron 2 with a fully convolutional input text encoder whose weights are predicted by a separate parameter generator network. To boost voice cloning, the model uses an adversarial speaker classifier with a gradient reversal layer that removes speaker-specific information from the encoder. We arranged two experiments to compare our model with baselines using various levels of cross-lingual parameter sharing, in order to evaluate: (1) stability and performance when training on low amounts of data, (2) pronunciation accuracy and voice quality of code-switching synthesis. For training, we used the CSS10 dataset and our new small dataset based on Common Voice recordings in five languages. Our model is shown to effectively share information across languages and according to a subjective evaluation test, it produces more natural and accurate code-switching speech than the baselines.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem workshopu je nabídnout platformu pro diskuse o stavu a budoucnosti hodnocení systémů generování přirozeného jazyka (NLG). Workshop vyzýval k pubilkaci jak archivních článků, tak abstraktů, zaměřených na evaluaci NLG včetně osvědčených postupů lidského hodnocení, kvalitativních studií, kognitivního zkreslení při lidském hodnocení atd. Sešlo se 12 přihlášených prezentací, bylo přijato deset článků a abstraktů, které byly na workshopu prezentovány jako postery. Tento sborník obsahuje pět archivních článků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of the workshop is to offer a platform for discussions on the status and the future of the evaluation of Natural Language Generation (NLG) systems. The workshop invited archival papers and abstracts on NLG evaluation including best practices of human evaluation, qualitative studies, cognitive bias in human evaluations etc.  The workshop received twelve submissions. Ten papers and abstracts were accepted and were presented as posters at the workshop. This proceedings volume contains the five archival papers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve svém projevu shrnu naše nedávné aktivity v překladu textu a řeči. Začnu projektem EU ELITR (https://elitr.eu/), kde se zaměřujeme na vysoce mnohojazyčný živý překlad řeči, který upozorňuje na technické problémy (pravděpodobně je všechny znáte), ale dotknu se i toho, jak získat lepší vstupy od koncových uživatelů, aby byla možná lepší kvalita překladu (naše aktivita na "odchozím překladu" v projektu EU Bergamot, https://browser.mt/). Na závěr požádám o spolupráci na svém celkovém úkolu základního výzkumu: správně určit význam a modelovat ho lidštějším způsobem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In my talk, I will summarize our recent activities in text and speech
translation. I will start with the EU project ELITR (https://elitr.eu/) where we
are aiming at highly multi-lingual live translation of speech, highlighting the
technical challenges (you probably know all of them) but I will also touch upon
getting a better input from the end users so that better translation quality is
possible (our activity on 'outbound translation' in the EU project Bergamot,
https://browser.mt/). Finally, I will ask for collaboration on my
overall basic-research quest: to get the meaning right and to model it in a more
human-like way.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Strojový překlad:

Projekt ELITR (European Live Translator)
usiluje o vytvoření systému pro překlad řeči
pro současné titulkování konferencí
a on-line schůzky až do 43
jazyků. Technologii testuje společnost
Nejvyššího kontrolního úřadu ČR a prostřednictvím alfaview®, německého online
konferenční systém. Další cíle projektu
mají posunout úroveň dokumentů a vícejazyčný strojový překlad, automatický
rozpoznávání řeči a shrnutí setkání.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>ELITR (European Live Translator) project
aims to create a speech translation system
for simultaneous subtitling of conferences
and online meetings targetting up to 43
languages. The technology is tested by
the Supreme Audit Office of the Czech Republic and by alfaview®, a German online
conferencing system. Other project goals
are to advance document-level and multilingual machine translation, automatic
speech recognition, and meeting summarization.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>MorfFlex CZ 2.0 je český morfologický slovník, který původně vyvinul Jan Hajič jako slovník kontroly pravopisu a lemmatizace. MorfFlex je seznam trojic lemma-značka-slovní forma. Pro každou slovní formu je kompletní morfologická informace kódována poziční značkou. Slovní formy jsou uspořádány do skupin (paradigma instancí nebo paradigmat ve zkratce) podle jejich formálního morfologického chování. Paradigma (množina slovník forem) je identifikováno jedinečným lemmatem. Kromě tradičních morfologických kategorií obsahuje popis také některé sémantické, stylistické a odvozené informace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>MorfFlex CZ (the latest version is MorfFlex CZ 2.0)  is the Czech morphological dictionary developed originally by Jan Hajič as a spelling checker and lemmatization dictionary. MorfFlex is a flat list of lemma-tag-wordform triples. For each wordform, full inflectional information is coded in a positional tag. Wordforms are organized into entries (paradigm instances or paradigms in short) according to their formal morphological behavior. The paradigm (set of wordforms) is identified by a unique lemma. Apart from traditional morphological categories, the description also contains some semantic, stylistic and derivational  information.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Bohatě anotovaný a žánrově diverzifikovaný jazykový zdroj, Prague Dependency Treebank - Consolidated 1.0 (PDT-C 1.0) je konsolidovaným vydáním stávajících PDT-korpusů s českými texty, jednotně anotovanými podle standardního anotačního schématu PDT. Korpusy zahrnuté do vydání PDT-C: Prague Dependency Treebank (psané noviny a texty časopisů tří žánrů); Česká část Prague Czech-English Dependency Treebank (přeložené finanční texty, z angličtiny), Prague Dependency Treebank of Spoken Czech (mluvená data, včetně audia a přepisu a anotace rekonstrukce řeči); PDT-Faust (texty generované uživateli). Rozdíl oproti samostatně publikovaným původním korpusům lze stručně popsat následovně: korpusy jsou publikovány v jednom balíčku, aby bylo umožněno jejich snadnější zpracování; data jsou doplněna o manuální lingvistickou anotaci na morfologické rovině a je přiložena nová verze morfologického slovníku; je přiložen společný valenční lexikon pro všechny čtyři původní části. Dokumentace poskytuje dva nástroje pro procházení a úpravy  (TrEd a MEd) a korpus je také k dispozici online pro vyhledávání pomocí PML-TQ.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A richly annotated and genre-diversified language resource, The Prague Dependency Treebank – Consolidated 1.0 (PDT-C 1.0, or PDT-C in short in the sequel) is a consolidated release of the existing PDT-corpora of Czech data, uniformly annotated using the standard PDT scheme. PDT-corpora included in PDT-C: Prague Dependency Treebank (the original PDT contents, written newspaper and journal texts from three genres); Czech part of Prague Czech-English Dependency Treebank (translated financial texts, from English), Prague Dependency Treebank of Spoken Czech (spokem data, including audio and transcripts and multiple speech reconstruction annotation); PDT-Faust (user-generated texts). The difference from the separately published original treebanks can be briefly described as follows: it is published in one package, to allow easier data handling for all the datasets; the data is enhanced with a manual linguistic annotation at the morphological layer and new version of morphological dictionary is enclosed; a common valency lexicon for all four original parts is enclosed. Documentation provides two browsing and editing desktop tools (TrEd and MEd) and the corpus is also available online for searching using PML-TQ.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme bohatě anotovaný a žánrově diverzifikovaný jazykový zdroj Prague Dependency Treebank-Consolidated 1.0 (PDT-C 1.0), jehož účelem je - jak tomu vždy bylo u rodiny Pražských závislostních korpusů - sloužit jako trénovací data pro různé typy úkolů NLP i pro jazykově orientovaný výzkum. PDT-C 1.0 obsahuje čtyři různé datové soubory s českými texty, jednotně anotované podle standardního schématu PDT. Texty pocházejí z různých zdrojů: novinové články, český překlad Wall Street Journal, přepsané dialogy a malé množství uživatelem vytvořených krátkých, často nestandardních jazykových segmentů, které se zadávají do webového překladače. Celkem obsahuje strom kolem 180 000 vět s jejich morfologickou, povrchovou a hlubokou syntaktickou anotací.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a richly annotated and genre-diversified language resource, the Prague Dependency Treebank-Consolidated 1.0 (PDT-C 1.0), the purpose of which is - as it always been the case for the family of the Prague Dependency Treebanks - to serve both as a training data for various types of NLP tasks as well as for linguistically-oriented research. PDT-C 1.0 contains four different datasets of Czech, uniformly annotated using the standard PDT scheme. The texts come from different sources: daily newspaper articles, Czech translation of the Wall Street Journal, transcribed dialogs and a small amount of user-generated, short, often non-standard language segments typed into a web translator. Altogether, the treebank contains around 180,000 sentences with their morphological, surface and deep syntactic annotation. The diversity of the texts and annotations should serve well the NLP applications as well as it is an invaluable resource for linguistic research, including comparative studies regarding texts of different genres. The corpus is publicly and freely available.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přestavujeme Ústav formální a aplikované lingvistiky u příležitosti relokace jeho časti do nové trojské budovy Impakt.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the Institute of formal and applied linguistics at the occasion of the relocation of its part to the new Impakt building in Troja.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve vzpomínkovém článku se vyčíslují několikerá setkání dvou významných českých lingvistů na poli editorském, pedagogickém, ale zejména jde o jejich styčné body v článcích gramatických.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the memorial number the meetings of two famous Czech linguists are described and evaluated with the focus on their grammatical contributions shared by them.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje hlavní principy Funkčního generativního popisu, jak ho navrhl Petr Sgall a jeho spolupracovníci, současně však podává i stručnou charakteristiku vybraných gramatických jevů popsaných v rámci tohoto přístupu. K těmto jevům patří zejména slovesná a substantivní valence a dále aktuální členění věty, zkoumané zejména ve vztahu k negaci a presupozici. Dále je věnována pozornost třídění elips a souvisejícímu jevu zvanému všeobecný aktant. Hlavní principy FGP byly aplikovány, ověřovány a dále zdokonalovány při budování jazykových zdrojů, rovněž v článku krátce popsaných, zejména v rodině Pražských závislostních korpusů a ve valenčních slovnících.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the present contribution, the main constituting features of the Functional Generative Description as proposed by Petr Sgall and his collaborators are introduced together with short characteristics of selected Czech grammatical phenomena within this framework. These phenomena include mainly the verbal and nominal valency and related issues and topic-focus articulation, esp. in relation to negation and presupposition. Further, attention is paid to the categorization of deletions and the related phenomenon of a general participant. Main tenets of FGD have been applied, verified and further refined in the Prague Dependency Treebanks family and valency lexicons, which are briefly characterized here as well.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku analyzujeme adverbiální určení s časovým významem se zvláštním důrazem na jejich formální realizaci. Tradiční klasifikace významů podle toho na jakou otázku adverbiální určení odpovídá (Kdy?, Od kdy?, Do kdy?, Jak dlouho?, Jak často?) vyžaduje přesnější subkategorizaci. Pro primární časové výrazy, které odpovídají na otázky Kdy? a Jak dlouho?, je navržen systém subfunktorů, zatímco pro sekundární významy Od kdy?, Do kdy?, nejsou odpovídající funktory rozděleny na subfunktory. Významy spojené s opakováním a frekvencí zde nejsou diskutovány, protože patří k popisu hranice mezi lexikonem a gramatikou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The system of adverbials with temporal meaning is analyzed here with specific emphasis on their surface expressions. The traditional classification for the meanings applied as responses for the question When?, Since when?, Till when?, How long?, How often? requires a more precise subcategorization with regard to systemic behavior of the adverbials. For the primary temporal expressions answering the questions When? and How long?, a system of subfunctors is proposed, while for the secondary meanings Since when?, Till when?, the corresponding functors are not splitted into subfunctors. The meanings connected with the repetition and the frequency are not discussed here as they belong to the description of the boundary between lexicon and grammar.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku se zaměřujeme na souhru reciprocity a valence. V češtině lze -- s ohledem na reciprocitu -- vymezit tři skupiny sloves: inherentně reciproční slovesa, odvozená inherentně reciproční slovesa a slovesa, který rys vzájemnosti ve svém významu nenesou, ale reciprocitu umožňují. Ukazujeme, že slovesa těchto tří skupin vyžadují jinou reprezentaci ve slovníku a v gramatice a že reflexivum se v jejich konstrukcích má různou slovnědruhovou klasifikací. Dále se soutředíme na ty aspekty reciprocity, které přesahují jazykový význam.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The notion of reciprocity is analyzed in linguistics from many different points of view. In this contribution, we focus on an interplay between reciprocity and valency. In Čzech (as in other languages), three groups of reciprocal verbs can be delimited: inherently reciprocal verbs, derived inherently reciprocal verbs and verbs without reciprocal feature in their lexical meaning that nevertheless allow reciprocity of some of their valency complementations. We show that verbs from the three given groups require different representation in lexicon and grammar and that in their syntactically reciprocalized constructions the clitic reflexive is of different part-of-speech classification. Moreover, they exhibit different types of ambiguity. Finally, we mention those aspects of reciprocity that cross the boundary between linguistically structured meaning and cognitive content.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek přináší přehled způsobů zachycení větného významu v jedenácti hloubkově-syntaktických formalismech, od takových, které jsou založené na lingvistických teoriích vyvíjených řadu desetiletí, až po jednodušší přístupy motivované zpracováním přirozeného jazyka. Nastiňujeme nejdůležitější charakteristiky každého formalismu a poté podrobněji rozebíráme způsoby zachycení konkrétních jazykových jevů napříč všemi formalismy. Snažíme se přitom objasnit společné rysy i rozdíly.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article gives an overview of how sentence meaning is represented in eleven deep-syntactic frameworks, ranging from those based on linguistic theories elaborated for decades to rather lightweight NLP-motivated approaches. We outline most important characteristics of each framework and then discuss how particular language phenomena are treated across those frameworks, while trying to shed light on commonalities as well as differences.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Autorka předkládá analýzu jisté kategorie v aktuálním členění věty, která se v češtině nazývá kulisa a zahrnuje místní a časové určení věty. Ukazuje se, že pokud má jít o kulisu, je toto určení v češtině součástí základu (tématu) věty a stojí na jejím začátku. V angličtině pak vzhledem k zásadě gramatického slovosledu může takové určení stát i na konci věty, v tom případě ovšem není nositelem větného přízvuku. Materiálem analýzy je anotovaný česko-anglický paralelní korpus PCEDT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The author presents analysis of a certain category of the functional sentence perspective, called in Czech "kulisa" (setting), and comprising local and temporal modifications of the sentence. The analysis demonstrates that in Czech, such a setting is a part of the topic (theme) of the sentence and stands at its beginning. In English, due to its grammatical word order, such a setting may be placed at the end of the sentence. In such a case it is not a bearer of the sentential pitch accent.  The analysis is based on the data from the annotated Czech-English corpus PCEDT.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nekrolog věnovaný životu a dílu zesnulého profesora Petra Sgalla, zakladatele počítačové lingvistiky v Československu, člena Pražského lingvistického kroužku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An obituary devoted to the life and work of the late Professor Petr Sgall, the founder of computational linguistics in Czechoslovakia, a member of the Prague Linguistic Circle.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Analýza tzv. fokalizátorů, tj. částic jako jsou anglické also, only, even, a jejich českých protějšků také, jenom, dokonce, vycházející z údajů anglicko-českého paralelního korpusu PCEDT, zaměřená na (i) za jakých podmínek lze o těchto fokalizátorech říci, že slouží jako diskurzní konektory, (ii) které konkrétní diskurzní vztahy jsou dotyčnými fokalizátory značeny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The analysis of the so-called focalizers, i.e. particles such as E. also, only, even, and their Czech counterparts také, jenom, dokonce, based on the data from the English–Czech parallel corpus PCEDT, focused on (i) in which respects and under which conditions these focalizers may  be said to serve as discourse connectives, (ii) which particular discourse relations are indicated by the  focalizers in question.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Naše podrobná analýza dat z anglicko-českého anotovaného paralelního korpusu PCEDT potvrzuje hypotézu, že anglické částice also, only a even i jejich české ekvivalenty hrají v zásadě diskurzivní roli explicitních konektorů, i když jiným způsobem a v jiné míře.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Our detailed analysis of the data from the English–Czech annotated parallel corpus PCEDT confirms the hypothesis that the particles also, only and even as well as their Czech equivalents play basically a discoursive role of explicit connectives, though in a different way and to a different extent.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek představí službu Evropské komise pro strojový překlad - eTranslation</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk will introduce a machine translation service - eTranslation</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Každodenně vzniká na světě nepřeberné množství dat, které mají různou formu a jsou z různých oblastí lidské činnosti. Značnou část tvoří data textová, a to v různých jazycích, např. novinové zprávy, odborné publikace, blogposty, ale i soukromá korespondence, konverzace na sociálních sítích a další. V oblasti lékařství a péče o zdraví tomu není jinak. Širokou škálu útvarů tvoří na jedné straně vysoce důvěryhodné a recenzované publikace pro odborníky, na straně druhé jsou to zcela nekontrolované rady na ten či onen neduh od virtuálních přátel, či dokonce záměrné dezinformace. Zvláštní místo pak mají lékařské zprávy typicky psané velmi specifickým jazykem.   

Příspěvek poskytne přehled výzkumu v oblasti zpracování přirozeného jazyka se zaměřením na různé typy textů z oblasti lékařství a zdraví, a
to zejména v kontextu evropských projektů Khresmoi a KConnect, které byly řešeny na Ústavu formální a aplikované lingvistiky, MFF UK.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Every day, a vast amount of data is created in the world, in various forms and areas of human activity. A significant part consists of textual data in various languages, such as news reports, professional publications, blog posts, but also private correspondence, conversations on social networks and more. This is no different in the field of medicine and health care. A wide range of data consists on the one hand of highly credible and peer-reviewed publications for experts, on the other hand they are completely uncontrolled advices on this or that illness from virtual friends, or even deliberate misinformation. A special place is given to medical reports typically written in a very specific language.

The paper will provide an overview of research in the field of natural language processing with a focus on various types of texts in the field of medicine and health, especially in the context of European projects Khresmoi and KConnect, which were addressed at the Institute of Formal and Applied Linguistics, MFF UK.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>SynSemClass je slovník česko-anglických slovesných synonym. Základními hesly ve slovníku jsou dvojjazyčné česko-anglické slovesné synonymní třídy, v nichž jsou obsažena synonymní česká a anglická slovesa (členy třídy), reprezentovaná jako valenční rámce (tj. slovesné významy), jejichž pojetí vychází z teorie Funkčně generativního popisu jazyka. Sémantická ekvivalence jednotlivých členů třídy byla stanovena na základě jejich kontextového valenčního chování usouvztažněného k situačně-kognitivnímu obsahu (sémantickým rolím). Synonymické vztahy jsou ve slovníku chápány volně, jednotlivé členy třídy jsou ve vztahu nikoli striktní (úplné) synonymie, ale ve vztahu významové podobnosti, tj. částečné synonymie. Předností slovníku je použití paralelního česko-anglického korpusu, PCEDT(https://catalog.ldc.upenn.edu/LDC2012T08 ) jako hlavního zdroje jazykových dat, které umožňuje tzv. "bottom-up" přístup, tj. od praxe k teorii.  Předností slovníku je rovněž propojení všech členů jednotlivých synonymních tříd s dalšími lexikálními zdroji, a to s hesly valenčních slovníků (PDT-Vallex - http://hdl.handle.net/11858/00-097C-0000-0023-4338-F, EngVallex- http://hdl.handle.net/11858/00-097C-0000-0023-4337-2), CzEngVallex - http://hdl.handle.net/11234/1-1512 a Vallex -  http://ufal.mff.cuni.cz/vallex/3.5/), a s hesly sémantických  databází (FrameNet - https://framenet.icsi.berkeley.edu/fndrupal/, VerbNet  - http://verbs.colorado.edu/verbnet/index.html, PropBank - http://verbs.colorado.edu/%7Empalmer/projects/ace.html, Ontonotes - http://verbs.colorado.edu/html_groupings/ a Wordnet - https://wordnet.princeton.edu/).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The SynSemClass synonym verb lexicon is a result of a project investigating semantic ‘equivalence’ of verb senses and their valency behavior in parallel Czech-English language resources, i.e., relating verb meanings with respect to contextually-based verb synonymy. The lexicon entries are linked to PDT-Vallex (http://hdl.handle.net/11858/00-097C-0000-0023-4338-F), EngVallex (http://hdl.handle.net/11858/00-097C-0000-0023-4337-2), CzEngVallex (http://hdl.handle.net/11234/1-1512), FrameNet (https://framenet.icsi.berkeley.edu/fndrupal/), VerbNet (http://verbs.colorado.edu/verbnet/index.html), PropBank (http://verbs.colorado.edu/%7Empalmer/projects/ace.html), Ontonotes (http://verbs.colorado.edu/html_groupings/), and English Wordnet (https://wordnet.princeton.edu/). Part of the dataset are files reflecting interannotator agreement.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>SynSemClass je slovník česko-anglických slovesných synonym. Základními hesly ve slovníku jsou dvojjazyčné česko-anglické slovesné synonymní třídy, v nichž jsou obsažena synonymní česká a anglická slovesa (členy třídy), reprezentovaná jako valenční rámce (tj. slovesné významy), jejichž pojetí vychází z teorie Funkčně generativního popisu jazyka. Sémantická ekvivalence jednotlivých členů třídy byla stanovena na základě jejich kontextového valenčního chování usouvztažněného k situačně-kognitivnímu obsahu (sémantickým rolím). Synonymické vztahy jsou ve slovníku chápány volně, jednotlivé členy třídy jsou ve vztahu nikoli striktní (úplné) synonymie, ale ve vztahu významové podobnosti, tj. částečné synonymie. Předností slovníku je použití paralelního česko-anglického korpusu, PCEDT(https://catalog.ldc.upenn.edu/LDC2012T08 ) jako hlavního zdroje jazykových dat, které umožňuje tzv. "bottom-up" přístup, tj. od praxe k teorii.  Předností slovníku je rovněž propojení všech členů jednotlivých synonymních tříd s dalšími lexikálními zdroji, a to s hesly valenčních slovníků (PDT-Vallex - http://hdl.handle.net/11858/00-097C-0000-0023-4338-F, EngVallex- http://hdl.handle.net/11858/00-097C-0000-0023-4337-2), CzEngVallex - http://hdl.handle.net/11234/1-1512 a Vallex -  http://ufal.mff.cuni.cz/vallex/3.5/), a s hesly sémantických  databází (FrameNet - https://framenet.icsi.berkeley.edu/fndrupal/, VerbNet  - http://verbs.colorado.edu/verbnet/index.html, PropBank - http://verbs.colorado.edu/%7Empalmer/projects/ace.html, Ontonotes - http://verbs.colorado.edu/html_groupings/ a Wordnet - https://wordnet.princeton.edu/).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>he SynSemClass synonym verb lexicon is a result of a project investigating semantic ‘equivalence’ of verb senses and their valency behavior in parallel Czech-English language resources, i.e., relating verb meanings with respect to contextually-based verb synonymy. The lexicon entries are linked to PDT-Vallex (http://hdl.handle.net/11858/00-097C-0000-0023-4338-F), EngVallex (http://hdl.handle.net/11858/00-097C-0000-0023-4337-2), CzEngVallex (http://hdl.handle.net/11234/1-1512), FrameNet (https://framenet.icsi.berkeley.edu/fndrupal/), VerbNet (http://verbs.colorado.edu/verbnet/index.html), PropBank (http://verbs.colorado.edu/%7Empalmer/projects/ace.html), Ontonotes (http://verbs.colorado.edu/html_groupings/), and English Wordnet (https://wordnet.princeton.edu/). Part of the dataset are files reflecting interannotator agreement.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slovník synonym propojuje anglická slovesa s jejich překladovými českými protějšky. Vychází z valence, tak jak je zpracovaná ve valenčních slovnících PDT_Vallex, EngVallex a CzEngVallex, a je založen na datech PCEDT. Sdružuje slovesná synonyma podle významu a chování ohledně sémantických rolí. Hesla jsou propojena s existujícími slovníky pro angličtinu i češtinu (FrameNet, VALLEX, VerbNet, PropBank, OntoNotes, WordNet).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This lexicon stores cross-lingual semantically similar verb senses in synonym classes extracted from a richly annotated parallel corpus, the Prague Czech-English Dependency Treebank. When building the lexicon, we make use of predicate-argument relations (valency) and link them to semantic roles; in addition, each entry is linked to several external lexicons of more or less “semantic” nature, namely FrameNet, WordNet, VerbNet, OntoNotes and PropBank, and Czech VALLEX. The aim is to provide a linguistic resource that can be used to compare semantic roles and their syntactic properties and features across languages within and across synonym groups (classes, or ’synsets’), as well as gold standard data for automatic NLP experiments with such synonyms, such as synonym discovery, feature mapping, etc. However, perhaps the most important goal is to eventually build an event type ontology that can be referenced and used as a human-readable and human-understandable “database” for all types of events, processes and states. While the current paper describes primarily the content of the lexicon, we are also presenting a preliminary design of a format compatible with Linked Data, on which we are hoping to get feedback during discussions at the workshop. Once the resource (in whichever form) is applied to corpus annotation, deep analysis will be possible using such combined resources as training data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento dokument shrnuje závěry tříleté studie o slovesných synonymech v překladu, založené na syntaktických i sémantických kritériích. Primárními jazykovými zdroji jsou stávající české a anglické lexikální a korpusové zdroje, konkrétně valenční lexikony ve stylu Pražského závislostního korpusu, FrameNet, VerbNet, PropBank, WordNet a paralelní Pražský česko-anglický závislostní korpus. Výsledný lexikon slovesných synonym (dříve nazývaný CzEngClass, nyní SynSemClass) a všechny související zdroje spojené se stávajícími lexikony  jsou veřejně a volně dostupné. Projekt samotný sice předpokládá ruční práci s anotacemi, ale předpokládáme, že výsledný zdroj (spolu se stávajícími) použijeme jako nezbytný zdroj pro vývoj automatických metod rozšíření takového lexikonu nebo vytvoření podobných lexikonů pro další jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper summarizes findings of a three-year study on
verb synonymy in translation based on both syntactic and semantic criteria
and reports on recent results extending this work. Primary language
resources used are existing Czech and English lexical and corpus
resources, namely the Prague Dependency Treebank-style valency lexicons,
FrameNet, VerbNet, PropBank, WordNet and the parallel Prague
Czech-English Dependency Treebank, which contains deep syntactic
and partially semantic annotation of running texts. The resulting lexicon
(called formerly CzEngClass, now SynSemClass) and all associated
resources linked to the existing lexicons and corpora following from
this project are publicly and freely available. While the project proper
assumes manual annotation work, we expect to use the resulting resource
(together with the existing ones) as a necessary resource for developing
automatic methods for extending such a lexicon, or creating similar lexicons
for other languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Při zpracování valence adjektiv a substantiv ve valenčním slovníku je třeba rozhodnout, zda je možné jejich afirmativní a negované formy zpracovat v jednom hesle, např. (ne)závislý, (ne)závislost. Kromě rozdílů ve významu afirmativní a negované formy, např. volnost (svoboda) vs. nevolnost (nepříjemný tělesný stav), by k vydělení zvláštního hesla vedla také jejich odlišná valence (srov. anglické dependent on vs. independent of). Naše korpusová analýza ukazuje, že negované formy českých adjektiv a substantiv mají až na výjimky stejnou předložkovou valenci jako jejich afirmativní formy (např. (ne)spokojený s čím, (ne)spokojenost s čím), liší se však jejich frekvence. Při zpracování afirmativních a negovaných forem v jednom hesle je vedle běžných případů třeba jednotným způsobem ošetřit jak méně doložené negované formy a jejich valenci (např. nevděčný za něco), tak případy, kdy frekvence negované formy a její valence naopak převažuje nad afirmativní formou (nepostradatelný pro někoho).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>When treating valency of adjectives and nouns in a valency lexicon it is necessary to decide whether their affirmative and negative form can be captured in one entry, e.g., (ne)závislý ‘(in)dependent’ and (ne)závislost ‘(in)dependence’. There are two cases that would lead to creating a separate entry in the lexicon, namely a difference between the meaning of an affirmative form and the corresponding negative form, e.g., volnost ‘freedom’ vs. nevolnost ‘indisposition’, and a difference in their valency, cf. Eng. dependent on and independent of. Focusing on valency complementations expressed by a prepositional group, we show that valency of negative forms of Czech adjectives and nouns in used corpus data is, with few exceptions, the same as valency of the corresponding affirmative forms, e.g. (ne)spokojený s čím ‘(un)satisfied with sth’, (ne)spokojenost s čím  ‘(dis)satisfaction with sth’. However, it often differs as for its frequency. When capturing valency of affirmative and negative forms in one entry in a lexicon it is important to treat not only the common cases but also less frequent negative forms and their valency (e.g., nevděčný za něco ‘ungrateful for sth’) as well as the opposite cases in which the frequency of a negative form and its valency considerably outnumbers the frequency of the affirmative form (e.g., nepostradatelný pro někoho ‘indispensable to sb’).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valenční slovník NomVallex I. zachycuje valenci českých deverbativních substantiv, která alespoň v jednom ze svých významů patří k sémantické třídě Communication (např. dotaz, dotázání se – dotazování se), Mental Action (např. plán, plánování) nebo Psych State (např. nenávist, nenávidění), a to v celkovém počtu 248 lexémů zahrnujících 505 lexikálních jednotek. Problematiku substantivní valence zpracovává v teoretickém rámci Funkčního generativního popisu, přičemž se důsledně opírá o korpusový materiál (korpusy řady SYN Českého národního korpusu a korpus Araneum Bohemicum Maximum). Navazuje na valenční slovník VALLEX, odkud přebírá anotační schéma, v relevantních případech rozdělení základových slovesných lexémů do lexikálních jednotek a jejich přiřazení k sémantickým třídám. U substantiv zachycuje všechny jejich lexikální významy, přičemž rozlišuje základní kategoriální významy událost (např. žádání, dovtípení se), abstraktní výsledek děje (např. žádost), vlastnost (např. důvtip), substance (např. komunikace (silnice)) a kontejner (např. počet). S cílem demonstrovat valenční chování různých typů substantivních derivátů jsou do slovníku zařazeny jak kmenové deriváty (odvozené sufixy -ní/-tí, např. žádání, navrhování - navržení, namítání - namítnutí), tak kořenové deriváty (odvozené různými sufixy, včetně sufixu nulového, např. žádost, návrh, námitka), je-li možné je od příslušných základových sloves utvořit. Kritériem pro zařazení do slovníku je příslušnost k jedné ze tří výše uvedených sémantických tříd, označení události nebo abstraktního výsledku děje a projev nesystémového valenčního chování, zejména nesystémové formy valenčních doplnění. Valenci zachycuje v podobě valenčního rámce, který pro každé valenční doplnění uvádí funktor a množinu možných morfematických vyjádření, a příkladů, které se vyskytly v použitých korpusech. Jeho cílem je na korpusových datech ukázat, v jakých syntaktických strukturách se zkoumaná substantiva mohou vyskytovat, proto dokládá všechny kombinace aktantů (ve všech formách uvedených ve valenčním rámci), které se u daných substantiv v použitých korpusových datech vyskytly.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The NomVallex I. lexicon describes valency of Czech deverbal nouns belonging to three semantic classes, namely Communication (e.g. dotaz 'question'), Mental Action (e.g. plán 'plan') and Psych State (e.g. nenávist 'hatred'). It covers stem-nominals (e.g. dotazování 'asking') as well as root-nominals (e.g. dotaz 'question'). It captures all lexical meanings of the nouns, differentiating between basic “categorial” meanings of action (e.g. žádání (si) ‘asking’, dovtípení (se) ‘inferring’), abstract result of an action (e.g. žádost ‘request’), property / quality (e.g. důvtip ‘ingenuity’), material object (e.g. pohled ‘postcard’), or container / quantity (e.g. počet ‘number’). Nouns matching the following criteria were included in the lexicon: its semantic class is either Communication, Mental Action or Psych State, its categorial meaning is action or abstract result of an action, and it exhibits non-systemic valency behaviour (especially non-systemic forms of participants) in at least one of its meanings. In total, the lexicon includes 505 lexical units in 248 lexemes. Valency properties are captured in the form of valency frames, specifying valency slots and their morphemic forms. The lexicon aims to illustrate the full range of syntactic structures of noun phrases, and thus the syntactic behaviour of every lexical unit is exemplified with all combinations of its participants (in all forms specified in the valency frame) which were found in the corpus data (CNC SYNv8 and Araneum Bohemicum Maximum). The lexicon is created within the theoretical framework of Functional Generative Description and was inspired by the VALLEX lexicon; it adopts the VALLEX annotation scheme, and in relevant cases, deverbal nouns captured in NomVallex I. mirror the division of lexemes into lexical units and the assignment of lexical units to semantic classes of the base verbs captured in the VALLEX lexicon.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valenční slovník NomVallex I. zachycuje valenci českých deverbativních substantiv, která alespoň v jednom ze svých významů patří k sémantické třídě Communication (např. dotaz, dotázání se – dotazování se), Mental Action (např. plán, plánování) nebo Psych State (např. nenávist, nenávidění), a to v celkovém počtu 248 lexémů zahrnujících 505 lexikálních jednotek. Problematiku substantivní valence zpracovává v teoretickém rámci Funkčního generativního popisu, přičemž se důsledně opírá o korpusový materiál (korpusy řady SYN Českého národního korpusu a korpus Araneum Bohemicum Maximum). Navazuje na valenční slovník VALLEX, odkud přebírá anotační schéma, v relevantních případech rozdělení základových slovesných lexémů do lexikálních jednotek a jejich přiřazení k sémantickým třídám. U substantiv zachycuje všechny jejich lexikální významy, přičemž rozlišuje základní kategoriální významy událost (např. žádání, dovtípení se), abstraktní výsledek děje (např. žádost), vlastnost (např. důvtip), substance (např. komunikace (silnice)) a kontejner (např. počet). S cílem demonstrovat valenční chování různých typů substantivních derivátů jsou do slovníku zařazeny jak kmenové deriváty (odvozené sufixy -ní/-tí, např. žádání, navrhování - navržení, namítání - namítnutí), tak kořenové deriváty (odvozené různými sufixy, včetně sufixu nulového, např. žádost, návrh, námitka), je-li možné je od příslušných základových sloves utvořit. Kritériem pro zařazení do slovníku je příslušnost k jedné ze tří výše uvedených sémantických tříd, označení události nebo abstraktního výsledku děje a projev nesystémového valenčního chování, zejména nesystémové formy valenčních doplnění. Valenci zachycuje v podobě valenčního rámce, který pro každé valenční doplnění uvádí funktor a množinu možných morfematických vyjádření, a příkladů, které se vyskytly v použitých korpusech. Jeho cílem je na korpusových datech ukázat, v jakých syntaktických strukturách se zkoumaná substantiva mohou vyskytovat, proto dokládá všechny kombinace aktantů (ve všech formách uvedených ve valenčním rámci), které se u daných substantiv v použitých korpusových datech vyskytly.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The NomVallex I. lexicon describes valency of Czech deverbal nouns belonging to three semantic classes, namely Communication (e.g. dotaz 'question'), Mental Action (e.g. plán 'plan') and Psych State (e.g. nenávist 'hatred'). It covers stem-nominals (e.g. dotazování 'asking') as well as root-nominals (e.g. dotaz 'question'). It captures all lexical meanings of the nouns, differentiating between basic “categorial” meanings of action (e.g. žádání (si) ‘asking’, dovtípení (se) ‘inferring’), abstract result of an action (e.g. žádost ‘request’), property / quality (e.g. důvtip ‘ingenuity’), material object (e.g. pohled ‘postcard’), or container / quantity (e.g. počet ‘number’). Nouns matching the following criteria were included in the lexicon: its semantic class is either Communication, Mental Action or Psych State, its categorial meaning is action or abstract result of an action, and it exhibits non-systemic valency behaviour (especially non-systemic forms of participants) in at least one of its meanings. In total, the lexicon includes 505 lexical units in 248 lexemes. Valency properties are captured in the form of valency frames, specifying valency slots and their morphemic forms. The lexicon aims to illustrate the full range of syntactic structures of noun phrases, and thus the syntactic behaviour of every lexical unit is exemplified with all combinations of its participants (in all forms specified in the valency frame) which were found in the corpus data (CNC SYNv8 and Araneum Bohemicum Maximum). The lexicon is created within the theoretical framework of Functional Generative Description and was inspired by the VALLEX lexicon; it adopts the VALLEX annotation scheme, and in relevant cases, deverbal nouns captured in NomVallex I. mirror the division of lexemes into lexical units and the assignment of lexical units to semantic classes of the base verbs captured in the VALLEX lexicon.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V nejnovější mluvnici Františka Štíchy a kol. jsou v kapitolách věnovaných slovotvorbě  bezpříponová dějová substantiva představena v kontextech, v nichž jsou užívána dokonavě i nedokonavě, tedy ve vztahu k oběma motivujícím slovesům tvořícím vidový pár (např. hvizd < hvízdat_impf|hvízdnout_pf). V návaznosti na Štíchovu analýzu se v této práci věnujeme bezpříponovým substantivům s kořeny klád a lož (např. překládat_impf|přeložit_pf), obsahujícím různé sufixy, např. -ba (sklad-ba), -ka (pře-klád-ka) nebo -0 (pře-klad-0). Přestože tato substantiva formálně odkazují pouze k jednomu členu vidové dvojice (přeložka – přeložit_pf, překládka < překládat_impf), korpusová data ukazují, že jsou užívána ve stejných kontextech a jsou vzájemně zaměnitelná. Tato skutečnost podporuje naši hypotézu, že bezpříponová dějová substantiva je třeba systematicky vztahovat k celé vidové dvojici základových sloves, a to i v případě jejich formálně odlišných kořenů (tedy přeložka < překládat_impf|přeložit_pf, překládka < překládat_impf|přeložit_pf). Součástí naší analýzy je důkladný popis významů a valence jednotlivých substantiv s kořeny klád a lož.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the word-formation chapters of František Štícha’s newest grammar, suffixless action nouns are provided with contexts demonstrating that they can be interpreted both perfectively and imperfectively, i.e., in relation to the verbs with both the perfective and imperfective aspect (e.g. hvizd ‘whistle’ < hvízdat_impf ‘to whistle_impf’|hvízdnout_pf ‘to whistle_pf’). In support of Štícha’s analysis, the present paper deals with action nouns derived from verbs with the suppletive roots klád and lož (e.g. překládat_impf ‘to replace_impf’|přeložit_pf ‘to replace_pf’), containing various suffixes, e.g. -ba (sklad-ba ‘composition’), -ka (pře-klád-ka ‘replacement’) or -0 (pře-klad-0 ‘translation’). Although these nouns refer formally to a particular member of the aspectual pair (přeložka ‘replacement’ – přeložit_pf ‘to replace_pf’, překládka ‘replacement’ < překládat_impf ‘to replace_impf’), corpus data documents that they are used in the same contexts and are mutually interchangeable, which is in line with our proposal to analyze each of the nouns with respect to both verbs, though with formally different roots (přeložka ‘replacement’ < překládat_impf ‘to replace_impf’|přeložit_pf ‘to replace_pf’, překládka ‘replacement’ < překládat_impf ‘to replace_impf’| přeložit_pf ‘to replace_pf’). An elaborate description of the meanings and valency of the nominalizations with the lož/klád roots is provided in the paper.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CzeDLex je elektronický slovník českých diskurzních konektorů s daty pocházejícími z velkého korpusu anotovaného diskurzními vztahy. Jeho nová verze CzeDLex 0.6 přináší podstatně větší podíl ručně zpracovaných položek. Struktura slovníku byla upravena, aby umožňovala výskyt primárních konektorů s více položkami pro jeden diskurzní typ. Představujeme novou verzi slovníku a ukazujeme možnosti vyhledávání různých typů informací ve slovníku pomocí PML-Tree Query.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CzeDLex is an electronic lexicon of Czech discourse connectives with its data coming from a large treebank annotated with discourse relations. Its new version CzeDLex 0.6 is significantly larger with respect to manually processed entries. Also, its structure has been modified to allow for primary connectives to appear with multiple entries for a single discourse sense. We present the new version of the lexicon and demonstrate possibilities of mining various types of information from the lexicon using PML-Tree Query.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Mnohé lingvistické teorie a anotační projekty obsahují hloubkově-syntaktickou a/nebo sémantickou rovinu. I když většina z těchto projektů mířila na více než jeden jazyk, žádný z nich se ani zdaleka neblíží počtu jazyků, které jsou pokryté projektem Universal Dependencies (UD). Ve své přednášce nejprve proberu tzv. rozšířené univerzální závislosti (Enhanced Universal Dependencies, EUD), sadu sémanticky orientovaných rozšíření, která byla navržena v rámci projektu Universal Dependencies (ale v současné době jsou k dispozici pouze pro malý počet jazyků). Představím také předběžná pozorování z právě probíhající soutěže v automatickém větném rozboru do EUD (https://universaldependencies.org/iwpt20/). Ve druhé části představím další rozšíření, která navrhujeme v rámci projektu Deep UD a která přesahují rámec současných anotačních pravidel UD. Zaměřím se na dva aspekty: jak tato rozšíření mohou být užitečná při porozumění přirozenému jazyku strojem a do jaké míry je můžeme získat z povrchově syntaktické anotace automaticky, pro mnoho typologicky odlišných jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Many linguistic theories and annotation frameworks contain a deep-syntactic and/or semantic layer. While many of these frameworks have been applied to more than one language, none of them is anywhere near the number of languages that are covered in Universal Dependencies (UD). In my talk, I will first discuss Enhanced Universal Dependencies (EUD), a set of semantically-oriented enhancements that have been proposed within the framework of Universal Dependencies (but which are still available only for a small number of languages). I will also present some preliminary observations from the current shared task on parsing into EUD (https://universaldependencies.org/iwpt20/). In the second part, I will present some additional enhancements, called Deep UD, which extend beyond the official UD guidelines. I will focus on two aspects: how can these enhancements be useful for natural language understanding, and to what extent can they be obtained semi-automatically from the surface annotation for many typologically different languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies (univerzální závislosti) je mezinárodní projekt a komunita, která se snaží poskytnout morfologicky a syntakticky anotovaná data pro mnoho jazyků v jednotném anotačním stylu. Informujeme o českém grantovém projektu MANYLA, který byl jednou z hnacích sil UD.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is an international project and community that seeks to provide morphologically and syntactically annotated data for many languages, using a uniform annotation style. We report on the Czech grant project MANYLA that was one of the driving forces behind UD.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Po nejméně dvě desetiletí jsou syntakticky anotované korpusy (treebanky) důležitým zdrojem jak pro jazykovědný výzkum, tak pro vývoj počítačových aplikací, které potřebují porozumět přirozenému jazyku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>For at least two decades, syntactically annotated corpora (treebanks) have been instrumental both in linguistic research and in development of natural language understanding applications. Even though the application aspect somewhat diminished with the current surge of neural networks, the classical who-did-what-to-whom type of questions still cannot be answered without understanding the syntax of the sentence.
In order to facilitate the usage of treebanks, it is desirable that they capture same phenomena the same way, across languages and domains. This is exactly the goal of Universal Dependencies (UD): a community effort to define cross-linguistically applicable annotation guidelines for morphology and syntax, and to provide data annotated following those guidelines. In my talk, I will introduce UD, its main principles and the current state, and I will discuss some of the challenges that harmonization and multi-lingual annotation presents. In the last part of the talk, I will touch upon the latest development towards Enhanced UD and Deep UD.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pražské tektogramatické grafy (PTG) představují reprezentaci pro zachycení významu, která má kořeny v tektogramatické rovině Pražského závislostního korpusu (PDT) a je teoreticky podložena Funkčním generativním popisem jazyka (FGP). Ve své současné podobě byly PTG připraveny pro soutěž CoNLL 2020 v analýze významových reprezentací napříč formalismy (MRP). Jsou automaticky generovány z pražských závislostních korpusů a uloženy v grafovém formátu založeném na JSONu. Převod je částečně ztrátový; v tomto článku popisujeme, které části anotace byly do PTG zahrnuty a jak jsou v PTG reprezentovány.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Prague Tectogrammatical Graphs (PTG) is a meaning representation framework that originates in the tectogrammatical layer of the Prague Dependency Treebank (PDT) and is theoretically founded in Functional Generative Description of language (FGD). PTG in its present form has been prepared for the CoNLL 2020 shared task on Cross-Framework Meaning Representation Parsing (MRP). It is generated automatically from the Prague treebanks and stored in the JSON-based MRP graph interchange format. The conversion is partially lossy; in this paper we describe what part of annotation was included and how it is represented in PTG.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je třinácté vydání treebanků UD, verze 2.7.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the thirteenth release of UD Treebanks, Version 2.7.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je dvanácté vydání treebanků UD, verze 2.6.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the twelfth release of UD Treebanks, Version 2.6.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme systém pro předpovídání rysů z World Atlas of Language Structures (WALS), který se účastnil soutěže pořádané u příležitosti typologického workshopu SIGTYP 2020.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our system for prediction of typological features from the World Atlas of Language Structures (WALS), which participated in the shared task organized as a part of the SIGTYP 2020 workshop.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Parlament České republiky se skládá ze dvou komor: Poslanecké sněmovny (dolní komora) a Senátu (horní komora). V naší práci se zaměřujeme na agendu a dokumenty týkající se Poslanecké sněmovny. Konkrétně věnujeme zvláštní pozornost stenografickým protokolům, které zaznamenávají schůze Poslanecké sněmovny. Naším cílem je kontinuálně kompilovat protokoly do korpusu ParCzech kódovaného TEI a zpřístupnit ho uživatelsky přívětivějším způsobem, než tak činí Parlament ČR. V první fázi kompilace ParCzech obsahuje protokoly z let 2013+, které zpřístupňujeme a prohledáváme ve webové platformě TEITOK.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Parliament of the Czech Republic consists of two chambers: the Chamber of Deputies (Lower House) and the Senate (Upper House). In our work, we focus on agenda and documents that relate to the Chamber of Deputies. Namely, we pay particular attention to stenographic protocols that record the Chamber of Deputies’ meetings. Our overall goal is to continually compile the protocols into the TEI encoded corpus ParCzech and make the corpus accessible in a more user friendly way than the Parliament publishes the protocols. In the very first stage of the compilation, the ParCzech corpus consists of the 2013+ protocols that we make accessible and searchable in the TEITOK web-based platform.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Korpus ParCzech PS7 1.0 je úplně první část rodiny korpusů z Parlamentu České republiky. ParCzech PS7 1.0 obsahuje stenoprotokoly z Poslanecké sněmovny sedmého volebního období z let 2013-2017. Audio záznamy jsou přiloženy. Přepisy jsou poskytnuty v původním HTML formátu a navíc zkonvertovány do TEI-odvozeném formátu pro korpusového správce TEITOK. Korpus je automaticky obohacen o morfologii a jmenné entity programy MorphoDita a NameTag.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The ParCzech PS7 1.0 corpus is the very first member of the corpus family of data coming from the Parliament of the Czech Republic. ParCzech PS7 1.0 consists of stenographic protocols that record the Chamber of Deputies' meetings held in the 7th term between 2013-2017. The audio recordings are available as well. Transcripts are provided in the original HTML as harvested, and also converted into TEI-derived XML format for use in TEITOK corpus manager. The corpus is automatically enriched with the morphological and named-entity annotations using the procedures MorphoDita and NameTag.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Korpus ParCzech PS7 2.0 je druhá verze korpusu ParCzech PS7, který obsahuje stenografické protokoly sedmého volebního období v letech 2013-2017. Protokoly jsou v jejich puvodním HTML formátu, v TEI formátu a TEI-odvozeném formátu pro korpusového správce TEITOK. Audio záznamy jsou přiloženy. Korpus je automaticky obohacen o morfologii, syntax a jmenné entity programy UDPipe 2 a NameTag 2.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The ParCzech PS7 2.0 corpus is the second version of ParCzech PS7 consisting of stenographic protocols that record the Chamber of Deputies' meetings held in the 7th term between 2013-2017. The protocols are provided in their original HTML format, TEI format and TEI-derived format to make them searchable in the TEITOK corpus manager. Their audio recordings are available as well. The corpus is automatically enriched with the morphological, syntactic, and named-entity annotations using the procedures UDPipe 2 and NameTag 2.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ačkoli název této zprávy přebírá slovo „Manuál“ z předchozích verzí, její účel již primárně není sloužit jako návod pro anotátory. Spíše se pokouší popsat současný stav morfologické anotace ve vydání Prague Dependency Treebank - Consolidated 1.0 (PDT-C 1.0) Věříme, že pokyny mohou být užitečné pro uživatele dat PDT-C 1.0, stejně jako pro přípravu nových.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Although the title of this report inherits the word "Manual" from the previous versions, it is no more intended to guide the annotators. Rather it attempts to describe the current state of the morphological annotation in the Prague Dependency Treebank – Consolidated 1.0 (PDT-C 1.0) We believe that the guidelines can be of use to the users of the PDT-C 1.0 data, as well as for possible preparation of new data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Na základě analýzy bohatého materiálu Pražské databáze syntaktických forem a funkcí (ForFun), která obsahuje psané texty i přepisy dialogů, popisujeme funkce a formy prostorových určení a sledujeme rozdíly v jejich vyjádření v psané i mluvené podobě text. Zaměřujeme se na základní formální vyjádření příslovečných určení: (i) příslovce, (ii) závislá věta a (iii) předložková fráze. Pro každý formální výraz (i) - (iii) popisujeme charakteristické rysy prostorových určení v mluvené komunikaci, které jsme získali z databáze ForFun.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>. Based on an analysis of the rich material of the Prague Database of Syntactic Forms and Functions (ForFun), which contains written texts as well as transcriptions of dialogues, we describe functions and forms of spatial adverbials and observe differences in their expression in written and spoken text. We focus on basic formal expressions of adverbials: (i) an adverb, (ii) a dependent clause and (iii) a prepositional phrase. For each formal expression (i) – (iii), we describe the characteristic features of the spatial adverbials in the spoken communication that we gained from the ForFun database.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valenční slovník českých sloves (též VALLEX 4.0) poskytuje informace o valenční struktuře českých sloves v jejich jednotlivých významech, které charakterizuje pomocí glos a příkladů; tyto základní údaje jsou doplněny o některé další syntaktické a sémantické charakteristiky. VALLEX 4.0 zachycuje 4 700 českých sloves, která odpovídají více než 11 000 lexikálním jednotkám, tedy vždy danému slovesu v daném významu.
VALLEX je budován jako odpověď na potřebu rozsáhlého a kvalitního veřejně dostupného slovníku, který by obsahoval syntaktické a sémantické charakteristiky českých sloves a byl přitom široce uplatnitelný v aplikacích pro zpracování přirozeného jazyka. Při navrhování koncepce slovníku byl proto kladen důraz na všestranné využití jak pro lidského uživatele, tak pro automatické zpracování češtiny.
VALLEX 4.0 obohacuje informace z předchozích verzí o charakteristiku sloves vyjadřujících reflexivní a reciproční významy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>VALLEX 4.0 provides information on the valency structure (combinatorial potential) of verbs in their particular senses, which are characterized by glosses and examples. VALLEX 4.0 describes 4 700 Czech verbs in more than 11 000 lexical units, i.e., given verbs in the given senses.
VALLEX 4.0 is a is a collection of linguistically annotated data and documentation, resulting from an attempt at formal description of valency frames of Czech verbs. In order to satisfy different needs of different potential users, the lexicon is distributed (i) in a HTML version (the data allows for an easy and fast navigation through the lexicon) and (ii) in a machine-tractable form, so that the VALLEX data can be used in NLP applications.
češtiny.
VALLEX 4.0 provides (in addition to information from previous versions) also characteristics of verbs expressing reciprocity and reflexivity.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento balíček obsahuje data použitá v soutěži IWPT 2020. Obsahuje trénovací, vývojová a testovací (vyhodnocovací) datové množiny. Data jsou založena na podmnožině vydání 2.5 Universal Dependencies (http://hdl.handle.net/11234/1-3105), ale některé treebanky obsahují další obohacené anotace nad rámec UD 2.5.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This package contains data used in the IWPT 2020 shared task. It contains training, development and test (evaluation) datasets. The data is based on a subset of Universal Dependencies release 2.5 (http://hdl.handle.net/11234/1-3105) but some treebanks contain additional enhanced annotations. Moreover, not all of these additions became part of Universal Dependencies release 2.6 (http://hdl.handle.net/11234/1-3226), which makes the shared task data unique and worth a separate release to enable later comparison with new parsing algorithms. The package also contains a number of Perl and Python scripts that have been used to process the data during preparation and during the shared task. Finally, the package includes the official primary submission of each team participating in the shared task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje úlohu parsingu (syntaktické analýzy) do rozšířených Universal Dependencies, popisuje data použitá pro trénování a vyhodnocení, jakož i evaluační metriky. Stručně shrnujeme jednotlivé přístupy a probíráme výsledky úlohy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This overview introduces the task of parsing into enhanced universal dependencies, describes the datasets used for training and evaluation, and evaluation metrics. We outline various approaches and discuss the results of the shared task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Náš příspěvek chce představit novou verzi tzv. „pražského“ morfologického slovníku MorfFlex.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes the new version of the "Prague" morphological dictionary MorfFlex.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Metoda stylometrie nejčastějšími slovy neumožňuje přímé srovnání původních textů a jejich překladů, tj. Napříč jazyky. Například v dvojjazyčné česko-německé textové sbírce obsahující paralelní texty (originály a překlady v obou směrech spolu s českými a německými překlady z jiných jazyků) by autoři neshlukovali mezi jazyky, protože seznamy četných slov pro jakékoli české texty jsou zjevně bude se více podobat německému textu a naopak. Pokusili jsme se přijít s interlinguou, která by odstranila rysy specifické pro jazyk a případně zachovala jazykově nezávislé rysy signálu jednotlivého autora, pokud existují. Každý jazykový protějšek jsme označili, lemmatizovali a analyzovali odpovídajícím jazykovým modelem v UDPipe, který poskytuje jazykové označení, které je do značné míry vícejazyčné. Odstranili jsme výstup jazykově závislých položek, ale to samo o sobě moc nepomohlo. V dalším kroku jsme transformovali lemma obou jazykových protějšků na sdílená pseudolemata na základě velmi hrubého česko-německého glosáře s 95,6% úspěšností. Ukazujeme, že u stylometrických metod založených na nejčastějších slovech se můžeme obejít bez překladů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The method of stylometry by most frequent words does not allow direct comparison of original texts and their translations, i.e. across languages. For instance, in a bilingual Czech-German text collection containing parallel texts (originals and translations in both directions, along with Czech and German translations from other languages), authors would not cluster across languages, since frequency word lists for any Czech texts are obviously going to be more similar to each other than to a German text, and the other way round. We have tried to come up with an interlingua that would remove the language-specific features and possibly keep the linguistically independent features of individual author signal, if they exist. We have tagged, lemmatized, and parsed each language counterpart with the corresponding language model in UDPipe, which provides a linguistic markup that is cross-lingual to a significant extent. We stripped the output of language-dependent items, but that alone did not help much. As a next step, we transformed the lemmas of both language counterparts into shared pseudolemmas based on a very crude Czech-German glossary, with a 95.6% success. We show that, for stylometric methods based on the most frequent words, we can do without translations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vzájemná provázanost přejímání slov a slovotvorby (specificky derivace) je demonstrována na příkladu přípon -ismus a -ita,  které jsou uváděny mezi nejběžnějšími příponami v přejatých substantivech v češtině. Obě přípony odvozují abstraktní substantiva, nicméně v mnoha dalších ohledech se liší. Přípona -ismus se kombinuje se základy, které vytvářejí větší derivační rodiny než základy kombinované s -ita, ale i substantiva na -ita většinou sdílejí svůj kořen s několika dalšími deriváty. Analýzou vybraných derivátů a jejich vzájemných vztahů napříč velkého množství derivačních rodin ukazuji, že velikost a vnitřní struktura derivačních rodin může poskytnout informace o významu analyzovaných derivátů. Význam přípon je popsán pomocí vzorců, do kterých jsou zahrnuty relevantní deriváty s explicitně vyznačenými derivačními vztahy. S použitím těchto vzorců je možné vysvětlit sémantické nuance, které u přejaých slov v češtině zatím nebyly popsány.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The interplay between borrowing and word formation (in particular, derivation) is documented on the example of the suffixes -ismus and -ita,  which are listed among the most common suffixes in loan nouns in Czech. They are both used to form abstract nouns but differ in many aspects. The suffix -ismus combines with bases that form larger derivational families than those of -ita but still most nouns in -ita share their root with several other derivatives, too. By analysing selected derivatives and their mutual relations across a large amount of derivational families, I demonstrate that the size and inner structure of derivational families can provide significant knowledge about the meaning of the formations analysed. The meanings of the suffixes are described using patterns which involve the most relevant derivatives with explicitly marked derivational relations. Using the patterns, it is possible to explain semantic nuances that have not been described with loan words in Czech so far.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V návaznosti na aktuální diskuzi o derivačních paradigmatech ve slovotvorbě přednáška představí dvě případové studie, jejichž cílem je identifikace opakujících se vzorců (paradigmat) v derivační morfologii češtiny. První studie se zabývá odvozováním bezpříponových dějových substantiv, druhá studie slovotvorných chováním přejatých sloves v češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Concurring with the recent discussion on derivational paradigms (Bonami &amp; Strnadová 2018, Fernandéz-Alcaina &amp; Čermák 2018, Štekauer 2014, and others), I will present two case studies which aim at identification of repeating patterns (paradigms) in derivational morphology of Czech.

First, suffixless action nouns derived from verbs (skok 'jump') are analysed and contrasted with unmotivated suffixless nouns (noc 'night') from which denominal verbs are derived. The corpus data reveal that suffixless action nouns correspond mostly to a pair of verbs with aspect-changing suffixes (cf. skákat : skočit 'to jump.PFV|IPFV' > skok 'jump'). In contrast, verbs that are based on nouns use a prefix to change the aspect (noc 'night' > nocovat 'to stay.IPFV overnight' > přenocovat 'to stay.PFV overnight').

The difference between verbs with verbal roots vs. nominal roots is elaborated into two patterns which are exploited in the second case study on verbal morphology. Assuming that, according to my analysis, Czech native verbs with verbal roots prefer to change the aspect by substituting the suffix (navrhnout : navrhovat 'to propose.PFV|IPFV') while verbs with nominal roots attach a prefix (nocovat : přenocovat 'to stay.PFV|IPFV overnight'), a clear dominance of the prefixation pattern with loan verbs (e.g. kontrolovat : zkontrolovat 'to control.PFV|IPFV') over the suffixation pattern (riskovat : risknout 'to risk.PFV|IPFV') suggests that loan verbs in Czech resemble native denominal verbs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V češtině se nulový odvozovací sufix předpokládá u substantiv s významem děje nebo výsledku. Tato substantiva jsou považována za deverbální deriváty, protože tyto významy jsou primárně vyjadřovány slovesy. V příspěvku jsou substantiva s nulovou příponu analyzována ve vztahu k základovým slovesům i ve vztahu ke konkurenčním substantivům s nenulovými příponami.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In  Czech, derivational  zero  suffixes  are  assumed  to  occur  in  nouns  that  denote  an  action  or  a  result. These nouns are viewed as derived from verbs because the action meaning is primarily expressed by verbs. As verbs in Czech are obligatorily marked by a thematic suffix conveying grammatical aspect, deverbal  zero-suffix  nouns  belong  to  the  minority  of  derivations  that  have  a  simpler  morphemic structure  than  their  base  words  and  violate  thus  the  general  account  of  derivation  as  an  affix-adding process. In the paper, zero-derived nouns are analysed with respect to the motivating verbs and to competing nominalizations with overt suffixes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je komunitní projekt, jehož cílem je vytvořit mnohojazyčnou sbírku korpusů anotovaných způsobem, který je konzistentní napříč jazyky, v rámci závislostního lexikalistického přístupu. Anotace sestává z lingvisticky motivované segmentace na slova, z morfologické roviny obsahující lemmata, univerzální slovní kategorie a standardizované morfologické rysy, jakož i ze syntaktické roviny, která se zaměřuje na syntaktické vztahy mezi predikáty, argumenty a volnými rozvitími.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is an open community effort to create cross-linguistically consistent treebank annotation for many languages within a dependency-based lexicalist framework. The annotation consists in a linguistically motivated word segmentation; a morphological layer comprising lemmas, universal part-of-speech tags, and standardized morphological features; and a syntactic layer focusing on syntactic relations between predicates, arguments and modifiers. In this paper, we describe version 2 of the guidelines (UD v2), discuss the major changes from UD v1 to UD v2, and give an overview of the currently available treebanks for 90 languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Mezijazyčné vyhledávání informací (pro Elitr LangTools workshop při Eurosai 2020)</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Cross-Lingual Information Retrieval (for Elitr LangTools workshop at Eurosai 2020)</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nástroj Cross-Lingual Information Retrieval (CLIR) vám umožňuje vyhledávat v dokumentech v různých jazycích a pomocí vlastního jazyka zadávat vyhledávací dotaz i zobrazovat výsledky vyhledávání díky automatizovanému strojovému překladu.
V ukázce můžete vyhledávat v auditech a dalších dokumentech publikovaných českými a belgickými nejvyššími kontrolními institucemi. Demo funguje v angličtině, němčině, francouzštině a češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Cross-Lingual Information Retrieval (CLIR) tool allows you to search in documents in various languages, using your own language both to enter the search query as well as to display the search results, thanks to automated machine translation.
In the demo, you can search in audits and other documents published by the Czech and Belgian Supreme Audit Institutions. The demo works in English, German, French and Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento deliverable reportuje přípravu workshopu LangTools na kongres EUROSAI 2020, zaměřeného na prezentaci jazykových technologií zástupcům nejvyšších kontrolních úřadů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This deliverable reports on the preparation of the LangTools workshop at EUROSAI Congress 2020, aimed at presenting NLP Technologies to supreme audit institution (SAI) representatives.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme projekt THEaiTRE, ve kterém se snažíme počítačově vygenerovat scénář divadelní hry.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the THEaiTRE project, in which we are trying to computationally generate a theatre play script.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku představíme projekt THEaiTRE, který si dává za cíl automaticky vygenerovat scénář divadelní hry. Podíváme se, jak to děláme, jak se nám to zatím daří a na jaké problémy narážíme.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the paper, we introduce the THEaiTRE project, which aims to automatically generate a theatre play script. We look at how this is done, how successful we have been so far and what problems we are facing.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Umělá inteligence stále více zasahuje do našich životů a doplňuje či nahrazuje lidi v různých činnostech, jako je řízení auta, překlad textu či vytváření krátkých novinových zpráv. My jsme se rozhodli jí zadat opravdu náročný úkol: napsat novou divadelní hru. Jak se s tím popere? Zvládne umělá inteligence tvořit umělecká díla? A měli bychom jí to vůbec dovolit? Jaká by měla být role umělé inteligence ve společnosti?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Artificial intelligence is increasingly intruding into our lives, complementing or replacing people in various activities such as driving a car, translating text or making short news stories. We decided to give it a really challenging assignment: to write a new play. How's he going to deal with it? Can artificial intelligence create works of art? Should we even let it? What should be the role of artificial intelligence in society?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Karel Čapek psal v roce 1920 o robotech, jsou v dnešní době roboti schopni napsat divadelní hru o Karlu Čapkovi? Chceme, aby umělá inteligence zasahovala do umění? Jaké je povědomí společnosti o využívání robotiky? A jak vývoj nastane v budoucnu? Odpovědi na tyto otázky přinese právě projekt THEAITRE a Rudolf ve své prezentaci na InnoCampu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Karel Čapek wrote about robots in 1920, are robots capable of writing a play about Karel Čapek these days? Do we want artificial intelligence to interfere with art? What is society's awareness of the use of robotics? And what will happen in the future? It is THEAITRE and Rudolf that will provide answers to these questions in their presentation at the InnoCamp.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme THEaiTRE, začínající výzkumný projekt zaměřený na automatické generování scénářů divadelních her. Tento článek podává přehled související literatury a návrh přístupu, který plánujeme použít.
Konkrétně jde o generativní neuronové jazykové modely a metody hierarchického generování, s podporou automatické sumarizace a strojového překladu, doplněné o přístupy používající manuální lidské vstupy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present THEaiTRE, a starting research project aimed at automatic generation of theatre play scripts.
This paper reviews related work and drafts an approach we intend to follow.
We plan to adopt generative neural language models and hierarchical generation approaches, supported by summarization and machine translation methods, and complemented with a human-in-the-loop approach.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Projekt THEaiTRE se provokativně ptá, zda robot zvládne napsat divadelní hru. V uplynulých měsících vědci z MFF UK ve spolupráci s odborníky z DAMU, dramaturgy a herci programovali umělou inteligenci tak, aby zvládla generovat scénář hry. Experiment má plánovanou premiéru na 25. 1. ve Švandově divadle – přesně 100 let od premiéry hry Karla Čapka R.U.R., jejíž výročí tím zároveň připomíná.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The THEaiTRE project provocatively asks if a robot can write a play. Over the past few months, scientists at MFF UK, in collaboration with DAMU experts, dramatists and actors, have programmed artificial intelligence to be able to generate a play script. The experiment is scheduled to premiere at 25. 11 in the Švanda Theatre – exactly 100 years since the premiere of Karel Čapek's R.U.R. play, whose anniversary it also marks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Mnohé studie zkoumaly reprezentace vznikající v neuronových sítích trénovaných pro úkoly NLP a zkoumaly, jaké jazykové informace na úrovni slov mohou být v reprezentacích zakódovány.
V klasickém sondování je klasifikátor trénován na reprezentacích k získání cílové jazykové informace.
Hrozí však, že si klasifikátor pouze zapamatuje jazykové popisky pro jednotlivá slova, místo toho, aby z vyjádření vytěžil jazykové abstrakce, čímž by vykázal falešně pozitivní výsledky.
I když bylo vynaloženo značné úsilí na minimalizaci problému s memorizací, úkol skutečně změřit množství memorizace odehrávající se v klasifikaci byl zatím podceněn.
V naší práci navrhujeme jednoduchou obecnou metodu měření memorizačního efektu, založenou na symetrickém výběru srovnatelných sad viděných a neviděných slov pro trénování a testování.
Naši metodu lze použít k explicitní kvantifikaci množství memorování, které se děje, aby bylo možné zvolit adekvátní nastavení a výsledky sondování bylo možné spolehlivěji interpretovat.
To dokládáme ukázkou naší metody na případové studii sondování slovních druhů v natrénovaném enkodéru neuronového strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Multiple studies have probed representations emerging in neural networks trained for end-to-end NLP tasks and examined what word-level linguistic information may be encoded in the representations.
In classical probing, a classifier is trained on the representations to extract the target linguistic information.
However, there is a threat of the classifier simply memorizing the linguistic labels for individual words, instead of extracting the linguistic abstractions from the representations, thus reporting false positive results.
While considerable efforts have been made to minimize the memorization problem, the task of actually measuring the amount of memorization happening in the classifier has been understudied so far.
In our work, we propose a simple general method for measuring the memorization effect, based on a symmetric selection of comparable sets of test words seen versus unseen in training.
Our method can be used to explicitly quantify the amount of memorization happening in a probing setup, so that an adequate setup can be chosen and the results of the probing can be interpreted with a reliability estimate.
We exemplify this by showcasing our method on a case study of probing for part of speech in a trained neural machine translation encoder.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento dokument představuje výsledky WMT20
Metriky sdíleného úkolu. Účastníci byli dotázáni
k hodnocení výstupů překladatelských systémů
soutěžících v WMT20 News Translation s automatickými metrikami. Deset výzkumů
skupiny předložily 27 metrik, z nichž čtyři
jsou „metriky“ bez odkazů. Kromě toho jsme
vypočítali  pět základních metrik, včetně SENT BLEU, BLEU, TER a CHR F us-
SacreBLEU. Všechny metriky dobře korelují
na úrovni systému, dokumentu a segmentu s
oficiálním prekladem.
Předkládáme rozsáhlou analýzu vlivu
referenčních překladů o metrické spolehlivosti,
jak dobře automatické metriky hodnotí lidské preklady a také upozorňujeme na velké nesrovnalosti
mezi metrickým a lidským skóre při hodnocení 
systémem MT. Nakonec zkoumame,
zda můžeme použít automatické metriky k označení
nesprávného hodnocení lidí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the WMT20
Metrics Shared Task. Participants were asked
to score the outputs of the translation systems
competing in the WMT20 News Translation
Task with automatic metrics. Ten research
groups submitted 27 metrics, four of which
are reference-less “metrics”. In addition,
we computed five baseline metrics, including SENT BLEU, BLEU, TER and CHR F using the SacreBLEU scorer. All metrics were evaluated on how well they correlate at the
system-, document- and segment-level with
the WMT20 official human scores.
We present an extensive analysis on influence
of reference translations on metric reliability,
how well automatic metrics score human trans-
lations, and we also flag major discrepancies
between metric and human scores when eval-
uating MT systems. Finally, we investigate
whether we can use automatic metrics to flag
incorrect human ratings.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Čtení s porozuměním je značně studovaný úkol s obrovskými trénovacími datasety v angličtině. Tato práce se zaměřuje na tvorbu systému čtení s porozuměním pro češtinu, aniž by byla potřeba ručně anotovaná česká trénovací data. Nejprve jsme automaticky přeložili datasety SQuAD 1.1 a SQuAD 2.0 do češtiny, abychom vytvořili trénovací a validační data, která zveřejňujeme na http://hdl.handle.net/11234/1-3249. Poté jsme natrénovali a vyhodnotili několik referenčních modelů založených na architekturách BERT a XLM-RoBERTa. Náš hlavní příspěvek však spočívá v modelech mezijazykového přenosu. Model XLM-RoBERTa, trénovaný na anglických datech a vyhodnocený na češtině, dosahuje velmi konkurenceschopných výsledků, jen přibližně o 2 procenta horší než model trénovaný na přeložených českých datech. Tento výsledek je mimořádně dobrý, vezmeme-li v úvahu skutečnost, že model během trénování neviděl žádná česká data. Mezijazykový přenos je velmi flexibilní a je pomocí něj možné vytvořit model v jakémkoli jazyce, pro který máme dostatek čistých dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Reading comprehension is a well studied task, with huge training datasets in English. This work focuses on building reading comprehension systems for Czech, without requiring any manually annotated Czech training data. First of all, we automatically translated SQuAD 1.1 and SQuAD 2.0 datasets to Czech to create training and development data, which we release at http://hdl.handle.net/11234/1-3249. We then trained and evaluated several BERT and XLM-RoBERTa baseline models. However, our main focus lies in cross-lingual transfer models. We report that a XLM-RoBERTa model trained on English data and evaluated on Czech achieves very competitive performance, only approximately 2 percent points worse than a model trained on the translated Czech data. This result is extremely good, considering the fact that the model has not seen any Czech data during training. The cross-lingual transfer approach is very flexible and provides a reading comprehension in any language, for which we have enough monolingual raw texts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento dokument představuje výsledky sdílených
úkolů ze 7. workshopu o překladech do asijských jazyků (WAT2020). WAT2020 se účastnilo 20 týmů a 14 týmů předložilo výsledky překladů pro lidské hodnocení. Obdrželi jsme také 12 písemných podání k výzkumu, z nichž 7 bylo přijato
s výjimkou. Zhruba 500 výsledků překladů bylo
odevzdáno na automatickém hodnotícím serveru
a vybraná podání byla vyhodnocena ručně.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the shared
tasks from the 7th workshop on Asian transla­tion (WAT2020). For the WAT2020, 20 teams
participated in the shared tasks and 14 teams
submitted their translation results for the hu­man evaluation. We also received 12 research paper submissions out of which 7 were ac­cepted. About 500 translation results were
submitted to the automatic evaluation server,
and selected submissions were manually eval­uated.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Automatické metody a metriky, které hodnotí různá kritéria kvality automaticky generovaných textů, jsou důležité pro vývoj systémů NLG, protože vytvářejí opakovatelné výsledky a umožňují rychlý vývojový cyklus. Představujeme zde pokus automatizovat hodnocení přirozenosti textu, což je velmi důležitá charakteristika metod generování přirozeného jazyka. Namísto spoléhání se na lidské účastníky při hodnocení nebo označování textových vzorků navrhujeme automatizovat proces pomocí metriky lidské pravděpodobnosti, kterou definujeme, a diskriminačního postupu založeného na velkých předtrénovaných jazykových modelech s jejich rozděleními pravděpodobnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Automatic methods and metrics that assess various quality criteria of automatically generated texts are important for developing NLG systems because they produce repeatable results and allow for a fast development cycle. We present here an attempt to automate the evaluation of text naturalness which is a very important characteristic of natural language generation methods. Instead of relying on human participants for scoring or labeling the text samples, we propose to automate the process by using a human likeliness metric we define and a discrimination procedure based on large pretrained language models with their probability distributions. We analyze the text probability fractions and observe how they are influenced by the size of the generative and discriminative models involved in the process. Based on our results, bigger generators and larger pretrained discriminators are more appropriate for a better evaluation of text naturalness. A comprehensive validation procedure with human participants is required as follow up to check how well this automatic evaluation scheme correlates with human judgments.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Schopnost předpovědět délku vědecké práce může být užitečná v mnoha situacích. Tato práce definuje úlohu predikce délky papíru jako regresní problém a uvádí několik experimentálních výsledků pomocí populárních modelů strojového učení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Being able to predict the length of a scientific paper may be helpful in numerous situations. This work defines the paper length prediction task as a regression problem and reports several experimental results using popular machine learning models. We also create a huge dataset of publication metadata and the respective lengths in
number of pages. The dataset will be freely available and is intended to foster research in this domain. As future work, we would like to
explore more advanced regressors based on neural networks and big pretrained language models.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Automatic evaluation of various text quality criteria produced by data-driven intelligent methods is very common and useful because it is cheap, fast, and usually yields repeatable results. In this paper, we present an attempt to automate the human likeliness evaluation of the output text samples coming from natural language generation methods used to solve several tasks. We propose to use a human likeliness score that shows the percentage of the output samples from a method that look as if they were written by a human. Instead of having human participants label or rate those samples, we completely automate the process by using a discrimination procedure based on large pretrained language models and their probability distributions. As follow up, we plan to perform an empirical analysis of human-written and machine-generated texts to find the optimal setup of this evaluation approach. A validation procedure involving human participants will also check how the automatic evaluation correlates with human judgments.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Automatické vyhodnocení různých kritérií kvality textu vytvořených inteligentními metodami založenými na datech je velmi běžné a užitečné, protože je levné, rychlé a obvykle přináší opakovatelné výsledky. V tomto příspěvku prezentujeme pokus automatizovat hodnocení lidské pravděpodobnosti výstupních textových vzorků pocházejících z metod generování přirozeného jazyka používaných k řešení několika úkolů.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Poslední vývoj v učení neuronovými sítěmi výrazně zvýšilo kvalitu automaticky generovaných souhrnů a klíčových slov dokumentu s tím, že je třeba ještě větších trénovacích korpusů. V tomto příspěvku představujeme dvě velké datové sady pro sumarizaci textu (OAGSX) a generování klíčových slov bsahující 34 milionů, resp. 23 milionů záznamů. Data byla získána ze sítě Open Academic Graph obsahující výzkumné profily a publikace. Pečlivě jsme zpracovávali každý záznam a také zkoušeli
několik extraktivních a abstraktivních metod pro obě úlohy, abychom vytvořili základ pro další výzkum. Dále jsme ukázali výkon těchto metod
kontrolou jejich výstupu. Brzy bychom rdi užili modelování témat na dvou množinách, abychom vytvořili dvě podmnožiny článků ze specifičtějších
disciplín.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Recent developments in sequence-to-sequence learning with neural networks have considerably improved the quality of automatically generated text summaries and document keywords, stipulating the need for even bigger training corpora. Metadata of research articles are usually easy to find online and can be used to perform research on various tasks. In this paper, we introduce two huge datasets for text summarization (OAGSX) and keyword generation (OAGKX) research, containing 34 million and 23 million records, respectively. The data were retrieved from the Open Academic Graph which is a network of research profiles and publications. We carefully processed each record and also tried several extractive and abstractive methods of both tasks to create performance baselines for other researchers. We further illustrate the performance of those methods previewing their outputs. In the near future, we would like to apply topic modeling on the two sets to derive subsets of research articles from more specific disciplines.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V rámci hodnotící kampaně Mezinárodní konference o překladu mluveného jazyka (IWSLT 2020) bylo letos zařazeno šest náročných úloh: i) simultánní překlad řeči, ii) překlad videořeči, iii) offline překlad řeči, iv) překlad konverzační řeči, v) Open domain překlad a vi) překlad řeči nerodilých mluvčí. Tento dokument uvádí cíle každé trati, údaje a metriky hodnocení a informuje o výsledcích obdržených podání.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The evaluation campaign of the International Conference on Spoken Language Translation (IWSLT 2020) featured this year six challenge tracks: (i) Simultaneous speech translation, (ii) Video speech translation, (iii) Offline speech translation, (iv) Conversational speech translation, (v) Open domain translation, and (vi) Non-native speech translation. A total of teams participated in at least one of the tracks. This paper introduces each track’s goal, data and evaluation metrics, and reports the results of the received submissions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme společný příspěvek Edinburské univerzity a Univerzity Karlovy do soutěže v česko-anglickém strojovém překladu - WMT 2020 Shared Task on News Translation. Naše rychlé a kompaktní studentské modely destilují znalosti z většího, pomalejšího učitelského modelu. Jsou navrženy tak, aby nabízely dobrý kompromis mezi kvalitou překladu a efektivitou inference. Na česko-anglických testovacích sadách WMT 2020 dosahují rychlosti překladu přes 700 zdrojových slov za sekundu na jednom procesoru, což umožňuje neuronový strojový překlad na spotřebním hardwaru bez GPU.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe the joint submission of the University of Edinburgh and Charles University, Prague, to the Czech/English track in the WMT 2020 Shared Task on News Translation. Our fast and compact student models distill knowledge from a larger, slower teacher. They are designed to offer a good trade-off between translation quality and inference efficiency. On the WMT 2020 Czech-English test sets, they achieve translation speeds of over 700 whitespace-delimited source words per second on a single CPU thread, thus making neural translation feasible on consumer hardware without a GPU.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentuji způsob, jak využít stenografované záznamy jednání PSPČR pro účely trénování systémů rozpoznávání řeči. V článku je uvedena metoda pro získání dat, zarovnání na úrovni slov a výběr spolehlivých částí nepřesného přepisu. Konečně prezentuji systém rozpoznávání řeči natrénovaný na těchto i jiných datech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I present a way to leverage the stenographed recordings of the Czech parliament meetings for purposes of training a speech-to-text system. The article presents a method for scraping the data, acquiring word-level alignment and selecting reliable parts of the imprecise transcript. Finally, I present an ASR system trained on these and other data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Praxe tetování je součástí lidské kultury od počátku dějin. Navzdory kulturním posunům směrem k dobrovolné povaze a společenské přijatelnosti tetování se v tomto textu zaměřujeme na některé případy tetování jako nedobrovolného „značkování“, což je způsob vykonávání naprosté fyzické kontroly nad člověkem, jako v případě lidských otroků. Ve své empirické dimenzi vychází naše studie z Archivu vizuální historie nadace USC Shoah.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The practice of tattooing has been part of human culture since the dawn of history. In this paper, despite the mentioned cultural shifts toward the voluntary nature and social acceptability of tattooing, we focus on some cases of tattooing as an involuntary “branding” practice, which is a historical form of exercising total physical control over one’s person, such as in the case of human slaves. In terms of empirical material, our study is based on the USC Shoah Foundation's Visual History Archive.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme PyVallex, nástroj pro prezentaci, prohledávání/filtrování, editaci/rozšiřování a automatické zpracování strojově čitelných lexikografických dat v původním textovém formátu. Tento systém se skládá z několika komponent, parseru, nástrojů pro validaci dat, vyhledávání pomocí regulárních výrazů, frameworku typu map-reduce pro sestavování komplexnějších dotazů a analýz a webového rozhraní.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present PyVallex, a Python-based system for presenting, searching/filtering, editing/extending and automatic processing of machine readable lexicon data originally available in a text-based format. The system consists of several components: a parser for the specific lexicon format used in several valency lexicons, a data-validation framework, a regular expression based search engine, a map-reduce style framework for querying the lexicon data and a web-based interface integrating complex search and some basic editing capabilities. PyVallex provides most of the typical functionalities of a Dictionary Writing System (DWS), such as multiple presentation modes for the underlying lexical database, automatic evaluation of consistency tests, and a mechanism of merging updates coming from multiple sources. The editing functionality is currently limited to the client-side interface and edits of existing lexical entries, but additional script-based operations on the database are also possible. The code is published under the open source MIT license and is also available in the form of a Python module for integrating into other software.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozpoznávání řeči a strojový překlad učinily v posledních desetiletích velký pokrok a vedly ke vzniku praktických systémů, které dovedou mapovat jednu jazykovou posloupnost na druhou. Přestože jsou stále dostupnější data ve více modalitách jako je zvuk a video, nejmodernější systémy jsou ze své podstaty unimodální v tom smyslu, že jako vstup berou jedinou modalitu - ať už řeč nebo text. Zkušenosti z toho, jak se učí lidé učí jazyk, ukazují, že různé modality nesou navzájem se dolňující se signály, které jsou často klíčové pro řešení mnoha jazykových úkolů. V tomto článku popisujeme datovou sadu How2, rozsáhlou, kolekci videí s přepisy a jejich překlady. Ukazujeme, jak lze tuto datovou sadu využít k vývoji systémů pro různé jazykové úlohy a představujeme řadu modelů. V rámci řešení těchto úloh zjišťujeme, že budování multimodálních architektur, které by fungovaly lépe, než jejich unimodální protějšek, zůstává i nadále velkou výzvou. To ponechává velký prostor pro zkoumání pokročilejších řešení, která plně využívají multimodální povahu datového souboru How2, a také obecného směřování multimodálního učení s využitím jiných multimodálních datových sad.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Speech recognition and machine translation have made major progress over the past decades, providing practical systems to map one language sequence to another. Although multiple modalities such as sound and video are becoming increasingly available, the state-of-the-art systems are inherently unimodal, in the sense that they take a single modality⁠—either speech or text⁠—as input. Evidence from human learning suggests that additional modalities can provide disambiguating signals crucial for many language tasks. Here, we describe the How2 dataset, a large, open-domain collection of videos with transcriptions and their translations. We then show how this single dataset can be used to develop systems for a variety of language tasks and present a number of models meant as starting points. Across tasks, we find that building multi-modal architectures that perform better than their unimodal counterpart remains a challenge. This leaves plenty of room for the exploration of more advanced solutions that fully exploit the multi-modal nature of the How2 dataset, and the general direction of multimodal learning with other datasets as well.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>I když se při strojovém vyhodnocování překladů ve velké míře používají metriky se středem věty, výkonnost na úrovni dokumentů je pro profesionální použití přinejmenším stejně důležitá. V tomto dokumentu upozorňujeme na podrobné hodnocení na úrovni dokumentů zaměřené na markables (výrazy nesoucí většinu významu dokumentu) a negativní dopad různých markable error fenomenů na překlad.

Pro anotační experiment dvou fází jsme vybrali české a anglické dokumenty přeložené systémy, které byly předány do WMT20 News Translation Task. Tyto dokumenty jsou z domén News, Audit a Lease. Ukazujeme, že kvalita a také druh chyb se mezi doménami výrazně liší. Tento systematický rozptyl je v protikladu k automatickým výsledkům hodnocení.

Zkoumáme, které specifické značení je problematické pro systémy MT, a zakončíme analýzou vlivu značených chybových typů na výkonnost MT měřenou lidmi a automatickými hodnotícími nástroji.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Even though sentence-centric metrics are
used widely in machine translation evaluation,
document-level performance is at least equally
important for professional usage. In this paper,
we bring attention to detailed document-level
evaluation focused on markables (expressions
bearing most of the document meaning) and
the negative impact of various markable error
phenomena on the translation.
For an annotation experiment of two phases,
we chose Czech and English documents translated
by systems submitted to WMT20 News
Translation Task. These documents are from
the News, Audit and Lease domains. We show
that the quality and also the kind of errors
varies significantly among the domains. This
systematic variance is in contrast to the automatic
evaluation results.
We inspect which specific markables are problematic
for MT systems and conclude with an
analysis of the effect of markable error types
on the MT performance measured by humans
and automatic evaluation tools.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Často musí uživatelé internetu vyprodukovat text v cizím jazyce, který znají jen velmi málo a nejsou schopni ověřit kvalitu překladu. Úkol nazýváme "outbound translation" a zkoumáme ho zavedením open-source modulárního systému Ptakopět. Jeho hlavním účelem je kontrola interakce člověka se systémy MT posílenými o další subsystémy, jako je zpětný překlad a odhad kvality. Navazujeme na experiment s (českými) lidskými anotátory, kteří mají za úkol vytvářet otázky v jazyce, kterým nemluví (němčina), s pomocí Ptakopětu. Zaměřujeme se na tři případy využití v reálném světě (komunikace s IT podporou, popis administrativních záležitostí a kladení encyklopedických otázek), z nichž získáváme vhled do různých strategií, které uživatelé používají, když čelí outbound translation. Je známo, že zpětný překlad je pro hodnocení systémů MT nespolehlivý, ale naše experimentální hodnocení dokládá, že pro uživatele funguje velmi dobře, přinejmenším u systémů MT střední kvality.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>It  is  not  uncommon  for  Internet  users  to  have  to  produce  a  text  in  a  foreign  language  they  have  very  little  knowledge  of  and  areunable  to  verify  the  translation  quality.We  call  the  task  “outbound  translation”  and  explore  it  by  introducing  an  open-sourcemodular system Ptakopˇet.  Its main purpose is to inspect human interaction with MT systems enhanced with additional subsystems,such  as  backward  translation  and  quality  estimation.    We  follow  up  with  an  experiment  on  (Czech)  human  annotators  tasked  toproduce  questions  in  a  language  they  do  not  speak  (German),  with  the  help  of  Ptakopˇet.   We  focus  on  three  real-world  use  cases(communication with IT support, describing administrative issues and asking encyclopedic questions) from which we gain insight intodifferent strategies users take when faced with outbound translation tasks.  Round trip translation is known to be unreliable for evaluat-ing MT systems but our experimental evaluation documents that it works very well for users, at least on MT systems of mid-range quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje systém Ptakopět, který byl vyvinut pro zkoumání chování užívatelů při práci s interaktivním systémem pro tzv. odchozí překlad. Při něm se překládá do jazyka, kterému užívatel sice nerozumí, ale zodpovědnost za kvalitu překladu ostáva na něm.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The problems of outbound translation, machine translation user confidence and user interaction are not yet fully explored. The goal of the online modular system Ptakopět is to provide tools for studying these phenomena. Ptakopět is a proof-of-concept system for examining user interaction with enhanced machine translation. It can be used either for actual translation or running experiments on human annotators. In this article, we aim to describe its main components and to show how to use Ptakopět for further research. We also share tips for running experiments and setting up a similar online annotation environment.

Ptakopět was already used for outbound machine translation experiments, and we cover the results of the latest experiment in a demonstration to show the research potential of this tool. We show quantitatively that even though backward translation improves machine-translation user experience, it mainly increases users' confidence and not the translation quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Strojový překlad z angličtiny:

V tomto dokumentu předkládáme naše podání k úkolu překládat nenarozené projevy pro IWSLT 2020. Naším hlavním příspěvkem je navržený systém rozpoznávání řeči, který se skládá z akustického modelu a modelu foném-tographeme. Jako mezičlánek používáme telefony. Dokazujeme, že navrhovaný ropovod překonává komerčně využívané automatické rozpoznávání řeči (ASR) a zavádí jej na dráhu ASR. Doplňujeme toto ASR o běžně dostupné systémy MT, abychom se zapojili také do skladby pro překlad řeči.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present our submission to
the Non-Native Speech Translation Task for
IWSLT 2020. Our main contribution is a proposed speech recognition pipeline that consists of an acoustic model and a phoneme-to grapheme model. As an intermediate representation, we utilize phonemes. We demonstrate that the proposed pipeline surpasses
commercially used automatic speech recognition (ASR) and submit it into the ASR track.
We complement this ASR with off-the-shelf
MT systems to take part also in the speech
translation track.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška pojednává o vztahu implicitnosti diskurzních vztahů a dalších faktorů, jako je jejich frekvence, specifičnost jejich sémantiky, a o  signálech spoluvytvářejících význam implicitního diskurzního vztahu (konfrontace: slovosled, dlouhé a krátké tvary zájmen, přízvuk; přípustka: intonace).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The lecture deals with the relation of implicit discourse relations and other factors, such as their frequency, specificity of their semantics, and features signalling the meaning of implicit discourse relation (confrontation: word order, long and short forms of pronouns, accent; concession: intonation).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme velký soubor plenárních zasedání českého parlamentu. Korpus se skládá z přibližně 1200 hodin řečových dat a odpovídajících textových přepisů. Celý korpus byl segmentován na krátké zvukové segmenty, takže je vhodný jak pro trénink, tak pro hodnocení systémů automatického rozpoznávání řeči (ASR). Zdrojovým jazykem korpusu je čeština, což z něj činí cenný zdroj pro budoucí výzkum, protože v českém jazyce je k dispozici pouze několik veřejných datových souborů. Vydání dat doplňujeme experimenty dvou základních systémů ASR trénovaných na prezentovaných datech: tradičnější přístup implementovaný v Kaldi ASR toolkit, který kombinuje skryté Markovovy modely a hluboké neurální sítě (NN), a moderní ASR architekturu implementovanou v Jasper toolkit, který využívá NN v podobě end-to-end.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a large corpus of Czech parliament plenary sessions. The corpus consists of approximately 1200 hours of speech data and corresponding text transcriptions. The whole corpus has been segmented to short audio segments making it suitable for both training and evaluation of automatic speech recognition (ASR) systems. The source language of the corpus is Czech, which makes it a valuable resource for future research as only a few public datasets are available in the Czech language. We complement the data release with experiments of two baseline ASR systems trained on the presented data: the more traditional approach implemented in the Kaldi ASRtoolkit which combines hidden Markov models and deep neural networks (NN) and a modern ASR architecture implemented in Jaspertoolkit which uses deep NNs in an end-to-end fashion.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Žákovské korpusy, čili korpusy, které dokumentují jazyk tak, jak jej používají nerodilí mluvčí, poskytují důležité informace pro výzkum osvojování jazyka i pedagogickou praxi. Tato monografie představuje CzeSL – korpus češtiny nerodilých mluvčích, a to na pozadí teoretických a praktických otázek současného výzkumu v oboru žákovských korpusů.

Jazyky s bohatou morfologií a volným slovosledem, včetně češtiny, jsou pro analýzu osvojovaného jazyka obzvláště náročné. Autoři se zabývají složitostí chybové anotace a popisují tři vzájemně se doplňující anotační schémata. Věnují se také popisu nerodilé češtiny z hlediska standardních jazykových kategorií.

Kniha podrobně rozebírá praktické aspekty tvorby korpusu: proces sběru a anotace, potřebné nástroje, výsledná data, jejich formáty a vyhledávací rozhraní. Kapitola o aplikacích korpusu ilustruje jeho užitečnost pro výuku, výzkum akvizice i počítačovou lingvistiku. Každý, kdo se zabývá tvorbou žákovských korpusů, jistě ocení závěrečnou kapitolu, shrnující úskalí, kterým je třeba se vyhnout.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Learner corpora, linguistic collections documenting a language as used by learners, provide an important empirical foundation for language acquisition research and teaching practice. This book presents CzeSL, a corpus of non-native Czech, against the background of theoretical and practical issues in the current learner corpus research.

Languages with rich morphology and relatively free word order, including Czech, are particularly challenging for the analysis of learner language. The authors address both the complexity of learner error annotation, describing three complementary annotation schemes, and the complexity of description of non-native Czech in terms of standard linguistic categories.

The book discusses in detail practical aspects of the corpus creation: the process of collection and annotation itself, the supporting tools, the resulting data, their formats and search platforms.
The chapter on use cases exemplifies the usefulness of learner corpora for teaching, language acquisition research, and computational linguistics. Any researcher developing learner corpora will surely appreciate the concluding chapter listing lessons learned and pitfalls to avoid.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku se zabýváme možnostmi zpracování výrazů s výraznou pragmatickou složkou významu v jednojazyčném výkladovém slovníku. Nejprve obecněji nastiňujeme formální lexikografické nástroje pro zpracování takových slov, jako je využití kvalifikátorů, komentářů a usage notes. Dále se konkrétněji zaměřujeme na výrazy vyjadřující etnickou a rasovou příslušnost lidí, které reprezentují jeden z typů s výraznou pragmatickou složkou významu. Široce přitom analyzujeme problematiku těchto výrazů, která se projevuje např. v posunech vnímání významů výrazů z této oblasti, v komplikované či nejasné terminologii, v obtížné zachytitelnosti některých konotací apod. Rozbory dokládáme na příkladech z Českého národního korpusu a ze stávajících slovníků, především českých (včetně vznikajícího ASSČ), ale i některých slovenských a anglických. V příloze pak uvádíme příklady zpracování několika vybraných slovníkových hesel.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper deals with the possibilities of the dictionary treatment of words whose meanings include a significant pragmatic component. Firstly, the formal lexicographical tools for their treatment are outlined, including (style) labels, glosses, and usage notes. Furthermore, the paper concentrates in detail on words referring to the ethnic and racial identity of people, representing the type of words whose meanings include a significant pragmatic component. The issues connected with their use and their dictionary treatment are analysed, e.g. shifts in their meaning and perception, complicated and unclear terminology, and the difficulty of capturing particular connotations. The analyses are illustrated using examples from the Czech National Corpus and from dictionaries of contemporary Czech, and selected dictionaries of Slovak and English. The paper also introduces and explains some decisions made by the authors of Akademický slovník současné češtiny (Academic Dictionary of Con­temporary Czech) related to the treatment of the words under scrutiny. In addition, examples of sev­eral dictionary entries or proposals thereof, accompanied by the author’s comments, are attached.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku popisujeme MorfFlex, Morfologický slovník češtiny, jako cenný zdroj pro zkoumání formálního chování slov. Ukazujeme, že MorfFlex poskytuje bohatá na korpusu založená data umožňující podrobně zkoumat různé morfologické jevy. MorfFlex obsahuje slova z celé slovní zásoby, včetně nestandardních jednotek, vlastních jmen, zkratek atd. Navíc ve srovnání s typickými monolingválními slovníky češtiny MorfFlex zachycuje i nestandardní varianty, což je pro češtinu jako jazyk s bohatou flexí velmi důležité.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we describe MorfFlex, the Morphological Dictionary of Czech, as an invaluable resource for exploring the formal behavior of words. We demonstrate that MorfFlex provides valuable and rich data allowing to elaborate on various morphological issues in depth, which is also connected with the fact that the MorfFlex dictionary includes words throughout the whole vocabulary, including non-standard units, proper nouns, abbreviations, etc. Moreover, in comparison with typical monolingual dictionaries of Czech, MorfFlex also captures non-standard wordforms, which is very important for Czech as a language with a rich inflection. In the paper we also demonstrate how particular information on lemmas and wordforms (e.g. variants, homonymy, style information) is marked and structured. The dictionary is provided as a digital open access source available to all scholars via the LINDAT/CLARIAH-CZ language resource repository. It is available in an electronic format, and also in a more human-readable, browsable and partly searchable form.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>UDPipe 2 je nástroj pro tokenizaci, tagging, lemmatizaci a závislostní parsing CoNLL-U souborů.  Předtrénované jazykové modely jsou k dispozici pro téměř všechny UD korpusy a dosahují úspěšnosti srovnatelné s nejlepšími známými výsledky.

UDPipe 2 je svobodný software licencovaný pod Mozilla Public License 2.0 a jazykové modely jsou k dispozici pro nekomerční použití pod licencí CC BY-NC-SA, nicméně původní data použitá k vytvoření modelů mohou v některých případech ukládat další licenční omezení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>UDPipe 2 is an pipeline for tokenization, tagging, lemmatization and dependency parsing of CoNLL-U files. Trained models are provided for nearly all UD treebanks and achieve performance competitive with state-of-the-art.

UDPipe 2 is a free software under Mozilla Public License 2.0 and the linguistic models are free for non-commercial use and distributed under CC BY-NC-SA license, although for some models the original data used to create the model may impose additional licensing conditions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme náš příspěvek k shared tasku EvaLatin, což je první hodnotící kampaň věnovaná hodnocení nástrojů zpracování přirozeného textu pro latinu. Předložili jsme systém založený na UDPipe 2, jednom z vítězů soutěže CoNLL 2018 Shared Task, dále The 2018 Shared Task on Extrinsic Parser Evaluation a také SIGMORPHON 2019 Shared Task. Náš systém získal s velkým náskokem první místo jak v lemmatizaci tak v značkování v režimu otevřené modality, kde jsou povoleny další trénovací data, v kterémžto případě využíváme všechny latinské korpusy Universal Dependencies. V režimu uzavřené modality, kdy jsou povoleny pouze EvaLatin trénovací data, dosahuje náš systém nejlepších výsledků v lemmatizaci a značkování klasických textů a zároveň dosahuje druhého místa v nastavení napříč žánry a napříč časem. V ablačních experimentech hodnotíme vliv BERT a XLM-RoBERTa kontextualizovaných embeddingů a také kódování různých druhů latinských korpusů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our contribution to the EvaLatin shared task, which is the first evaluation campaign devoted to the evaluation of NLP tools for Latin. We submitted a system based on UDPipe 2.0, one of the winners of the CoNLL 2018 Shared Task, The 2018 Shared Task on Extrinsic Parser Evaluation and SIGMORPHON 2019 Shared Task. Our system places first by a wide margin both in lemmatization and POS tagging in the open modality, where additional supervised data is allowed, in which case we utilize all Universal Dependency Latin treebanks. In the closed modality, where only the EvaLatin training data is allowed, our system achieves the best performance in lemmatization and in classical subtask of POS tagging, while reaching second place in cross-genre and cross-time settings. In the ablation experiments, we also evaluate the influence of BERT and XLM-RoBERTa contextualized embeddings, and the treebank encodings of the different flavors of Latin treebanks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Derivations (UDer) je kolekce harmonizovaných lexikalních sítí zachycujících slovotvorné, zvl. derivační vztahy jednotlivých jazyků v témže anotačním schématu. Základní datovou strukturou tohoto anotačního schématu jsou zakořeněné stromy, ve kterých uzly odpovídají lexémům a hrany reprezentují derivační, příp. kompozitní vztahy. Stávající verze UDer v0.1 obsahuje 27 harmonizovaných zdrojů pokrývajících 20 různých jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Derivations (UDer) is a collection of harmonized lexical networks capturing word-formation, especially derivational relations, in a cross-linguistically consistent annotation scheme for many languages. The annotation scheme is based on a rooted tree data structure, in which nodes correspond to lexemes, while edges represent derivational relations or compounding. The current version of the UDer collection contains twenty-seven harmonized resources covering twenty different languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá harmonizací existujících datových zdrojů zachycujících slovotvorbu různých jazyků, konkrétně převodem originálně zachycených slovotvorných příznaků do stejného souborového formátu a zčásti též do téhož anotačního schématu. Shrnuty jsou rozdíly i podobnosti mezi harmonizovanými zdroji. Popsány jsou jednotlivé kroky prezentované harmonizační procedury, jež zahrnuje manuální anotace i aplikaci technik z oblasti strojového učení. Výsledká kolekce 'Universal Derivations 1.0' obsahuje 27 harmonizovaných datových zdrojů, které dohromady pokrývají 20 různých jazyků. Kolekce je volně dostupná v repozitáři LINDAT/CLARIAH CZ a data jednotlivých zdrojů lze též dotazovat pomocí nástroje DeriSearch.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper deals with harmonisation of existing data resources containing word-formation features by converting them into a common file format and partially aligning their annotation schemas. We summarise (dis)similarities between the resources and describe individual steps of the harmonisation procedure, including manual annotations and application of Machine Learning techniques. The resulting 'Universal Derivations 1.0' collection contains  27 harmonised resources covering 20 languages. It is publicly  available  in the LINDAT/CLARIAH CZ repository and can be queried via the DeriSearch tool.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zavádíme novou symetrickou míru (zvanou θpos), která využívá asymetrickou míru KLcpos3 (Rosa a Žabokrtský, 2015), abychom porovnali konzistenci anotace mezi různými anotovanými treebanky téhož jazyka, jestliže jsou anotované podle téhož anotačního schématu. Pro tuto míru můžeme nastavit práh a říct, že dva treebanky lze považovat za harmonické, pokud jde o jejich anotaci, jestliže θpos nepřekročí daný práh. Při stanovování prahové hodnoty posuzujeme vliv (i) různých velikostí dat a (ii) různého žánrového složení dat. Naše odhady vycházejí z dat z různých jazykových rodin, takže práh není tolik závislý na vlastnostech jednotlivých jazyků. Užití navržené míry demonstrujeme na treebancích z vydání 2.5 Universal Dependencies (Zeman et al., 2019): tam, kde je více než jeden treebank pro daný jazyk, uvádíme míru konzistence pro každý pár treebanků. Navržená míra může být nicméně využita pro vyhodnocení konzistence i v jiných anotačních schématech než Universal Dependencies.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a new symmetric metric (called θpos) that utilises the non-symmetric KLcpos3 metric (Rosa and Žabokrtský, 2015) to allow us to compare the annotation consistency between different annotated treebanks of a given language, when annotated under the same guidelines. We can set a threshold for this new metric so that a pair of treebanks can be considered harmonious in their annotation consistency if θpos surpasses the threshold. For the calculation of the threshold, we estimate the effects of (i) the size variation, and (ii) the genre variation in the considered pair of treebanks. The estimations are based on data from treebanks of distinct language families, making the threshold less dependent on the properties of individual languages. We demonstrate the utility of the proposed metric by listing the treebanks in Universal Dependencies version 2.5 (UDv2.5) (Zeman et al., 2019) data that are annotated consistently with other treebanks of the same language. However, the metric could be used to assess inter-treebank annotation consistency under other (non-UD) annotation guidelines as well.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rudolf Rosa z Matematicko-fyzikální fakulty Univerzity Karlovy už od střední školy vystupuje pod nickem R.U.R. a shodou okolností se mu do rukou dostal možná trochu bláznivý nápad zinscenovat ke stoletému výročí uvedení stejnojmenné hry od Karla Čapka, v níž poprvé zaznělo slovo „robot”, divadelní představení, jejímž autorem bude umělá inteligence. Přečtěte si rozhovor o tom, jak se nadšenci z Matfyzu spojili s herci ze Švandova divadla, aby vytvořili nový světový unikát.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Rudolf Rosa of Charles University's Faculty of Mathematics and Physics has been appearing under the nickname R.U.R. since high school, and as it happens, he has become involved in a perhaps slightly crazy idea of, at the occasion of the 100-year anniversary of the introduction of the play R.U.R. by Karel Čapek, in which the word "robot" was first mentioned, staging a theatrical performance written by an artificial intelligence. Read an interview about how Matfyz enthusiasts have teamed up with actors from the Švanda Theatre to create a new world-class unique.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Původně české slovo „robot“ oslaví v lednu příštího roku sto let od vstupu do našeho slovníku. Čapkovo drama R.U.R. bylo první hrou právě o tomto tématu. Jaké by to ale bylo, kdyby naopak roboti napsali hru o člověku, možná přímo o Karlu Čapkovi? Se současným stavem poznání si už lze troufnout prakticky na cokoliv. Umělá inteligence se bude čím dál víc stávat běžnou součástí našich životů a divadlo může být jedním z experimentálních prostorů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Originally, the Czech word "robot" will celebrate one hundred years since entering our dictionary in January next year. Chapek's R.U.R. drama was the first play on this very subject. But what would it be like if, on the other hand, the robots wrote a play about man, perhaps directly about Karel Čapek? With the current state of knowledge, practically anything can be ventured. Artificial intelligence will increasingly become a normal part of our lives, and theatre can be one of the experimental spaces.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Uživatelské rozhraní vytvářené pro potřeby Centra vizuální historie Malach zpřístupňuje několik velkých kolekcí a databází orálně historických pramenů k výzkumu dějin 20. století, se zvláštním zřetelem na problematiku holokaustu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Malach User Interface created for the needs of the Malach Center for Visual History makes several large collections and databases of oral history sources available for the research of 20th century history, with special attention to the issue of the Holocaust History.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Neurální sítě trénované na zpracování přirozeného jazyka zachycují syntaxi, i když není poskytována jako signál dohledu. To naznačuje, že syntaktická analýza je nezbytná pro podcenění jazyka v systémech umělé inteligence. Tento přehledný dokument se zabývá přístupy k hodnocení množství syntaktických informací obsažených v reprezentacích slov pro různé architektury neuronových sítí. Shrnujeme především výzkum anglických jednojazyčných dat o úkolech jazykového modelování a vícejazyčných dat pro systémy neurálního strojového překladu a vícejazyčné jazykové modely. Popisujeme, které předcvičené modely a znázornění jazyka jsou nejvhodnější pro přenos do syntaktických úloh.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Neural networks trained on natural language processing tasks capture syntax even though it is not provided as a supervision signal. This indicates that syntactic analysis is essential to the understating of language in artificial intelligence systems. This overview paper covers approaches of evaluating the amount of syntactic information included in the representations of words for different neural network architectures. We mainly summarize research on English monolingual data on language modeling tasks and multilingual data for neural machine translation systems and multilingual language models. We describe which pre-trained models and representations of language are best suited for transfer to syntactic tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce se zaměřuje na analýzu formy a rozsahu syntaktické abstrakce zachycené BERT extrahováním označených stromů závislosti ze sebepozornosti.
Předchozí práce ukázaly, že jednotlivé hlavy BERT mají tendenci kódovat konkrétní typy vztahů závislosti. Rozšiřujeme tato zjištění explicitním porovnáním vztahů BERT s anotacemi Universal Dependencies (UD), což ukazuje, že se často neshodují jedna ku jedné.
Navrhujeme metodu pro identifikaci vztahu a syntaktickou stavbu stromu. Náš přístup vytváří podstatně více konzistentních stromů závislosti než předchozí práce, což ukazuje, že lépe vysvětluje syntaktické abstrakce v BERT. Zároveň ji lze úspěšně aplikovat jen s minimální mírou dohledu a dobře zobecňuje napříč jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This work focuses on analyzing the form and extent of syntactic abstraction captured by BERT by extracting labeled dependency trees from self-attentions.
Previous work showed that individual BERT heads tend to encode particular dependency relation types. We extend these findings by explicitly comparing BERT relations to Universal Dependencies (UD) annotations, showing that they often do not match one-to-one.
We suggest a method for relation identification and syntactic tree construction. Our approach produces significantly more consistent dependency trees than previous work, showing that it better explains the syntactic abstractions in BERT. At the same time, it can be successfully applied with only a minimal amount of supervision and generalizes well across languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace je věnována implicitnosti diskurzních vztahů, jako je například podspecifikované užití konektivních prostředků nebo sémantická signalizace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presentation wis devoted to the current discourse-oriented projects/research directions at ÚFAL institute in connection with the implicitness of discourse relations, such as underspecified connectives or semantic signalling.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje první verzi lexikonu GeCzLex, online jazykového zdroje překladových ekvivalentů českých a německých anaforických konektorů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce the first version of GeCzLex, an online electronic resource for translation equivalents of Czech and German discourseconnectives.  The lexicon is one of the outcomes of the research on anaphoricity and long-distance relations in discourse, it containsat  present  anaphoric  connectives  (ACs)  for  Czech  and  German  connectives,  and  further  their  possible  translations  documented  inbilingual parallel corpora (not necessarily anaphoric). As a basis, we use two existing monolingual lexicons of connectives: the Lexiconof  Czech  Discourse  Connectives  (CzeDLex)  and  the  Lexicon  of  Discourse  Markers  (DiMLex)  for  German,  interlink  their  relevantentries via semantic annotation of the connectives (according to the PDTB 3 sense taxonomy) and statistical information of translationpossibilities from the Czech and German parallel data of the InterCorp project.  The lexicon is, as far as we know, the first bilingualinventory of connectives with linkage on the level of individual entries, and a first attempt to systematically describe devices engaged inlong-distance, non-local discourse coherence. The lexicon is freely available under the Creative Commons License.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Studie se věnuje analýze vzájemného uspořádání tzv. lokálních vztahů textové koherence a zkoumá využitelnost zjištěných výsledků při modelování globální koherence textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Descriptive approaches to discourse (text) structure and coherence typically proceed either in a bottom-up or a top-down analytic way. The former ones analyze how the smallest discourse units (clauses, sentences) are connected in their closest neighbourhood, locally, in a linear way. The latter ones postulate a hierarchical organization of smaller and larger units, sometimes also represent the whole text as a tree-like graph. In the present study, we mine a Czech corpus of 50k sentences
annotated in the local coherence fashion (Penn Discourse Treebank style) for indices signalling higher discourse structure. We analyze patterns of overlapping discourse relations and look into hierarchies they form. The
types and distributions of the detected patterns correspond to the results for English local annotation, with patterns not complying with the tree-like interpretation at very low numbers. We also detect hierarchical organization of local discourse relations of up to 5 levels in the Czech data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CzeDLex 0.7 je třetí vývojová verze slovníku českých diskurzních konektorů. Slovník obsahuje konektory částečně automaticky extrahované z Pražského diskurzního korpusu 2.0 (PDiT 2.0) a z dalších zdrojů. Nejfrekventovanější slovníková hesla (pokrývající více než 95% diskurzních vztahů anotovaných v PDiT 2.0) byla ručně zkontrolována, přeložena do angličtiny a doplněna dalšími lingvistickými informacemi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CzeDLex 0.7 is the third development version of a lexicon of Czech discourse connectives. The lexicon contains connectives partially automatically extracted from the Prague Discourse Treebank 2.0 (PDiT 2.0) and other resources. The most frequent entries in the lexicon (covering more than 95% of the discourse relations annotated in the PDiT 2.0) have been manually checked, translated to English and supplemented with additional linguistic information.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje první závislostní korpus indo-árijského jazyka bhódžpurštiny. Bhódžpurština je jedním z indických jazyků, pro které není k dispozici dostatek zdrojů v oblasti strojového zpracování jazyka. Účelem projektu Bhódžpurského závislostního korpusu (BHTB) je poskytnout velký, syntakticky anotovaný korpus bhódžpurštiny, který pomůže při tvorbě nástrojů pro automatické zpracování jazyka. Tento projekt také pomůže s mezijazykovým strojovým učením a s typologickým výzkumem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the first dependency treebank for Bhojpuri, an Indo-Aryan language. Bhojpuri is one of the resource-poor Indian languages. The objective of the Bhojpuri Treebank (BHTB) project is to provide a substantial, syntactically annotated treebank for Bhojpuri which helps in building language technological tools. This project will also help in cross-lingual learning and typological research. Currently, the treebank consists of 4,881 tokens using the annotation scheme of Universal Dependencies (UD). We develop a Bhojpuri tagger and parser using the machine learning approach. The accuracy of the model is 57.49% UAS, 45.50% LAS, 79.69% UPOS accuracy and 77.64% XPOS accuracy. Finally, we discuss linguistic analysis and annotation process of the Bhojpuri UD treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Existující modely vícejazyčných větných vektorových reprezentací (embeddingů) vyžadují rozsáhlé paralelní datové zdroje, které nejsou k dispozici pro všechny jazyky. Navrhujeme novou metodu neřízeného učení pro získání vícejazyčných větných embeddingů pouze z jednojazyčných dat. Nejprve pomocí neřízeného strojového překladu vytvoříme syntetický paralelní korpus a použijeme jej k doladění předtrénovaného cross-lingválního maskovaného jazykového modelu (XLM) a k odvození vícejazyčných větných reprezentací. Kvalita reprezentací je hodnocena na dvou úlohách dolování paralelních dat se zlepšením F1 skóre až o 22 bodů oproti standardnímu XLM. Dále pozorujeme, že jeden syntetický dvojjazyčný korpus je schopen vylepšit výsledky i pro jiné jazykové páry.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Existing models of multilingual sentence embeddings require large parallel data resources which are not available for low-resource languages. We propose a novel unsupervised method to derive multilingual sentence embeddings relying only on monolingual data. We first produce a synthetic parallel corpus using unsupervised machine translation, and use it to fine-tune a pretrained cross-lingual masked language model (XLM) to derive the multilingual sentence representations. The quality of the representations is evaluated on two parallel corpus mining tasks with improvements of up to 22 F1 points over vanilla XLM. In addition, we observe that a single synthetic bilingual corpus is able to improve results for other language pairs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje popis soutěžních systémů Univerzity Karlovy pro úlohu WMT20 ve strojovém překladu mezi němčinou a lužickou srbštinou při nedostatku dat. Provedli jsme experimenty s trénováním na syntetických datech a předtrénováním na příbuzných jazykových párech. V plně neřízeném režimu jsme dosáhli 25,5 a 23,7 BLEU při překladu z a do lužické srbštiny. Ve volnějším režimu jsme použili transfer learning z německo-českých paralelních dat a dosáhli 57,4 BLEU a 56,1 BLEU, což je zlepšení o 10 BLEU bodů oproti baseline natrénované pouze na malém množství dostupných německo-lužickosrbských paralelních vět.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a description of CUNI systems submitted to the WMT20 task on unsupervised and very low-resource supervised machine translation between German and Upper Sorbian. We experimented with training on synthetic data and pre-training on a related language pair. In the fully unsupervised scenario, we achieved 25.5 and 23.7 BLEU translating from and into Upper Sorbian, respectively. Our low-resource systems relied on transfer learning from German-Czech parallel data and achieved 57.4 BLEU and 56.1 BLEU, which is an improvement of 10 BLEU points over the baseline trained only on the available small German-Upper Sorbian parallel corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nový přístup pro generování textu z dat založený na postupných úpravách textu. Náš přístup maximalizuje úplnost a sémantickou přesnost výstupního textu a zároveň využívá současných předtrénovaných modelů pro editaci textu (LaserTagger) a modelování jazyka (GPT-2) pro zlepšení plynulosti textu. Za tímto účelem nejprve převádíme data na text pomocí triviální lexikalizace zvlášť pro každou položku a následně výsledný text postupně vylepšujeme neuronovým modelem natrénovaným na spojování vět. Náš přístup vyhodnocujeme na dvou používaných datových sadách (WebNLG, Cleaned E2E) a analyzujeme jeho přínosy a úskalí. Dále ukazujeme, že náš přístup umožňuje generování textu z dat bez dat z konkrétní domény za pomocí obecné datové sady pro spojování vět.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a novel approach to data-to-text generation based on iterative text editing. Our approach maximizes the completeness and semantic accuracy of the output text while leveraging the abilities of recent pretrained models for text editing (LaserTagger) and language modelling (GPT-2) to improve the text fluency. To this end, we first transform data to text using trivial per-item lexicalizations, iteratively improving the resulting text by a neural model trained for the sentence fusion task. The model output is filtered by a simple heuristic and reranked with an off-the-shelf pretrained language model. We evaluate our approach on two major data-to-text datasets (WebNLG, Cleaned E2E) and analyze its caveats and benefits. Furthermore, we show that our formulation of data-to-text generation opens up the possibility for zero-shot domain adaptation using a general-domain dataset for sentence fusion.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme náš systém pro generování textu z RDF pro soutěž WebNLG Challenge 2020. Svůj přístup zakládáme na modelu mBART, který je předtrénován pro vícejazyčný denoising. To nám umožňuje použít jednoduchý, identický end-to-end přístup pro angličtinu i ruštinu. S minimálními nároky specifickými pro konkrétní jazyk nebo úlohu se náš model umístil v první třetině žebříčku pro angličtinu a na prvním nebo druhém místě pro ruštinu v automatických metrikách. Podle lidského hodnocení se dostal do nejlepší nebo druhé nejlepším skupiny systémů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe our system for the RDF-to-text generation task of the WebNLG Challenge 2020. We base our approach on the mBART model, which is pre-trained for multilingual denoising. This allows us to use a simple, identical, end-to-end setup for both English and Russian. Requiring minimal task or language-specific effort, our model placed in the first third of the leaderboard for English and first or second for Russian on automatic metrics, and it made it into the best or second-best system cluster on human evaluation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Aplikace architektury Transformeru na úrovni znaků obvykle vyžaduje velmi hluboké architektury, které se obtížně a pomalu trénují. V článku ukazujeme, že předtrénováním podslovního modelu a jeho finetuningem na znaky můžeme získat kvalitní model pro neuronový strojový překlad, který funguje na úrovni znaků bez nutnosti tokenizace vstupu. Používáme pouze základní šestivrstvou architekturu Transformer Base. Naše modely na úrovni znaků lépe zachycují morfologické jevy a vykazují větší odolnost vůči šumu za cenu poněkud horší celkové kvality překladu. Naše studie je tak významným krokem ke kvalitním a snadno trénovatelným modelům, které modelují překlad na útrovni znaků a zároveň nejsou extrémně velké.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Applying the Transformer architecture on the character level usually requires very deep architectures that are difficult and slow to train. These problems can be partially overcome by incorporating a segmentation into tokens in the model. We show that by initially training a subword model and then finetuning it on characters, we can obtain a neural machine translation model that works at the character level without requiring token segmentation. We use only the vanilla 6-layer Transformer Base architecture. Our character-level models better capture morphological phenomena and show more robustness to noise at the expense of somewhat worse overall translation quality. Our study is a significant step towards high-performance and easy to train character-based models that are not extremely large.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Mnohojazyčné kontextové embedinky, jako vícejazyčný BERT (mBERT) a XLM-RoBERTa, se osvědčily pro mnoho vícejazyčných úloh. Předchozí práce zkoumala mnohojazyčnost reprezentací nepřímo s využitím nulového transferového učení na morfologických a syntaktických úkolech. Místo toho se zaměřujeme na jazykovou neutralitu mBERTu s ohledem na lexikální sémantiku. Naše výsledky ukazují, že kontextové embedinky jsou jazykově neutrálnější a obecně informativnější než zarovnané statické slovní embedinky, které jsou explicitně trénovány na jazykovou neutralitu. Kontextové embedinky jsou stále standardně pouze mírně jazykově neutrální, nicméně ukazujeme dvě jednoduché metody, jak dosáhnout silnější jazykové neutrality: zaprvé neřízeným vystředěním reprezentace pro jazyky a zadruhé explicitní projekcí na malých paralelních datech. Kromě toho ukazujeme, jak překonat nejlepší dosažené přesnosti při identifikaci jazyka a zarovnávání slov v paralelních větách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Multilingual contextual embeddings, such as multilingual BERT (mBERT) and XLM-RoBERTa, have proved useful for many multi-lingual tasks. Previous work probed the cross-linguality of the representations indirectly using zero-shot transfer learning on morphological and syntactic tasks. We instead focus on the language-neutrality of mBERT with respect to lexical semantics. Our results show that contextual embeddings are more language-neutral and in general more informative than aligned static word-type embeddings which are explicitly trained for language neutrality. Contextual embeddings are still by default only moderately language-neutral, however, we show two simple methods for achieving stronger language neutrality: first, by unsupervised centering of the representation for languages, and second by fitting an explicit projection on small parallel data. In addition, we show how to reach state-of-the-art accuracy on language identification and word alignment in parallel sentences.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme příspěvek do soutěže a kombinace automatického překladu a parafrázování ve výuce jazyků (STAPLE). Pro překlad jsme použili standardní model Transformer, doplněný kroslinguálním klasifikátorem pro filtrování překladových hypotéz. Abychom zvýšili rozmanitost výstupů, použili jsme další trénovací data a vyvinuli jsme parafrázovací model založený na architektuře Levenshtein Transformer, který generuje další synonymní překlady. Výsledky parafrázování byly opět filtrovány kroslinguálním klasifikátorem. Zatímco použití dalších dat a náš filtr zlepšily výsledky, parafrázování generovalo příliš mnoho neplatných výstupů, aby dále zlepšilo kvalitu výstupů. Náš model bez parafrázování skončil přibližně uprostřed soutěžního pořadí a přinesl zlepšení o 10-22% vážený F1 bodů oproti základnímu řešení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our submission to the Simultaneous Translation And Paraphrase for Language Education (STAPLE) challenge. We used a standard Transformer model for translation, with a crosslingual classifier predicting correct translations on the output n-best list. To increase the diversity of the outputs, we used additional data to train the translation model, and we trained a paraphrasing model based on the Levenshtein Transformer architecture to generate further synonymous translations. The paraphrasing results were again filtered using our classifier. While the use of additional data and our classifier filter were able to improve results, the paraphrasing model produced too many invalid outputs to further improve the output quality. Our model without the paraphrasing component finished in the middle of the field for the shared task, improving over the best baseline by a margin of 10-22 % weighted F1 absolute.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme naše systémy pro WMT20 Very Low Resource MT Task k překladu mezi němčinou a hornolužickou srbštinou. Pro trénink našich systémů generujeme syntetická data zpětným i dopředným překladem. Trénvací data navíc obohacujeme o německo-české překlady z češtiny do hornolužické srbštiny pomocí neřízeného statistického MT systému, který obsahuje ortograficky podobné slovní dvojice a transliterace slov mimo slovník. Náš nejlepší překladový systém mezi němčinou a srbštinou je založen na transferu modelu z česko-německého systému a má o 12 až 13 BLEU vyšší skóre než základní systém vytvořený pouze s využitím dostupných paralelních dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our systems for the WMT20 Very Low Resource MT Task for translation between German and Upper Sorbian. For training our systems, we generate synthetic data by both back- and forward-translation. Additionally, we enrich the training data with German-Czech translated from Czech to Upper Sorbian by an unsupervised statistical MT system incorporating orthographically similar word pairs and transliterations of OOV words. Our best translation system between German and Sorbian is based on transfer learning from a Czech-German system and scores 12 to 13 BLEU higher than a baseline system built using the available parallel data only.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme naše dva neuronové překladové systémy pro anglicko-český a anglicko-polský překlad, které se zúčastnily soutěže v překladu novinových článků WMT 2020. První systém překládá každou větu nezázvisle. Druhý systém je tzv. document-level, tedy překládá více vět naráz a je trénovaný na vícevětných sekvencích dlouhých až 3000 znaků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe our two NMT systems submitted to the WMT 2020 shared task in English-Czech and English-Polish news translation. One system is sentence level, translating each sentence independently. The second system is document level, translating multiple sentences, trained on multi-sentence sequences up to 3000 characters long.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška úvodem krátce představí některé úlohy, kterými se zabývá počítačová lingvistika, například automatickou opravu překlepů/gramatiky či automatický větný rozbor.

Hlavní pozornost bude věnována úloze strojového překladu, zejména vývoji různých typů překladačů z angličtiny do češtiny během posledního desetiletí. Dnešní nejlepší překladače jsou založeny na technologiích umělé inteligence, konkrétně hlubokých neuronových sítí, a kvalita výsledného překladu se blíží úrovni profesionální překladatelské agentury. Vyvstávají otázky, jak tento pokrok ve kvalitě strojového překladu dosažený během posledních let ovlivní výuku jazyků, ale též zda se na strojové překlady můžeme spolehnout.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The lecture will briefly introduce some of the tasks dealt with in computer linguistics, such as automatic correction of misspellings/grammar or automatic sentence analysis.

The main focus will be on the role of machine translation, in particular the development of different types of English-to-Czech translators over the last decade. Today's best translators are based on artificial intelligence technologies, namely deep neural networks, and the quality of the resulting translation is close to that of a professional translation agency. Questions are being asked about how this progress in machine translation quality made over recent years will affect language learning, but also whether we can rely on machine translations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kvalita lidského překladu byla dlouho považována za nedosažitelnou pro počítačové překladové systémy. V této studii představujeme systém hlubokého učení CUBBITT, který tento názor zpochybňuje. V zaslepeném lidském hodnocení překladu novinových článků z angličtiny do češtiny CUBBITT výrazně předčil lidský překlad od profesionální agentury v zachování významu textu (adequacy, přesnosti překladu). Zatímco lidský překlad je stále hodnocen jako plynulejší, ukázalo se, že CUBBITT je podstatně plynulejší než dosavadní překladače. Většina účastníků překladového Turingova testu navíc nedokázala rozlišit překlady CUBBITT od překladů lidských. Tato práce se blíží kvalitě lidského překladu a za určitých okolností jej dokonce v přiměřenosti překonává. To naznačuje, že hluboké učení může mít potenciál nahradit člověka v aplikacích, kde je hlavním cílem zachování významu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The quality of human translation was long thought to be unattainable for computer translation systems. In this study, we present a deep-learning system, CUBBITT, which challenges this view. In a context-aware blind evaluation by human judges, CUBBITT significantly outperformed professional-agency English-to-Czech news translation in preserving text meaning (translation adequacy). While human translation is still rated as more fluent, CUBBITT is shown to be substantially more fluent than previous state-of-the-art systems. Moreover, most participants of a Translation Turing test struggle to distinguish CUBBITT translations from human translations. This work approaches the quality of human translation and even surpasses it in adequacy in certain circumstances. This suggests that deep learning may have the potential to replace humans in applications where conservation of meaning is the primary aim.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento dokument představuje náš pokrok směrem k zavedení univerzální komunikační platformy v úkolu vysoce mnohojazyčného živého projevu
překlady pro konference a vzdálené schůzky živé titulkování. Platforma byla navržena se zaměřením na velmi nízkou latenci a
vysoká flexibilita při umožnění snadného propojení výzkumných prototypů nástrojů pro zpracování řeči a textu, bez ohledu na to, kde
fyzicky běhat. Nastíníme naše řešení architektury a také ho krátce porovnáme s platformou ELG. Technické podrobnosti jsou uvedeny
o nejdůležitějších součástech a shrnujeme události zkušebního nasazení, které jsme zatím provedli.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents our progress towards deploying a versatile communication platform in the task of highly multilingual live speech
translation for conferences and remote meetings live subtitling. The platform has been designed with a focus on very low latency and
high flexibility while allowing research prototypes of speech and text processing tools to be easily connected, regardless of where they
physically run. We outline our architecture solution and also briefly compare it with the ELG platform. Technical details are provided
on the most important components and we summarize the test deployment events we ran so far.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme naši práci zkoumající možnosti společného prostoru embedingů mezi textovou a vizuální modalitou.
Narozdíl od současného trendu zakotvení slov či vět v přidružených obrázcích, navrhujeme zakotvení visuálních reprezentací objektů v prostoru slovních embedingů systému generujícího popisků.
K tomu využíváme textové povahy popisků detekovaných objektů a předpokládanou expresivitu reprezentací těchto objektů.
Na základě předchozích poznatků aplikujeme dodatečné objektivní funkce k základní popiskovací objektivní funkci, jejichž cílem je tvorba heterogenních klastrů závislých na jejich třídě a napodobení sémantické struktury prostoru slovních embedingů.
Kromě toho také analyzujeme natrénované projekce prostoru vizualních objektů a jejich vliv na výkon popiskovacího systému.
I přes mírné zhoršení kvality generovaných popisků, modely se zakotvením konvergují výrazně rychleji během trénovaní vyžadujíce dvakrát až třikrát méně trénovacích iterací.
Zlepšení strukturální korelace mezi slovními embedingy a nejen původními objektovými reprezentacemi, ale i jejich projekcí naznačuje, že zakotvení je vzájemné.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our work in progress exploring the possibilities of a shared embedding space between textual and visual modality.
Leveraging the textual nature of object detection labels and the hypothetical expressiveness of extracted visual object representations, we propose an approach opposite to the current trend, grounding of the representations in the word embedding space of the captioning system instead of grounding words or sentences in their associated images.
Based on the previous work, we apply additional grounding losses to the image captioning training objective aiming to force visual object representations to create more heterogeneous clusters based on their class label and copy a semantic structure of the word embedding space.
In addition, we provide an analysis of the learned object vector space projection and its impact on the IC system performance.
With only slight change in performance, grounded models reach the stopping criterion during training faster than the unconstrained model, needing about two to three times less training updates.
Additionally, an improvement in structural correlation between the word embeddings and both original and projected object vectors suggests that the grounding is actually mutual.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje průběh jubilejního 45. ročníku Olympiády v českém jazyce. Představuje soutěž jako takovou, přibližuje konkrétní soutěžní úlohy včetně jejich řešení, komentuje řešení účastníků a přináší jména vítězů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article describes the course of the jubilee 45th year of the Olympiad in the Czech Language, presenting its general settings as well as some of the tasks, their solutions, the approaches of the participants and the names of the winners.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem příspěvku je představit česko-německou slovníkovou databázi obsahující frekventované jazykové výrazy podílející se na strukturaci textu – anaforické diskurzní konektory.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of the paper is to introduce a Czech-German dictionary database containing frequently used language expressions involved in the text structure – anaphoric discourse connectives.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vyhodnotili jsme strojový překlad na vztazích v rámci dokumentů. Ukázalo se, že systémy se na mezivětných vztazích příliš neliší.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>As the quality of machine translation rises and
neural machine translation (NMT) is moving
from sentence to document level translations,
it is becoming increasingly difficult to evaluate
the output of translation systems.
We provide a test suite for WMT19 aimed at
assessing discourse phenomena of MT systems participating in the News Translation
Task. We have manually checked the outputs
and identified types of translation errors that
are relevant to document-level translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme aplikace EVALD (Evaluator of Discourse) pro automatické vyhodnocování českých textů. Podrobně analyzujeme nově získaná jazyková data - texty psané cizími mluvčími dosahující první úrovně osvojování českého jazyka. Představujeme také nové pravopisné "featury" přidané do systému.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present EVALD applications (Evaluator of Discourse) for automated essay scoring. We analyze in detail newly acquired language data – texts written by non-native speakers reaching the threshold level of the Czech language acquisition. We also present new spelling features added in the system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>GeCzLex je databáze překladových ekvivalentů českých a německých textových konektorů. Je založena na značkovaných datech několika elekronických jazykových zdrojů: pro češtinu je jeho základem Pražský diskurzní korpus 2.0 a slovník českých konektorů CzeDLex 0.6, pro němčinu podobný slovník DiMLex, pro oba jazyky pak česko-německá část paralelního korpusu Intercorp 11. Současné, první vydání je pilotní verzí a zároveň výstupem výzkumného projektu o anaforičnosti českých a německých konektorů. Databáze tedy nyní obsahuje překladové ekvivalenty pro a) konektory utvořené zpravidla spojením předložky a anaforického prvku (jako např. "darum" v němčině a "proto" v češtině) a b) české konektory, u nichž byla demonstrována schopnost vázat se na nesousední, vzdálené textové segmenty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>GeCzLex - Lexicon of Czech and German Anaphoric Connectives  - is a translation equivalent database of Czech and German discourse connectives, based on the data of annotated corpora and lexicons: Prague Discourse Treebank 2.0 and CzeDLex 0.6 (for Czech), DiMLex 2.0 (for German) and Intercorp 11 (a large resource of parallel Czech - German texts). Its current, first release is a pilot version representing one of the outcomes of a research project on anaphoricity in Czech and German connectives. Thus, it contains translation equivalents for a) connectives originally formed from a preposition and an anaphoric element (e.g. "darum" in German, "proto" in Czech) and b) Czech connectives with the ability to relate "remotely" to non-adjacent text segments.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje podání Idiap pro WAT 2019 pro anglicko-hindský vícemodální překladatelský úkol. Použili jsme nejmodernější model Transformeru a jako dodatečný zdroj dat jsme použili anglicko-hindský paralelní korpus IITB. Z různých skladeb multimodálního úkolu jsme se zúčastnili skladby „Text-Only“ pro hodnocení a
testovací sady. Naše podání je mezi konkurenty špičkové jak z hlediska automatického, tak manuálního hodnocení. Na základě automatických skóre předčí naše pouze textové podání i systémy, které v úkolu „multimodální překlad“ zohledňují vizuální informace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the Idiap submission
to WAT 2019 for the English-Hindi MultiModal Translation Task. We have used
the state-of-the-art Transformer model and
utilized the IITB English-Hindi parallel corpus as an additional data source.
Among the different tracks of the multimodal task, we have participated in the
“Text-Only” track for the evaluation and
challenge test sets. Our submission tops in
its track among the competitors in terms
of both automatic and manual evaluation.
Based on automatic scores, our text-only
submission also outperforms systems that
consider visual information in the “multimodal translation” task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Visual Genome je dataset spojující strukturované obrazové informace s anglickým jazykem. Představujeme „Hindi Visual Genome“, multimodální datový soubor skládající se z textu a obrazů vhodný pro anglicko-hindský multimodální strojový překlad a multimodální výzkum.
Vybrali jsme krátké anglické segmenty (popisky) z Visual Genome spolu s přidruženými obrázky a automaticky je přeložili do hindštiny. Následovala pečlivá ruční kontrola, která vzala v úvahu související obrázky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Visual Genome is a dataset connecting
structured image information with English language. We present “Hindi Visual Genome”, a multi-modal dataset consisting of text and images suitable for English-Hindi multi-modal machine translation task and multi-modal research.
We have selected short English segments
(captions) from Visual Genome along with the associated images and automatically translated them to Hindi. A careful manual post-editing followed which took the associated images into account.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje sestavení dvou korpusů pro urijštinu (Oria): OdiEnCorp, paralelní korpus urijštiny a angličtiny, a OdiMonoCorp, jednojazyčný urijský korpus.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A multi-lingual country like India needs language corpora for
low resource languages not  only  to provide its citizens with technologies of natural language
processing (NLP) readily available in other countries, but also to support its people
in their education and cultural needs.

In this work, we focus on 
one of the low resource languages, Odia, and
build an Odia-English parallel (OdiEnCorp) and an Odia monolingual (OdiMonoCorp) corpus.
The parallel corpus is based on
Odia-English parallel texts extracted from online resources and formally
corrected by
volunteers. We also preprocess the parallel corpus for machine translation
research or training. The
monolingual corpus comes from a diverse set of 
online resources and we organize it into a collection of
segments and paragraphs, easy to handle by NLP tools.

OdiEnCorp parallel corpus contains 29346 sentence pairs and 756K English and 648K Odia
tokens. OdiMonoCorp contains 2.6 million tokens in  221K sentences in 71K
paragraphs.

Despite their small size, OdiEnCorp and OdiMonoCorp are still the largest Odia language resources, freely available for
non-commercial educational or research purposes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládáme testovací korpus audionahrávek a přepisů prezentací studentských firem spolu s jejich slidy a webovými stránkami. Korpus je určen k evaluaci automatického rozpoznávání řeči, primárně za podmínek, ve kterých je využitelná předchozí dostupnost terminologie a pojmenovaných entit z dané oblasti.
Korpus se zkládá z 39 prezentací v angličtině, každá trvá až 90 sekund.
Řečníci jsou studenti středních škol z evropských zemí. Angličtina je jejich druhý jazyk.
Na korpusu testujeme tři základní modely pro automatické rozpoznávání řeči a ukazujeme jejich nedostatky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a test corpus of audio recordings and transcriptions of presentations of students' enterprises together with their slides and web-pages. The corpus is intended for evaluation of automatic speech recognition (ASR) systems, especially in conditions where the prior availability of in-domain vocabulary and named entities is benefitable. 
The corpus consists of 39 presentations in English, each up to  90 seconds long. 
The speakers are high school students from European countries with English as their second language.
We benchmark three baseline ASR systems on the corpus and show their imperfection.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zkoumáme vícehlavou sebepozornost v enkodérech Transformer NMT pro tři zdrojové jazyky a hledáme vzory, které by mohly mít syntaktickou interpretaci. V mnoha hlavách pozornosti často nalézáme sekvence po sobě jdoucích stavů, které sledují stejnou pozici, které se podobají syntaktickým frázím. Navrhujeme transparentní deterministickou metodu kvantifikace množství syntaktické informace přítomné v sebepozornosti, založenou na automatickém vytváření a vyhodnocování frázových stromů z frázovitých sekvencí. Výsledné stromy porovnáváme se stávajícími syntaktickými korpusy, a to jak ručně, tak pomocí výpočtu přesnosti a úplnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We inspect the multi-head self-attention in Transformer NMT encoders for three source languages, looking for patterns that could have a syntactic interpretation. In many of the attention heads, we frequently find sequences of consecutive states attending to the same position, which resemble syntactic phrases. We propose a transparent deterministic method of quantifying the amount of syntactic information present in the self-attentions, based on automatically building and evaluating phrase-structure trees from the phrase-like sequences. We compare the resulting trees to existing constituency treebanks, both manually and by computing precision and recall.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slovní embedinky a hluboké neuronové sítě fungují skvěle. Nemají žádné explicitní znalosti jazykových abstrakcí. Jak fungují? Jaké emergentní abstrakce v nich můžeme pozorovat? Jak je můžeme interpretovat? Jsou emergentní struktury a abstrakce podobné klasickým lingvistickým strukturám a abstrakcím?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Word embeddings and Deep neural networks perform great. They do not have any explicit knowledge of linguistic abstractions. How do they work? What emergent abstractions can we observe in them? How can we interpret them? Are the emergent structures and abstractions similar to classical linguistic structures and abstractions?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci provádíme studii soustřeďující se na neuronový překlad (NMT) pro angličtinu-indonéštinu (EN-ID) a indonéštinu-angličtinu (ID-EN). Zaměřujeme se na doménu mluveného jazyka, jmenovitě na hovorový jazyk. Budujeme systémy NMT pomocí modelu Transformer pro oba směry překladu a implementujeme adaptaci domény, kde předtrénované systémy NMT trénujeme na datech mluveného jazyka (v doméně). Dále provádíme hodnocení toho, jak může metoda doménové adaptace v našem systému EN-ID vést k formálnějším výsledkům překladů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this  work, we conduct a study on Neural Machine Translation (NMT) for English-Indonesian (EN-ID) and Indonesian-English (ID-EN). We focus on spoken language domains, namely colloquial and speech languages. We build NMT systems using the Transformer model for both translation directions and implement domain adaptation, in which we train our pre-trained NMT systems on speech language (in-domain) data.  Moreover, we conduct an evaluation on how the domain-adaptation method in our EN-ID system can result in more formal translation out-puts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Neuronové systémy generování přirozeného jazyka jsou známy svými patologickými výstupy, tj. generováním textu, který nesouvisí se specifikovaným vstupem. V tomto článku ukazujeme vliv sémantického šumu na současné nejlepší neuronové generátory, které implementují různé mechanismy sémantické kontroly. Zjistili jsme, že vyčištění trénovacích dat může zlepšit sémantickou přesnost až o 97% při zachování plynnosti výstupů. Dále jsme zjistili, že nejčastějším typem chyby je vynechání informace, ne přidaná halucinovaná informace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Neural natural language generation (NNLG) systems are known for their pathological outputs, i.e. generating text which is unrelated to the input specification. In this paper, we show the impact of semantic noise on state-of-the-art NNLG models which implement different semantic control mechanisms. We find that cleaned data can improve semantic correctness by up to 97%, while maintaining fluency. We also find that the most common error is omitting information, rather than hallucination.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme systém pro automatický odhad kvality výstupů generování přirozeného jazyka založený na rekurentních neuronových sítích, který se učí zároveň přiřazovat numerická absolutní hodnocení jednotlivým výstupům a dodávat relativní hodnocení pro páry různých výstupů. Druhá úloha se trénuje pomocí párové hinge chyby nad skóre ze dvou kopií sítě pro absolutní hodnocení.
Pro zlepšení kvality absolutního hodnocení používáme i učení relativního hodnocení a syntetická trénovací data: syntetizujeme trénovací páry zašuměných výstupů generátorů a učíme systém preferovat ten méně zašuměný. Toto vedlo ke 12% zvýšení korelace s lidským hodnocením proti předchozí nejlepší dosažené hodnotě. Navíc ukazujeme první výsledky na datové sadě relativních hodnocení z E2E NLG Challenge (Dušek et al., 2019), kde syntetická data přinesla 4% zlepšení přesnosti oproti základnímu modelu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a recurrent neural network based system for automatic quality estimation of natural language generation (NLG) outputs, which jointly learns to assign numerical ratings to individual outputs and to provide pairwise rankings of two different outputs. The latter is trained using pairwise hinge loss over scores from two copies of the rating network.
We use learning to rank and synthetic data to improve the quality of ratings assigned by our system: we synthesise training pairs of distorted system outputs and train the system to rank the less distorted one higher. This leads to a 12% increase in correlation with human ratings over the previous benchmark. We also establish the state of the art on the dataset of relative rankings from the E2E NLG Challenge (Dušek et al., 2019), where synthetic data lead to a 4% accuracy increase over the base model.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme první datovou sadu zaměřenou na end-to-end generování jazyka v češtině v doméně restaurací, společně s několika silnými základními modely postavenými na architektuře sequence-to-sequence. Neanglické generování jazyka je obecně málo probádaný problém a čeština jakožto morfologicky bohatý jazyk představuje ještě těžší úkol: protože v češtině je třeba skloňovat jmenné entity, delexikalizace nebo jednoduché kopírovací mechanismy nefungují samy o sobě a lexikalizace výstupů generátoru je netriviální.

V našich experimentech představujeme dva různé přístupy k tomuto problému: (1) použití jazykového modelu pro výběr správné vyskloňované formy během lexikalizace, (2) dvoufázové generování: náš model sequence-to-sequence vygeneruje prokládanou sekvenci lemmat a morfologických značek, která je posléze zpracována morfologickým generátorem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the first dataset targeted at end-to-end NLG in Czech in the restaurant domain, along with several strong baseline models using the sequence-to-sequence approach. While non-English NLG is under-explored in general, Czech, as a morphologically rich language, makes the task even harder: Since Czech requires inflecting named entities, delexicalization or copy mechanisms do not work out-of-the-box and lexicalizing the generated outputs is non-trivial.

In our experiments, we present two different approaches to this problem: (1) using a neural language model to select the correct inflected form while lexicalizing, (2) a two-step generation setup: our sequence-to-sequence model generates an interleaved sequence of lemmas and morphological tags, which are then inflected by a morphological generator.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento dokument představuje výsledky premiérově sdíleného úkolu organizovaného společně s Konferencí o strojovém překladu (WMT) 2019.
Účastníci byli požádáni, aby sestavili systémy strojového překladu pro kterýkoli z 18 jazykových párů, které budou vyhodnoceny na základě testovací sady novinek. Hlavním metrem pro tento úkol je lidský úsudek o kvalitě překladu. Úkol byl také otevřen pro další testovací sady, které zkoumají specifické aspekty překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the premier
shared task organized alongside the Conference on Machine Translation (WMT) 2019.
Participants were asked to build machine
translation systems for any of 18 language
pairs, to be evaluated on a test set of news
stories. The main metric for this task is human judgment of translation quality. The task
was also opened up to additional test suites to
probe specific aspects of translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této kapitole chápeme vztah mezi budovaným prostředím a jeho obyvateli jako zpětnou vazbu a naším cílem je zachytit temporalitu této vazby v různých formách adaptace. Zaměřujeme se zejména na nově vznikající formy přizpůsobení, které jsou založeny na digitálně pořízených osobních datech, což vede k automatizaci či k různým opatřením ze strany stavebních subjektů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Recognizing the relation between inhabitants and their built environments as a feedback loop, our aim is to capture the temporality of this loop in various scenarios of adaptation. We specifically focus on the emerging types of adaptation that are motivated by digitally acquired personal data, leading to either automation or action taken by the building stakeholders. Between the microscopic daily mutations (e.g. automated adaptation to occupants’ presence or activity) and the macroscopic evolution of built environments, we identify a “mesoscopic” scale and argue for broadening its consideration in the research domain of adaptive built environments. In mesoscopic adaptations, inhabitants’ data undergo a process of thorough analysis and scrutiny, the results of which inform the re-envisioning of building design for its next cycles over the course of months-years. This contribution distinguishes and elaborates on four temporal scales of adaptation (minutes-hours, days-weeks, months-years, decades-centuries) and then exemplifies the meso-scale with a study conducted over three years within a living lab context. Through this example, we also aim to demonstrate the opportunity for living lab methodologies to contribute to the research on adaptive built environments at the mesoscopic scale.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento dokument představuje výsledky sdíleného úkolu WMT19 Metrics. Účastníci byli požádáni, aby pomocí automatických metrik ohodnotili výstupy překladatelských systémů soutěžících v WMT19 News Translation Task. 13 výzkumných skupin předložilo 24 metrik, z nichž 10 jsou „metriky“ bez odkazů a představují podání ke společnému úkolu s WMT19 Quality Estimation Task, „QE as a Metric“. Navíc jsme vypočítali 11 základních
metriky, s 8 běžně používanými výchozími hodnotami (BLEU, SentBLEU, NIST, WER, PER, TER, CDER a chrF) a 3 reimplementy (chrF+, sacreBLEU-BLEU a sacreBLEU-chrF). Metriky byly hodnoceny na systémové úrovni, jak dobře daná metrika koreluje s oficiálním manuálním řazením WMT19 a na úrovni segmentu, jak dobře metrika koreluje s lidskými úsudky o kvalitě segmentu. Letos používáme přímé hodnocení (DA) jako jedinou formu manuálního hodnocení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the
WMT19 Metrics Shared Task. Participants were asked to score the outputs of the translations systems competing in the WMT19 News Translation
Task with automatic metrics. 13 research
groups submitted 24 metrics, 10 of which
are reference-less “metrics” and constitute
submissions to the joint task with WMT19
Quality Estimation Task, “QE as a Metric”. In addition, we computed 11 baseline
metrics, with 8 commonly applied baselines (BLEU, SentBLEU, NIST, WER,
PER, TER, CDER, and chrF) and 3 reimplementations (chrF+, sacreBLEU-BLEU,
and sacreBLEU-chrF). Metrics were evaluated on the system level, how well a given
metric correlates with the WMT19 official manual ranking, and segment level,
how well the metric correlates with human
judgements of segment quality. This year,
we use direct assessment (DA) as our only
form of manual evaluation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku byla diskutována role reflexiv v popisu češtiny a představena zejména ta reflexiva, která jsou součástí lemmatu slovesa.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this contribution, a comprehensive theoretical account of the reflexives in Czech elaborated within the Functional Generative Description, with the main emphasis put on the role of the reflexives in valency behavior of Czech verbs, has been provided.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku se diskutují reflexiva v češtině, která tvoří konstrukce obdobné jako osobní zájmena, a jejich popis v rámci Funkčního generativního popisu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the paper, Czech reflexives and their description in the dependency-oriented theory,
Functional Generative Description, are addressed. The primary focus lies in the reflexives
that form analogous syntactic structures as personal pronouns.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Reflexiva v češtině plní nerůznější funkce. V tomto příspěvku jsme se zaměřily na změny ve valenci derivovaných reflexivních sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Reflexives in Czech are highly ambiguous. In this contribution, changes in valency of derived reflexive verbs have been discussed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zatímco reciproční slovesa jsou v současné jazykovědě často diskutována, popis reciprocity dalších slovních druhů je teprve na počátku. V tomto příspěvku využíváme poznatky o reciprocitě sloves k popisu jejich deverbálních protějšků. Ukazujeme, že mnoho rysů recipročních konstrukcí sloves reciproční struktury deverbálních jmen sdílejí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Reciprocal verbs are widely debated in the current linguistics. However, other parts of speech can be characterized by reciprocity as well – in
contrast to verbs, their analysis is underdeveloped so far. In this paper, we make
an attempt to fill this gap, applying results of the description of Czech
reciprocal verbs to nouns derived from these verbs. We show that many aspects
characteristic of reciprocal verbs hold for reciprocal nouns as well.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Soutěžní úkol pro rok 2019 na konferenci Computational Language Learning (CoNLL) byl věnován sémantickému parsingu (Meaning Representation Parsing, MRP) napříč různými přístupy. Soutěž zahrnuje pět formálně a lingvisticky rozdílných přístupů k reprezentaci významu (DM, PSD, EDS, UCCA a AMR). Do soutěže se přihlásilo osmnáct týmů, z nichž pět se neúčastnilo oficiálního hodnocení, protože jejich výsledky dorazily až po uzávěrce, nebo tým využil dodatečných trénovacích dat, popřípadě byl jeden ze spoluorganizátorů soutěže mezi zástupci týmu. Veškeré technické informace týkající se soutěže jsou k dispozici na webových stránkách úkolu: http://mrp.nlpl.eu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The 2019 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks. Five distinct approaches to the representation of sentence meaning in the form of directed graph were represented in the training and evaluation data for the task, packaged in a uniform abstract graph representation and serialization. The task received submissions from eighteen teams, of which five do not participate in the official ranking because they arrived after the closing deadline, made use of additional training data, or involved one of the task co-organizers. All technical information regarding the task, including system submissions, official results, and links to supporting resources and software are available from the task web site at: http://mrp.nlpl.eu.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku navrhujeme dvě neuronové architektury pro rozpoznávání vnořených pojmenovaných entit, což je úloha, ve které se pojmenované entity mohou překrývat a také být označeny více než jednou značkou. Vnořené značky zakódováváme pomocí linearizovaného schématu. V prvním navrženém přístupu jsou vnořené značky modelovány jako multiznačky náležející kartézkému součinu vnořených značek ve standardní LSTM-CRF architektuře. V druhém navrženém přístupu přistupujeme k úloze rozpoznávání vnořených pojmenovaných entit jako k sequence-to-sequence problému, ve kterém vstupní sekvence sestává z tokenů a výstupní sekvence ze značek, přičemž používáme vynucený mechanismus attention na slově, které právě značkujeme. Navržené metody překonávají současný stav poznání v úloze rozpoznávání vnořených pojmenovaných entit na čtyřech korpusech: ACE-2004, ACE-2005, GENIA a českém CNEC. Naše architektury jsme dále obohatili nedávno publikovanými kontextovými embeddingy: ELMo, BERT a Flair, čímž jsme dosáhli dalšího zlepšení na všech čtyřech korpusech. Navíc publikujeme nejlepší známé výsledky v ropoznávání pojmenovaných entit na korpusech CoNLL-2002 v nizozemštině a španělštině a korpusu CoNLL-2003 v angličtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We propose two neural network architectures for nested named entity recognition (NER), a setting in which named entities may overlap and also be labeled with more than one label. We encode the nested labels using a linearized scheme. In our first proposed approach, the nested labels are modeled as multilabels corresponding to the Cartesian product of the nested labels in a standard LSTM-CRF architecture. In the second one, the nested NER is viewed as a sequence-to-sequence problem, in which the input sequence consists of the tokens and output sequence of the labels, using hard attention on the word whose label is being predicted. The proposed methods outperform
the nested NER state of the art on four corpora: ACE-2004, ACE-2005, GENIA and Czech CNEC. We also enrich our architectures with the recently published contextual embeddings: ELMo, BERT and Flair, reaching further improvements for the four nested entity corpora. In addition, we report flat NER state-of-the-art results for CoNLL-2002 Dutch and Spanish and for CoNLL-2003 English.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Metody hlubokého učení (deep learning) umělých neuronových sítí zaznamenaly v posledních letech výrazný nástup a jejich pomocí se podařilo překonat a posunout úspěšnost automaticky řešených úloh v mnoha oborech. Výjimkou není ani oblast počítačové (či komputační) lingvistiky (Computational Linguistics) a její aplikační odnož, zpracování přirozeného jazyka (Natural Language Processing) s klasickými úlohami, jako jsou morfologické značkování (POS tagging), závislostní analýza (dependency parsing), rozpoznávání pojmenovaných entit (named entity recognition) a strojový překlad (machine translation). Tento příspěvek přináší přehled nedávných pokroků v uvedených úlohách se vztahem k českému jazyku a představuje zcela nové výsledky v oblastech morfologického značkování a rozpoznávání pojmenovaných entit v češtině, spolu s detailní chybovou analýzou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The deep learning methods of artificial neural networks have seen a significant uptake in recent years, and have succeeded in overcoming and advancing the success of auto-solving tasks in many fields. The field of computational linguistics and its application offshoot, natural language processing with classic tasks such as morphological tagging, dependency analysis, named entity recognition and machine translation are not exceptions. This post provides an overview of recent advances in these tasks related to the Czech language and presents completely new results in the areas of morphological marking and recognition of named entities in Czech, along with detailed error analysis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Je známo, že neuronový strojový překlad vyžaduje velké množství paralelních
trénovacích vět, které obecně brání tomu, aby vynikal na párech jazyků s nedostatečným množstvím zdrojů. Tato práe se zabývá využitím translingválního
učení na neuronových sítích jako způsobu řešení problému nedostatku zdrojů.
Navrhujeme několik přístupů k transferu znalostí za účelem opětovného využití
modelu předtrénovaného na jiné jazykové dvojici s velkým množstvím zdrojů.
Zvláštní pozornost věnujeme jednoduchosti technik. Studujeme dva scénáře:
a) když používáme předtrénovaný model bez jakýchkoli předchozích úprav
jeho trénovacího procesu a b) když můžeme předem připravit prvostupňový
model pro transfer znalostí pro potřeby dítěte. Pro první scénář představujeme
metodu opětovného využití modelu předtrénovaného jinými výzkumníky. V
druhém případě předkládáme metodu, která dosáhne ještě většího zlepšení.
Kromě navrhovaných technik se zaměřujeme na hloubkovou analýzu technik
transferu znalostí a snažíme se vnést trochu světla do pochopení transferového
učení. Ukazujeme, jak naše techniky řeší specifické problémy jazyků s málo
daty a že jsou vhodné i pro jazykové páry s velkým množstvím dat. Potenciální nevýhody a chování hodnotíme studiem transferového učení v různých
situacích, například pod uměle poškozeným trénovacím korpusem, nebo se
zafixovanými částmi modelu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Neural machine translation is known to require large numbers of parallel training sentences, which generally prevent it from excelling on low-resource language pairs. This thesis explores the use of cross-lingual transfer learning on
neural networks as a way of solving the problem with the lack of resources. We
propose several transfer learning approaches to reuse a model pretrained on a
high-resource language pair. We pay particular attention to the simplicity of
the techniques. We study two scenarios: (a) when we reuse the high-resource
model without any prior modifications to its training process and (b) when we
can prepare the first-stage high-resource model for transfer learning in advance.
For the former scenario, we present a proof-of-concept method by reusing a
model trained by other researchers. In the latter scenario, we present a method
which reaches even larger improvements in translation performance. Apart
from proposed techniques, we focus on an in-depth analysis of transfer learning
techniques and try to shed some light on transfer learning improvements. We
show how our techniques address specific problems of low-resource languages
and are suitable even in high-resource transfer learning. We evaluate the potential drawbacks and behavior by studying transfer learning in various situations,
for example, under artificially damaged training corpora, or with fixed various
model parts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto tutoriálu se naučíte používat tensor2tensor a jak aplikovat přenos znalostí na nízko-zdrojové jazykové páry. Tutorial je vhodný i pro účastníky, kteří nemají zkušenosti s trénováním neuronových MT modelů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This tutorial will show you how to use the Tensor2Tensor and how to apply Transfer Learning to low-resource languages. It should be easy to follow for everyone, even people that never trained Machine Translation models.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje systém CUNI do News WMT 2019 pro jazyky s nedostatečnými zdroji: Gujarati-Angličtina  a Kazakština-Angličtina. Zúčastnili jsme se na obou jazykových párech v obou směrech překladu. Náš systém kombinuje přenos znalostí z jiného dvojice jazyků s vysokým množstvím paralelních dat, po kterém následuje trénování na zpětně přeložených jednojazyčných dat. Díky simultánnímu tréninku v obou směrech můžeme iterovat proces zpětného překladu. Používáme Transformer model v constrained podmínkách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the CUNI submission
to the WMT 2019 News Translation Shared
Task for the low-resource languages: GujaratiEnglish and Kazakh-English. We participated
in both language pairs in both translation directions. Our system combines transfer learning from a different high-resource language
pair followed by training on backtranslated
monolingual data. Thanks to the simultaneous training in both directions, we can iterate
the backtranslation process. We are using the
Transformer model in a constrained submission.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku poskytujeme přehled zkušeností z první ruky a výhodných bodů pro osvědčené postupy z projektů v sedmi evropských zemích věnovaných výzkumu korpusu studentů (LCR) a vytváření korpusů studentů jazyků. Korpusy a nástroje zapojené do LCR jsou stále důležitější, stejně jako pečlivá příprava a snadné vyhledávání a opětovné použití korpusů a nástrojů. Nedostatek společně dohodnutých řešení pro mnoho aspektů LCR, interoperabilita mezi korpusy studentů a výměna dat z různých korpusových projektů studentů však zůstává výzvou. Ukážeme, jak mohou být koncepty jako metadata, anonymizace, taxonomie chyb a jazykové anotace, jakož i nástroje, řetězce nástrojů a datové formáty individuálně náročné a jak lze výzvy řešit.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this article we provide an overview of first-hand experiences and vantage points for best practices from projects in seven European countries dedicated to learner corpus research (LCR) and the creation of language learner corpora. The corpora and tools involved in LCR are becoming more and more important, as are careful preparation and easy retrieval and reusability of corpora and tools. But the lack of commonly agreed solutions for many aspects of LCR, interoperability between learner corpora and the exchange of data from different learner corpus projects remains a challenge. We show how concepts like metadata, anonymization, error taxonomies and linguistic annotations as well as tools, toolchains and data formats can be individually challenging and how the challenges can be solved.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento dokument poskytuje komplexní přehled datového souboru mezery pro ruštinu, který se skládá z 7,5k vět s mezerou (stejně jako 15k relevantních negativních vět) a obsahuje údaje z různých žánrů: zprávy, beletrie, sociální média a technické texty. Dataset byl připraven pro automatický sdílený úkol pro řešení rozdílů ruských dat (AGRR-2019) - soutěž zaměřená na stimulaci vývoje nástrojů a metod NLP pro zpracování elipsy. V tomto článku věnujeme zvláštní pozornost metodám rozlišování mezer, které byly zavedeny v rámci sdíleného úkolu, a také alternativní testovací sadě, která ukazuje, že náš korpus je různorodá a reprezentativní podmnožina mezery ruského jazyka dostatečná pro efektivní využití technik strojového učení. .</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper provides a comprehensive overview of the gapping dataset for Russian that consists of 7.5k sentences with gapping (as well as 15k relevant negative sentences) and comprises data from various genres: news, fiction, social media and technical texts. The dataset was prepared for the Automatic Gapping Resolution Shared Task for Russian (AGRR-2019) - a competition aimed at stimulating the development of NLP tools and methods for processing of ellipsis. In this paper, we pay special attention to the gapping resolution methods that were introduced within the shared task as well as an alternative test set that illustrates that our corpus is a diverse and representative subset of Russian language gapping sufficient for effective utilization of machine learning techniques.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje účastnický systém „ÚFAL–Oslo“ v soutěži Cross-Framework Meaning Representation Parsing (MRP, Oepen et al. 2019). Systém je postaven na několika existujících parserech třetích stran. V rámci oficiálních výsledků soutěže se umístil na 12. místě z celkem 14 účastníků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the “ÚFAL–Oslo” system submission to the shared task on Cross-Framework Meaning Representation Parsing (MRP, Oepen et al. 2019). The submission is based on several third-party parsers. Within the official shared task results, the submission ranked 12th out of 14 participating systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Mnohé lingvistické teorie a anotační schémata obsahují hloubkově-syntaktickou a/nebo sémantickou vrstvu. Ačkoli řada z nich byla aplikována na více než jeden jazyk, žádná se nepřibližuje množství jazyků, které jsou pokryty univerzálními závislostmi (Universal Dependencies, UD). V tomto článku představujeme prototyp hloubkových univerzálních závislostí (Deep Universal Dependencies): dvourychlostního konceptu, ve kterém určitá minimální hloubková anotace je automaticky odvozena z povrchových stromů UD, zatímco bohatší anotaci je možné přidat ručně u korpusů, kde jsou k dispozici potřebné zdroje. Data Deep UD zpřístupňujeme v repozitáři Lindat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Many linguistic theories and annotation frameworks contain a deep-syntactic and/or semantic layer. While many of these frameworks have been applied to more than one language, none of them is anywhere near the number of languages that are covered in Universal Dependencies (UD). In this paper, we present a prototype of Deep Universal Dependencies, a two-speed concept where minimal deep annotation can be derived automatically from surface UD trees, while richer annotation can be added for datasets where appropriate resources are available. We release the Deep UD data in Lindat.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme UDify, vícejazyčný víceúlohový model schopný přesně předpovědět univerzální slovní druhy, morfologické rysy, lemmata a závislostní stromy současně pro všech 124 treebanků Universal Dependencies napříč 75 jazyky. Využitím vícejazyčného modelu BERT předcvičeného na 104 jazycích jsme zjistili, že jeho dotrénování na všech zřetězených treebancích spolu s jednoduchými softmax klasifikátory pro každý úkol UD ústí v nejlepší známé výsledky pro UPOS, UFeats, lemmatizaci, UAS a LAS metriky, aniž by vyžadovalo jakékoli rekurentní nebo jazykově specifické komponenty. Hodnocení UDify ukazuje, že vícejazyčné učení nejvíce prospívá jazykům s málo daty. Vícejazykové trénovaní poskytuje kvalitní předpovědi i pro jazyky, které nebyly zastoupeny v trénovacích datech, naznačují, že i pro ně posky vícejazyčné školen. Zdrojový kód UDify je dostupný na https://github.com/hyperparticle/udify.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present UDify, a multilingual multi-task model capable of accurately predicting universal part-of-speech, morphological features, lemmas, and dependency trees simultaneously for all 124 Universal Dependencies treebanks across 75 languages. By leveraging a multilingual BERT self-attention model pretrained on 104 languages, we found that fine-tuning it on all datasets concatenated together with simple softmax classifiers for each UD task can result in state-of-the-art UPOS, UFeats, Lemmas, UAS, and LAS scores, without requiring any recurrent or language-specific components. We evaluate UDify for multilingual learning, showing that low-resource languages benefit the most from cross-linguistic annotations. We also evaluate for zero-shot learning, with results suggesting that multilingual training provides strong UD predictions even for languages that neither UDify nor BERT have ever been trained on. Code for UDify is available at https://github.com/hyperparticle/udify.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nedávný vývoj v oblasti strojového překladu experimentuje s myšlenkou, že model může zlepšit kvalitu překladu provedením více úloh, např. překládáním ze zdroje na cíl a také označováním každého zdrojového slova syntaktickými informacemi. Intuice je taková, že síť by zobecňovala znalosti na více úloh a zlepšila tak výkon překladu, zejména v podmínkách nízkých zdrojů. Vymysleli jsme experiment, který tuto intuici zpochybňuje. Podobné experimenty provádíme jak v multidekodérech, tak v prokládacích sestavách, které označují každé cílové slovo buď syntaktickou značkou, nebo úplně náhodnou značkou. Překvapivě ukazujeme, že model si vede skoro stejně dobře na nekorespondovaných náhodných značkách jako na skutečných syntaktických značkách. Naznačujeme některá možná vysvětlení tohoto chování. Hlavním poselstvím našeho článku je, že experimentální výsledky s hlubokými neuronovými sítěmi by měly být vždy doplněny triviálními výchozími hodnotami dokumentujícími, že pozorovaný přírůstek je
ne kvůli některým nesouvisejícím vlastnostem systému nebo tréninkovým efektům. Skutečná důvěra v to, odkud zisky pocházejí, bude pravděpodobně i nadále problematická.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Recent developments in machine translation experiment with the idea that a model can
improve the translation quality by performing multiple tasks, e.g., translating from source to
target and also labeling each source word with syntactic information. The intuition is that the
network would generalize knowledge over the multiple tasks, improving the translation performance, especially in low resource conditions. We devised an experiment that casts doubt on
this intuition. We perform similar experiments in both multi-decoder and interleaving setups
that label each target word either with a syntactic tag or a completely random tag. Surprisingly, we show that the model performs nearly as well on uncorrelated random tags as on true
syntactic tags. We hint some possible explanations of this behavior.
The main message from our article is that experimental results with deep neural networks
should always be complemented with trivial baselines to document that the observed gain is
not due to some unrelated properties of the system or training effects. True confidence in where
the gains come from will probably remain problematic anyway.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto textu chceme nastínit shody a rozdíly dvou tagsetů užívaných k automatické morfologické analýze češtiny. Ukážeme, nakolik původně nezáměrná a časem udržovaná dvojkolenost tzv. pražského a tzv. brněnského systému může býti v dohledné době překonána v rámci projektu NovaMorf.  Budeme se zabývat vztahy mezi značkováním morfologických kategorií a hodnot v návrhu NovaMorf v porovnání s oběma staršími systémy. Při posuzování brněnského systému vycházíme z článku Czech Morphological Tagset Revisited. (Jakubíček, Kovář, Šmerk, 2011). Poznatky o pražském systému zakládáme na popisu pražského pozičního tagsetu (viz http://ufal.mff.cuni.cz/pdt/Morphology_and_Tagging/Doc/hmptagqr.html) a na monografii Jana Hajiče (Hajič, 1994, 2004). Naším cílem bude ukázat, kterak zkušenosti s užíváním obou systémů vyústily ve snahu inspirovat se pozitivy a vyhnout se neúspěšným řešením na obou stranách (srv. Osolsobě et al., 2017). Ke vzájemné konverzi značek dosavadního pražského systému na brněnský viz Pořízka, Schäfer, 2009.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this text, we want to outline the coincidences and differences of the two tagsets used for automatic morphological analysis of Czech. We will show how much the originally unintentional and time-sustained double-knee of the so-called Prague and so-called Brno systems can be overcome in the foreseeable future under the NovaMorf project. We will look at the relationships between the branding of morphological categories and values in the NovaMorf proposal compared to the two older systems. We base our assessment of the Brno system on an article by the Czech Morphological Tagset Revisited. (Jakubíček, Kovář, Šmerk, 2011). We base our knowledge of the Prague system on a description of Prague's position tagset (see http://ufal.mff.cuni.cz/pdt/Morphology_and_Tagging/Doc/hmptagqr.html) and on a monograph by Jan Hajic (Hajic, 1994, 2004). Our goal will be to show how the experience of using both systems has resulted in an effort to inspire positives and avoid failed solutions on both sides (Srv. Osolsoba et al., 2017). For a mutual conversion of the marks of the Prague system to the Brno system, see Pořízka, Schäfer, 2009.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá praktikou pozorovanou v souboru videozáznamů z prostředí školní třídy. Studenti pracovali s on-line materiály ve skupinách po 2-3 osobách s jedním počítačem a vyplňovali papírový list s otázkami. Účastníci často ukazovali na obrazovku počítače, aby zdůraznili aspekty zobrazených textů, videoklipů, obrázků atd. V některých okamžicích ukazovalo na zobrazené objekty současně více osob. V jednom z takových případů se jedna účastsnice (P1) jemně dotkla natažené ruky své kolegyně (P2). Dotek nevyvolal žádnou pozorovatelnou reakci a po 2,5 sekundách se opakoval. Teprve po tomto druhém dotyku P2 ukončí ukazování a stáhne ruku z obrazovky.
Tato posloupnost interakcí je analyzována jako jediný případ, který zdůrazňuje etnometodologický význam „fenoménu opakování“. V žité realitě natočené situace pouze díky prvnímu dotyku (T1) získává druhý dotyk (T2) svůj význam jako „opakovaná akce“. Dvojitý dotyk je činností, jež je sama tvořena dvěma „samostatnými“ dotyky (T1 + T2) a není jen jedním úkonem, který se jen dvakrát opakuje.
Zdá se, že některé praktiky je třeba znásobit, aby se z nich staly smysluplně vytvořené aktivity (např. klepání na dveře kanceláře nebo klepání na něčí rameno), odlišené od pouhých nehod (např. jediný dotek něčího ramene nebo jediný náraz do dveří).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I will discuss a practice observed in a corpus of video-recordings of a classroom setting. Students worked with on-line materials in groups of 2-3 persons with one computer, filling a paper sheet with questions. Participants often pointed to the computer screen in order to topicalize relevant aspects of displayed texts, video clips, images etc. At times, more persons were concurrently pointing to the displayed objects. In one of such instances, one participant (P1) gently touched the pointing hand of her colleague (P2) – see Figure above. Inciting no observable reaction, after 2.5 seconds, the touch occurred again. Only after this second touch, P2 terminates the pointing and withdraws her hand from the screen.
This interactional sequence is analysed as a single case that highlights the ethnomethodological relevance of “the phenomenon of repetition”. In lived reality of the videotaped situation, it is only because of the first touch (T1), that the second touch (T2) gains its significance as a “repeated action”. In contrast to repeated utterances, which are usually accountable in CA as repair or registering, the double touch is a course of action that is itself constituted by the two “separate” touches (T1 + T2) rather than being a single action that is just repeated twice.
It seems that some acts must be multiplied in order to become meaningfully produced actions (e.g., knocking on the office door, or tapping someone’s shoulder), distinguished from mere accidents (e.g., a single touch of someone’s shoulder, or a single bump into the door). The topic of repeated “hand touching hand” also illustrates pertinent methodological issues in EM/CA research, such as the transcription of touch. Considering that T1 constitutes the phenomenal field of T2, I argue for the importance of profoundly ethnomethodological approach to studies of human interaction.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška poskytne teoretické základy pro navazující workshop zaměřený na jeden z největších digitálních archivů ústní historie: Archiv vizuální historie USC Shoah Foundation. Tato digitální databáze se skládá z více než 54 500 audiovizuálních záznamů orálněhistorických rozhovorů. Během workshopu, který kombinuje teorii a praxi, se zaměříme na sekundární analýzu archivních rozhovorů, diskusi o jejich úloze v současné společnosti a také na otázky interpretace při sekundárním využití archivované orální historie. V úvodní přednášce pojednám orální historii obecněji jako společenský fenomén a archivované rozhovory jako sociální sémiotické objekty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The lecture Sociological Aspects of Oral History in the Digital Age is going to provide theoretical foundations for the follow-up workshop focused on one of the largest digital archives of oral history: the USC Shoah Foundation’s Visual History Archive. This digital database consists of more than 54,500 audio-visual recordings of oral history interviews. The majority of the interviews were collected between 1994 and 2000, mainly in the USA and Europe. Most of the interviews (nearly 50%) are in English, but more than 35 other languages are also represented in the Visual History Archive. Over 150 interviews from the Archive are related to Luxembourg. During the workshop, combining theory and praxis, we will focus on secondary analysis of archival interviews, discussion of their role in contemporary society, as well as interpretive issues in secondary use of archived oral history. In the introductory lecture, we will consider oral history more generally as a social phenomenon, and archived oral history interviews as social semiotic objects.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V současné společnosti se otázky sociální spravedlnosti často týkají dokumentárních videoklipů a jejich interpretace. Pochopení způsobů práce s videem a jeho okolí v každodenním prostředí by mohlo přinést nové pohledy na tradiční témata. Tato práce vychází z etnometodologické studie videonahrávek kolektivní práce studentů s on-line materiály sestavenými z textů, obrázků a fragmentů orálněhistorických rozhovorů. Studenti pracovali s jedním počítačovým zařízením a jedním papírovým listem v malých skupinách po dvou nebo třech. Časová organizace práce v učebně s digitálními ústními dějinami je v tomto uspořádání rozdělena do tří fází: 1) příprava na sledování videoklipu, což vyžaduje vytvoření optimálního uspořádání hmotných artefaktů a tělesné orientace účastníků; 2) sledování videoklipu, během něhož je upřednostňováno nepřerušované sledování od začátku do konce, jakož i omezení hovoru účastníků na "průběžné komentáře"; 3) reflexe videoklipu, během níž se projevuje orientace na formulaci požadované odpovědi a její společné zapsání do papírového listu, včetně případného druhého sledování videoklipu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In contemporary society, social justice issues are often related to documentary video clips and their interpretation. Understanding the ways of working with and around video in everyday settings could provide novel perspectives on traditional themes. This paper is based on an ethnomethodological study of videotaped episodes of students’ collaborative work with on-line material constructed from texts, images and video clip fragments of oral history interviews. Students worked with a single computer device and one paper sheet in small groups of two or three. The temporal organization of classroom work with digital oral history in this classroom setting is divided to three phases: (1) preparation for watching the video clip, which requires the establishment of an optimal arrangement of material artifacts and participants’ bodily orientations; (2) watching the video clip, which points to a preference for uninterrupted watching from start to end, as well as limitation of participants’ talk to "running commentary"; (3) reflecting the video clip, which shows orientation to the formulation of required answer and its collaborative writing into the paper sheet, including occasional second watching.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pro efektivní a smysluplné používání audiovizuálních orálněhistorických materiálů (OH) je důležité pochopit, jak lidé chápou smysl takových videozáznamů v sociální interakci. Vyvstávají tak zásadní otázky týkající se sekundární analýzy a praktického využití archivovaného audiovizuálního materiálu OH, jako například: Jaké jsou rysy OH rozhovoru jako společenského objektu? Co jej činí smysluplným a interpretovatelným? Kolik toho potřebujeme vědět o sociálně situovaném charakteru rozhovoru, abychom ho správně pochopili? Existuje ve vztahu k OH „příliš málo“ nebo „příliš mnoho“ kontextu? Jaký smysl mají rozhovory na OH ve společenské praxi a jaký je jejich vztah k širším historickým znalostem? V rámci workshopu uchopíme tyto dalekosáhlé otázky z velice empirického a praktického hlediska. Oslovujeme akademické pracovníky, kteří se zajímají o mezioborové přístupy a pracují s interview, orální historií a digitalizovanými nebo digitálními kvalitativními daty obecně.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In order to use audiovisual oral history (OH) materials efficiently and meaningfully, it is important to understand how people make sense of such video recordings in social interaction. Fundamental questions thus emerge in regard to secondary analysis and practical utilization of archived audiovisual OH material, such as: What are the features of OH interview as a social object? What makes it meaningful and interpretable? How much do we need to know about the socially situated character of the interview in order to understand it properly? Is there “too little” or “too much” context in relation to OH? How do people make sense of OH interviews in social practice, and relate it to their broader historical knowledge? In the workshop, we will grasp such far-reaching questions from a very empirical and practical perspective. We are reaching out to scholars who are interested in cross-disciplinary approaches and work with interviews, oral history, and digitized or digital qualitative data in general.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce popisuje testovací sadu dokumentů z auditorské domény pro strojový překlad a její použití jako jedné ze „testovacích sad“ v WMT19 News Translation Task pro překladatelské směry zahrnující češtinu, angličtinu a němčinu.

Naše hodnocení naznačuje, že současné MT systémy optimalizované pro oblast všeobecného zpravodajství mohou docela dobře fungovat i v konkrétní oblasti auditních zpráv. Podrobné manuální hodnocení však ukazuje, že hluboká faktická znalost dané oblasti je nezbytná. Pouhým okem neodborníka se překlady mnoha systémů zdají téměř dokonalé a automatické hodnocení MT s jednou referencí je pro zvážení těchto detailů prakticky zbytečné.

Dále na ukázkovém dokumentu z oblasti smluv ukazujeme, že i ty nejlepší systémy zcela selhávají v zachování sémantiky smlouvy, specificky zachování identit stran.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes a machine translation test set of documents from the auditing domain and its use as one of the “test suites” in the WMT19 News Translation Task for translation directions involving Czech, English and German.

Our evaluation suggests that current MT systems optimized for the general news domain can perform quite well even in the particular domain of audit reports. The detailed manual evaluation however indicates that deep factual knowledge of the domain is necessary. For the naked eye of a non-expert, translations by many systems seem almost perfect and automatic MT evaluation with one reference is practically useless for considering these details.

Furthermore, we show on a sample document from the domain of agreements that even the best systems completely fail in preserving the semantics of the agreement, namely the identity of the parties.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CzeDLex 0.6 je druhá vývojová verze slovníku českých diskurzních konektorů. Slovník obsahuje konektory částečně automaticky extrahované z Pražského diskurzního korpusu 2.0 (PDiT 2.0), rozsáhlého korpusu s ručně anotovanými diskurzními vztahy. Nejfrekventovanější slovníková hesla (pokrývající více než 90% diskurzních vztahů anotovaných v PDiT 2.0) byla ručně zkontrolována, přeložena do angličtiny a doplněna dalšími lingvistickými informacemi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CzeDLex 0.6 is the second development version of a lexicon of Czech discourse connectives. The lexicon contains connectives partially automatically extracted from the Prague Discourse Treebank 2.0 (PDiT 2.0), a large corpus annotated manually with discourse relations. The most frequent entries in the lexicon (covering more than 90% of the discourse relations annotated in the PDiT 2.0) have been manually checked, translated to English and supplemented with additional linguistic information.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce se zabývá opomínejou otázkou valence příslovcí. Po stručném teoretickém úvodu představíme postup při extrahování seznamu potenciálních valenčních příslovcí ze dvou českých syntakticky značkovaných korpusů, SYN2015 a PDT. Dále zmíníme metodologické a teoretické problémy spojené s tímto problémem, zejména ty, které se týkají nejasných hranic slovních druhů, a charakterizujeme typy získaných valenčních příslovcí. Tam, kde je to vhodné, komentujeme lexikografické zpracování valenčních příslovcí a případně navrhujeme jeho úpravu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper deals with the neglected issue of the valency of adverbs. After providing a brief theoretical background, a procedure is presented of extracting the list of potentially valent adverbs from two syntactically parsed corpora of Czech, SYN2015 and PDT. Taking note of the methodological and theoretical problems surrounding this task, especially those relating to the fuzzy boundaries of word classes, we outline the types of adverbs identified as having valency properties. Where appropriate, we comment on – and occasionally suggest improvements in – the lexicographic treatment of valent adverbs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>DeriNet je rozsáhlý lingvistický zdroj obsahující více než 1 milion českých lexémů spojených téměř 810 tisíci derivačních vztahů. Jeho předchozí verze, DeriNet 1.7, kromě derivací neobsahovala další anotace – byly v ní uvedeny lemmata a slovnědruhové kategorie každého lexému, a od verze 1.5 binární příznak kompozitnosti.
Tento článek představuje rozšířenou verzi zdroje, nazvanou DeriNet 2.0, která přináší řadu nových anotací: všechny lexémy mají vyznačené základní morfologické kategorie (vid, rod a životnost), 250 tisíc lexémů má identifikované kořenové morfémy, 150 tisíc derivačních vztahů je označeno svou sémantickou kategorií (zdrobňování, přivlastňování, přechylování, opakovanost a změna vidu), některá kompozita jsou v rámci pilotního projektu přiřazena ke svým základovým slovům a přibylo několik tzv. fiktivních lexémů spojujících příbuzné derivační rodiny bez společného předka. Tyto nové anotace mohly být přidány díky novému souborovému formátu, který je obecný a rozšiřitelný a tedy potenciálně využitelný i v jiných podobných projektech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>DeriNet is a large linguistic resource containing over 1 million lexemes of Czech connected by
almost 810 thousand links that correspond to derivational relations. In the previous version,
DeriNet 1.7, it only contained very sparse annotations of features other than derivations – it listed
the lemma and part-of-speech category of each lexeme and since version 1.5, a true/false flag
with lexemes created by compounding.
The paper presents an extended version of this network, labelled DeriNet 2.0, which adds a number
of features, namely annotation of morphological categories (aspect, gender and animacy) with all
lexemes in the database, identification of root morphemes in 250 thousand lexemes, annotation
of five semantic labels (diminutive, possessive, female, iterative, and aspect) with 150 thousand
derivational relations, a pilot annotation of parents of compounds, and another pilot annotation
of so-called fictitious lexemes, which connect related derivational families without a common
synchronous parent. The new pieces of annotation could be added thanks to a new file format
for storing the network, which aims to be general and extensible, and therefore possibly usable to
other similar projects.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>DeriNet je lexikální síť zachycující derivační vztahy mezi českými slovy. Vrcholy sítě představují české lexémy a hrany reprezentují derivační nebo kompoziční vztahy mezi odvozeným a základovým/základovými slovem/slovy. Tato verze, DeriNet 2.0, obsahuje 1 027 665 lexémů převzatých ze slovníku MorfFlex spojených 808 682 derivačními a 600 kompozičními vztahy.
V porovnání s předchozími verzemi, verze 2.0 používá nový formát a obsahuje nové typy anotace: kompozice, anotace několika morfologických kategorií lexémů, identifikované kořenové morfy u 244 198 lexémů, sémantické označení 151 005 hran pomocí pěti značek a identifikaci několika fiktivních lexémů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>DeriNet is a lexical network which models derivational relations in the lexicon of Czech. Nodes of the network correspond to Czech lexemes, while edges represent derivational or compositional relations between a derived word and its base word / words. The present version, DeriNet 2.0, contains 1,027,665 lexemes (sampled from the MorfFlex dictionary) connected by 808,682 derivational and 600 compositional links.
Compared to previous versions, version 2.0 uses a new format and contains new types of annotations: compounding, annotation of several morphological categories of lexemes, identification of root morphs of 244,198 lexemes, semantic labelling of 151,005 relations using five labels and identification of several fictitious lexemes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozvoj a využívání jazykových zdrojů často zahrnuje zpracování osobních údajů. Obecné nařízení o ochraně osobních údajů (GDPR) stanoví celoevropský rámec pro zpracování osobních údajů pro výzkumné účely a zároveň umožňuje určitou flexibilitu na straně členských států. Dokument pojednává o právním rámci pro výzkum lan- guage po vstupu GDPR v platnost. V první části představíme některé základní pojmy ochrany údajů, které jsou důležité pro výzkum jazyků. Ve druhé části je diskutován obecný rámec zpracování osobních údajů pro výzkumné účely. V poslední části se zaměříme na modely, které některé členské státy EU používají k regulaci zpracování dat pro výzkumné účely.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The development and use of language resources often involve the processing of personal data. The General Data Protection Regulation (GDPR) establishes an EU-wide framework for the processing of personal data for research purposes while at the same time allowing for some flexibility on the part of the Member States. The paper discusses the legal framework for lan- guage research following the entry into force of the GDPR. In the first section, we present some fundamental concepts of data protection relevant to language research. In the second section, the general framework of processing personal data for research purposes is discussed. In the last section, we focus on the models that certain EU Member States use to regulate data processing for research purposes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek zachycuje funkce a překlady významově vyprázdněných dikurzních markerů AND, BUT a SO v angličtině, češtině, maďarštině, francouzštině a litevštině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Discourse markers are highly polyfunctional, particularly in spoken settings. Because of their syntactic optionality, they are often omitted in translations, especially in the restricted space of subtitles such as the parallel transcripts of TED Talks. In this study, we combine discourse annotation and translation spotting to investigate English discourse markers, focusing on their functions, omission and translation equivalents in Czech, French, Hungarian and Lithuanian. In particular, we study them through the lens of
underspecification, of which we distinguish one monolingual and two multilingual types. After making an inventory of all discourse markers in the dataset, we zoom in on the three most frequent and, but and so. Our small-scale yet fine-grained corpus study suggests that the processes of underspecification are based on the semantics of discourse markers and are therefore shared cross-linguistically. However, not all discourse marker types nor their functions are equally affected by underspecification. Moreover, monolingual and multilingual underspecification do not always map for a particular marker. Beyond the empirical
analysis of three highly frequent discourse markers in a sample of TED Talks, this study
illustrates how translation and annotation can be combined to explore the multiple facets of underspecification in a monolingual and multilingual perspective.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Diateze, a to jak ty tvořené pomocí pasivního participia (pasivum, prostý a posesivní rezultativ, recipientní diateze), tak i tzv. zvratné pasivum (deagentizace) byly v minulosti předmětem řady studií jak v bohemistické, tak i
v mezinárodní lingvistice, pro češtinu ale dosud chybělo jejich důkladné slovníkové zpracování. V této dizertační práci se zabývám zachycením diatezí tvořených pomocí pasivního participia a s nimi příbuzných verbonominálních konstrukcí
v gramatické komponentě valenčního slovníkuVALLEX.
Vlastnímu tématu práce předchází krátký historický úvod a podrobné shrnutí pojetí valence ve Funkčním generativním popisu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Diatheses have been the topic of a number of linguistic studies in Czech as well as international linguistics. Previous investigations have led to the delimitation of diatheses formed with the passive participle (the passive, objective (with auxiliary být ‘to be’) and possessive (with auxiliary mít ‘to have’)
resultative, and recipient diatheses) and the so-called reflexive passive (deagentive diathesis), but systematic dictionary treatment has not been carried out yet.
This dissertation is concerned with the diatheses that are formed with the passive participle and their treatment in the Functional Generative Description (FGD). After a thorough description of the valency theory of FGD, I discuss the design of the Grammatical Component of the valency lexicon VALLEX, concentrating on the rules for the formation of the diatheses that are built with the passive participle. Related verbonominal constructions and a new candidate for treatment as a diathesis (the subjective resultative) are also discussed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Automaticky vytvořené zápisy ze schůzek mohou výrazně zlepšit efektivitu práce. Abychom připravili rozumnou automatizaci potřebujeme mít přehled a pochopení, jaké typy schůzek a zápisů existují, jaké mají společné rysy.
V tomto článku shrnujeme jazykové vlastnosti zápisek a schůzek, analyzujeme existující metody sumarizace a možnost jejich aplikace k danému úkolu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Many meetings of different kinds will potentially benefit from technological support like automatic creation of meeting minutes. To prepare a reasonable automation, we need to have a detailed understanding of common types of meetings, of the linguistic properties and commonalities in the structure of meeting minutes, as well as of methods for their automation. 
In this paper, we summarize the quality criteria and linguistic properties of meeting minutes, describe the available meeting corpora and meeting datasets and propose a classification of meetings and minutes types. Furthermore, we  analyze the methods and tools for automatic minuting with respect to their use with existing types of datasets. We summarize the obtained knowledge with respect to our goal of designing automatic minuting and present our first steps in this direction.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Srovnáváme použítí korelativních konstrukcí v českých, ruských a německých překladech z angličtiny. Hledáme, kde je překlad s použitím korelativní konstrukce povinný, kde je možný a kde je vyloučen. Analyzujeme různé typy kontextů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Our research concerns correlative constructions in German, Czech and Russian translations and the corresponding structures in English that trigger these correlatives. In total, 100 parallel segments have been analysed manually for this study. For the study of optionality, we compare our results to original (not translated texts) in Czech and Russian National corpora. The occurrence of the constructions under analysis, as well as cross-linguistic differences in the transformation patterns, can be explained by the influence of a set of factors described in the presentation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nový ručně anotovaný morfematicky segmentovaný slovník pro perštinu, a dále algoritmus, který s využitím tohoto slovníku zkonstruuje morfologickou síť. Výsledná síť zachycuje jak derivační, tak flektivní vztahy mezi slovními formami. Algoritmus pro tvorbu sítě aproximuje rozdíl mezi kořenovými a afixovými morfémy na základě frekvenční informace o morfémech. Vyhodnocujeme kvalitu (ve smyslu lingvistické správnosti) výsledné sítě v konfiguraci s ručně označenými nekořenovými morfémy. V další části naší práce vyhodnocujeme různé strategie pro přidání nových (ve slovníku dosud nezachycených) do sítě s využitím systému MORFESSOR (v řízené i neřízené verzi). Experimenty potvrzují, že navržený postup lze použít pro přidávání dosud nepokrytých slov s přijatelnou úspěšností.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this work, we introduce a new large hand-annotated morpheme-segmentation lexicon of Persian words and present an algorithm that builds a morphological network using this segmented lexicon. The resulting network captures both derivational and inflectional relations. The algorithm for inducing the network approximates the distinction between root morphemes and affixes using the number of morpheme occurrences in the lexicon.
We evaluate the quality (in the sense of linguistic correctness) of the resulting network empirically and compare it to the quality of a network generated in a setup based on manually distinguished non-root morphemes. 
In the second phase of this work, we evaluated various strategies to add new words (unprocessed in the segmented lexicon) into an existing morphological network automatically. For this purpose, we created primary morphological networks based on two initial data: a manually segmented lexicon and an automatically segmented lexicon created by unsupervised MORFESSOR. Then new words are segmented using MORFESSOR and are added to the network. In our experiments, both supervised and unsupervised versions of MORFESSOR are evaluated and the results show that the procedure of network expansion could be performed automatically with reasonable accuracy.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>ITAT (Informačné Technológie - Aplikácie a Teória) je tradiční česko-slovenská konference sdružující vědce a odborníky pracující v širokém spektru informatiky. ITAT nabízí nejen možnost výměny nápadů a informací, ale klade také důraz na neformální komunikaci a diskuse mezi účastníky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>ITAT (Information Technologies—Applications and Theory) is a traditional Czecho-Slovak conference gathering scientists and experts working within a broad scope of computer science. ITAT not only offers a possibility to exchange ideas and information, it also emphasizes informal communication and discussions among participants.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje úvodní výzkum větné reprezentace ve spojitém prostoru. Získali jsme páry velmi podobných vět, které si liší pouze drobnou změnou (jako změna substantiva, přidání adjektiva) z datasetů na odvozování v přirozeném jazyce pomocí metody jednoduchých vzorců. Zkoumáme nakolik drobná změna ve větě ovlivní její reprezentaci ve spojitém prostoru a jak jsou tyto změny zobrazeny v některých populárních modelech větných embeddingů. Zjistili jsme, že některé embeddingy pěkně reflektují malé větné změny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an introductory investigation into continuous-space vector 
representations of sentences. We acquire pairs of very similar sentences
differing only by a small alterations (such as change of a noun, adding 
an adjective, noun or punctuation) from datasets for natural language 
inference using a simple pattern method. We look into how such a small 
change within the sentence text affects its representation in the continuous 
space and how such alterations are reflected by some of the popular sentence 
embedding models. We found that vector differences of some embeddings
actually reflect small changes within a sentence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme sbírku testů pro získávání informací napříč jazyky v medicínské doméně. Tato sbírka je postavena na zdrojích používaných laboratoří CLEF pro hodnocení elektronického zdravotnictví v letech 2013–2015 při úkolech vyhledávání informací zaměřených na pacienta a zlepšuje využitelnost a opakovanou použitelnost oficiálních údajů. Soubor dokumentů je totožný s oficiálním souborem, který byl pro tento úkol použit v roce 2015, a obsahuje asi milion anglických lékařských webových stránek. Sada dotazů obsahuje 166 položek použitých během tří let jako testovací dotazy, které jsou nyní dostupné v osmi jazycích. Rozšířená sbírka testů poskytuje další relevantní hodnocení, která téměř zdvojnásobily množství oficiálně posuzovaných párů dotaz-dokument. Tato práce popisuje obsah rozšířené sbírky, podrobnosti překladu dotazů a posouzení relevance a nejmodernější výsledky získané z této sbírky.
https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-2925</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a test collection for medical cross-lingual information retrieval. It is built on resources used by the CLEF eHealth Evaluation Lab 2013–2015 in the patient-centered information retrieval tasks and greatly improves applicability and reusability of the official data. The document set is identical to the official one used for the task in 2015 and contains about 1 million English medical webpages. The query set consists of 166 queries used during the three years of the campaign as test queries, now available in eight languages (the original English and human translations into Czech, French, German, Hungarian, Polish, Spanish and Swedish). The extended test collection provides extensively improved relevance judgements which almost doubled the amount of the officially assessed query-document pairs. This paper describes the content of the extended version of the test collection, details of query translation and relevance assessment, and state-of-the-art results on this collection.
The dataset can be obtained publicly in the following url: https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-2925</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozšířená testovací kolekce CLEF eHealth pro vyhledávání napříč jazyky v textech z medicíny</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This package contains an extended version of the test collection used in the CLEF eHealth Information Retrieval tasks in 2013--2015. Compared to the original version, it provides complete query translations into Czech, French, German, Hungarian, Polish, Spanish and Swedish and additional relevance assessment.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme metodu automatického rozšíření dotazů pro vyhledávání informací mezi jazyky v oblasti medicíny. Metoda využívá strojový překlad dotazů ze zdrojového jazyka do jazyka dokumentu a lineární regresi k předvídání výkonu vyhledávání pro každý přeložený dotaz, který je rozšířen o kandidátský termín. Kandidátské termíny (v jazyce dokumentu) pocházejí z více zdrojů: hypotézy pro překlad dotazů získané ze systému strojového překladu, články na Wikipedii a abstrakty PubMed. Rozšíření dotazu je použito pouze v případě, že model předpovídá skóre pro kandidátní termín přesahující vyladěnou hranici, která umožňuje rozšiřovat dotazy pouze se silně příbuznými termíny. Naše experimenty jsou prováděny s využitím kolekce testů elektronického zdravotnictví CLEF 2013–2015 a vykazují významná zlepšení jak v mnohojazyčném, tak jednojazyčném nastavení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a method for automatic query expansion for cross-lingual information retrieval in the medical domain. The method employs machine translation of source-language queries into a document language and linear regression to predict the retrieval performance for each translated query when expanded with a candidate term. Candidate terms (in the document language) come from multiple sources: query translation hypotheses obtained from the machine translation system, Wikipedia articles and PubMed abstracts. Query expansion is applied only when the model predicts a score for a candidate term that exceeds a tuned threshold which allows to expand queries with strongly related terms only. Our experiments are conducted using the CLEF eHealth 2013–2015 test collection and show significant improvements in both cross-lingual and monolingual settings.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku porovnáváme strukturu českých slovních embeddingů pro anglicko-český strojový překlad (NMT), word2vec a analýzu sentimentu. Ukazujeme, že i když je možné úspěšně předvídat část slovních druhů (POS) z embeddingů word2vec a různých překladových modelů, ne všechny prostory embeddingů vykazují stejnou strukturu. Informace o POS jsou přítomny v embeddingu word2vec, ale vysoký stupeň organizace POS v dekodéru NMT naznačuje, že tyto informace jsou důležitější pro strojový překlad, a proto je model NMT reprezentuje přímějším způsobem. Naše metoda je založena na korelaci dimenzí PCA s kategorickými lingvistickými údaji. Také ukazujeme, že další zkoumání histogramů tříd podél dimenzí PCA je důležité pro pochopení struktury znázornění informací v embeddinzích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we compare structure of Czech word embeddings for English-Czech neural machine translation (NMT), word2vec and sentiment analysis. We show that although it is possible to successfully predict part of speech (POS) tags from word embeddings of word2vec and various translation models, not all of the embedding spaces show the same structure. The information about POS is present in word2vec embeddings, but the high degree of organization by POS in the NMT decoder suggests that this information is more important for machine translation and therefore the NMT model represents it in more direct way. Our method is based on correlation of principal component analysis (PCA) dimensions with categorical linguistic data. We also show that further examining histograms of classes along the principal component is important to understand the structure of representation of information in embeddings.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Derivace je typ slovotvorného procesu, který odvozuje nová slova z existujících přidáváním, měněním či odebíráním afixů. V tomto článku zkoumáme potenciál slovních embeddingů pro identifikaci vlastností derivací v češtině. Extrahujeme derivační vztahy mezi páry slov z DeriNetu, sítě českých derivací, která sdružuje zhruba milion českých lemmat do derivačních stromů. Pro každý pár vypočteme rozdíl embeddingů obou forem a neřízeně clusterujeme výsledné vektory. Naše výsledky ukazují, že tyto clustery zhruba odpovídají manuálně označeným sémantickým kategoriím derivačních vztahů (čili vztah „péct-pekař“ patří do třídy „aktor“ a správné clusterování ho přiřadí do stejného clusteru jako „řídit-ředitel“).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Derivation is a type of a word-formation process which creates new words from existing ones by adding, changing or deleting affixes. In this paper, we explore the potential of word embeddings to identify properties of word derivations in the morphologically rich Czech language. We extract derivational relations between pairs of words from DeriNet, a Czech lexical network, which organizes almost one million Czech lemmas into derivational trees. For each such pair, we compute the difference of the embeddings of the two words, and perform unsupervised clustering of the resulting vectors. Our results show that these clusters largely match manually annotated semantic categories of the derivational relations (e.g. the relation ‘bake–baker’ belongs to category ‘actor’, and a correct clustering puts it into the same cluster as ‘govern–governor’).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje vážený konečný morfologický převodník pro krymskou tatarštinu, který je schopen analyzovat a generovat slova jak v latince, tak v cyrilici. Tento převodník byl vyvinut týmem, který sestává z člena komunity a jazykového experta, polního lingvisty, který pracuje s krymskotatarskou komunitou, turkologa se zkušeností s počítačovou lingvistikou a zkušeného počítačového lingvisty se zkušenostmi s turkickými jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes a weighted finite-state
morphological transducer for Crimean Tatar able to analyse and generate in both Latin and Cyrillic orthographies. This transducer was developed by a team including a community member and language expert, a field linguist who works with the community, a Turkologist with computational linguistics expertise, and an experienced computational linguist with Turkic expertise.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení oboru počítačové lingvistiky a zejména úlohy strojového překladu středoškolákům, účastníkům semináře 100vědců.cz: Strojové zpracování přirozeného jazyka v posledních několika málo letech zasáhla vlna hlubokého učení, "umělé inteligence", a podobně jako v jiných oblastech vidíme revoluční skoky v kvalitě výstupu. V příspěvku představím postupně základy techniky zpracování vět hlubokými neuronovými sítěmi, podrobněji se zaměřím na úlohu strojového překladu, kde stroje v posledním roce začaly dosahovat lidských kvalit, a nakonec se vrátím k obecnějším úvahám, zda nám stroje konečně začínají rozumět.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An introduction of computational linguistics and machine translation in particular to high school students, participants of the seminar 100vedcu.cz.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce představím technologii strojového překladu, její předpoklady a omezení, to jest zejména závislost na dostupnosti přeložených textů a obecně spolehlivost. Dále popíši zkušenosti s naším projektem, kdy překládáme SMS z vietnamštiny do češtiny pro účely Národní protidrogové centrály Policie České republiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the lecture, I will present machine translation technology, its assumptions and limitations, that is, in particular, the reliance on the availability of translated texts and also reliability. Next, I will describe the experience of our project, where we translate SMS from Vietnamese into Czech for the purposes of the National Drug Headquarters of the Police of the Czech Republic.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento dokument slouží jako stručný přehled zvláštního vydání JNLE o znázornění významu věty, který spojuje tradiční symbolické a moderní kontinuální přístupy. Uvedeme pozoruhodné aspekty významu věty a jejich slučitelnost s oběma proudy výzkumu a poté shrneme podklady vybrané pro tuto zvláštní otázku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper serves as a short overview of the JNLE special issue on representation of the meaning of the sentence, bringing together traditional symbolic and modern continuous approaches. We indicate notable aspects of sentence meaning and their compatibility with the two streams of research and then summarize the papers selected for this special issue.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládáme dvě případové studie, které prokazují, že soužití různých (pod)oborů komputační lingvistiky a nauky o přirozeném jazyce může vést k novým zjištěním a pomoci rozvoji oboru (oborů). Jeden příklad pokrývá studium synonymie vyhledáváním informací v různých lexikálních zdrojích s ohledem na mnohojazyčnost a druhý ukazuje na studium některých jevů týkajících se informační struktury a pořadí slov v angličtině a češtině, jak lze paralelní vícejazyčný a/nebo vícevrstevný korpus, pokud je správně opatřen anotací, použít pro studium některých aspektů hloubkové syntaktické struktury. Oba případy podporují naše přesvědčení, že tvorba jazykových zdrojů je základním krokem v hlavní úloze počítačové lingvistiky, že dobře definovaná anotace je důležitým krokem vpřed jak k testování původní jazykové teorie, tak k jejímu dalšímu rozvoji.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present two case studies that demonstrate that co-habitation of various (sub)disciplines of Computational Linguistics and Natural Language Pro-cessing can reach novel findings and help the advancement of the field(s). One example covers a study of synonymy by searching for information in different lexical resources with regard to multilinguality, and the other demonstrates on the study of some phenomena concerning information structure and word order in English and Czech how a parallel multilingual and/or multilayered corpus if properly annotated can be used for a study of some aspects of the deep syntactic structure. Both cases support our convic-tion that the creation of language resources is a fundamental step in the do-main of computational linguistics, that well-founded annotation is an im-portant step forward towards both testing the original linguistic theory and developing it further.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jako první v rámci oddílu I je zařazena stať Relativní čas napsanáu ve spoluautorství s Petrem Sgallem.P. Sgall je také autorem teoretického rámce pro závislostní a vícerovinný popis jazyka, Funkčního generativního popisu, na nějž  zde otištěné příspěvky navazují a dále ho rozvíjejí. Oddíl II o obecných otázkách syntaxe je zakončen statí o přínosu Vladimíra Šmilauera do novočeské syntaxe. Oddíl III obsahuje studie o valenci jakožto „novém paradigmatu“ v syntaxi. Valenci je věnováno hodně pozornosti, jí byly věnovány četné diskuse i polemiky týkající se např. kritérií pro určování sémantické obligatornosti a platnosti „dialogového testu“.Do oddílu IV jsou zařazena témata, v nichž se kumulují problémy morfologické se syntaktickými (zejména s valencí). Jejich zvládnutí a konsistentní popis se stává měřítkem úspěšnosti zvoleného teoretického rámce. Jde především otázky koreference a kontroly.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the present volume selected papers written during more than 50 years of the author´s linguistic career are presented. Twenty five articles are divided into four general Sections, namely (I) Studies on morphological meanings, (II) General issues on syntax, (III) Issues on valency, (IV) Reflexivity, reciprocity, coreference and control. The Functional Generative Description used as the theoretical framework provides the basis for the studies of form and function relations between the units of several language levels and for the dependency approach to the syntax.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tradiční třídění příslovečných určení na lokální, časová, způsobová a kauzální, s jakými se pracuje v našich syntaxích, a jejich aplikace pro školní výuku jsou pro tyto účely vyhovující. Pro empirické zpracování v rámci Funkčního generativního popis, který slouží mj. i pro úlohy automatického zpracování jazyka (zejména pro strojový překlad, ale v podstatě i pro translatologické problémy spojené s výukou češtiny jako cizího jazyka), se jeví jako potřebná jemnější klasifikace. V kapitole na analýze materiálu vybraných adverbiálních určení a z hlediska problémů, které jejich řešení doprovázejí, navrhujeme způsob jejich zachycení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The traditional classification of adverbials into local, temporal, modal, and causal, such as those used in our syntaxes, and their applications for schooling are suitable for these purposes. For empirical processing under the Functional Generational Description, which also serves, among other things, the tasks of automatic language processing (especially for machine translation, but essentially for the translatological problems associated with the teaching of Czech as a foreign language), a finer classification seems necessary. On the analysis of the material of selected adverbials and in terms of the problems that accompany their solution, we propose a way of capturing them.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku se předkládají argumenty pro začlenění reprezentace informační struktury věty do reprezentace větného významu. Tyto argumenty vycházejí ze skutečnosti, že informační struktura je sémanticky relevantní a je třeba ji brát v úvahu při významové interpretaci negace, presupozice i diskurzní koheze.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Arguments are presented why the information structure of the sentence should be represented in the representation of the meaning. This claim is supported i.a. by the fact that information structure is semantically relevant and is important for the interpretation of negation, presupposition and discourse connectedness.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve stati se rozebírá vliv kybernetiky na vědní obor počítačové lingvistiky, a to jak v historické perspektivě, tak i v současné metologii.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper analyses the impact of cybernetics on the scientific domain of computational linguistics both in the historical perspective as well as in the influence of modern methodology.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vzpomínka na Michaela Alexandra Kirkwooda Hallidaye se zvláštní pozorností věnovanou jeho spojení s pražskými učenci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Obituary of Michael Alexander Kirkwood Halliday with special attention on his connection with the Prague scholars.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nekrolog shrnuje zájmy a přínosy Petra Sgalla k oborům obecné lingvistiky, typologie, počítačové lingvistiky v oblasti vědecké, stejně jako jeho přínosy pedagogické a organizační.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the obituary the contributions of Petr Sgall to the general linguistics, language typology, computational linguistics as well as his participation in the pedagogical and organization activities are enumerated.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Uvádíme několik příkladů dokládajících možnosti využití dat z Pražského závislostního korpusu pro lingvistický výzkum, kde je možné na týchž textech sledovat vztah mezi povrchovou syntaktickou strukturou, hloubkovou syntaktickou strukturou včetně aktuálního členění i celkovou strukturou textu. Mohli jsme tak dojít k následujícím závěrům:(1) Nelze obecně tvrdit, že při charakteristice dichotomie základ – jádro (ať již v jakékoliv terminologii) je možné vycházet z rozdílu mezi starou (danou) a novou informací. (2) Vedle globálního členění věty na základ a jádro je pro popis informační struktury věty vhodné pracovat s členěním lokálním, uplatněným na všech úrovních větné struktury. (3) Pro návaznost vět v textu z hlediska jejich členění na základ a jádro se ukazuje, že je typičtější (pro češtinu, na rozdíl od angličtiny) návaznost základu dané věty na ohnisko věty předcházející (tzv. posloupnost lineární).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present examples how the data from the Prague Dependency Treebank can be used in linguistic research, allowing to study the relationship among surface syntactic structure, deep syntactic structure including the sentence information structure, and the overall text structure. Thus we could have come to the following conclusions: (1) In general, it cannot be claimed that the topic-focus dichotomy (in any terminology) can be based on the difference between the old (given) and the new information. (2) In addition to the global division of a sentence to topic and focus, it is appropriate to work with local segmentation applied at all layers of the sentence structure. (3) For linking the sentences in the text in terms of their division into topic and focus, it seems to be more typical (for Czech, unlike English) linking of the topic of the given sentence to the focus of the preceding sentence (so-called linear sequence).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Data paralelního anotovaného anglicko–českého korpusu byla použita ke zkoumání variability vzájemné pozice LOC a TWHEN v češtině a angličtině a k analýze vztahu mezi informační strukturou a daným pořadím v těchto dvou jazycích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Data from a parallel annotated English–Czech corpus serve for testing the variability of the mutual position of LOC and TWHEN in Czech and English and for the analysis of the relation between information structure and the given order in the two languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Na základě pražské teorie Topic/Focus Articulation a s využitím paralelního anglicko–českého korpusu jsme se zaměřili na dvě otázky: (i) Do jaké míry se shoduje Focus proper v angličtině a v češtině, (ii) pokud se Focus proper liší, platí alespoň, že Focus proper v angličtině je členem (globálního) Focusu v české větě?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Based on the Praguian theory of Topic/Focus Articulation and using a parallel English–Czech corpus, we have followed two research questions: (i) How far does the assignment of Focus proper agree in English and in Czech, (ii) If the assignment of Focus proper differs, is the Focus-proper element in English at least a member of the (global) Focus of the Czech sentence?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CLARIN-DSpace je rozšířená verze softwaru DSpace. Je uzpůsoben potřebám úložiště jazykových dat, ale byl využit i pro jiné účely a není omezen na konkrétní vědecký obor.

Poskytované prostředí umožňuje bez námahy integraci se službami třetích stran: automaticky doplňovat granty z OpenAIRE a výsledky naopak do OpenAIRE, Clarivate Data Citation Index a jinam.

Nejenže je prostředí pro uložení dat nakonfigurováno tak, aby vyžadovalo potřebná metadata, ale také poskytuje návod pro výběr vhodné licence v rámci otevřeného přístupu prostřednictvím integrace projektu Public license selector a komplexního frameworku pro licencování vč. podepisování licencí a kontrolz přístupu.

Pro dodatečnou ochranu dat lze systém snadno nakonfigurovat tak, aby automaticky zálohoval nové záznamy prostřednictvím služby EUDAT B2SAFE.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CLARIN-DSpace is an enhanced fork of the DSpace repository software. The out of the box it is tailored to suit the needs of a language data repository, but has been also deployed for contemporary history or film archives, and is not limited to specific scientific field.

The provided workflows make integrations with third party services auto-suggest grants from OpenAIRE, reporting to CLARIN Virtual Language Observatory, Clarivate Data Citation Index, or OpenAIRE, effortless.

Not only is the submission workflow preconfigured to require necessary metadata, but also provides a guide for Open Access licensing by integrating the Public License Selector. And because not all data can be open, there is also a support for submitters to assign custom licenses to their datasets, for users to sign the licenses, and for repository managers to manage and keep track of all of it.

When the software is configured to connect with Piwik (Matomo) analytics platform, the submitters of data are provided with concise periodic reports about popularity of their submissions.

To offer additional layer of protection, the system can be easily configured to automatically backup submissions via EUDAT B2SAFE service.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme softwarové řešení a zkušenosti s provozem repozitáře pro jazyková data a nástroje pro zpracování přirozených jazyků - LINDAT/CLARIN. Představíme unikátní podporu licencování s důrazem na Open Access a to, jak podporujeme všechny 4 klíčové principy FAIR. Ukážeme vytváření záznamů včetně volby licence, jejich schvalování a publikaci editory, i prostředí pro administraci repozitáře včetně definice licencí, jejich podepisování a kontroly přístupu. Ukážeme také integrace repozitáře s dalšími službami a provozní statistiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a software solution for and experience in running a digital repository for language data and natural language processing tools - LINDAT/CLARIN. We will present unique support for licensing with an emphasis on Open Access, and how we support all 4 key FAIR principles. We will show the submission workflow including license choice, approval and publishing or submissions by editors, as well as the repository administration environment including license definition, signing and access control. We will also present repository integration with other services, and statistics of operation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Overview of recent advances in cross-lingual information retrieval in the patient-centered web search.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Přehled posledních výsledků ve výkzumu multilingválního vyhledávání v oblasti zdraví.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přehledná přednáška na téma vyhledávání informací napříč jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An overview talk on the topic of cross-lingual information retrieval.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Podrobný morfologický popis slovních forem v jakémkoli jazyce je nezbytnou podmínkou úspěšného automatického zpracování jazykových údajů. Prezentujeme nový popis morfologických kategorií, zejména podkategorizaci slovních druhů v češtině v rámci projektu NovaMorf. NovaMorf se zaměřuje na popis morfologických vlastností českých slov kompaktnějším a konzistentnějším způsobem a s vyšší explikační silou než dosud používané přístupy. Cílem projektu je také sjednocení různých přístupů k morfologické anotaci češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A detailed morphological description of word forms in any language is a necessary condition for a successful automatic processing of linguistic data. The paper focuses on a new description of morphological categories, mainly on the subcategorization of parts of speech in Czech within the NovaMorf project. NovaMorf focuses on the description of morphological properties of Czech wordforms in a more compact and consistent way and with a higher explicative power than approaches used so far. It also aims at the unification of diverse approaches to morphological annotation of Czech. NovaMorf approach will be reflected in a new morphological dictionary to be exploited for a new automatic morphological analysis (and disambiguation) of corpora of contemporary Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato technická zpráva je anotačním manuálem k projektu zaměřenému na kontextovou synonymii a valenci sloves v dvojjazyčném
prostředí. Analýza sémantické „ekvivalence“  slovesných významů a jejich valenčního chování v paralelních česko-anglických jazykových zdrojích je jádrem probíhajícího výzkumu. Použití překladového kontextu podporuje více jazykově nezávislé určení vlastností slovesných tříd synonym a vede ke generalizace napříč jazyky. Hlavní přínos projektu
je hlubší vhled do tématu významu slovesa v
kontextu založeném na teorii funkčního generačního popisu, čímž se tento popis rozšiřuje směrem k popisu synonymie sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This technical report is the guideline connected with the project focuses on contextually-based synonymy and valency of verbs in a bilingual
setting. The analysis of semantic ‘equivalence’ (synonymy or near synonymy) of verb senses and their valency behavior in parallel Czech-English language resources is the core of the
proposed research. Using the translational context supports more language-independent
specification of properties of verb sense classes of synonyms and leads towards
generalization across languages. An initial sample bilingual verb lexicon of classes
representing synonym or near-synonym pairs of verbs (verb senses) based on richly annotated
corpora and existing lexical resources will be created. The main contribution of the project
will be a deeper insight from the bilingual perspective into the topic of verb meaning in
context based on the Functional Generative Description theory, thus extending it towards
the appropriate description of contextually-based verb synonymy.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce se zaměřuje na sémantické role, důležitou součást studia lexikální sémantiky, neboť jsou zachyceny jako součást dvojjazyčného (česko-anglického) slovníku slovesných synonym (CzEngClass). Tento slovník navazuje na
existující valenční lexikony zahrnuté v rámci anotace různých pražských závislostních korpusů. Současná analýza sémantických rolí
je nastíněna z pohledu FGP a doložena příklady z korpusu, z Pražského česko-anglického závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper focuses on Semantic Roles, an important component of
studies in lexical semantics, as they are captured as part of a bilingual (Czech-
English) synonym lexicon called CzEngClass. This lexicon builds upon the
existing valency lexicons included within the framework of the annotation of
the various Prague Dependency Treebanks. The present analysis of Semantic
Roles is being approached from the Functional Generative Description point of
view and supported by the textual evidence taken specifically from the Prague
Czech-English Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme probíhající projekt obohacující anotaci paralelního závislostního korpusu, konkrétně Pražského česko-anglického Treebanku, sémantickou anotací používající dvojjazyčný lexikon slovesných synonymních tříd, CzEngClass. Tento lexikon propojuje slovesné predikáty v korpusu s různými externími lexikony. V tomto článku popisujeme první milník dlouhodobého projektu; zatím je k dispozici 100 tříd CzEngClass, které obsahují asi 1800 různých sloves pro češtinu i angličtinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an ongoing project of enriching an annotation of a parallel dependency treebank, namely the Prague Czech-English Dependency Treebank, with verb-centered semantic annotation using a bilingual synonym verb class lexicon, CzEngClass. This lexicon, in turn, links the predicate occurrences in the corpus to various external lexicons. This paper describes a first milestone of a long-term project; so far, approx. 100 CzEngClass classes, containing about 1800 different verbs each for both Czech and English, are available for such annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>SynSemClass je slovník česko-anglických slovesných synonym. Základními hesly ve slovníku jsou dvojjazyčné česko-anglické slovesné synonymní třídy, v nichž jsou obsažena synonymní česká a anglická slovesa (členy třídy), reprezentovaná jako valenční rámce (tj. slovesné významy), jejichž pojetí vychází z teorie Funkčně generativního popisu jazyka. Sémantická ekvivalence jednotlivých členů třídy byla stanovena na základě jejich kontextového valenčního chování usouvztažněného k situačně-kognitivnímu obsahu (sémantickým rolím). Synonymické vztahy jsou ve slovníku chápány volně, jednotlivé členy třídy jsou ve vztahu nikoli striktní (úplné) synonymie, ale ve vztahu významové podobnosti, tj. částečné synonymie. Předností slovníku je použití paralelního česko-anglického korpusu, PCEDT(https://catalog.ldc.upenn.edu/LDC2012T08 ) jako hlavního zdroje jazykových dat, které umožňuje tzv. "bottom-up" přístup, tj. od praxe k teorii.  Předností slovníku je rovněž propojení všech členů jednotlivých synonymních tříd s dalšími lexikálními zdroji, a to s hesly valenčních slovníků (PDT-Vallex - http://hdl.handle.net/11858/00-097C-0000-0023-4338-F, EngVallex- http://hdl.handle.net/11858/00-097C-0000-0023-4337-2), CzEngVallex - http://hdl.handle.net/11234/1-1512 a Vallex -  http://ufal.mff.cuni.cz/vallex/3.5/), a s hesly sémantických  databází (FrameNet - https://framenet.icsi.berkeley.edu/fndrupal/, VerbNet  - http://verbs.colorado.edu/verbnet/index.html, PropBank - http://verbs.colorado.edu/%7Empalmer/projects/ace.html, Ontonotes - http://verbs.colorado.edu/html_groupings/ a Wordnet - https://wordnet.princeton.edu/).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The SynSemClass synonym verb lexicon is a result of a project investigating semantic ‘equivalence’ of verb senses and their valency behavior in parallel Czech-English language resources, i.e., relating verb meanings with respect to contextually-based verb synonymy. The lexicon entries are linked to PDT-Vallex (http://hdl.handle.net/11858/00-097C-0000-0023-4338-F), EngVallex (http://hdl.handle.net/11858/00-097C-0000-0023-4337-2), CzEngVallex (http://hdl.handle.net/11234/1-1512), FrameNet (https://framenet.icsi.berkeley.edu/fndrupal/), VerbNet (http://verbs.colorado.edu/verbnet/index.html), PropBank (http://verbs.colorado.edu/%7Empalmer/projects/ace.html), Ontonotes (http://verbs.colorado.edu/html_groupings/), and English Wordnet (https://wordnet.princeton.edu/). Part of the dataset are files reflecting interannotator agreement.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se věnuje popisu nesystémového valenčního chování českých deverbativních substantiv, jak se jeví na základě automatického srovnání valenčních rámců vzájemně propojených substantivních a slovesných lexikálních jednotek ve valenčních slovnících NomVallex a VALLEX. Nesystémové valenční chování se v NomVallexu nejčastěji projevuje nesystémovými formami aktantů, zatímco změny v počtu a typu aktantů jsou co do počtu případů zanedbatelné. Nesystémové formy významně přispívají k všeobecnému nárůstu počtu forem ve valenčních rámcích substantiv, ve srovnání s počtem forem ve valečních rámcích jejich základových sloves. Nesystémové formy jsou častější ve valenčních rámcích neproduktivně tvořených substantiv než ve valenčních rámcích substantiv tvořených produktivně.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In order to describe non-systemic valency behavior of Czech deverbal nouns, we present results of an automatic comparison of valency frames of interlinked noun and verbal lexical units included in valency lexicons NomVallex
and VALLEX. We show that the non-systemic valency behavior of the nouns is mostly manifested by non-systemic forms of their actants, while changes in the number or type of adnominal actants are negligible as for their frequency. Non-systemic forms considerably contribute to a general increase in the number of forms in valency frames of nouns compared to the number of forms in valency frames of their base verbs. The non-systemic forms are more frequent in valency frames of non-productively derived nouns than in valency frames of productively derived ones.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zkoumáme anotaci reflexiv v závislostních korpusech Universal Dependencies (UD) (Nevre et al., 2018), se silnějším zaměřením na slovanské jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We explore the annotation of reflexives in Universal Dependencies (UD) treebanks (Nivre et al., 2018), with a stronger focus on Slavic languages. We will show a number of examples where reflexive markers (in particular reflexive clitics) are used as true (argumental) reflexives, reciprocal arguments, passive/middle/impersonal markers etc.; we will confront the UD guidelines for the individual phenomena with the current state of the data. Improvements will be proposed, sometimes of the guidelines, but mostly of the annotation.

This is an extended version of our talk at TLT 2018 in Oslo.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Mnohé lingvistické teorie a anotační projekty zahrnují hloubkově-syntaktickou a/nebo sémantickou rovinu. I když mnohé z nich byly aplikovány na více než jeden jazyk, žádný syntakticko-sémantický projekt se ani zdaleka neblíží množství jazyků pokrytých projektem Universal Dependencies (UD).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Many linguistic theories and annotation frameworks contain a deep-syntactic and/or semantic layer. While many of these frameworks have been applied to more than one language, none of them is anywhere near the number of languages that are covered in Universal Dependencies (UD). I will present a prototype of Deep Universal Dependencies, a two-speed concept where minimal deep annotation can be derived automatically from surface UD trees, while richer annotation can be added for datasets where appropriate resources are available. The talk is based on joint work with Kira Droganova.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je jedenácté vydání treebanků UD, verze 2.5.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the eleventh release of UD Treebanks, Version 2.5.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek je příspěvkem k diskusi o hranici mezi kognitivním obsahem a jazykovým významem. V příspěvku se na příkladu určování repertoáru adverbiálních významů probírají praktické důsledky rozlišování mezi obsahem a významem. Je popsána metodologie práce při určování adverbiálních významů, zejména princip zaměnitelnosti synonym. Teroretickým východiskem je Funkční generativní popis.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The necessity to distinguish between cognitive content and linguistic meaning arose in European structural linguistics (Saussere, 1916) and was further elaborated in the Prague Linguistic Circle (Mathesius, 1942; Dokulil – Daneš, 1958; Daneš, 1974). In the contribution, we describe the practical aspects of applying the principle of distinguishing meaning and content to the task of delimitation of adverbial meanings, expressed by prepositional groups.  We present methodology and main principles we work with in completing the set of meanings of adverbials. We describe how we use the principle of substitutability of synonyms. All the examples in the contribution relate to spatial adverbials but the principles apply to adverbials in general.  Our theoretical framework is Functional Generative Description (Sgall et al., 1986).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Data Pražského česko-anglického závislostního korpusu (Prague Czech-English Dependency Treebank, PCEDT) posloužila jako materiál pro srovnávací studii věnovanou vydělování adverbiálních významů místního určení "v rámci daného místa". České předložkové skupiny obsahující předložky v, na a u vyjadřující výše uvedené místní určení byly srovnány s jejich anglickými ekvivalenty a byly dále rozděleny do tří sémantických podskupin, konkrétně "uvnitř", "na povrchu" a "v daném místě". Naše analýza potvrdila, že přestože oba jazyky strukturují realitu jiným způsobem, lze vypozorovat určité tendence ve vztahu forem a jejich funkcí, které vedou k jemnější klasifikaci daných adverbiálních významů. Jedná se o studii popisující aktuálně probíhající výzkum.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The data of the Prague Czech-English Dependency Treebank (a member of the family of Prague Dependency Treebanks) have served as a basis for the comparative study of delimiting adverbial meanings of the local relation “within the given place”. The Czech prepositional groups containing the prepositions v, na, and u with the above meaning are compared with their English equivalents, using a more subtle differentiation into three semantic subgroups of "inside", "on the surface" and "at the given place". Our analysis confirms that though every language structures the reality in a different way, certain tendencies may be observed in the relation of the forms and their functions that eventually result in a more detailed classification. The contribution presents results of an ongoing work.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Existuje několik morfologických slovníků pro češtinu. Liší se pouze řešením komplikovaných morfologických rysů. Byly učiněny různé pokusy o sjednocení jejich přístupů, ale jen některé z nich byly realizovány. Dokument se zabývá několika takovými rysy a porovnává jejich řešení přijatá ve dvou různých projektech, konkrétně příprava nového vydání PDT (Prague Dependency Treebank) a NovaMorf. Charakteristickými rysy prezentovanými v tomto dokumentu jsou: agregáty (slovní formy bez jasné části řeči, např. užs, oč, naň) a varianty - flektivní (více slovní formy pro konkrétní kombinaci lemmy a morfologické značky) i globální (zejména ortografické varianty vyjádřené ve všech slovních formách paradigmatu).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>There exist several morphological dictionaries
for Czech. They differ only in solutions of complicated morphological features. Various attempts have been made to unify their approaches, but only some of them were implemented. The paper deals with several such features and compares their solutions taken in two different projects,
namely preparation of the new edition of PDT (Prague Dependency Treebank) and NovaMorf. The features presented in this paper are: aggregates (the word-forms without a clear part of speech, e.g. užs, oč, naň), and variants – inflectional (more wordforms for a particular combination of lemma and morphological tag) as well as global ones (mainly orthographic variants expressed in all wordforms of a paradigm).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku jsou popsány systematické změny, které jsou realizovány v českém morfologickém slovníku v souvislosti s anotací nových dat v Pražských závislostních korpusech. Přináší řešení několika komplikovaných morfologických jevů, které se objevují v českých textech. Představeny jsou dva nové slovní druhy: cizí slovo a segment. Popisují se pravidla pro reprezentaci variantních a homonymních tvarů a slov (lemmat), pravidla pro zachycení zkratek a tzv. agregátů (např. naň). Změny ve slovníku jsou prováděny za účelem vyšší konzistence mezi daty a slovníkem a  v slovníku samotném.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe systematic changes that have been made to the Czech morphological dictionary related to annotating new data within the project of Prague Dependency Treebank (PDT). We bring new solutions to several complicated morphological features that occur in Czech texts. We introduced two new parts of speech, namely foreign word and segment. We adopted new principles for morphological analysis of global and inflectional variants, homonymous lemmas, abbreviations and aggregates. The changes were initiated by the need of consistency between the data and the dictionary and of the dictionary itself.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Připomenutí od Googlu ohledně ochrany soukromíPřečíst nyníPřečtu si je později

4027/5000
Metoda stylometrie nejčastějšími slovy neumožňuje přímé srovnání původních textů a jejich překladů, tj. Napříč jazyky. Pokusili jsme se odstranit tuto jazykovou bariéru pro přímé stylometrické srovnání literárního překladu tím, že jsme textové tokeny nahradili křížovými lingválními morfologickými značkami poskytovanými v rámci programu Universal Dependencies (http://universaldependencies.org). Připravili jsme sbírku přibližně 50 textů v češtině a němčině: obsahovala české originály a jejich německé překlady; Německé originály a jejich české překlady; a díla původně psaná v angličtině, francouzštině nebo ruštině, ale pouze v českých a německých překladech. Pomocí analyzátoru UD-Pipe jsme převedli texty na sekvence značek. Výstupem byla tabulka CONLL-u (jeden token na řádek, různé typy jazykových značek ve sloupcích: token, lemma, hrubý tag části řeči, jemnozrnné morfologické znaky, závislost na řídícím uzlu, ID řídící uzel). Hrubé tagy pro část řeči (POS) se nazývají UPOS a jemnozrnné morfologické znaky se nazývají FEATS. Závislostní vztahy se nazývají DEPREL. Zatímco UPOS jsou skutečně univerzální, soupis příslušných funkcí je závislý na jazyce. Například jazyk, který nepoužívá definitivitu substantiva, nepoužívá funkci `Definitivní 's hodnotami` Def` a `Indef`. Protože se české a německé soupisy FEATS liší, odstranili jsme atributy funkcí a hodnoty specifické pro oba jazyky, přičemž zachovali pouze funkce a hodnoty sdílené oběma jazyky.
V dalším kroku jsme původní lemma nahradili `` FLEMMA '', tj. "Falešné lemma". FLEMMA je mezikulturní lemma pro německo-český pár. Navrhli jsme to jako číselné ID. Získali jsme glosář FLEMMAS, který čerpal z česko-německého glosáře (Škrabal a Vavřín 2017), který byl automaticky generován z vícejazyčného paralelního korpusu InterCorp (Rosen 2016). Počítali jsme různé kombinace značek získaných pro čtyři úrovně parsingu podle obvyklého postupu Delta (Burrows 2002). Každý značkovací řetězec byl považován za jediný prvek (protějšek typů slov v tradiční stylometrické analýze podle nejčastějších slov) a jejich frekvence byly spočteny v textech v korpusu a porovnány v textových párech za účelem vytvoření matice měr vzdálenosti; v této studii byly vzdálenosti stanoveny pomocí modifikované Cosine Delta (Smith and Aldridge 2011), která je nyní považována za nejspolehlivější verzi (Evert et al. 2017). Funkce classify () v balíčku stylů (Eder et al. 2016) pro R (R Core Team 2016) byla použita k pokusu o posouzení úspěšnosti přiřazení autorství, když její referenční sada obsahovala texty v jednom jazyce a testovací sada obsahovala texty v jiný. Úspěch přiřazení byl počítán vždy, když byla hodnota Delta pro dvojici překladů stejného textu nejnižší. Nejúspěšnější kombinací pro přiřazování autorství byl FLEMMAS + UPOS (95,6%), zatímco samotný FLEMMAS dosáhl pouze (3,7%) a stejně tak UPOS sám. To ukazuje, že i velmi hrubý překlad ze slova na slovo (polysemy zanedbaný), spolu s hrubou částí řeči, pomáhá obejít jazykovou bariéru. Dalším zajímavým nálezem je zdánlivě skromná 20,3% úspěšnost pro přiřazení kombinací UPOS + FEATS + DEPREL. To je konec konců mnohem víc než jen hod pro mince pro takový počet textů. Ještě důležitější je, že hádání bylo pro stejné dvojice textů trvale úspěšné, což naznačuje, že tyto konkrétní dvojice by bylo snadnější uhodnout. To by zase mohlo naznačovat, že překlady v těchto případech aplikovaly strategie vedoucí k gramatickým a syntaktickým strukturám podobným původnímu, jevu často pozorovanému v překladu slovo za slovem, nebo obecněji v překladech zaměřených na tzv. nazývána „formální rovnocennost“ (Nida 1964). Tato práce byla financována z LTC18020.
Odeslat zpětnou vazbu
Historie
Uloženo
Komunita</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The method of stylometry by most frequent words does not allow direct comparison of original texts and their translations, i.e. across languages. We have tried to remove this language barrier for direct stylometric comparison of literary translation by replacing the text  tokens  with  the  cross-lingual  morphological  tags  provided  by  the  Universal Dependencies scheme (http://universaldependencies.org). We have prepared a collection of approximately 50 texts in Czech and German: it contained Czech originals and their German translations; German originals and their Czech translations; and works originally written  in  English,  French  or  Russian,  but  only  in  Czech  and  German  translations.  We converted  the  texts  into sequences  of  tags  using  the  UD-Pipe  parser.  The  output  was  a CONLL-u  table  (one  token  per  line,  different  types  of  linguistic  markup  in  columns: token,   lemma,   coarse   part-of-speech   tag,   fine-grained   morphological   features, dependency relation to the governing node, ID of the governing node).The   coarse   part-of-speech   (POS)   tags   are   called   UPOS   and   the   fine-grained morphological features are called FEATS. The dependency relations are called DEPREL. While  the  UPOS  are  truly  universal,  the  inventory  of  relevant  FEATS  is  language-dependent. For instance, a language that does not use noun definiteness does not use the `Definite` feature with its values `Def` and `Indef`. Since the FEATS inventories of Czech and  German  differ,  we  have  stripped  feature  attributes  and  values  specific  to  either language, keeping only features and values shared by both languages.
As a next step, we replaced the original lemmas with a ``FLEMMA'', i.e. a "fake lemma". FLEMMA is a cross-lingual lemma for the German-Czech pair. We designed it as a numerical ID. Weobtained a glossary of FLEMMAS drawing on a Czech-German glossary (Škrabal and Vavřín 2017) that was  automatically  generated  from the multilingal parallel  corpus  InterCorp (Rosen 2016). We counted various  combinations of tags obtained for the four parsinglevels according the the usual Delta procedure (Burrows 2002). Each tagging string was treated as a single element (the counterpart of word-types in traditional stylometric analysis by most frequent words), and their frequencies were counted in the texts in the corpus and compared in text pairs to produce a matrix of distance measures; in this study, the distances were established by means of the modified Cosine Delta (Smith and Aldridge 2011), which is now seen as the most reliable version (Evert et al. 2017). The function classify() in the stylo package (Eder et al. 2016) for R (R Core Team 2016) was used to try to assess authorship attribution success when its reference set contained texts in one language and the test set contained texts in the other. Attribution success was counted whenever the Delta value for the pair of the translations of the same text was lowest.The most successful combination for authorship attribution was FLEMMAS + UPOS (95.6%), while FLEMMAS alone achieved only (3.7%), and so did UPOS alone. This shows that even a very crude word-to-word translation (polysemy neglected), along with the coarse part of speech, helps bypass the language barrier. Another interesting finding is the seemingly modest 20.3% success rate for attribution by the combination UPOS + FEATS + DEPREL. This is, after all, much more than a coin toss for such a number of texts. More importantly, guessing was consistently successful for the same pairs of texts, suggesting that these particular pairs might be easier to guess. This, in turn, might indicate that the translations in these case applied strategies resulting in grammatical and syntactic structures somehow similar to the original, a phenomenon oftenobserved in word-for-word translation, or, more generally, in translations aiming for the so-called “formal equivalence” (Nida 1964).This work was funded by LTC18020.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vyhodnocujeme výběr aktuálně dostupných značkovačů na beletristických textech z 19. století. Texty patří do sbírky ELTeC vytvořené a udržované týmem WG1 projektu COST CA16204 „Vzdálené čtení pro evropskou literární historii“. Jedná se o mnohojazyčnou sbírku beletrie 19. století v následujících jazycích: čeština, angličtina, francouzština, němčina, maďarština, italština, norština (obě odrůdy), portugalština, srbština a slovinština (s novými jazyky). Každý jazyk je zastoupen nejméně 20 romány ve formátu TEI.
Hodnotili jsme značkovače pro následující jazyky:
Česky, anglicky, francouzsky, německy, maďarsky, norsky-nynorsk, portugalsky a slovinsky. Všechny jsme označili štítkem UD-Pipe. Kromě toho byla maďarština označena také e-magyar. UD-Pipe jsme vybrali kvůli přitažlivé křížové jazykové značce a jednotné tvorbě modelů pro všechny jazyky. Již dříve bylo známo, že maďarský model je velmi malý a že maďarské značkování je špatné. Proto jsme také vyhodnotili e-magyar, nejlepší značkovač v současnosti dostupný pro maďarštinu
Schéma anotací UD-Pipe
Značkovač a analyzátor UD-Pipe čerpá z Universal Dependencies (universaldependencies.org), dále od UD. UD je schéma vícejazyčných anotací pro morfologii a syntaxi. V současné době má více než 70 jazyků vlastní UD stromovou banku, na které lze trénovat syntaktický analyzátor. Kategorie, které anotují UD-Pipe, jsou: lemma, hrubá část řeči (např. VERB), univerzální rysy (např. Množné číslo, minulé napětí) a vztahy syntaktické závislosti (např. Předmět, predikát).
Postup
Vybrali jsme náhodný vzorek pro ruční opravu značkování v každém jazyce. Každý vzorek obsahoval přibližně 5 000 žetonů v celých větách. Věty byly extrahovány z ELTeC napříč všemi dokumenty v každém jazyce a označeny ve formátu CONLL-u: jeden token na řádek, sloupce: token, lemma, univerzální POS, univerzální funkce. Anotace provedla anotaci v tabulce ve čtyřech dalších sloupcích pro tokenizaci, lemma, POS a funkce. V prvních třech sloupcích měly označovat odpovídající typ chyby s písmenem „F“. Ve čtvrtém sloupci měli anotátoři zadat počet nesprávně rozpoznaných jazykových prvků.
Proto anotace zachycuje čtyři různé typy chyb pro každý token. Počet chyb funkcí měří pouze přesnost, nikoli stažení, značkovače. Je to proto, že morfologická anotace je poněkud složitá a bylo by příliš náročné na čas a zdroje na to, aby bylo možné vyškolit plně kompetentního morfologického anotátora pro každý jazyk, který by byl schopen korigovat výstup automatického značkovače na nový zlatý standard.
Pro každý jazyk jsme vytvořili popisné a explorativní statistiky a vizualizace, abychom porovnali výkon UD-Pipe mezi jazyky a analyzovali chyby. Jako druhý krok jsme porovnali slovní zásobu jednotlivých vzorků se slovní zásobou referenčních stromů příslušných jazyků, abychom zjistili, do jaké míry se doména beletrie 19. století liší od domény referenční stromové struktury. Ručně jsme rozdělili tokeny specifické pro vzorky z 19. století do následujících kategorií: Cizí slovo / těžký dialekt, archaické hláskování / morfologie, morfologická dvojznačnost, archaické tokenizace, typo, vlastní jméno. Když anotátor nechal žádnou buňku prázdnou, když se žádný ze štítků neshodoval. Štítky kategorií lze také úmyslně kombinovat.
Na základě obou anotací jsme schopni pro každý jazyk říct, kde má tagger největší problémy (POS, lemmatizace? Tokenizace? Funkce?) As jakými slovy (kolik archaického pravopisu souvisí s výkonem lemmatizace?). Schopnost značkovače „hádat“ neznámá slova se v každém jazyce liší. Díky této dvojité anotaci jsme schopni zahájit spolupráci s vývojáři UD-Pipe na možné adaptaci domény pro vybrané jazyky a vytvářet spolehlivé modely pro označování fikce 19. století.
Tato práce byla financována z LTC18020.
Odeslat zpětnou vazbu</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We  evaluate  a  selection  of  currently  available  taggers  on  fiction  texts  from  the  19th century. The texts belong to the ELTeC collection created and maintained by the WG1 team of the COST project CA16204 "Distant Reading for European Literary History". It is  a  multilingual  collection  of  19th-century  fiction  in  the  following  languages:  Czech, English,  French,  German,  Hungarian,  Italian,  Norwegian  (both  varieties),  Portuguese, Serbian, and Slovene (with new languages being added). Each language is represented by at least 20 novels in the TEI format.We have evaluated the taggers for the following languages:Czech,  English,  French,  We evaluate a selection of currently available taggers on fiction texts from the 19th century. The texts belong to the ELTeC collection created and maintained by the WG1 team of the COST project CA16204 "Distant Reading for European Literary History". It is a multilingual collection of 19th-century fiction in the following languages: Czech, English, French, German, Hungarian, Italian, Norwegian (both varieties), Portuguese, Serbian, and Slovene (with new languages being added). Each language is represented by at least 20 novels in the TEI format.
We have evaluated the taggers for the following languages:
Czech, English, French, German, Hungarian, Norwegian-Nynorsk, Portuguese, and Slovene. We have tagged them all with the UD-Pipe tagger. On top of that, Hungarian was also tagged with e-magyar. We have selected UD-Pipe because of the appealing cross-lingual markup and uniform model building for all languages. It was known before that the Hungarian model is very small and that the Hungarian tagging is poor. Therefore we have also evaluated e-magyar, the best tagger currently available for Hungarian
The annotation scheme of UD-Pipe
The UD-Pipe tagger and parser draws on Universal Dependencies (universaldependencies.org), henceforth UD. UD is a cross-lingual annotation scheme for morphology and syntax. At the moment, more than 70 languages have their own UD treebank to train a parser on. The categories that UD-Pipe annotates are: lemma, coarse part of speech (e.g. VERB), universal features (e.g. Plural, Past Tense), and syntactic dependency relations (e.g. Subject, Predicate).
The procedure
We have selected a random sample for a manual tagging correction in each language. Each sample comprised approximately 5,000 tokens in entire sentences. The sentences had been extracted from ELTeC across all documents in each language and tagged in the CONLL-u format: one token per line, columns: token, lemma, universal POS, universal features. The annotators performed the annotation in a spreadsheet, in four additional columns for tokenization, lemma, POS, and features, respectively. In the first three columns, they were to indicate the corresponding error type(s) with an "F". In the fourth column, the annotators were to type the number of incorrectly recognized linguistic features.
Hence, the annotation captures four different error types for each token. The number of feature errors measures only the precision, not the recall, of the tagger. This is because the morphological annotation is rather complex and it would have been too time- and resource-consuming to train a fully competent morphological annotator for each language, who would have been able to correct the automatical tagger output to a new gold standard.
We have produced descriptive and explorative statistics and visualizations for each language to compare the performance of UD-Pipe among languages and to analyze the errors. As a second step, we have compared the vocabulary of the individual samples to the vocabulary of the referential treebanks of the respective languages to find out how much the domain of the 19th-century fiction differs from the domain of the referential treebank. We have manually classified the tokens specific to the 19-century samples into the following categories: Foreign word/heavy dialect, archaic spelling/morphology, morphological ambiguity, archaic tokenization, typo, proper noun. The annotator was allowed to leave a cell blank when none of the labels matched. The category labels could also be deliberately combined.
Based on both annotations, we are able to tell, for each language, where the tagger has most problems (POS, lemmatization? Tokenization? Features?) and with which words (how much is archaic spelling associated with lemmatization performance?). The ability of the tagger to "guess" unknown words differs in each language. With this double annotation, we are able to start a collaboration with UD-Pipe developers on a possible domain adaptation for selected languages and build reliable models to tag 19-th century fiction.
This work was funded by LTC18020.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci prezentujeme analýzu hypotetických preferencí žánru specifických pro žánry u spontánních příběhů v korpusu 286 přepsaných záznamů mladších školních dětí (ve věku 6–11). Vycházíme z Nicolopoulouvé pojetí vyprávění jako symbolické aktivity s tendencemi danými pohlavím: dívky dávají přednost „rodinnému“ žánru a chlapci preferují „hrdinně agonistický“. Operacionalizovali jsme žánrovou klasifikaci podle Nikolopoulou do 10 tematických vyprávěcích rysů a zaznamenali jsme jejich příslušnou přítomnost nebo nepřítomnost v každém vyprávění. Kombinace statistických metod na náš vzorek odhalila, že narativní preference spojené s pohlavím jsou tak slabé, že nemají žádné důsledky, např. pro školní praxi. Nejvýznamnější výsledek ve skutečnosti dokonce odporuje apriornímu předpokladu pohlavních narativních preferencí: bez ohledu na věk a pohlaví, děti projevují silnou preferenci „vzájemné pomoci a spolupráce“.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present an analysis of hypothesized sex-specific genre preferences in spontaneous narratives in a corpus of 286 transcribed recordings of younger school children (aged 6–11). We draw upon Nicolopoulou’s conception of narratives as a symbolic activity with sex-specific tendencies: girls prefer the “family” genre and boys prefer the “heroic-agonistic” one. We operationalized Nikolopoulou’s genre classification into 10 thematic narrative features and an-notated their respective presence or absence in each narrative. The application of a combination of statistical methods to our sample revealed that the sex-correlated narrative preferences are so weak that they have no implications, e.g. for school practice. The most prominent result, in fact, even coun-ters the a priori assumption of sex-based narrative preferences: no matter what sex or age, children show a strong preference for “mutual aid and cooperation”.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá tvořením vidového protějšku u přejatých slovesných neologismů v češtině. To, zda je vidový protějšek tvořen sufixací či prefixací, závisí - podle mé hypotézy diskutované v příspěvku - na tom, zda je báze neologismu v češtině interpretovaná jako slovesná nebo nominální.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper focuses on how loan verbal neologisms form their aspectual counterpart in Czech, either by changing the thematic suffix in a simplex verb, or by deriving a prefixed verb. The main hypothesis is that the formation of aspectual pairs is determined by whether the neologism is interpreted as having either a verbal base, or a noun base.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje poloautomatickou proceduru, při níž byly vztahy mezi základovými slovy a jejich deriváty v lexikální síti DeriNet označkovány sémantickými značkami. V prezentovaném pilotním experimentu, který byl omezen na pět sémantických značek (diminutiva, posesiva, ženské protějšky maskulin, iterativa a vidové významy), byla data značkována metodami strojového učení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes a semi-automatic procedure introducing semantic labels into the DeriNet network, which is a large, freely available resource modelling derivational relations in the lexicon of Czech. The data
were assigned labels corresponding to five semantic categories (diminutives, possessives, female nouns, iteratives, and aspectual meanings) by a machine learning model, which achieved excellent results in terms of both precision and recall.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je desáté vydání treebanků UD, verze 2.4.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the tenth release of UD Treebanks, Version 2.4.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme návrh projektu pro automatické generování scénářů divadelních her. Navrhujeme hierarchický postup generování založený na expanzi textu (inverze sumarizace textu), která iterativně rozgenerovává název hry, dokud není vygenerován celý scénář. Diskutujeme výzvy projektu a navrhujeme pro ně řešení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a proposal for a project for automatic generation of theatre play scripts. We suggest a hierarchical generation scenario based on text expansion (an inverse of text summarization), iteratively expanding the title of the play until the whole script is generated. We review the challenges of the project and solutions that we envision for them.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Podobně jako v jiných oborech, zpracování přirozeného jazyka prošlo nedávno revolucí zavedením hlubokého učení. V mé přednášce se zaměřím na dvě specifické vlastnosti přírozeného textu jako vstupu pro systém strojového učení a na současné způsoby, jakými jsou v hlubokých neuronových sítích řešeny:
- Reprezentace masivně vícehodnotových diskrétních dat (slov) kontinuálními nízkodimenzionálními vektory (slovní embedinky).
- Zpracování vstupních sekvencí s proměnnou délkou pomocí vztahů na dlouhou vzdálenost mezi elementy (větami) neurálními jednotkami pevné velikosti (mechanismy pozornosti).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Similarly to other fields, Natural language processing has recently been
revolutionized by the introduction of Deep learning. In my talk, I will
focus on two specific properties of natural text as input to a machine
learning system, and the current ways in which these are addressed in
Deep neural networks:
- Representing massively multi-valued discrete data (words) by
continuous low-dimensional vectors (word embeddings).
- Handling variable-length input sequences with long-distance relations
between elements (sentences) by fixed-sized neural units (attention
mechanisms).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CoNLL 2018 Shared Task, tým CUNI-x-ling -- zdrojové kódy pro soutěž ve vícejazyčném syntaktickém parsingu</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CoNLL 2018 Shared Task Team CUNI-x-ling -- source codes for a competition in multilingual parsing</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Používáme anglický model BERT a zkoumáme, jak vypuštění jednoho slova ve větě mění reprezentace jiných slov. Naše hypotéza je taková, že odstranění redukovatelného slova (např. přídavného jména) neovlivní reprezentaci jiných slov natolik, jako odstranění např. hlavního slovesa, které činí větu pro jazykový model negramatickou a „velmi překvapivou“. Odhadujeme reducibility jednotlivých slov a také delších souvislých frází (slovních n-gramů), studujeme jejich syntaktické vlastnosti a poté je také použijeme k odvození plných závislostních stromů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We use the English model of BERT and explore how a deletion of one word in a sentence changes representations of other words. Our hypothesis is that removing a reducible word (e.g. an adjective) does not affect the representation of other words so much as removing e.g. the main verb, which makes the sentence ungrammatical and of "high surprise" for the language model. We estimate reducibilities of individual words and also of longer continuous phrases (word n-grams), study their syntax-related properties, and then also use them to induce full dependency trees.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Reprezentace textu v počítači.
Reprezentace slov (slovní embedinky).
Generování vět I. (n-gramové jazykové modely).
Základní principy strojového učení.
Generování vět II. (umělé neuronové sítě).
Úvahy nad silněji řízeným generováním.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Representing text on a computer.
Representation of words (word embeddings).
Generating Sentences I. (n-gram language models).
Basic principles of machine learning.
Generating Sentences II (artificial neural networks).
Reflections on more powerfully managed generation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zkoumáme, do jaké míry lze flexi automaticky oddělit od derivace, jen na základě slovních forem. Očekáváme, že při použití vhodné míry vzdálenosti budou páry vyskloňovaných tvarů stejného lemmatu k sobě blíže než páry vyskloňovaných forem dvou různých lemmat (stále odvozených od stejného kořene). Vzdálenosti slovních tvarů odhadujeme pomocí editační vzdálenosti, která představuje podobnost založenou na znacích, a pomocí podobnosti slovních embedinků, která slouží jako proxy k významové podobnosti. Konkrétně zkoumáme Levenshteinovu a Jarovu-Winklerovu editační vzdálenost a kosinovou podobnost FastTextových slovních embedinků. Vyhodnocujeme oddělitelnost flexe a derivace na vzorku z databáze DeriNet, což je databáze slovotvorných vztahů v češtině. Zkoumáme míry vzdálenosti slov jednak přímo a jednak a jako složku shlukovacího postupu. Nejlepších výsledků je dosaženo kombinací Jarovy-Winklerovy editační vzdálenosti a kosionové podobnosti slovních embedinků, která překonává míry použité samostatně. Další analýza ukazuje, že metoda funguje lépe pro některé třídy flexí a derivací než pro jiné, což ukazuje některá omezení metody, ale také podporuje myšlenku nahrazení binární dichotomie flexe-derivace kontinuální škálou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We investigate to what extent inflection can be automatically separated from derivation, just based
on the word forms. We expect pairs of inflected forms of the same lemma to be closer to each other
than pairs of inflected forms of two different lemmas (still derived from a same root, though),
given a proper distance measure. We estimate distances of word forms using edit distance, which
represents character-based similarity, and word embedding similarity, which serves as a proxy
to meaning similarity. Specifically, we explore Levenshtein and Jaro-Winkler edit distances, and
cosine similarity of FastText word embeddings. We evaluate the separability of inflection and
derivation on a sample from DeriNet, a database of word formation relations in Czech. We
investigate the word distance measures directly, as well as embedded in a clustering setup. Best
results are achieved by using a combination of Jaro-Winkler edit distance and word embedding
cosine similarity, outperforming each of the individual measures. Further analysis shows that the
method works better for some classes of inflections and derivations than for others, revealing some
limitations of the method, but also supporting the idea of replacing a binary inflection-derivation
dichotomy with a continuous scale.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zaměřujeme se na úlohu neřízené lemmatizace, tj. seskupení vyskloňovaných forem jednoho slova pod jeden štítek (lemma) bez použití anotovaných trénovacích dat. Navrhujeme provádět aglomerativní shlukování slovních forem s novou mírou vzdálenosti. Naše míra vzdálenosti je založena na pozorování, že flexe jednoho slova mají tendenci být podobné řetězcově i významově. Proto kombinujeme kosinovou podobnost slovních embedinků, která slouží jako proxy k významové podobnosti, s editační vzdáleností Jaro-Winklera. Naše experimenty na 23 jazycích ukazují, že náš přístup je slibný a překonal baseline pro 23 z 28 testovacích sad.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We focus on the task of unsupervised lemmatization, i.e. grouping together inflected forms of one word under one label (a lemma) without the use of annotated training data. We propose to perform agglomerative clustering of word forms with a novel distance measure. Our distance measure is based on the observation that inflections of the same word tend to be similar both string-wise and in meaning. We therefore combine word embedding cosine similarity, serving as a proxy to the meaning similarity, with Jaro-Winkler edit distance. Our experiments on 23 languages show our approach to be promising, surpassing the baseline on 23 of the 28 evaluation datasets.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení výzkumu automatického vyhodnocování koherence v češtině s pomocí naanotovaných velkých dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Introduction of the research of automatic coherence evaluation in Czech using unlabeled large data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Využíváme velkých neanotovaných dat (n-gramový model, odhady hustoty featur) ke zlepšení kvality automatického hodnocení koherence textů v češtině. Spolu s novými featurami z různých jazykových rovin přispělo využití neanotovaných dat k signifikantnímu zlepšení výsledků systému.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We use large unlabeled data (n-gram model, density estimates of features) to improve quality of surface coherence evaluation in Czech texts. Along with new features across layers of language description, the additions significantly improve the performance of the system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Počítačový nástroj Evaluátor diskurzu pro začátečníky (EVALD 4.0 pro začátečníky) slouží k automatickému hodnocení povrchové koherence (koheze) textů v češtině psaných začínajícími nerodilými mluvčími češtiny. Software hodnotí úroveň předloženého textu z hlediska jeho povrchové výstavby (zohledňuje např. frekvenci a rozmanitost užitých spojovacích prostředků, bohatost slovní zásoby, koreferenční vztahy, aktuální členění, obtížnost čtení apod.) a vyhodnocuje, zda text dostahuje alespoň základní úrovně A1.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Evaluator of Discourse for Beginners (EVALD 4.0 for Beginners) is a software that serves for automatic evaluation of surface coherence (cohesion) in Czech texts written by non-native speakers of Czech. Software evaluates the level of the given text from the perspective of its surface coherence (i.e. it takes into consideration, e.g., the frequency and diversity of connectives, richness of vocabulary, coreference realations, topic-focus articulation, readability etc.), and reports whether the text reaches the basic A1 level.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Počítačový nástroj Evaluátor diskurzu pro cizince (EVALD 4.0 pro cizince) slouží k automatickému hodnocení povrchové koherence (koheze) textů v češtině psaných nerodilými mluvčími češtiny. Software hodnotí úroveň předloženého textu z hlediska jeho povrchové výstavby (zohledňuje např. frekvenci a rozmanitost užitých spojovacích prostředků, bohatost slovní zásoby, koreferenční vztahy, aktuální členění apod.). Nová verze (4.0) přidává množinu rysů zaměřených především na obtížnost čtení textu a nově také využívá velká neanotovaná data.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Evaluator of Discourse for Foreigners (EVALD 4.0 for Foreigners) is a software that serves for automatic evaluation of surface coherence (cohesion) in Czech texts written by non-native speakers of Czech. Software evaluates the level of the given text from the perspective of its surface coherence (i.e. it takes into consideration, e.g., the frequency and diversity of connectives, richness of vocabulary, coreference relations, topic-focus articulation etc.). The new version (4.0) adds a set of features related to readability of the text and also newly uses large unlabeled data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Počítačový nástroj Evaluátor diskurzu (EVALD 4.0) slouží k automatickému hodnocení povrchové koherence (koheze) textů v češtině psaných rodilými mluvčími češtiny. Software tedy hodnotí (známkuje) úroveň předloženého textu z hlediska jeho povrchové výstavby (zohledňuje např. frekvenci a rozmanitost spojovacích prostředků, bohatost slovní zásoby, koreferenční vztahy, aktuální členění apod.). Nová verze (4.0) přidává množinu rysů zaměřených především na obtížnost čtení textu a nově také využívá velká neanotovaná data.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Evaluator of Discourse (EVALD 4.0) is a software that serves for automatic evaluation of surface coherence (cohesion) in Czech texts written by native speakers of Czech. Software evaluates the level of the given text from the perspective of its surface coherence (i.e. it takes into consideration, e.g., the frequency and diversity of connectives, richness of vocabulary, coreference relations, topic-focus articulation etc.). The new version (4.0) adds a set of features related to readability of the text and also newly uses large unlabeled data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku popisujeme naše systémy předložené v rámci soutěže Building Educational Applications (BEA) 2019 Shared Task (Bryant a kol., 2019). Zúčastnili jsme se všech tří variant. Naše modely jsou systémy NMT založené na architektuře Transformer, který vylepšujeme začleněním několika vylepšení: dropout celých zdrojových a cílových slov, vážení cílových podslov, průměrování modelu a použití trénovaného modelu iterativním způsobem. Systém v Restricted Track je trénován na poskytnutých korpusech s nadměrně zesílenými "čistšími" větami a na testovací sadě dosahuje skóre 59,39 F0,5. Systém v režimu nízkých zdrojů je trénován z historie revizí Wikipedie a dosahuje skóre 44,13 F0,5. V neomezeném režimu jsme dotrénováním systému z režimu nízkých zdrojů dosáhli 64.55 F0.5 skóre a obsadili tak třetí místo.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we describe our systems submitted to the Building Educational Applications (BEA) 2019 Shared Task (Bryant et al., 2019). We participated in all three tracks. Our models are NMT systems based on the Transformer model, which we improve by incorporating several enhancements: applying dropout to whole source and target words, weighting target subwords, averaging model checkpoints, and using the trained model iteratively for correcting the intermediate translations. The system in the Restricted Track is trained on the provided corpora with oversampled "cleaner" sentences and reaches 59.39 F0.5 score on the test set. The system in the Low-Resource Track is trained from Wikipedia revision histories and reaches 44.13 F0.5 score. Finally, we finetune the system from the Low-Resource Track on restricted data and achieve 64.55 F0.5 score, placing third in the Unrestricted Track.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Automatická oprava gramatiky v angličtině je dlouho studovaný problém s mnoha existujícími systémy a datovými zdroji. Výzkum oprav chyb v jiných jazycích je však pouze omezený. V této práci představujeme nový dataset AKCES-GEC pro gramatickou korekci chyb pro češtinu. Dále provádíme experimenty na češtině, němčině a ruštině a ukazujeme, že při využití syntetického paralelního korpusu může model neuronového strojového překladu Transformer dosáhnout na těchto datasetech nejlepších známých výsledků. AKCES-GEC vychází pod licencí CC BY-NC-SA 4.0 na adrese https://hdl.handle.net/11234/1-3057 a zdrojový kód modelu GEC je k dispozici na adrese https://github.com/ufal/low-resource-gec-wnut2019.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Grammatical error correction in English is a long studied problem with many existing systems and datasets. However, there has been only a limited research on error correction of other languages. In this paper, we present a new dataset AKCES-GEC on grammatical error correction for Czech. We then make experiments on Czech, German and Russian and show that when utilizing synthetic parallel corpus, Transformer neural machine translation model can reach new state-of-the-art results on these datasets. AKCES-GEC is published under CC BY-NC-SA 4.0 license at https://hdl.handle.net/11234/1-3057 and the source code of the GEC model is available at https://github.com/ufal/low-resource-gec-wnut2019.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje výsledky sdílených úkolů z 6. workshopu o asijském překladu (WAT2019), včetně dílčích úkolů Ja↔En, Ja↔Zh pro překlad vědeckých článků, Ja↔En, Ja↔Ko, Ja↔En pro překlad patentů, Hi↔En, My↔En, Km↔En, Ta↔En pro překlad smíšených doménových dílčích úkolů, Ru↔Ja komentář pro překlad zpráv a En pro multimodální překlad. V rámci programu WAT2019 se sdílených úkolů účastnilo 25 týmů.
Obdrželi jsme také 10 písemných podání k výzkumu, z nichž 71 bylo přijato. Automatickému hodnotícímu serveru bylo předloženo asi 400 výsledků překladu a vybraná podání byla ručně vyhodnocena.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the
shared tasks from the 6th workshop on
Asian translation (WAT2019) including
Ja↔En, Ja↔Zh scientific paper translation subtasks, Ja↔En, Ja↔Ko, Ja↔En
patent translation subtasks, Hi↔En,
My↔En, Km↔En, Ta↔En mixed domain
subtasks, Ru↔Ja news commentary
translation task, and En→Hi multi-modal
translation task. For the WAT2019, 25
teams participated in the shared tasks.
We also received 10 research paper submissions out of which 71 were accepted.
About 400 translation results were submitted to the automatic evaluation server,
and selected submissions were manually
evaluated.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento výzkum se zaměřuje na různé funkce diskurzního konektivu "a" v anotovaných zapsaných mluvených textech TED Talks v angličtině a litevštině. Anotace litevských textů bylo zahájena teprve nedávno, proto tvoří litevské texty ve srovnání prozatím menší vzorek. Výsledky výzkumu ukazují, že výraz "and" a jeho ekvivalenty v litevštině vyjadřují řadu vágních významů, včetně prostého přidávání, řízení a strukturace diskurzu</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present research focuses on the multiple functions performed by the discourse marker "and" in annotated spoken-like texts of TED Talks in English and Lithuanian. The annotation of TED Talks in Lithuanian has started only recently, which results in the limitation regarding the quantity of annotated texts. The research findings show that and and its Lithuanian counterparts perform multiple fuzzy functions, including the function of addition, discourse management and structuring discourse.
It was also established that the most frequent variants of translation of the discourse marker and are those provided by bilingual English–Lithuanian dictionaries and that translators choose paraphrases to convey the pragmatics of the spoken-like texts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Využití modelů řízených daty pro sumarizaci textu nebo podobné úlohy se v posledních letech stává velmi běžným. Zatímco většina studií hlásí pouze základní přesnost, není nic známo o schopnosti zmíněných modelů se zlepšit, jsou-li trénovány na větších datech. V tomto příspěvku definujeme a navrhujeme tři metriky efektivity dat: efektivita úspěšnosti dat, časové nedostatečnosti dat a celkové účinnosti dat. Navrhujeme také jednoduché schema využívající těchto metod a využívající je pro ucelenější hodnocení populárních metod sumarizace textů a generování nadpisů. Pro druhou z úloh zpracováváme a uvol%nujeme rozsáhlou kolekci 35 miliónů párů abstrakt-název vědeckých článků. Naše výsledky odhalují, že mezi tetovanými metodami je Transformer nejúčinnější pro obě úlohy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Using data-driven models for solving text summarization or similar tasks has become very common in the last years. Yet most of the studies report basic accuracy scores only, and nothing is known about the ability of the proposed models to improve when trained on more data. In this paper, we define and propose three data efficiency metrics: data score efficiency, data time deficiency and overall data efficiency. We also propose a simple scheme that uses those metrics and apply it for a more comprehensive evaluation of popular methods on text summarization and title generation tasks. For the latter task, we process and release a huge collection of 35 million abstract-title pairs from scientific articles. Our results reveal that among the tested models, the Transformer is the most efficient on both tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Výzkum získávání klíčových slov probíhá od devadesátých let, ale pokročilé přístupy jako kodér-dekodér a učení z posloupností byly objeveny až v poslední době. Více než tucet abstrakčních metod poskytujících smysluplná klíčová slova a dosahujících aktuálních výsledků bylo navrženo v posledních třech letech. Testujeme zde různé aspekty metod generování klíčových slov.
Soustředíme se zejména na metody z poslední doby založené na neuronových sítích. Zvláštní důraz klademe na mechanismy řízení přesnosti těch posledně jmenovaných. Byla vytvořena velmi rozsáhlá kolekce metadat o vědeckých článcích a uvolněna vědecké komunitě. Také jsme prezentovali různé vzory výzkumu generování klíčových slov a sumarizace textu a trendy posledních dvou dekád.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Extractive keyphrase generation research has been around since the nineties, but the more advanced abstractive approach based on the encoder-decoder framework and sequence-to-sequence learning has been explored only recently. In fact, more than a dozen of abstractive methods have been proposed in the last three years, producing meaningful keyphrases and achieving state-of-the-art scores. In this survey, we examine various aspects of the extractive keyphrase generation methods and focus mostly on the more recent abstractive methods that are based on neural networks. We pay particular attention to the mechanisms that have driven the perfection of the later. A huge collection of scientific article metadata and the corresponding keyphrases is created and released for the research community. We also present various keyphrase generation and text summarization research patterns and trends of the last two decades.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Klíčová slova, která svým vědeckým článkům přiřadili jejich autoři jsou nepostradatelná pro rozpoznání obsahu a témat dané článku. Většina
řízených i neřízených metod generování klíčových slov není schopna přiřazovat termíny, které to dobře vystihují, ale nevyskytují se v textu. V tomto příspěvku zkoumáme možnost klíčových slov coby shrnutím názvu práce a abstraktu. Nejdříve sesbíráme, zpracujme a vydáme velkou sadu metadat vědeckých článků čítajících 2,2 milionu záznamů. Pak vyzkoušíme populární neurální architektury pro sumarizaci textů. Na rozdíl od pokročilých metod hlubokého učení, velkých objemů dat a mnoha
dní výpočtů naše systematické vyhodnocování na čtyřech testovacích sadách dat ukázalo, že zkoumané metody sumarizace textu nemohou vytvořit
lepší klíčová slova než jednoduché neřízené či řízené metody.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Authors' keyphrases assigned to scientific articles are essential for recognizing content and topic aspects. Most of the proposed supervised and unsupervised methods for keyphrase generation are unable to produce terms that are valuable but do not appear in the text. In this paper, we explore the possibility of considering the keyphrase string as an abstractive summary of the title and the abstract. First, we collect, process and release a large dataset of scientific paper metadata that contains 2.2 million records. Then we experiment with popular text summarization neural architectures. Despite using advanced deep learning models, large quantities of training data and many days of computation, our systematic evaluation on four test datasets reveals that the explored text summarization methods could not produce better keyphrases than the much simpler unsupervised methods or the existing supervised ones.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V oblasti online komunikace, komerce a překladů, se analýza polarity sentimentu textů napsaných v různých přirozených jazycích stává zásadní. Zatímco pro angličtinu je k dispozici mnoho příspěvků a zdrojů, "menší" jazyky, jako je čeština, se zatím netěší větší pozornosti. V tomto
přehledu zkoumáme efektivitu mnoha algoritmů strojového učení pro analýzu sentimentu příspěvků na českém Facebooku a recenzí různých produktů. Sepíšeme sady optimálních hodnot parametrů pro každý algoritmus a ohodnocení v obou datasetech. Nakonec zaznamenáme, že metoda podpůrných vektorů je nejlepším klasifikátorem a snahy dále zlepšit výkon pomocí baggingu, boostingu, či hlasovacích schemat selhaly.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the area of online communication, commerce and transactions, analyzing sentiment polarity of texts written in various natural languages has become crucial. While there have been a lot of contributions in resources and studies for the English language, "smaller" languages like Czech have not received much attention. In this survey, we explore the effectiveness of many existing machine learning algorithms for sentiment analysis of Czech Facebook posts and product reviews. We report the sets of optimal parameter values for each algorithm and the scores in both datasets. We finally observe that support vector machines are the best classifier and efforts to increase performance even more with bagging, boosting or voting ensemble schemes fail to do so.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Morfologická segmentace slov je proces rozdělování slova na menší jednotky nazývané morfémy, což je úloha, která je obtížná zejména v případě morfologicky bohatých nebo polysyntetických jazyků. V této práci navrhujeme pro řešení této úlohy několik rekurzivních neuronových sítí a dalších přístupů založených na strojovém učení. Jako trénovací data používáme ručně segmentované slovníky. K vyhodnocení vlivu velikosti slovníku na kvalitu segmentace používáme rozsáhlý, ručně anotovaný segmentační slovník perštiny. Dále používáme menší segmentační slovníky pro češtinu a finštinu. Na těchto jazycích zkoumáme rovněž vliv nastavení hyperparametrů a zvolených rekurentních architektur.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Morphological segmentation of words is the process of dividing a word into smaller units called morphemes; it is tricky especially when a morphologically rich or polysynthetic language is under question. In this work, we designed and evaluated several Recurrent Neural Network (RNN) based models as well as various other machine learning based approaches for the morphological segmentation task. We trained our models using annotated segmentation lexicons. To evaluate the effect of the training data size on our models, we decided to create a large hand-annotated morphologically segmented corpus of Persian words, which is, to the best of our knowledge, the first and the only segmentation lexicon for the Persian language. In the experimental phase, using the hand-annotated Persian lexicon and two smaller similar lexicons for Czech and Finnish languages, we evaluated the effect of the training data size, different hyper-parameters settings as well as different RNN-based models.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku studujeme abstraktiví sumarizaci videí bez doménového omezení. Na rozdíl od tradiční sumarizace zpravodajských textů není cílem "komprimovat" textové informace, ale spíše poskytnout plynulé textové shrnutí informací, které byly shromážděny z různých zdrojových modalit, v našem případě videozáznamů a audio přepisů (nebo textu). Ukazujeme, jak vícezdrojový model sekvenčního učení s hierarchickým mechanismem pozorností dokáže integrovat informace z různých modalit do uceleného výstupu, porovnáváme různé modely trénované s různými modalitami a prezentuje pilotní experimenty na How2 korpusu instruktážních videí. Navrhujeme také novou hodnotící metriku (Conent F1) pro abstraktivní sumarizaci, která měří spíše sémantickou adekvátnost než plynulost, kterou naopak zachcují tradiční metriky jako jako ROUGE a BLEU.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we study abstractive summarization for open-domain videos. Unlike the traditional text news summarization, the goal is less to "compress" text information but rather to provide a fluent textual summary of information that has been collected and fused from different source modalities, in our case video and audio transcripts (or text). We show how a multi-source sequence-to-sequence model with hierarchical attention can integrate information from different modalities into a coherent output, compare various models trained with different modalities and present pilot experiments on the How2 corpus of instructional videos. We also propose a new evaluation metric (Content F1) for abstractive summarization task that measures semantic adequacy rather than fluency of the summaries, which is covered by metrics like ROUGE and BLEU.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje experimentální výzkum srovnávající, jak jsou texty psané nerodilými mluvčími češtiny hodnoceny softwarovou aplikací EVALD a učiteli češtiny jako cizího jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce an experimental probe comparing how texts written by non-native speakers of Czech are evaluated by a software application EVALD and by teachers of Czech as a foreign language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme první kompletní hlasový dialogový systém řízený multidimenzionálním statistickým dialogovým manažerem. Tento framework prokazatelně významně snižuje potřebu dat využitím doménově nezávislých dimenzí, jako jsou společenské konvence nebo zpětná vazba, které (jak ukazujeme) lze přenášet mezi doménami. V tomto článku provádíme uživatelskou sudii a ukazujeme, že výkon multidimenzionálního systému, který lze adaptovat ze zdrojové domény, je ekvivalentní výkonu jednodimenzionální baseline, kterou je třeba natrénovat od nuly.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the first complete spoken dialogue system driven by a multiimensional statistical dialogue manager. This framework has been shown to substantially reduce data needs by leveraging domain-independent dimensions, such as social obligations or feedback, which (as we show) can be transferred between domains. In this paper, we conduct a user study and show that the performance of a multi-dimensional system, which can be adapted from a source domain, is equivalent to that of a one-dimensional baseline, which can only be trained from scratch.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška představuje výsledky anotace implicitních vztahů v češtině na základě korpusu PDiT-EDA 1.0. Zaměřuje se na distribuci implicitních a explicitních vztahů, jejich sémantiku, přítomnost větné negace a vztah implicitnosti k textovému žánru.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The lecture presents the results of the annotation of implicit relations in Czech based on the PDiT-EDA 1.0 corpus. It focuses on the distribution of implicit and explicit relations, their semantics, the presence of sentence negation and the relation of implicitness to the text genre.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek podává základní přehled o implicitních diskurzních vztazích v korpusu PDiT-EDA 1.0. Zabývá se vztahem implicitnosti k následujícím jazykovým faktorům: sémantika diskurzního vztahu, přítomnost větné negace, textový žánr.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper gives a basic overview of implicit discourse relations in the PDiT-EDA 1.0 corpus. It deals with the relation of implicitness to the following language factors: semantics of discourse relation, presence of sentence negation, text genre.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Analyzujeme některé faktory ovlivňující explicitnost/implicitnost diskurzních vztahů, např. žánr textu, sémantický typ diskurzního vztahu a přítomnost negace v diskurzních argumentech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We analyze some of the factors influencing the explicitness/implicitness of discourse relations, such as the text genre, semantic type of the discourse relation and the presence of negation in discourse arguments.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Recenze knihy Syntax mluvené češtiny, jejímiž editory jsou Jana Hoffmannová, Jiří Homoláč a Kamila Mrázková.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Review of Syntax of Spoken Czech, a book edited by Jana Hoffmannová, Jiří Homoláč and Kamila Mrázková.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Koherence textů může být zajištěna řadou jazykových prostředků, např. diskurzními konektory, anaforickým odkazováním, lexikálním opakováním apod. Představujeme výzkum nejčastějších chyb v koherenci žákovských textů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Coherence may be manifested through various language means, e.g. by discourse connectives, anaphoric devices, lexical repetition etc. We present research of the most common errors in coherence occurring in learners’ essays.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládáme popis našeho příspěvku do soutěže CoNLL 2019, Cross-Framework Meaning Representation Parsing (MRP 2019). Navržená architektura je naším prvním pokusem o sémantický parsing v rámci UDPipe 2.0, nástroje pro lemmatizaci, POS tagging a závislostní parsing.

Pro MRP 2019, který zahrnuje pět formálně a lingvisticky rozdílných přístupů k reprezentaci významu (DM, PSD, EDS, UCCA a AMR), navrhujeme uniformní, jazykově a formálně agnostickou architekturu založenou na transformaci grafů pomocí umělých neuronových sítí. Bez jakékoli znalosti grafové struktury, a specificky bez jakýchkoli lingvisticky nebo formálně motivovaných klasifikačních rysů náš systém implicitně modeluje reprezentaci významu v grafu.

Po opravě člověkem způsobené chyby (použili jsme nesprávnou verzi poskytnutých analýz testovacích dat) se náš příspěvek umístil na třetím místě v soutěžním hodnocení. Zdrojový ḱód našeho systému je dostupný na adrese https://github.
com/ufal/mrpipe-conll2019.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a system description of our contribution to the CoNLL 2019 shared task, Cross-
Framework Meaning Representation Parsing (MRP 2019). The proposed architecture is our first attempt towards a semantic parsing extension of the UDPipe 2.0, a lemmatization, POS tagging and dependency parsing pipeline.

For the MRP 2019, which features five formally and linguistically different approaches to meaning representation (DM, PSD, EDS, UCCA and AMR), we propose a uniform, language and framework agnostic graph-to-graph neural network architecture. Without any knowledge about the graph structure, and specifically without any linguistically or framework motivated features, our system implicitly models the meaning representation graphs.

After fixing a human error (we used earlier incorrect version of provided test set analyses),
our submission would score third in the competition evaluation. The source code of our
system is available at https://github.
com/ufal/mrpipe-conll2019.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nedávno byly navrženy kontextové embeddingy, které vhodně zachycují význam slova v závislosti na kontextu. V tomto příspěvku vyhodnocujeme dvě metody pro výpočet takových embeddingů, BERT a Flair, na čtyřech úlohách zpracování přirozeného jazyka v češtině: značkování slovních druhů (POS tagging), lemmetizace, závislostní parsing a rozpoznávání pojmenovaných entit. První tři úlohy jsou vyhodnoceny na dvou korpusech: Pražský závislostní korpus 3.5 a Universal Dependencies 2.3. Rozpoznávání pojmenovaných entit je vyhodnoceno na Českém korpusu pojmenovaných entit (Czech Named Entity Corpus) 1.1 a 2.0. Publikujeme state-of-the-art výsledky ve všech výše zmíněných úlohách na všech korpusech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Contextualized embeddings, which capture appropriate word meaning depending on context, have recently been proposed. We evaluate two methods for precomputing such embeddings, BERT and Flair, on four Czech text processing tasks: part-of-speech (POS) tagging, lemmatization, dependency parsing and named entity recognition (NER). The first three tasks, POS tagging, lemmatization and dependency parsing, are evaluated on two corpora: the Prague Dependency Treebank 3.5 and the Universal Dependencies 2.3. The named entity
recognition (NER) is evaluated on the Czech Named Entity Corpus 1.1 and 2.0. We report state-of-the-art results for the above mentioned tasks and corpora.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme rozsáhlé hodnocení tří nedávno navržených metod pro kontextualizované embeddiny na 89 korpusech v 54 jazycích projektu Universal Dependencies 2.3 ve třech úkolech: POS tagging, lemmatizace a závislostní analýza. Využitím BERT, Flair a ELMo jako předtrénovaných embeddingů do systému UDPipe 2.0, jednoho z vítězů CoNLL 2018 Shared Task a celkového vítěze EPE 2018, představujeme porovnání těchto tří kontextualizovaných metod, word2vec předtrénovaných embedingů a trénovatelných embedingů založených na znacích slov. Popsané metody dosahují nejlepších známých výsledků na UD 2.2 v porovnání s výsledky na CoNLL 2018 Shared Task.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an extensive evaluation of three recently proposed methods for contextualized embeddings on 89 corpora in 54 languages of the Universal Dependencies 2.3 in three tasks: POS tagging, lemmatization, and dependency parsing. Employing the BERT, Flair and ELMo as pretrained embedding inputs in a strong baseline of UDPipe 2.0, one of the best-performing systems of the CoNLL 2018 Shared Task and an overall winner of the EPE 2018, we present a one-to-one comparison of the three contextualized word embedding methods, as well as a comparison with word2vec-like pretrained embeddings and with end-to-end character-level word embeddings. We report state-of-the-art results in all three tasks as compared to results on UD 2.2 in the CoNLL 2018 Shared Task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme náš příspěvek k shared tasku SIGMORPHON 2019: Crosslingualita a kontext v morfologii, úkol 2: kontextová morfologická analýza a lemmatizace.

Odevzdali jsme modifikaci UDPipe 2.0, jednoho z výherního systému CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies a celkového vítěze The 2018 Shared Task on Extrinsic Parser Evaluation. Jako první vylepšení používáme předtrénované kontextualizované embeddingy (BERT) jako další vstupy do sítě, za druhé používáme jednotlivé morfologické vlastnosti jako regularizaci a nakonec slučujeme vybrané korpusy stejného jazyka.

V lemmatizačním úkolu náš systém výrazně převyšuje všechny ostatní systémy s přesností lemmatizace 95,78 (druhý nejlepší byl 95,00, třetí 94,46). V morfologické analýze se náš systém umístil těsně druhý: přesnost naší morfologické analýzy byla 93,19, vítězný systém měl 93,23.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our contribution to the SIGMORPHON 2019 Shared Task: Crosslinguality and Context in Morphology, Task 2: contextual morphological analysis and lemmatization.

We submitted a modification of the UDPipe 2.0, one of best-performing systems of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies and an overall winner of the The 2018 Shared Task on Extrinsic Parser Evaluation. As our first improvement, we use the pretrained contextualized embeddings (BERT) as additional inputs to the network; secondly, we use individual morphological features as regularization; and finally, we merge the selected corpora of the same language.

In the lemmatization task, our system exceeds all the submitted systems by a wide margin with lemmatization accuracy 95.78 (second best was 95.00, third 94.46). In the morphological analysis, our system placed tightly second: our morphological analysis accuracy was 93.19, the winning system’s 93.23.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Derivations (UDer) je sada lexikalních sítí zachycujících slovotvorné, zvl. derivační vztahy jednotlivých jazyků, tyto sítě byly harmonizovány do jednotného formátu. Stávající verze UDer v0.5 obsahuje 11 harmonizovaných zdrojů pokrývajících 11 různých jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Derivations (UDer) is a collection of harmonized lexical networks capturing word-formation, especially derivational relations, in a cross-linguistically consistent annotation scheme for many languages. The annotation scheme is based on a rooted tree data structure, in which nodes correspond to lexemes, while edges represent derivational relations or compounding.
The current version of the UDer collection contains eleven harmonized resources covering eleven different languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem tohoto článku je otevřít diskusi o harmonizaci existujících datových zdrojů zabývajících se derivační morfologií. Představujeme nově vytvořený soubor jedenácti harmonizovaných zdrojů pojmenovaný „Universal Derivations“ (zjevně inspirovaný úspěchem iniciativy Universal Dependencies mezi syntakticky anotovanými korpusy) a harmonizační proces, kterým jsme tyto zdroje sjednotili do stejného anotačního schématu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of this paper is to open a discussion on harmonization of existing data resources related
to derivational morphology. We present a newly assembled collection of eleven harmonized
resources named “Universal Derivations” (clearly being inspired by the success story of the
Universal Dependencies initiative in treebanking), as well as the harmonization process that
brings the individual resources under a unified annotation scheme.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje příspěvek týmu Univerzity Karlovy a Maltské univerzity do soutěže CoNLL-SIGMORPHON 2019 v morfologické analýze a lematizaci v kontextu. Předkládáme lematizační model postavený na dříve publikované metodě neuronových převodníků (Makarov 2018; Aharoni a Goldberg 2017). Klíčovým rozdílem je, že náš model transformuje celý slovní tvar každého kmene, místo aby ho konzumoval znak po znaku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the submission by the Charles University-University of Malta team to
the CoNLL--SIGMORPHON 2019 Shared Task on Morphological Analysis and Lemmatization in context. We present a lemmatization model based on previous work on neural transducers \cite{makarov2018neural,aharoni-goldberg-2017-morphological}. The key difference is that our model transform the whole word form in every stem, instead of consuming it character by character. We propose a merging strategy inspired by Byte-Pair-Encoding that reduces the space of valid operations by merging frequent adjacent operations. The resulting operations not only encode the action/s to be performed but the relative position in the word token and
how characters need to be transformed.

Our morphological tagger is a vanilla biLSTM tagger that operates over operation representations, encoding operations and words in a hierarchical manner.

Even though relative performance according to metrics is below the baseline, experiments show that our models capture important associations between interpretable operation labels and fine-grained morpho-syntax labels.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Digitální sbírky audiovizuálních orálněhistorických rozhovorů (OHI) mohou být pojaty jako zvláštní kategorie velkých textových korpusů, které vyžadují implementaci metod vzdáleného čtení. Po digitální revoluci ve výzkumu orální historie (např. Thomson, 2007; Boyd &amp; Larson, 2014) dnes mnoho institucí poskytuje přístup k mnoha digitálním kolekcím najednou, což uživatelům a výzkumníkům poskytuje možnost zkombinovat a porovnat například několik OHI provedených se stejnou osobou v různorodém situačním a sociohistorickém rámci jako součást různých projektů. To klade před takové přístupové instituce řadu otázek: Jak můžeme uživatelům co nejvíce usnadnit práci s několika samostatnými digitálními archivy OHI? Které výpočetní metody mohou usnadnit vzdálené čtení a efektivní využití rozsáhlého sběru audiovizuálních materiálů OHI? A jaké jsou přetrvávající problémy - technické, metodologické, etické atd. -, které musí instituce vyřešit?
Při nastínění odpovědí na tyto otázky budeme v naší prezentaci diskutovat o našich zkušenostech a technologických řešeních z Centra vizuální historie Malach (CVHM) na Univerzitě Karlově. 
Současná situace, kdy je zpřístupněno několik nesourodých sbírek najednou, představuje několik výzev na úrovni zlepšení účinnosti metod vyhledávání a textové analýzy korpusů. Na jedné straně uživatelé-výzkumníci potřebují úplný přehled metadat, aby mohli vytvářet příslušné soubory dat, objevovat duplicitní případy a analyzovat profily sbírek OHI. Zároveň vyvstává otázka prohledávání obsahu napříč celým korpusem. Pro tyto účely vyvinulo CVHM interní rozhraní, které integruje několik kolekcí najednou a poskytuje řešení obou těchto otázek. Díky tomuto řešení mohou uživatelé snadno získávat relevantní výsledky na několika úrovních bez nutnosti samostatného přístupu k jednotlivým sbírkám nebo k OHI.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Digital collections of audiovisual oral history interviews (OHIs) can be conceived as a specific category of large text corpora that require implementation of distant reading methods. Following the digital revolution in oral history research (e.g. Thomson, 2007; Boyd &amp; Larson, 2014), many institutions now provide access to divergent digital collections at once, which provides the users and researchers with an opportunity to combine and compare, for example, several OHIs conducted with the same person in varied situational and socio-historical framework as part of various "process-generated" (Freund, 2009) oral history projects. This constitutes a pertinent issue for such access institutions: How can we make it as easy as possible for users to work with several separate digital archives of OHIs? Which computational methods can facilitate distant reading and efficient use of large collection of audiovisual OHI materials? And what are the persistent problems -- technical, methodological, ethical, etc. -- that have to be solved by the institutions of multiple access?
In outlining answers to these questions, in our presentation, we will discuss our experience and technological solution from the Malach Center for Visual History (CVHM) at the Charles University in Prague (see Mlynář, 2015). Over the last decade, CVHM has been providing access for students, researchers and general public to several established collections of OHIs. Since 2009, CVHM is an access point to the USC Shoah Foundation's Visual History Archive (VHA), which is an ever-growing collection of interviews with witnesses and survivors of genocides, especially the Holocaust. At the present moment, the VHA contains nearly 56,000 audiovisual recordings of OHIs in more than 40 languages (see Gustman et al., 2002). Since 2018, the Fortunoff Video Archive for Holocaust Testimonies of the Yale University Library with more than 4,400 audiovisual recordings of OHIs is also available at CVHM. In addition, users in CVHM can work with smaller collections lacking an integrated user interface such as the Refugee Voices archive (150 English interviews), and a small portion of interviews from the Jewish Holocaust Center in Melbourne (15 interviews with people of Czechoslovak origin).
The present situation of hosting several disparate collections at once poses several challenges on the level of improving effectivity of search and textual corpora analysis methods. On one hand, users-researchers are in need of a complete metadata overview, in order to generate relevant datasets, discover duplicate cases and analyze OHI collections’ profiles. On the other hand, the question of performing content-based queries across the whole corpus is imminent. For these purposes, CVHM developed an in-house interface integrating several collection at once providing solutions to both of these issues. Besides providing general access to metadata, automated speech recognition based transcripts (generated by AMALACH algorithm, post-edited) serve simultaneously as textual data for the multilingual cross-corpus search in English, Czech and Slovak and searchable automatically generated keywords dataset (KER - Keyword Extractor provided by LINDAT/CLARIN). Owing to this approach, users are able to easily acquire relevant results on several levels without the necessity of separately accessing the individual collections or OHIs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Užitečnost jazykových anotací v překladu do neurálních strojů byla zřejmě prokázána v minulých pracích. Pokusy se však omezovaly na opakující se sekvenční architektury a relativně malé nastavení dat.

Zaměřujeme se na nejmodernější model Transformeru a používáme srovnatelně větší korporáty. Konkrétně se snažíme podporovat znalosti syntaxe na zdrojové straně pomocí víceúkolového učení buď pomocí jednoduchých technik manipulace s daty, nebo pomocí speciální modelové komponenty. Konkrétně jednoho cvičíme
Transformer se soustředí na vytvoření stromu závislosti na straně zdroje.

Celkově naše výsledky zpochybňují užitečnost víceúkolových sestav s jazykovými informacemi. Techniky manipulace s daty, doporučované v předchozích dílech, se v nastavení velkých dat ukazují jako neúčinné.

Zacházení se sebepozorností jako se závislostmi se zdá mnohem slibnější: pomáhá při překladu a odhaluje, že model Transformer dokáže velmi snadno uchopit syntaktickou strukturu.
Důležitým, ale kuriózním výsledkem však je, že identického zisku se dosáhne použitím triviálních ,,lineárních stromů`` namísto skutečných závislostí. Přínos tedy nemusí vyplývat z přidaných jazykových znalostí, ale z nějakého jednoduššího regularizačního efektu, který jsme navodili u matricí, které se věnují samy sobě.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The utility of linguistic annotation in neural machine translation seemed to had been established in past papers. The experiments were however limited to recurrent sequence-to-sequence architectures and relatively small data settings.

We focus on the state-of-the-art Transformer model and use comparably larger corpora. Specifically, we try to promote the knowledge of source-side syntax using multi-task learning either through simple data manipulation techniques or through a dedicated model component. In particular, we train one
of Transformer attention heads to produce source-side dependency tree.

Overall, our results cast some doubt on the utility of multi-task setups with linguistic information. The data manipulation techniques, recommended in previous works, prove ineffective in large data settings.

The treatment of self-attention as dependencies seems much more promising: it helps in translation and reveals that Transformer model can very easily grasp the syntactic structure.
An important but curious result is, however, that identical gains are obtained by using trivial ``linear trees'' instead of true dependencies. The reason for the gain thus may not be coming from the added linguistic knowledge but from some simpler regularizing effect we induced on self-attention matrices.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek přináší lingvistický i technický podklad pro vývoj diskurzního parseru pro češtinu. Zaměřuje se na diskurzní vztahy  mezi nesousedními segmenty textu signalizované (většinou) anaforickými konektory.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper is a linguistic as well as technical survey for the development of a shallow discourse parser for Czech. It focuses on long-distance discourse relations signalled by (mostly) anaphoric discourse connectives. Proceeding from the division of connectives on “structural” and “anaphoric” according to their (in)ability to accept distant (non-adjacent) text segments as their left-sided arguments, and taking into account results of related analyses on English data in the framework of the Penn Discourse Treebank, we analyze a large amount of language data in Czech. We benefit from the multilayer manual annotation of various language aspects from morphology to discourse, coreference and bridging relations in the Prague Dependency Treebank 3.0. We describe the linguistic parameters of long-distance discourse relations in Czechin connection with their anchoring connective, and suggest possible ways of their detection. Our empirical research also outlines some theoretical consequences for the underlying assumptions in discourse analysis and parsing, e.g. the risk of relying too much on different (language-specific?) part-of-speech categorizations of connectives or the different perspectives in shallow and global discourse analyses (the minimality principle vs. higher text structure).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku popisujeme překladový systém CUNI pro úlohu neřízeného strojového překladu novinových textů na ACL 2019 Fourth Conference on Machine Translation (WMT19).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we describe the CUNI translation system used for the unsupervised news shared task of the ACL 2019 Fourth Conference on Machine Translation (WMT19). We follow the strategy of Artetxe ae at. (2018b), creating a seed phrase-based system where the phrase table is initialized from cross-lingual embedding mappings trained on monolingual data, followed by a neural machine translation system trained on synthetic parallel data. The synthetic corpus was produced from a monolingual corpus by a tuned PBMT model refined through iterative back-translation. We further focus on the handling of named entities, i.e. the part of vocabulary where the cross-lingual embedding mapping suffers most. Our system reaches a BLEU score of 15.3 on the German-Czech WMT19 shared task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku přestavuje náš přísvek do soutěže v robustním strojovém překaladu na konferenci WMT19. Náš základní systém je CUNI Transformer systém trénovaný překlad novinových textů pro WMT18. Kvantitativní výsledky ukazují, že systém CUNI Transformer je již mnohem robustnější základní model založený na LSTM, který poskytli organizátoři soutěže. Kvalitu překladu našeho modelu jsme dále vylepšili vyladěním na zašuměných datech, která byla poskytnuta k soutěži.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our submission to the WMT19 Robustness Task. Our baseline system is the CUNI Transformer system trained for the WMT18 shared task on News Translation. Quantitative results show that the CUNI Transformer system is already far more robust to noisy input than the LSTM-based baseline provided by the task organizers. We further improved the performance of our model by fine-tuning on the in-domain noisy data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tradičně se většina úloh zpracování přirozeného jazyka řeší výhradně uvnitř jazyka, kdy modely spoléhají na distribuční vlastnosti slov. Hluboké učení se svojí schopností učit se vhodné reprezentace vstupních dat umožňuje využití více informací tím, že trénovací signál nepochází pouze z jazyka, ale o i z obrazové modality. Jednou z úloh, které se pokoušejí využít vizuální informace, je multimodální strojový překlad: překlad popisků obrázků, kdy je stále k dispozici původní obrázek, který lze využít jako vstup pro překladač. Tato práce shrnuje metody společného zpracovávání jazykových dat a fotografií s využitím hlubokého učení. Uvádíme přehled metod, které se využívají k řešení multimodálního strojového překladu a popisujeme náš původní příspěvek k řešení této úlohy. Představujeme metody kombinování více vstupů z potenciálně různých modalit v modelech sekvenčního učení založených na rekurentních neuronových sítích a neuronových sítí s mechanismem sebepozornosti. Uvádíme výsledky, kterých jsme dosáhli při řešení multimodálního strojového překladu a dalších úloh souvisejících se strojovým překladem. Na závěr analyzujeme, jak multimodalita ovlivňuje sémantické vlastnosti větných reprezentací, které v sítích vznikají, a jak sémantické vlastnosti reprezentací souvisí s kvalitou překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Traditionally, most natural language processing tasks are solved within the language, relying on distributional properties of words. Representation learning abilities of deep learning recently allowed using additional information source by grounding the representations in the visual modality. One of the tasks that attempt to exploit the visual information is multimodal machine translation: translation of image captions when having access to the original image. The thesis summarizes joint processing of language and real-world images using deep learning. It gives an overview of the state of the art in multimodal machine translation and describes our original contribution to solving this task. We introduce methods of combining multiple inputs of possibly different modalities in recurrent and self-attentive sequence-to-sequence models and show results on multimodal machine
translation and other tasks related to machine translation. Finally, we analyze how the multimodality influences the semantic properties of the sentence representation learned by the networks and how that relates to translation quality</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Většina úloh zpracování přirozeného jazyka se tradičně řeší v rámci jazyka a spoléhá se na distribuční vlastnosti slov. Schopnost modelů hlubokého učení učit se vhodnou reprezentaci vstupních dat nedávno umožnila využít také obrazovou informaci. Jedna z úloh, které se pokoušejí využít vizuální informaci, je multimodální strojový překlad, tj. překlad popisků obrázků, který má k původnímu obrázku.

Přednáška shrnuje metody kombinování více vstupů s možnou různou modalitou v model využívajích rekurentní neuronové sítě a modelu Tranformer a ukáže výsledky multimodálního a vícezdrojového strojového překladu. Nakonec probereme, jak multimodalita ovlivňuje sémantické vlastnosti reprezentace, které se sítě naučily, a jak to souvisí s kvalitou překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Traditionally, most NLP tasks are solved within the language, relying on distributional properties of words. Representation learning abilities of deep learning recently allowed using additional information source by grounding the representations in the visual modality. One of the tasks that attempt to exploit the visual information is multimodal machine translation, translation of image captions when having access to the original image.

The talk will summarize methods of combining multiple inputs of possibly different modality in recurrent and Transformer sequence-to-sequence models and show results on multimodal and multi-source machine translation. Finally, we will discuss how the multimodality influences the semantic properties of the representation learned by the networks and how does that relate to the translation quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Filtry konvolučních neuronových sítí používaných v počítačovém vidění se často vizualizjí jako malé čtevrcové obrázky, které maximalizují odezvu filtru. V tomto abstraktu používáme stejný postup při interpretaci váhových matic v jednoduchých architekturách pro úkoly zpracování přirozeného jazyka. Intepretujeme konvoluční neuronovou síť pro klasifikaci sentimentu jako slovní pravidla. Pomocí těchto pravidel jsme schopni rokonstruovat fungování původního modelu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Filters of convolutional networks used in computer vision are often visualized as image patches that maximize the response of the filter. We use the same approach to interpret weight matrices in simple architectures for natural language processing tasks. We interpret a convolutional network for sentiment classification as word-based rules. Using the rule, we recover the performance of the original model.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek populárně vysvětluje základy fungování umělých neuronových sítí a popisuje, jak fungují modely pro neuronový strojový překlad. Dále ukazuje problémy strojového překladu jako třeba genderové stereotypy, které modely vykazují.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article explains the basics of artificial neural networks and describes model for neural machine translation. Further, it discusses problem of the current models such as handling gender stereotypes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vícejazyčný BERT (mBERT) poskytuje větné reprezentace pro 104 jazyků, které jsou užitečné pro mnoho vícejazyčných úloh. Předchozí práce zkoumala mnohojazyčnost mBERTu s využitím nulového transferového učení na morfologických a syntaktických úkolech. Místo toho se soustředíme na sémantické vlastnosti mBERTu. Ukazujeme, že reprezentace mBERT mohou být rozděleny na jazykově specifickou složku a jazykově neutrální složku a že jazykově neutrální složka je dostatečně obecná, pokud jde o modelování sémantiky, aby umožnila vysoce přesné zarovnání slov a vyhledávání vět, ale zatím není dostatečně dobrá pro obtížnější úkol odhadu kvality MT. Naše práce přináší zajímavé výzvy, které musí být vyřešeny, aby bylo možné sestavit lepší jazykově neutrální reprezentace, zejména u úkolů vyžadujících jazykový přenos sémantiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Multilingual BERT (mBERT) provides sentence representations for 104 languages, which are useful for many multi-lingual tasks. Previous work probed the cross-linguality of mBERT using zero-shot transfer learning on morphological and syntactic tasks. We instead focus on the semantic properties of mBERT. We show that mBERT representations can be split into a language-specific component and a language-neutral component, and that the language-neutral component is sufficiently general in terms of modeling semantics to allow high-accuracy word-alignment and sentence retrieval but is not yet good enough for the more difficult task of MT quality estimation. Our work presents interesting challenges which must be solved to build better language-neutral representations, particularly for tasks requiring linguistic transfer of semantics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Z nedávné literatury vyplývá, že jazykové modelování na velkých datech poskytuje vynikající opakovaně použitelné větné reprezentace. Bylo rovněž prokázáno, že vizuální informace sloužit jako jeden ze způsobů ukotvení větných reprezentací. V tomto článku představujeme metastudii hodnotící kvalitu reprezentace modelů, kde trénovací signál pochází z různých modalit: jazykového modelování, předvídání reprezentace obrázku a textového a multimodálního strojového překladu. Hodnotíme textové a vizuální vlastnosti větných reprezentací na úlohách vyhledávání obrázků a sématické podobnosti textů. Naše experimenty odhalují, že na středně velkých datasetech poskytuje větný protějšek v cílovém jazyce nebo vizuální modalitě mnohem silnější trénovací signál pro reprezentaci vět než jazykové modelování. Důležité je, že zatímco modely Transformer dosahují vyšší kvality strojového překladu, reprezentace z modelů založených na rekurentních neuronové sítí dosahují výrazně lepších výsledků při hodnocení sématické relevance.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Recent literature shows that large-scale language modeling provides excellent reusable sentence representations with both recurrent and self-attentive architectures. However, there has been less clarity on the commonalities and differences in the representational properties induced by the two architectures. It also has been shown that visual information serves as one of the means for grounding sentence representations. In this paper, we present a meta-study assessing the representational quality of models where the training signal is obtained from different modalities, in particular, language modeling, image features prediction, and both textual and multimodal machine translation. We evaluate textual and visual features of sentence representations obtained using predominant approaches on image retrieval and semantic textual similarity. Our experiments reveal that on moderate-sized datasets, a sentence counterpart in a target language or visual modality provides much stronger training signal for sentence representation than language modeling. Importantly, we observe that while the Transformer models achieve superior machine translation quality, representations from the recurrent neural network based models perform significantly better over tasks focused on semantic relevance.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Lexikální desambiguace je úlohu, při které má být zvolen ten význam slova, který je v daném kontextu relevantní. V posledních letech můžeme pozorovat úspěšnou aplikaci vektorových reprezentací slov napříč různými úlohami v oblasti zpracování přirozeného jazyka. Vzhledem ke schopnosti těchto vektorových reprezentací odrážet distribuční sémantiku byla v nedávné době věnovaná pozornost i možnosti využití pro lexikální desambiguaci. V tomto článku navrhujeme novou neřízenou metodu pro lexikální desambiguace v jednom jazyce s využitím vektorových reprezentací natrénovaných pro jiný jazyk a překladového slovníku. V našich experimentech byly pro lexikální desambiguaci perských slov využity vektorové reprezentace anglických překladů slov z blízkého kontextu. Každý možný překlad polysémního slova je porovnán s vektorovými reprezentacemi okolních slov, z toho je vygenerováno podobnostní skóre, přičemž překladový ekvivalent s nejvyšším skóre reprezentuje zvolený význam. Tato metoda vyžaduje pouze neznačkovaný korpus a překladový slovník. Úspěšnost našeho přístupu je ilustrovaná na malém vzorku ručně desambiguovaných dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Word sense disambiguation is the task of assigning the correct sense of a polysemous word in the context in which it appears. In recent years, word embeddings have been applied successfully to many NLP tasks. Thanks to their ability to capture distributional semantics, more recent attention have been focused on utilizing word embeddings to disambiguate words.
In this paper, a novel unsupervised method is proposed to disambiguate words from the first language by deploying a trained word embeddings model of the second language using only a bilingual dictionary. While the translated words are useful clues for the disambiguation process, the main idea of this work is to use the information provided by English-translated surrounding words to disambiguate Persian words using trained English word2vec; well-known word embeddings model. Each translation of the polysemous word is compared against word embeddings of translated surrounding words to calculate word similarity scores and the most similar word to vectors of translated surrounding words is selected as the correct translation. This method only requires a raw corpus and a bilingual dictionary to disambiguate the word under question. The experimental results on a manually-created test dataset demonstrate the accuracy of the proposed method.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>ílem sdíleného úkolu o automatickém rozlišení mezer v ruštině (AGRR2019) v roce 2019 je boj proti netriviálnímu lingvistickému jevu, který se vyskytuje v koordinovaných strukturách, a eliminuje opakovaný predikát, obvykle
z druhé věty.
V tomto článku definujeme metriku úkolů a hodnocení, poskytujeme podrobné informace
informace o přípravě údajů, schématech anotace a metodice,
analyzovat výsledky a popsat různé přístupy účastníka
řešení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The 2019 Shared Task on Automatic Gapping Resolution for Russian (AGRR2019) aims to tackle non-trivial linguistic phenomenon, gapping, that occurs in coordinated structures and elides a repeated predicate, typically
from the second clause.
In this paper, we define the task and evaluation metrics, provide detailed
information on data preparation, annotation schemes and methodology,
analyze the results and describe different approaches of the participating
solutions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prakticky všechny metriky a techniky hodnocení strojového překladu, automatické i manuální, mají svá vlastní nebezpečí. Některá nebezpečí lze považovat za podjatá, pokud existují systémy MT, které mohou (nespravedlivě) těžit z daných aspektů hodnocení. Ve své prezentaci se zaměřuji na následující aspekty: hodnocení na úrovni vět vs. dokumentu (document-level vs. document-aware evaluation), source-based vs. reference-based, přímé hodnocení vs. hodnocení založené na srovnávání, plynulost vs. přesnost. Zabývám se také aspektem překladštiny (translationese) a rodným jazykem překladatelů a hodnotitelů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Virtually all MT evaluation metrics and techniques, both automatic and manual, have their own perils.
Some of the perils can be considered biases if there are MT systems which can (unfairly) benefit from a given evaluation aspects. In my presentation, I focus on the following aspects of evaluations: sentence-level vs. document-level vs.
document-aware, source-based vs. reference-based, direct assessment vs. comparison-based, fluency-biased vs. adequacy-biased. I also discuss the aspect of translationese and native target/source-language translators and evaluators.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje CUNI NMT systémy pro překlad z angličtiny do češtiny zaslané na WMT 2019 News Translation Shared Task. Systémy jsou založené na architektuře Transformer a její implementaci ve frameworcích Tensor2Tensor (T2T) a Marian. Snahou bylo zlepšit adekvátnost a koherenci přeložených dokumentů rozšířením kontextu na zdrojové a cílové straně. Namísto překladu izolovaných vět překládají popisované systémy potenciálně překrývající se vícevětné segmenty. V případě T2T implementace dosahuje takto trénovaný systém oproti systému trénovaném na izolovaných větách vylepšení +0.6 BLEU (p < 0.05). Poloautomatická analýza lexikální koherence odhalila jen málo příkladů, kde systém s větším kontext opravil chybu systému překládajíchího izolované věty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe our NMT systems submitted to the WMT19 shared task in English→Czech news translation. Our systems are based on the Transformer model implemented in either Tensor2Tensor (T2T) or Marian framework. We aimed at improving the adequacy and coherence of translated documents by enlarging the context of the source and target. Instead of translating each sentence independently, we split the document into possibly overlapping multi-sentence segments. In case of the T2T implementation, this “document-level”-trained system achieves a +0.6 BLEU improvement (p < 0.05) relative to the same system applied on isolated sentences. To assess the potential effect document-level models might have on lexical coherence, we performed a semi-automatic analysis,  which revealed only a few sentences improved in this aspect. Thus, we cannot draw any conclusions from this week evidence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme naše čtyři systémy neuronového strojového překladu (NMT), které jsme odeslali do shared tasku IWSLT19 pro anglicko-český překlad  TED Talks. Cílem této studie je porozumět interakcím mezi NMT na úrovni dokumentů a doménovou adaptací. Všechny naše systémy jsou založeny na modelu Transformer implementovaném ve frameworku Tensor2Tensor. Dva ze systémů slouží jako baseline a nejsou přizpůsobeny doméně TED Talks: SENTBASE je trénován na jednotlivých větách, DOCBASE na vícevětných (document-level) sekvencích. Další dva předložené systémy jsou přizpůsobeny doméně TED Talks: SENTFINE je adaptován na jednotlivých větách, DOCFINE na vícevětných sekvencích. Představujeme jak automatické metrické hodnocení, tak manuální analýzu kvality překladu se zaměřením na rozdíly mezi těmito čtyřmi systémy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe our four NMT systems submitted to the IWSLT19 shared task in English→Czech text-to-text translation of TED talks. The goal of this study is to understand the interactions between document-level NMT and domain adaptation. All our systems are based on the Transformer model implemented in the Tensor2Tensor framework. Two of the systems serve as baselines, which are not adapted to the TED talks domain: SENTBASE is trained on single sentences, DOCBASE on multi-sentence (document-level) sequences. The other two submitted systems are adapted to TED talks: SENTFINE is fine-tuned on single sentences, DOCFINE is fine-tuned on multi-sentence sequences. We present both automatic-metrics evaluation and manual analysis of the translation quality, focusing on the differences between the four systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce představuje náš probíhající výzkum předtrénování bez dohledu v oblasti neuronového strojového překladu (NMT). Naše metoda inicializuje váhy enkodéru a dekodéru pomocí dvou jazykových modelů, které jsou trénovány na jednojazyčných datech. Celý model pak dolaďujeme na paralelních datech s pomocí elastické konsolidace vah (EWC), abychom zabránili zapomenutí původní úlohy jazykového modelování. Srovnáváme regularizaci EWC s předchozí prací, která se zaměřuje na regularizaci s pomocí optimalizačních cílů jazykového modelování.

Pozitivním výsledkem je, že použitím EWC s dekodérem dosáhneme podobných hodnot BLEU jako předchozí práce. Model však konverguje 2-3krát rychleji a nevyžaduje původní jednojazyčná data během dolaďování.

Oproti tomu je EWC regularizace méně účinná, pokud spolu původní a navazující úloha úzce nesouvisí. Ukazujeme, že inicializace obousměrného NMT enkodéru pomocí jednosměrného jazykového modelu a nucení modelu zapamatovat si původní úlohu modelování jazyka zleva doprava omezuje schopnosti enkodéru naučit se oboustranný kontext.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This work presents our ongoing research of unsupervised pretraining in neural machine translation (NMT). In our method, we initialize the weights of the encoder and decoder with two language models that are trained with monolingual data and then fine-tune the model on parallel data using Elastic Weight Consolidation (EWC) to avoid forgetting of the original language modeling tasks. We compare the regularization by EWC with the previous work that focuses on regularization by language modeling objectives.

The positive result is that using EWC with the decoder achieves BLEU scores similar to the previous work. However, the model converges 2-3 times faster and does not require the original unlabeled training data during the finetuning stage.

In contrast, the regularization using EWC is less effective if the original and new tasks are not closely related. We show that initializing the bidirectional NMT encoder with a left-toright language model and forcing the model to remember the original left-to-right language modeling task limits the learning capacity of the encoder for the whole bidirectional context.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Hluboké učení přináší průlomy v mnoha podoborech strojového vidění, včetně rozpoznávání notopisu (Optical Music Recognition, OMR), kde bylo zaznamenáno množství pokroků v detekci notopisných symbolů pomocí obecných modelů hlubokého učení. Zatím však byly všechny tyto pokroky měřené pouze na některém z dostupných datasetů a využívaly různá evaluační kritéria, takže lze jen obtížně kvantifikovat nový stav poznání dosažený těmito metodami a porovnat jejich výhody a nevýhody v doméně hudební notace. V tomto článku prezentujeme  základní výsledky detekce symbolů hudební notace pomocí tří různých obecných detekčních modelů hlubokého učení, a to napříč třemi typologicky různými datasety, změřené standardizovaným postupem. Experimentální výsledky potvrzují, že přímá detekce notačních objektů pomocí hlubokého učení má velmi slibné výsledky, nicméně zároveň ilustruje omezení generických detektorů na této doméně. Kvalitativní porovnání poté naznačuje, jak detekci zlepšit: jak na základě vlastností detekčních modelů, tak na základě vlastností datasetů. Dle našich poznatků je toto poprvé, kdy je vícero špičkových metod detekce notačních objektů přímo porovnáváno. Doufáme, že tato práce bude sloužit jako reference pro měření dalších pokroků v OMR.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Deep learning is bringing breakthroughs to many computer vision subfields including Optical Music Recognition (OMR), which has seen a series of improvements to musical symbol detection achieved by using generic deep learning models. However, so far, each such proposal has been based on a specific dataset and different evaluation criteria, which made it difficult to quantify the new deep learning-based state-of-the-art and assess the relative merits of these detection models on music scores. In this paper, a baseline for general detection of musical symbols with deep learning is presented. We consider three datasets of heterogeneous typology but with the same annotation format, three neural models of different nature, and establish their performance in terms of a common evaluation standard. The experimental results confirm that the direct music object detection with deep learning is indeed promising, but at the same time illustrates some of the domain-specific shortcomings of the general detectors. A qualitative comparison then suggests avenues for OMR improvement, based both on properties of the detection model and how the datasets are defined. To the best of our knowledge, this is the first time that competing music object detection systems from the machine learning paradigm are directly compared to each other. We hope that this work will serve as a reference to measure the progress of future developments of OMR in music object detection.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Naším cílem je anotace korpusu CzeSL podle gramatiky jazyka, se kterou pracuje nerodilý mluvčí, a ne podle standartní gramatiky. Tento přístup přináší několik problémů. Za prvé nemáme dostatek dat na to, abychom analyzovali gramatiky jednotlivých autorů. Za druhé v jazyce nerodilých mluvčí je podstatně více složitějších jevů  než v jazyce rodilých mluvčí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Our goal has been to annotate the CzeSL corpus according to the non-native grammar in the mind of the author, not according to the standard grammar. However, this brings many challenges. First, we do not have enough data to get reliable insights into the grammar of each author. Second, many phenomena are far more complicated than they are in native languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CzeSL je žákovský korpus textů nerodilých mluvčích češtiny, který je cenným zdrojem unikátních znalostí o jazyce nerodilých mluvčích. Pomáhá pedagogům a výzkumníkům v oblasti osvojování druhého jazyka. V našem projektu se zaměřujeme na syntaktickou anotaci textů z CzeSL v rámci formalismu Universal Dependencies. Pokud je nám známo, jedná se o první pokus takové anotace pro jazyk s bohatou flexí. Naší ambicí je anotovat dle gramatiky, kterou používá nerodilý mluvčí, namísto standardní gramatiky. Relativné malé množství dat přináší celou řadu otázek.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CzeSL is a learner corpus of texts produced by non-native speakers of Czech. Such corpora area great source of information about specific features of learners’ language, helping language teachers and researchers in the area of second language acquisition. In our project, we have focused on syntactic annotation of the non-native text within the framework of Universal Dependencies. As far as we know, this is a first project annotating a richly inflectional non-native language. Our ideal goal has been to annotate according to the non-native grammar in the mind of the author, not according to the standard grammar. However, this brings many challenges. First, we do not have enough data to get reliable insights into the grammar of each author. Second, many phenomena are far more complicated than they are in native languages.  We believe that the most important result of this project is not the actual annotation, but the guidelines and principles that can be used as a basis for other non-native languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje průběh 44. ročníku Olympiády v českém jazyce. Představuje celkový průběh soutěže, soutěžní úkoly a jejich řešení, komentuje řešení účastníků soutěže a přináší jména vítězů celostátního kola.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article describes the course of the 44th year of the Olympiad in the Czech Language, presenting its general settings as well as the tasks, their solutions, the approaches of the participants and the names of the winners.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Diskurzní konektory představujeme z pohledu reference (tj. přítomnosti anaforického nebo kataforického prvku). Diskurzní konektory dělíme na: 1) konektory bez interní reference (např. "a", "ale", "nebo", "jestli", "však", "pak") a 2) konektory s interní referencí, která může být fakultativní (např. "výsledkem je" vs. "výsledkem toho je") nebo obligatorní – srov. již gramatikalizované konektory (tj. primární konektory typu "potom", "proto", "tímto") i dosud ne zcela gramatikalizované konektory (tj. sekundární konektory typu "kvůli tomu", "z tohoto důvodu").</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We examine discourse connectives from the perspective of reference (i.e. a presence of an anaphoric/cataphoric element). We introduce a division of connectives into: i) connectives without an inherent (internal) reference (e.g. "and", "but", "or", "if", "however", "then"), and ii) connectives with an inherent (internal) reference that is either optional (e.g. "as a result" vs. "as a result of this"), or obligatory – cf. already grammaticalized connectives (i.e. primary connectives like "thereafter", "therefore" or "thereby") vs. not yet grammaticalized connectives (i.e. secondary connectives like "because of this", "for this reason").</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá analýzou koherence textu v češtině a v němčině. Zaměřuje se na tzv. anaforické diskurzní konektory, které se významně podílejí na stavbě textu. Příspěvek analyzuje roli těchto konektorů v celkové komunikační kompetenci rodilých i nerodilých mluvčích češtiny a němčiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a contrastive study of text coherence in Czech and German. Specifically, we focus on the discourse-anaphoric devices (called anaphoric connectives) contributing to text coherence and we analyze their role in the overall communicative competence of both native and non-native speakers of Czech and German.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Koherentní text (diskurz) se skládá ze vzájemně propojených vztahů, které mohou být vyjádřeny explicitními jazykovými prostředky. V našem článku se zabýváme jedním typem těchto prostředků, diskurzními konektory. Přebíráme přitom dělení konektivních prostředků na primární konektory, sekundární konektory a volné konektivní fráze. Na základě dokladů z češtiny, angličtiny, francouzštiny a němčiny rozvíjíme definice těchto skupin, především s ohledem na mezijazykové rozdíly. U primárních a sekundárních konektorů navrhujeme způsob jejich zachycení ve slovnících. Představujeme konkrétní návrh vícejazyčného slovníku konektorů, do kterého je v současné době zapojeno pět různých jazyků. Další jazyky budou postupně přidávány.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Starting from the perspective that discourse structure arises from the presence of coherence relations, we provide a map of linguistic discourse structuring devices (DRDs), and then focus on those found  in  written  text: connectives. To subdivide  this  class  further, we  follow  the  recent idea of structuring the set of connectives by differentiating between primary and secondary connectives, on the one hand, and free connecting phrases, on the other. Considering examples from Czech, English, French and German, we develop definitions of these groups, with attention to certain cross-linguistic differences. For primary and secondary connectives, we propose that their behavior can be described to a large extent by declarative lexicons, and we demonstrate a concrete proposal which has been applied to five languages, with others currently being added in ongoing work. The lexical representations can be useful both for humans (theoretical investigations, transfer to other languages) and for machines (automatic discourse parsing and generation).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>This paper presents a case study in translating short image captions of the Visual Genome dataset from English into Hindi using out-of-domain data sets of varying size. We experiment with three NMT models: the shallow and deep sequence-to-sequence and the Transformer model as implemented in Marian toolkit. Phrase-based Moses serves as the baseline. The results indicate that the Transformer model outperforms others in the large data setting in a number of automatic metrics and manual evaluation, and it also produces the fewest truncated sentences. Transformer training is however very sensitive to the hyperparameters, so it requires more experimenting.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a case study in translating short image captions of the Visual Genome dataset from English into Hindi using out-of-domain data sets of varying size. We experiment with three NMT models: the shallow and deep sequence-to-sequence and the Transformer model as implemented in Marian toolkit. Phrase-based Moses serves as the baseline. The results indicate that the Transformer model outperforms others in the large data setting in a number of automatic metrics and manual evaluation, and it also produces the fewest truncated sentences. Transformer training is however very sensitive to the hyperparameters, so it requires more experimenting. The deep sequence-to-sequence model produced more flawless outputs in the small data setting and it was generally more stable, at the cost of more training iterations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V současných neuronových systémech pro strojový překlad textů přirozeného jazyka (NMT) se morfologicky příbuzná slova zpracovávají jejich rozdělením na podslovní jednotky takovým způsobem, aby se slovník jednotek vešel do limitů daných zvoleným NMT modelem a do paměti grafické karty. V tomto článku srovnáváme dva nejobvyklejší, nelingvistické, způsoby vytváření podslovních jednotek (BPE a STE, metody implementované v nástroji Tensor2Tensor) se dvěma lingvisticky motivovanými způsoby: Nástrojem Morfessor a námi vyvinutou metodou založenou na derivačních vztazích. Naše experimenty s překladem z němčiny do češtiny, morfologicky bohatých jazyků, ukazují, že prozatím mají lepší výsledky nelingvistické metody. K tomu identifikujeme důležitý rozdíl mezi BPE a STE a ukazujeme, že jednoduché předzpracování textu před BPE výrazně zvyšuje kvalitu překladu vyhodnocovanou automatickými metrikami.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The state of the art of handling rich morphology in neural machine translation (NMT) is to break word forms into subword units, so that the overall vocabulary size of these units fits the practical limits given by the NMT model and GPU memory capacity. In this paper, we compare two common but linguistically uninformed methods of subword construction (BPE and STE, the method implemented in Tensor2Tensor toolkit) and two linguistically-motivated methods: Morfessor and one novel method, based on a derivational dictionary. Our experiments with German-to-Czech translation, both morphologically rich, document that so far, the non-motivated methods perform better. Furthermore, we identify a critical difference between BPE and STE and show a simple pre-processing step for BPE that considerably increases translation quality as evaluated by automatic measures.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce porovnává kvalitu a rychlost systémů pro neuronový strojový překlad (Tensor2Tensor, Marian, Nematus, Neural Monkey, OpenNMT) na dvou srovnatelných překladových úlohách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This work is a comparison of a quality and speed of  NMT systems Tensor2Tensor, Marian, Nematus, Neural Monkey and OpenNMT on two comparable translation tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Extrakce syntaktických struktur z self-attentions NMT enkodéru.
Extrakce syntaktických struktur z self-attentions NMT enkodéru.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Extracting Syntactic Trees from NMT Encoder Self-Attentions.
Extracting Syntactic Trees from NMT Encoder Self-Attentions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Extrakce syntaktických struktur z self-attentions enkodéru Transformeru. Extrakce syntaktických struktur z self-attentions enkodéru Transformeru</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Extracting Syntactic Trees from Transformer Encoder Self-Attentions. Extracting Syntactic Trees from Transformer Encoder Self-Attentions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato prezentace shrnuje závěry týmu Grounded Sequence-to-Sequence Transduction na pátém Fred Jelinek Memorial Summer Workshop. V prezentovaných modelech kombinujeme psaný text, mluvené slovo a objekty a akce v pozorované . Když vidíme, že člověk krájí červené předměty a umístí je na hnědý povrch, je pravděpodobné, že spíše vysvětluje, jak vyrobit sendvič, než jak změnit pneumatiku. A- mohli bychom se dozvědět, že červené předměty se nazývají "rajčata". Náš tým pracoval na vývoji metod, které využívají multimodální informace pro zpracování a analýzu videoklipů, ve třech hlavních lohách: rozpoznávání řeči, sumarizace videa a textu a překlad do jiného jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This presentation summarizes output of the Grounded Sequence-to-Sequence Transduction team at the Fifth Fred Jelinek Memorial Summer Workshop. We combine written, spoken, and seen objects and actions in how-to videos: if we see a person slicing round, red objects and putting them on a brown surface, it is more likely that she or he is explaining how to make a sandwich than how to change a car tire. And we might learn that the red objects are called “tomatoes”. Our team  develop methods that exploit multimodality to process and analyze videos to accomplish three main tasks: speech captioning, video-to-text summarization and translation into a different language. These tasks are diverse but not unrelated. Therefore, we model them using a multi-task sequence-to-sequence learning framework where these (and other, auxiliary) tasks can benefit from shared representations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Modely strojového učení přinášejí slibné výsledky v mnoha oborech včetně zpracování přirozeného jazyka. Tyto modely jsou nicméně náchylné k adversálním příkladům. Jedná se o uměle vytvořené příklady s dvěma důležitými vlastnostmi: podobají se skutečným tréninkovým příkladům, ale matou již natrénovaný model. Tento článek zkoumá účinek používání adversálních příkladů při tréninku rekurentních neuronových sítí, jejichž vstup je ve formě slovních či znakových embeddingů. Účinky jsou studovány na kompilaci osmi datasetů. Na základě experimentů a charakteristik datasetů dospíváme k závěru, že použití adversálních příkladů pro úkoly zpracování přirozeného jazyka, které jsou modelovány pomocí rekurentních neuronových sítí, přináší efekt regularizace a umožňuje trénovat modely s větším počtem parametrů bez overfittingu. Na závěr popisujeme, které kombinace datasetů a nastavení modelů by mohly mít z adversálního tréninku největší prospěch.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Machine learning models have been providing promising results in many fields including natural language processing. These models are, nevertheless, prone to adversarial examples. These are artificially constructed examples which evince two main features: they resemble the real training data but they deceive already trained model. This paper investigates the effect of using adversarial examples during the training of recurrent neural networks whose text input is in the form of a sequence of word/character embeddings. The effects are studied on a compilation of eight NLP datasets whose interface was unified for quick experimenting. Based on the experiments and the dataset characteristics, we conclude that using the adversarial examples for NLP tasks that are modeled by recurrent neural networks provides a regularization effect and enables the training of models with greater number of parameters without overfitting. In addition, we discuss which combinations of datasets and model settings might benefit from the adversarial training the most.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek přináší výsledky soutěže ve vyhodnocování strojového překladu WMT18 Metrics Shared Task. Vyhodnocujeme 10 metrik získaných od 8 týmů a navíc 8 standardních metrik.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the
WMT18
Metrics Shared Task.  We
asked participants of this task to score
the outputs of the MT systems in-
volved in the
WMT18
News Transla-
tion Task with automatic metrics. We
collected scores of 10 metrics and 8 re-
search groups. In addition to that, we
computed scores of 8 standard met-
rics (BLEU, SentBLEU, chrF, NIST,
WER, PER, TER and CDER) as base-
lines. The collected scores were eval-
uated in terms of system-level corre-
lation (how well each metric’s scores
correlate with
WMT18
official man-
ual ranking of systems) and in terms
of segment-level correlation (how often
a metric agrees with humans in judging
the quality of a particular sentence rel-
ative to alternate outputs). This year,
we employ a single kind of manual eval-
uation: direct assessment (DA).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvěk se zabývá otázkou, jak může syntaktická vlastnost sloves, jako je reciprocita, být promítnuta do slovníka a přispět k dalšímu popisu a klasifikaci sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we deal with reciprocity, its representation in a lexicon and its possible contribution to the description of verbs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Na pozadí dvou skupin sloves, lexikálních a syntaktických reciprok v češtině, diskutujeme otázky homonymie a kombinovatelnosti reflexivity a reciprocity. Ukazujeme, že zdrojem homonymie i možnosti kombinace reflexivity a reciprocity je reflexivní zájmeno a že klíčovou úlohu sehrává jeho povrchověsyntaktická pozice.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this article, we focus on two groups of Czech verbs, lexical and syntactic reciprocals. We provide an analysis of their syntactic properties with respect to reciprocity and reflexivity, their possible ambiguity and combination. We demonstrate that it is the reflexive pronoun and its surface expression that play a key role in the studied phenomena.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku shrnujeme teoretickou analýzu syntaktického chování českých komplexních predikátů s kategoriálním slovesem, která byla ověřena v anotaci 1500 komplexních predikátů. Tato anotace je součátí slovníku VALLEX, verze 3.5.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we summarize a theoretical analysis of syntactic behavior of Czech light verb constructions and their verification in the data annotation of 1,500 Czech light verbs constructions, which have been integrated in VALLEX, version 3.5.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška se zabývala tzv. komplementovým systémem v češtině, a to zejm. vztahem mezi závislými obsahovými klauzemi a infinitivy sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk dealt with the so-called complement system in Czech as it manifests in the data from the substitutability of dependent content clauses with infinitives.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme první volně dostupný závislostní korpus sanskrtu. Je založen na textech z Paňčatantry, starověké indické sbírky bajek. Zvolili jsme formalismus Universal Dependencies, který v současnosti představuje faktickou normu mezijazykově srovnatelné morfologické a syntaktické anotace. V článku probíráme obtíže se segmentací textu na slova, představujeme inventář morfologických kategorií, jakož i některé syntaktické konstrukce, které jsou zajímavé ve světle pravidel Universal Dependencies. Dále popisujeme experiment s automatickou syntaktickou analýzou (parsingem).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the first freely available dependency treebank of Sanskrit. It is based on text from Panchatantra, an ancient Indian collection of fables. The annotation scheme we chose is that of Universal Dependencies, a current de-facto standard for cross-linguistically comparable morphological and syntactic annotation. In the present paper,
we discuss word segmentation issues, morphological inventory and certain interesting
syntactic constructions in the light of the Universal Dependencies guidelines. We also present an initial parsing experiment.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje mezinárodní projekt, který připravil univerzální terminologii a anotační postup pro slovesné víceslovné výrazy. Výstupem je korpus zveřejněný pod otevřenou licencí (18 jazyků, 5,4 milionů slov, 62 tisíc víceslovných výrazů) a univerzální anotační instrukce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Multiword expressions (MWEs) are known as a “pain in the neck” due to their idiosyncratic behaviour. While some categories of MWEs have been largely studied, verbal MWEs (VMWEs) such as to take a walk, to break one’s heart or to turn off have been relatively rarely modelled. We describe an initiative meant to bring about substantial progress in understanding, modelling and processing VMWEs. In this joint effort carried out within a European research network we elaborated a universal terminology and annotation methodology for VMWEs. Its main outcomes, available under open licenses, are unified annotation guidelines, and a corpus of over 5.4 million words and 62 thousand annotated VMWEs in 18 languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje CUNI system na WAT 2018 pro překlad mezi angličtinou a Hindštinou. Náš systém využívá transfer learningu z anglicko-českého modelu. Využíváme technologii neuronového transformeru.
Náš systém začíná trénováním na jazykovém páru s mnoho paralelními daty, česko-anglickém, na který naváže jazykový pár s nedostatkem dat. Náš systém se umístil první podle lidského hodnocení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the CUNI submission to WAT 2018 for the English-Hindi translation task using a transfer 
learning techniques which has proven effective under low resource conditions. We have used the Transformer model and utilized an English-Czech parallel corpus as additional data source. Our simple transfer learning approach first trains a “parent” model for a high-resource 
language pair (English-Czech) and then continues the training on the low-resource (English-Hindi) pair by replacing
the training corpus. This setup improves the performance compared with the baseline and in combination with back-translation of Hindi monolingual data, it allowed us to win the English-Hindi task. The automatic scoring by BLEU did not correlate well with human judgments.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Bylo dokázáno, že přetrénování natrénovaných modelů vede k zlepšení strojového překladu. Existující metody potřebují, aby zkoumané jazykové páry byly lingvisticky podobné. My představujeme přístup, který nepotřebuje lingvistickou podobnost jazyků ani specifické trénovací kroky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Transfer learning has been proven as an effective
technique for neural machine translation
under low-resource conditions. Existing
methods require a common target language,
language relatedness, or specific training
tricks and regimes. We present a simple
transfer learning method, where we first train
a “parent” model for a high-resource language
pair and then continue the training on a lowresource
pair only by replacing the training
corpus. This “child” model performs significantly
better than the baseline trained for lowresource
pair only. We are the first to show
this for targeting different languages, and we
observe the improvements even for unrelated
languages with different alphabets.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zúčastnili jsme se překladové soutěže WMT 2018 v překladu novinových článků. Zúčastnili jsme se v jazykových párech Angličtina-Estonština, Angličtina-Finština a Angličtina-Čeština. Náš hlavní cíl byly jazyky s nedostatkem trénovacích dat, tedy překlad mezi angličtinou a estonštinou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We participated in the WMT 2018 shared
news translation task in three language
pairs: English-Estonian, English-Finnish, and
English-Czech. Our main focus was the lowresource
language pair of Estonian and English
for which we utilized Finnish parallel
data in a simple method. We first train a
“parent model” for the high-resource language
pair followed by adaptation on the related lowresource
language pair. This approach brings
a substantial performance boost over the baseline
system trained only on Estonian-English
parallel data. Our systems are based on the
Transformer architecture. For the English
to Czech translation, we have evaluated our
last year models of hybrid phrase-based approach
and neural machine translation mainly
for comparison purposes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme náš překladový systém pro překlad z Baskičtiny do Angličtiny. K jeho natrénování jsme využili metodu přetrénování jiného překladového modelu, v našem případě anglicko-českého.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our submission to the IWSLT18 Low Resource task focused on the translation from
Basque-to-English. Our submission is based on the current state-of-the-art self-attentive
neural network architecture, Transformer. We further improve this strong baseline by
exploiting available monolingual data using the back-translation technique. We also
present further improvements gained by a transfer learning, a technique that trains a model using a high-resource language pair (Czech-English) and then fine-tunes the model using the target low-resource language pair (Basque-English).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje dvoustupňovou metodu přepisu historických rukopisů. V této metodě používá první krok reprezentaci na stránce, což usnadňuje přepis dokumentu po stránce a řádku po řádku, zatímco druhý krok to převádí do textového formátu TEI / XML, aby zajistit, aby byl dokument plně prohledávatelný.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article describes a two-step method for transcribing historic manuscripts. In this method, the first step uses a page-based representation making it easy to transcribe the document page-by-page and line-by-line, while the second step converts this to the TEI/XML text-based format, in order to make sure the document becomes fully searchable.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kapitola prezentuje kvantitativní analýzu vybraných závislostních vztahů v češtině. Zavádíme pojem valenčního rámec jako lingvistické jednotky a zkoumáme jeho základní charakteristiky, zejména frekvenční charakteristiky a dále vztah mezi typem syntaktické funkce a množstvím různých valenčních rámců, jež tato funkce nese. Analýzu jsou priváděny na korpusu Czech Universal Dependency Treebank.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article presents a quantitative analysis of some syntactic dependency properties in Czech. A dependency frame is introduced as a linguistic unit and its characteristics are investigated. In particular, a ranked frequencies of dependency frames are observed and modelled and a relationship between particular syntactic functions and the number of dependency frames is examined. For the analysis, the Czech Universal Dependency Treebank is used.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme uživatelské rozhraní mezi českým valenčním lexikonem, PDT-Vallex [1] a KonText1 -
webová aplikace pro dotazování na korpusy dostupné v rámci projektu LINDAT / CLARIN. KonText
umožňuje vyhodnocování jednoduchých a komplexních dotazů, zobrazování výsledků jako shodných linek,
výpočet distribuce frekvence, výpočet asociačních opatření pro kolokace a dále
práce s jazykovými daty. Pro každé sloveso ve shodné lince umožňuje naše rozhraní zobrazit
informace o jeho valenčním rámci v samostatném okně, pokud existují odpovídající položky
PDT-Vallex, stejně jako seznam možných valenčních rámců pro toto konkrétní sloveso. Informace
týkající se slovesného rámu obsahuje lemma slovesa, prvky rámce se sémantickými rolami, slovník slovníku
popis a příklady z PDT-Vallex, Praha Dependency Treebank [2] a Praha
Česko-anglická závislostní stromová banka [3]. Informace jsou požadovány z REST-API z
valenční lexikon PDT-Vallex, který obsahuje přes 11000 valenčních rámců pro více než 7000 sloves
která se objevila v pražské Dependency Treebank, Praha Czech-English Dependency Treebank nebo
Praha závislost Treebank mluveného češtiny. Používáme vidlici aplikace KonText (vyvinutá
Ústavem českého národního korpusu), který byl dále rozšířen Ústavem
formální a aplikované lingvistiky, aby vyhovovaly potřebám projektu LINDAT / CLARIN. Plugin jsme
present poskytuje unikátní řešení pro český jazyk, které integruje valenční informace z
Český valenční lexikon s prostředky dotazování na pražskou Závislostní bankou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a user interface between the Czech valency lexicon, PDT-Vallex [1], and KonText1 –
a web application for querying corpora available within the LINDAT/CLARIN project. KonText
allows evaluation of simple and complex queries, displaying their results as concordance lines,
computing frequency distribution, calculating association measures for collocations and further
work with language data. For every verb in a concordance line, our interface allows to display
information concerning its valency frame in a separate window if corresponding entries exist in
PDT-Vallex, as well as a list of possible valency frames for that particular verb. Information
concerning verb frame comprises verb lemma, frame elements with semantic roles, vocabularystyle
description and examples from PDT-Vallex, Prague Dependency Treebank [2] and Prague
Czech-English Dependency Treebank [3]. The information is requested by REST-API from the
valency lexicon PDT-Vallex that contains over 11000 valency frames for more than 7000 verbs
which occurred in Prague Dependency Treebank, Prague Czech-English Dependency Treebank or
Prague Dependency Treebank of Spoken Czech. We use a fork of KonText application (developed
by the Institute of the Czech National Corpus) that has been further extended by the Institute
of Formal and Applied Linguistics to suit the needs of LINDAT/CLARIN project. The plugin we
present provides a unique solution for Czech language that integrates valency information from the
Czech valency lexicon with the means of querying Prague Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozbalení pomocí křížových značek je založeno na nahrazení jedné vrstvy anotací za jinou při zpracování dat v jednom jazyce. Nejčastěji není k dispozici ani rodný značkovač nebo syntaktický analyzátor závislostí používaný v (před) anotaci Gold stromové banky. Přístup přes křížové štítky umožňuje anotovat nové texty pomocí volně dostupných nástrojů nebo nástrojů optimalizovaných podle potřeb uživatele. Vyhodnocujeme robustnost analyzování ruské závislostí s použitím různých morfologických a syntaktických tagů ve vstupu a výstupu. Kvalitativní analýza chyb ukazuje, že křížová substituce tří morfologických značek a dvou syntaktických značek způsobuje pouze mírný pokles výkonu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Cross-tagset parsing is based on the substitution of one annotation layer for another while processing data within one language. As often as not, either the native tagger or the dependency parser used in (pre-)annotation of the Gold treebank is not available. The cross-tagset approach allows one to annotate new texts using freely available tools or tools optimized to user's needs. We evaluate the robustness of Russian dependency parsing using different morphological and syntactic tagsets in input and output. A qualitative analysis of errors shows that the cross-substitution of three morphological tagsets and two syntactic tagsets causes only a mild drop in performance.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku se zaměřujeme na syntaktickou anotaci a její konzistenci v korpusech Universal Dependencies (UD) pro ruštinu: SynTagRus, GSD, Taiga a PUD. Popisujeme tyto čtyři korpusy, jejich distinktivní rysy a vývoj.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we focus on syntactic annotation consistency within Universal Dependencies (UD) treebanks for Russian: UD_Russian-SynTagRus, UD_Russian-GSD, UD_Russian-Taiga, and UD_Russian-PUD. We describe the four treebanks, their distinctive features and development. In order to test and improve consistency within the treebanks, we reconsidered the experiments by Martínez Alonso and Zeman; our parsing experiments were conducted using a state-of-the-art parser that took part in the CoNLL 2017 Shared Task. We analyze error classes in functional and content relations and discuss a method to separate the errors induced by annotation inconsistency and those caused by syntactic complexity and other factors.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Umělé závislostní stromy v anotačním stylu Universal Dependencies v2, zaměřené na druh elipsy zvaný anglicky gapping (v UD odpovídá vztahu 'orphan'). Motivace a popis těchto dat je obsažen v Droganova et al., 2018 (LREC, Miyazaki, Japonsko).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Artificial dependency trees in the Universal Dependencies v2 style, focused on gapping (the 'orphan' relation in UD). For motivation and description of the data, see Droganova et al., 2018 (LREC, Miyazaki, Japan).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci se zaměřujeme na konkrétní jazykový jev, elipsu, a zkoumáme výstupy současných parserů, abychom zjistili jejich úspěšnost a typické chyby s ohledem na elipsy. K tomuto účelu jsme sebrali a zpracovali výstupy několika nejlepších parserů, které se zúčastnily společné úlohy CoNLL 2017. Oficiální vyhodnocovací software jsme rozšířili, aby bylo možné zjistit chyby v analýze elips. Protože studované struktury jsou poměrně vzácné, a tudíž není k dispozici dostatečné množství dat pro experimenty, popisujeme dále tvorbu nového datového zdroje, polouměle vytvořeného závislostního korpusu elips.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this work we focus on a particular linguistic phenomenon, ellipsis, and explore the latest parsers in order to learn about parsing accuracy and typical errors from the perspective of elliptical constructions. For this purpose we collected and processed outputs of several state-of-the art parsers that took part in the CoNLL 2017 Shared Task. We extended the official shared task evaluation software to obtain focused evaluation of elliptical constructions. Since the studied structures are comparatively rare, and consequently there is not enough data for experimentation, we further describe the creation of a new resource, a semi-artificially constructed treebank of ellipsis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme pokusy s několika přístupy k automatickému rozšíření trénovacích dat pro závislostní syntaktické analyzátory s využitím velkých webových korpusů. Jedna sada metod je obecná, inspiruje se samotrénováním a trojtrénováním a přidává nový algoritmus, který napodobuje strukturální složitost původního treebanku. Metody ve druhé sadě se více zaměřují na eliptické konstrukce. Pokusy vyhodnocujeme na 5 jazycích: češtině, angličtině, finštině, ruštině a slovenštině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We report on experiments with several approaches to automatically extending training data for dependency parsers, using large crawled web corpora. One set of methods is general, draws upon self-training and tri-training and adds a novel algorithm of mimicking the structural complexity of the original treebank. Methods from the other set are more focused on elliptical constructions. We provide evaluation on 5 languages: Czech, English, Finnish, Russian and Slovak.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek shrnuje diskusi skupiny aktivních výzkumníků rozpoznávání notopisu (Optical Music Recognition, OMR), která se konala v rámci 12th IAPR International Workshop on Graphics Recognition, a prezentuje její výstupy: OMR by mělo zpřesnit svou terminologickou a taxonomickou základnu, a komunita výzkumníků v oboru by měla intenzivněji spolupracovat jak mezi sebou, tak se zájemci o OMR.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This document summarizes the discussion of the interest group on Optical Music Recognition (OMR) that took place in the 12th IAPR International Workshop on Graphics Recognition, and presents the main conclusions drawn during the session: OMR should revisit how it describes itself, and the OMR community should intensify its collaboration both internally and with other stakeholders.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme LemmaTag, architekturu neuronové sítě, která společně generuje morfologické značky a lemmata pomocí obousměrných rekurentních neuronových sítí pomocí slovních a znakových embeddingů. Demonstrujeme, že oběma úkolům pomáhá sdílet enkodér, předvídat podtypy značek a používat předpovězené značky na vstup lemmatizátoru. Vyhodnocujeme náš model na několika jazycích se složitou morfologií, a překonáváme nejlepší známé výsledky jak morfologického značkování tak lemmatizace v češtině, němčině a arabštině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present LemmaTag, a featureless neural network architecture that jointly generates part-of-speech tags and lemmas for sentences by using bidirectional RNNs with character-level and word-level embeddings. We demonstrate that both tasks benefit from sharing the encoding part of the network, predicting tag subcategories, and using the tagger output as an input to the lemmatizer. We evaluate our model across several languages with complex morphology, which surpasses state-of-the-art accuracy in both part-of-speech tagging and lemmatization in Czech, German, and Arabic.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace výzkumného projektu a jeho výsledků je v kontextu workshopu příkladem využití sbírek orálněhistorických interview jako příkladů narativního vyjádření uprchlické zkušenosti během druhé světové války. Důraz je kladen na vzdělávací uplatnění úryvků z rozhovorů prostřednictvím informačních technologií ve výuce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In 2017/2018, I was working on a project „Collective memory as an interactional practice: The case of the Czech-Jewish experience in Switzerland during the World War II period”, carried out at the University of Fribourg in Switzerland. My research consisted of three successive stages with mutually interrelated aims:
(1) Analysis of oral history interviews with Holocaust survivors born in the former Czechoslovakia, who have spent time in Switzerland as refugees, asylum seekers or displaced persons during the World War II period. The oral history interviews were selected from the USC Shoah Foundation’s Visual History Archive. 
(2) Short clips of interviews were then incorporated into on-line educational material, accompanied by text and images. The resulting on-line lesson had the following structure: (I) The context (Why did people leave Czechoslovakia?); (II) The decision (How do the narrators reflect the decision to leave Czechoslovakia?); (III) The journey (How was the emigration practically accomplished?); (IV) The memory: (How is the migration retrospectively reflected by the narrators?).
(3) The on-line lesson was tested at five schools in Switzerland and in the Czech Republic between January and March 2018. The sessions were videotaped and analysed in detail: not only to evaluate and improve the developed activities, but also to explore the interactional specifics of the educational setting.
The research project provides an example of using oral history interviews as cases of narrative expression of the refugee experience during the World War II period. It explored one of the possible ways of utilizing large archives of oral histories for the transmission of historical knowledge on refugeedom, focusing on personal aspects. Moreover, it highlights several issues in educational use of archival interviews, such as the necessity of sufficient context for the students’ comprehension, and also the more general topics of incorporating digital technologies into classroom interaction.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zaměřil na analýzu situovaného jednání v rámci testovacích vyučovacích hodin, jež byly realizovány ve školním roce 2017/18 na třech gymnáziích v ČR. Během těchto hodin studenti pracovali v menších skupinách 2–3 osob s on-line materiálem složeným z textů, obrázků a klipů vyňatých z audiovizuálních orálněhistorických rozhovorů. Videonahrávky pořízené v průběhu hodin byly následně podrobeny detailní analýze vycházející z tradice etnometodologie a konverzační analýzy. V prezentaci vybraných výsledků analýzy se soustředím především na specifika začlenění videoklipu do sociální interakce. Samotná práce s videoklipem je účastníky metodicky zasazena do širšího kontextu dalších aktivit, zvláště četby úvodních textů na monitoru a následného zapisování odpovědí do pracovního listu. Z hlediska sekvenčnosti práce s videoklipem lze rozlišit tři fáze, a to (i) přípravu na spuštění videa, během níž je třeba dosáhnout optimálního uspořádání materiálních artefaktů a tělesné orientace účastníků; (ii) sledování videa, kde je zřejmá preference nepřerušeného přehrávání od začátku do konce, stejně jako omezení řečových projevů účastníků na „průběžný komentář“; (iii) reflexe videa, která nejčastěji směřuje k rychlé orientaci na formulaci odpovědi a její zápis do pracovního listu. Ukážu, že videoklip je v analyzovaných interakcích přítomen jako zdroj (např. dočasné pozastavení klipu či jeho opakované sledování za účelem identifikace klíčových pasáží) i jako téma (např. komentáře týkající se délky klipu či jazyka mluvčího, jenž na nahrávce hovoří). Příspěvek tak skrze rozbor konkrétních událostí vypovídá i o roli moderních digitálních technologií v současné společnosti a ve vzdělávání.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper is based on analysis of situated action in testing lessons, which were implemented in the school year 2017/18 at three high schools in the Czech Republic. Video recordings taken over the course of the lessons were then subjected to a detailed analysis grounded in the tradition of ethnomethodology and conversation analysis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V kapitole je předložen základní rámec pro uchopení tématu připomínání holocaustu, založený na analýze vybraných interview z Archivu vizuální historie USC Shoah Foundation.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This chapter presents a conceptual framework for approaching the topic of Holocaust commemoration, based on the analysis of selected interviews from the USC Shoah Foundation's Visual History Archive.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Text pojednává o možných přístupech ke zkoumání umělé inteligence jako sociálního fenoménu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Social sciences have been always formed and influenced by the development of society, adjusting the conceptual, methodological, and theoretical frameworks to emerging social phenomena. In recent years, with the leap in the advancement of Artificial Intelligence (AI) and the proliferation of its everyday applications, “non-human intelligent actors” are increasingly becoming part of the society. This is manifested in the evolving realms of smart home systems, autonomous vehicles, chatbots, intelligent public displays, etc. In this paper, we present a prospective research project that takes one of the pioneering steps towards establishing a “distinctively sociological” conception of AI. Its first objective is to extract the existing conceptions of AI as perceived by its technological developers and (possibly differently) by its users. In the second part, capitalizing on a set of interviews with experts from social science domains, we will explore the new imaginable conceptions of AI that do not originate from its technological possibilities but rather from societal necessities. The current formal ways of defining AI are grounded in the technological possibilities, namely machine learning methods and neural network models. But what exactly is AI as a social phenomenon, which may act on its own, can be blamed responsible for ethically problematic behavior, or even endanger people’s employment? We argue that such conceptual investigation is a crucial step for further empirical studies of phenomena related to AI’s position in current societies, but also will open up ways for critiques of new technological advancements with social consequences in mind from the outset.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku popisujeme metody a výsledky výzkumu zaměřeného na vícejazyčné korpusové srovnání spojovacího výrazu "and" v angličtině a jeho ekvivalentů v češtině, francouzštině, maďarštině a litevštině u vybraných příspěvků z TED talks.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we report on the methods and findings of a multilingual corpus study focusing on the functions of and in English and its translations into Czech, French, Hungarian and Lithuanian, in a selection of TedTalks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Analýza funkcí spojovacího výrazu AND a jeho ekvivalentů v angličtině, češtině, litevštině, francouzštině a maďarštině podle doménové klasifikace Crible - Degand (2017).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Analysis of functions of the discourse connective AND and its counterparts in English, Czech, Lithuanian, French and Hungarian according to the taxonomy with cross-domain functions (Crible, Degand, 2017).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Mezijazykové srovnání funkcí spojky "a" a možností jejího překladu, založené na paralelních překladech titulků v TED talks. Výchozí studie o nespecifičnosti významu některých spojek v různých jazycích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Cross-linguistic comparison of the functions of the conjunction "and" and the possibilities of its translation, based on the parallel translations of subtitles in TED talks. Pilot study about the underspecification of semantics of some conjunctions in different languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V lingvistice se obvykle slova považují za složená z morfémů, což jsou dále nedělitelné jazykové jednotky nesoucí význam. Zadáním této práce je nalézt automatickou metodu dělení českých slov na morfémy, které by bylo možné přidat do DeriNetu, sítě derivačních vztahů mezi českými slovy.

Vytvořili jsme dvě různé takové metody. První nalézá hranice morfémů na základě hledání rozdílů mezi slovem a jeho derivačním předkem, a tranzitivně mezi všemi slovy v derivačním hnízdě. Tato metoda explicitně modeluje hláskové a morfologické alternace a nalézá nejvhodnější hranice morfémů pomocí metody maximální věrohodnosti. Ve srovnání s moderním systémem Morfessor FlatCat naše metoda přinejhorším mírně zaostává, ovšem v některých testech naopak dosahuje výsledků výrazně lepších.

Druhou metodou je neuronová síť pro současné předpovídání morfologické segmentace a derivačních předků, trénovaná na datech získaných první metodou a na derivačních vztazích ze sítě DeriNet. S naší hypotézou, že tento způsob trénování dvou úloh naráz pomůže k dosažení lepších výsledků oproti trénování samotné segmentace, jsou však ve shodě pouze některé provedené pokusy. Celkově dosahuje neuronová síť horších výsledků než první metoda, pravděpodobně kvůli trénování na datech obsahujících chyby, které se tím přidávají k chybám metody samotné.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In linguistics, words are usually considered to be composed of morphemes: units that carry meaning and are not further subdivisible. The task of this thesis is to create an automatic method for segmenting Czech words into morphemes, usable within the network of Czech derivational relations DeriNet.

We created two different methods. The first one finds morpheme boundaries by differentiating words against their derivational parents, and transitively against their whole derivational family. It explicitly models morphophonological alternations and finds the best boundaries using maximum likelihood estimation. At worst, the results are slightly worse than the state of the art method Morfessor FlatCat, and they are significantly better in some settings.

The second method is a neural network made to jointly predict segmentation and derivational parents, trained using the output of the first method and the derivational pairs from DeriNet. Our hypothesis that such joint training would increase the quality of the segmentation over training purely on the segmentation task seems to hold in some cases, but not in other. The neural model performs worse than the first one, possibly due to being trained on data which already contains some errors, multiplying them.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>DeriNet je lexikální síť modelující derivační vztahy v češtině. Uzly odpovídají lexémům, hrany odpovídají slovotvorným derivacím. Současná verze, DeriNet 1.6, obsahuje 1 027 832 lexémů převzatých ze slovníku MorfFlex, spojených 803 404 derivačními vztahy. Kromě toho obsahuje DeriNet, počínaje verzí 1.5, anotace kompozit (kompozita jsou označena vyhrazeným znakem ve svém záznamu o slovním druhu).
Oproti verzi 1.5 byla verze 1.6 rozšířena o vztahy extrahované ze slovníků dostupných pod svobodnými licencemi, například Wiktionary, a o množství kompozitních značek.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>DeriNet is a lexical network which models derivational relations in the lexicon of Czech. Nodes of the network correspond to Czech lexemes, while edges represent derivational relations between a derived word and its base word. The present version, DeriNet 1.6, contains 1,027,832 lexemes (sampled from the MorfFlex dictionary) connected by 803,404 derivational links. Furthermore, starting with version 1.5, DeriNet contains annotations related to compounding (compound words are distinguished by a special mark in their part-of-speech labels).
Compared to version 1.5, version 1.6 was expanded by extracting potential links from dictionaries available under suitable licences, such as Wiktionary, and by enlarging the number of marked compounds.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek analyzuje kompatibilitu stávající kategorizace licencí v CLARINu s paradigmatem Open Science. V první části se prezentují základní koncepty a teoretický rámec, druhá část řeší kategorizaci licencí v projektu CLARIN do tříd PUB, ACA a RES a možnosti do budoucna tento systém změnit. Varianty možných změn jsou analyzovány jako podklad k další diskusi o změně přístupu k licencím v CLARINu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article investigates the compatibility of the current CLARIN license categorization scheme with the open science paradigm. The first part presents the main concepts and theoretical framework required for the analysis, while the second part discusses the use of the CLARIN categorization system, divided into PUB (public), ACA (academic), and RES (restricted), and potential ways to change it. This paper serves to explore various suggestions for change and to begin discussion of a reformed CLARIN license category scheme.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zkoumáme anotaci zvratných zájmen v korpusech Universal Dependencies (UD) 2.2 (Nivre et al., 2018), přičemž se zaměřujeme zejména na slovanské jazyky. Snažíme se zjistit, zda jsou současná anotační pravidla dostatečně jasná, aby se jimi anotátoři dokázali řídit. Ukazujeme celou řadu nesrovnalostí napříč jazyky v současných datech a navrhujeme vylepšení–někdy anotačních pravidel, většinou však přímo anotace dat. Cílem článku je přispět ke konzistentnější a mezijazykově paralelnější anotaci reflexiv v budoucích vydáních UD.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We explore the annotation of reflexives in Universal Dependencies (UD) 2.2 treebanks (Nivre et al., 2018), with a stronger focus on Slavic languages. We have tried to find out if the current guidelines are transparent and clear enough for the annotators to follow them successfully. We point out a number of inconsistencies in the current annotation across languages, and propose improvements—sometimes of the guidelines, but mostly of the annotation. The goal of the paper is to contribute to more consistent and cross-linguistically parallel annotation of reflexives in the future releases of UD.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku popisujeme anotaci koreferenčních vztahů v multijazykovém paralelním korpusu PAWS. Zaměřujeme se na mezijazykové rozdíly ve vejadřování koreference, jako např. osobní a neosobní slovesné tvary v polypredikativních konstrukcích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we decribe the coreference annotation on a multi-lingual parallel treebank (PAWS), a portion of Wall Street Journal translated into Czech, Russian and Polish which continues the tradition of multilingual treebanks with coreference annotation. The paper focuses on language-specific differences. We analyse syntactic structures concerning anaphoric relations in the languages under analysis, such as personal and impersonal constructions in polypredicative constructions and pro-drop qualities.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>PAWS je anglicko-česko-rusko-polský korpus s anotací koreference.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>PAWS (Parallel Anaphoric Wall Street Journal) is a multi-lingual parallel treebank with coreference annotation. It consists of English texts from the Wall Street Journal translated into Czech, Russian and Polish. In addition, the texts are syntactically parsed and word-aligned. PAWS is based on PCEDT 2.0 and continues the tradition of multilingual treebanks with coreference annotation. PAWS offers linguistic material that can be further leveraged in cross-lingual studies, especially on coreference.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku představujeme PAWS, vícejazykový paralelní treebank s anotací koreference.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present PAWS, a multi-lingual parallel treebank with coreference annotation. It consists of English texts from the Wall Street Journal translated into Czech, Russian and Polish. In addition, the texts are syntactically parsed and word-aligned. PAWS is based on PCEDT 2.0 and continues the tradition of multilingual treebanks with coreference annotation. The paper focuses on the coreference annotation in PAWS and its language-specific differences. PAWS offers linguistic material that can be further leveraged in cross-lingual studies, especially on coreference.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Provádíme analýzu textových prostředků (na angličtině, češtině a němčitě), která je založená na textech, přeložených z angličtiny. Zvláštní důraz této publikace klademe na vlivu překladového faktoru na použití různých typů koheze.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Since translations are influenced by various factors of translation process, it is often difficult to explain the real reasons of a certain construction used in translation data. 
The present contribution describes a cross-linguistic analysis of the interplay between discourse-relational devices (DRDs) and other discourse-related phenomena, such as coreference and bridging relations. The difference between the phenomena under analysis lies in the type of relations, which is expressed by a corresponding device. DRDs express logico-semantic relations between propositions, such as contrast, time, addition and others). Coreference serves the task of linking identical objects or events (i.e. complex anaphors) and bridging anaphora expresses non-identical or near-identical relations between referents, linking them with semantic interconnection.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem výzkumu je srovnání souhry konektorů, koreference a asociační anafory ve němčině, češtině a angličtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The purpose of our study is to analyse the interplay between conjunctions and other discourse-related
phenomena, such as coreference and bridging relations in German, English and Czech. The difference between the phenomena under analysis lies in the type of relations, which is expressed by a
corresponding device.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku představíme kontrastivní analýzu zájmenných adverbií v němčině (dabei, daauf, damit aj.) a jejich ekvivalentů v angličtině, češtině a rušině. Analýza je založena na empirickém výzkumu paralelních textů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents a contrastive analysis of pronominal adverbs in German (dabei, darauf, damit etc.) and their equivalents in English, Czech and Russian. The analysis is based on an empirical study of parallel news texts. Our main focus is to show the interplay between cohesive devices expressed through German pronominal adverbs in text and explore their equivalents in English, Czech and Russian. As the dataset at hand contains translations, we also focus on the influence of the translation factor in parallel texts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek se věnuje přiřazování audia přímo k notopisu reprezentovanému jako obraz, bez jakýchkoliv abstratktních reprezentací. Navrhujeme metodu, která se naučí společný prostor pro reprezentaci krátkých útržků audia a jejich protějšků v obrázkách not pomocí multimodálních konvolučních neuronových sítí. Následně ukazujeme, jak s těmito naučenými reprezentacemi (1) identifikovat příslušnou skladbu podle nahrávky, (2) vyhledávat nahrávky pomocí obrázků not. Všechny vyhledávací modely jsou natrénované na novém velkém multimodálním datasetu audia a notopisu, který je spolu s tímto článkem dán veřejně k dispozici. Dataset obsahuje 479 detailně anotovaných klavírních skladeb od 53 skladatelů, celkem 1129 stran not a více než 15 hodin k nim zarovnaného audia, které bylo z příslušných not syntetizováno. Nad modelem natrénovaným těmito syntetickými daty však provádíme pokusy, které vyhledávají v databázi komplexních not (např. téměř celé dílo pro sólový klavír F. Chopina) a komerčních nahrávek špičkových klavíristů. Naše výsledky naznačují, že navržená metoda spolu s velkým datasetem vede k vyhledávacím modelům, které úspěšně zobecňují ze syntetických trénovacích dat na skutečná data.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This work addresses the problem of matching musical audio directly to sheet music, without any higher-level abstract representation. We propose a method that learns joint embedding spaces for short excerpts of audio and their respective counterparts in sheet music images, using multimodal convolutional neural networks. Given the learned representations, we show how to utilize them for two sheet-music-related tasks: (1) piece/score identification from audio queries and (2) retrieving relevant performances given a score as a search query. All retrieval models are trained and evaluated on a new, large scale multimodal audio–sheet music dataset which is made publicly available along with this article. The dataset comprises 479 precisely annotated solo piano pieces by 53 composers, for a total of 1,129 pages of music and about 15 hours of aligned audio, which was synthesized from these scores. Going beyond this synthetic training data, we carry out first retrieval experiments using scans of real sheet music of high complexity (e.g., nearly the complete solo piano works by Frederic Chopin) and commercial recordings by famous concert pianists. Our results suggest that the proposed method, in combination with the large-scale dataset, yields retrieval models that successfully generalize to data way beyond the synthetic training data used for model building.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Současné modely pro křížové vyhledávání mezi nahrávkami a notovými zápisy přes natrénované multimodální reprezentace používají konvoluční neuronové sítě, jenž očekávají na vstupu pro audio pevně daný časový úsek. Podle tempa nahrávky však toto okno zachycuje různá množství hudebních událostí (not), zatímco fixně dlouhé okno do not jich zachycuje množství, které na tempu téměř nezávisí. V této práci se snažíme tento problém obejít pomocí mechanismu měkké pozornosti, která umožňuje modelu zakódovat pouze ty části útržku audia, které jsou nejvíce relevantní pro vybudování efektivního vyhledávacího klíče. Experimentální výsledky na klasické klavírní hudbě ukazují, že pozornostní mechanismus vyhledávání zlepšuje, a má intuitivně jasné výhody.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Current models for audio–sheet music retrieval
via multimodal embedding space learning use con-
volutional neural networks with a fixed-size win-
dow for the input audio. Depending on the tempo
of a query performance, this window captures
more or less musical content, while notehead den-
sity in the score is largely tempo-independent. In
this work we address this disparity with a soft
attention mechanism, which allows the model to
encode only those parts of an audio excerpt that
are most relevant with respect to efficient query
codes. Empirical results on classical piano music
indicate that attention is beneficial for retrieval performance, and exhibits intuitively appealing
behavior.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek se zabývá parafrázovatelností českých slovesných víceslovných výrazů (kategoriálních slovesa a idiomů). V příspěvku je navržena lexikografická reprezentace parafrází a demonstrováno jejich praktické využití ve strojovém překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this chapter, we explore paraphrasability of Czech verbal MWEs (light verbs constructions and idioms) by single verbs in a semiautomatic experiment
using word embeddings. Further, we propose a lexicographic representation and demonstrate one of its practical application in a machine translation
experiment.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato zpráva popisuje účast týmu Univerzity Karlovy v Praze na CLEF eHealth Consumer Health
Search Task 2018</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present our participation in CLEF Consumer Health Search Task 2018, mainly, its monolingual and multilingual subtasks: IRTask1 and IRTask4. In IRTask1, we use language-model
based retrieval model, vector-space model and Kullback-Leiber divergence query expansion mechanism to build our runs. In IRTask4, we submitted 4 runs for each language of Czech, French and German. We
follow query-translation approach in which we employ a Statistical Machine Translation (SMT) system to get a ranked list of translation hypotheses in English. We use this list for two systems: the first one uses 1-best-list translation to construct queries, and the second one uses a hypotheses reranker to select the best translation (in terms of retrieval performance) to construct queries. We also present our term reranking model for query expansion, in which we deploy feature set from different
resources (the document collection, Wikipedia articles, translation hypotheses). These features are used to train a logistic regression model that can predict the performance when a candidate term is added to a base query.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Hegelovo prázdninové boogie aneb Mondrian, Magritte a avantgardní dialektika

Avantgarda, jako umění, které často tematizuje svoji vlastní formu, je z hlediska Hegelovy filosofie zajímavá jako příklad rozvoje sebe-vědomí. V tomto přípěvku se však naopak podíváme na to, proč avantgardu zajímala Hegelova filosofie. Na příkladech Mondrianovy abstrakce a Magrittova surrealismu ukážeme dva diametrálně odlišné přístupy k inspiraci Hegelem ve výtvarném umění.
Piet Mondrian se na Hegela odkazuje ve svých textech o teorii malířství. Pod vlivem hegelovské filosofie dospívá ke koncepci zobrazování, které označuje jako „abstraktně-reálné“, výraz „oživené skutečnosti abstraktního“.
René Magritte se k Hegelově filosofii vyjadřuje přímo názvy a obsahem svých obrazů. V některých z nich explicitně odkazuje k dialektice, která je však implicitně přítomná ve většině jeho díla.
Na závěr se zamyslíme nad tím, jak můžeme vývoj avantgardního malířství interpretovat jako dialektiku rozumného a skutečného.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Hegel's Holiday Boogie or Mondrian, Magritte and  Avantgarde Dialectics

In this talk we expore hegelian inspiration in Mondrian's theory of art and Magritte's paintings and propose to interpret the development of avantgarde painting as dialectics of the real and the reasonable.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Budou někdy počítače schopny pochopit psaný text? Přeložit ho bezchybně třeba z angličtiny do češtiny? Popsat co je na obrázku? Co jsou umělé neuronové sítě a jak mohou pomoci vyřešit tyto otázky? To se dozvíte v této přednášce. Když bude čas, zmíníme i jak neuronky hrají go nebo řídí auta.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Will computers ever be able to understand written text? Will they be able to translate it from English to Czech without mistakes? Will they be able to describe a picture? What are artificial neural networks and how are they connected to these questions?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace word embeddings a souvisejícího výzkumu probíhajícího na ÚFALu v rámci Jednoho dne s informatikou a matematikou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presentation of word embeddings and related research at ÚFALu within Jeden den s informatikou a matematikou.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Stručný úvod k panelu o umělé inteligenci a humanitních oborech představuje možnosti neuronového strojového překladu a hlubokého učení obecně pro lingvistiku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A brief opening presentation for a panel on AI and the Humanities</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Strojový překlad je královskou disciplínou umělé inteligence a počítačové lingvistiky. Jednu revoluci zažil v 90. letech minulého století (zavedení statistických metod), druhou zažívá právě teď, když neuronové sítě skokově zlepšily kvalitu překladu. Jak se nyní překlad dělá? Podívejme se na ten zázrak technicky, bez silných marketingových slov. Je snad konečně naděje, že počítače začnou textu rozumět?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Machine translation is the one of the key application of artificial inteligence and computational linguistics. Two revolutions happened recently: in the 1990's and these days. What's the current state of the art? Finally a chance that machines will understand the text?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve svém příspěvku podrobně vysvětlím, jakou hlavní výhodu má neuronový překlad oproti předešlým statistickým přístupům k MT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In my talk, I will highlight the benefit that neural machine translation (NMT) has over previous statistical approaches to MT. I will then present the current state of the art in neural machine translation, briefly describing the current best architectures and their performance and limitations. In the second part of the talk, I will outline my planned search for correspondence between sentence meaning as traditionally studied by linguistics (or even semantics and semiotics) and the continuous representations learned by neural networks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zvaná přednáška na seminář pořádaný na FJFI, představující zajímavá vědecká témata studentům magisterského programu. Ve své přednášce jsem představil počítačovou lingvistiku a zejména neuronový strojový překlad.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An invited talk for Master students of FJFI at a seminar introducing various interesting research topics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V prezentaci jsem představil současnou úroveň kvality dosahovanou neuronovým strojovým překladem a podrobně se v diskusi věnoval očekávaným výhodám a rizikům, která do překladatelské profese přináší projekt ELITR.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In my talk, I presented the current translation quality achieved by neural machine translation and thoroughly discussed the expected benefits and risks for interpreters arising from the EU project ELITR.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Hluboké strojové učení v roce 2016 zásadním způsobem změnilo techniky používané ve strojovém překladu. Kvalita strojových překladů se skokově zlepšila a strmý růst stále ještě pozorujeme. Neuronové sítě slibují možnost "učit se reprezentacím". Daří se je tedy učit reprezentacím, které by ladily s něčím jako je význam vět?

V krátkém příspěvku představím překotný vývoj na poli strojového překladu a otázky pro tento obor na příští týden i příštích deset let.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Deep machine learning has crucially changed the techniques used in machine translation in 2016. In my short talk, I will summarize the quick development and questions for the field for the next week and the next ten years.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Seznámení zájemců o obor informatiky a počítačové lingvistiky s nejnovějším vývojem na poli strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A quick introduction to prospective students of Computer Science and Computational Linguistics with recent advances in machine translation</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Automatizovaný systém pro dávkový překlad pro IBM. Prvním jazykovým párem byl překlad z angličtiny do češtiny, další jazyky (maďarština, arabština ap.) jsou přidávány dle potřeby.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Automated system for batch translation for IBM. The first supported translation direction was from English into Czech, other languages (e.g. Hungarian, Arabic) are added as needed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Užíváme systém EVALD k automatickému ohodnocení výstupů několika systémů strojového překladu; výsledky ukazují, že automatické ohodnocení diskurzu v textech umožňuje rozlišit různé systémy strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present results of automatic evaluation of discourse in machine translation (MT) outputs using the EVALD tool, showing that automatic evaluation of discourse in translated texts allows for distinguishing individual MT systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek přináší výsledky hlavní společné úlohy organizované při konferenci o strojovém překladu (WMT) v roce 2018.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the premier
shared  task  organized  alongside  the  Confer-
ence  on  Machine  Translation  (WMT)  2018.
Participants   were   asked   to   build   machine
translation systems for any of 7 language pairs
in both directions, to be evaluated on a test set
of news stories.  The main metric for this task
is human judgment of translation quality. This
year, we also opened up the task to additional
test suites to probe specific aspects of transla-
tion.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Závislostní korpus anotovaný na rovině morfologické (2 miliony slov), syntaktické (1,5 milionu slov) a sémantické (přes 0,8 milionu slov, tedy 49,5 tis. vět). Obsahuje všechny PDT anotace originálních textů, které vznikly v rámci různých projektů na UFALu mezi roky 1996 a 2018 (PDT 1.0, PDT 2.0, PDT 2.5, PDT 3.0, PDiT 1.0 and PDiT 2.0) a jejich opravy; seznam autorů pokrývá autory všech publikovaných korpusů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Prague Dependency Treebank 3.5 is the 2018 edition of the core Prague Dependency Treebank (PDT). It contains all PDT annotation made at the Institute of Formal and Applied Linguistics under various projects between 1996 and 2018 on the original texts, i.e., all annotation from PDT 1.0, PDT 2.0, PDT 2.5, PDT 3.0, PDiT 1.0 and PDiT 2.0, plus corrections, new structure of basic documentation and new list of authors covering all previous editions. The Prague Dependency Treebank 3.5 (PDT 3.5) contains the same texts as the previous versions since 2.0; there are 49,431 annotated sentences (over 800 thousand nodes) on all layers, from tectogrammatical to words, and additional sentences on the analytical (surface dependency syntax) and morphological layers of annotation (approx. 2 million words in total). Closely linked to the tectogarammtical layer is the annotation of sentence information structure, multiword expressions, coreference, bridging relations and discourse relations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve stati se předládají vedle přehledu konstrukcí pokládaných za gramatickou diatezi argumenty pro případy, kdy deagentní diateze spojená s reciproční diatezí může vyjádřit konatele pomocí předložkového výrazu mezi + Instrumentál.Jsou zde uvedena jistá omezení na tvorbu těchto konstrukcí. Ilustruje se také jejich častá homonymie s konstrukcemi s významem lokace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The short survey of the constructions understood as grammatical diathesis is given. The discussion of the generally accepted hypothesis that the surface expression of the actor is excluded in deagentive constructions. The counterexamples where the combination of the deagentive and reciprocal diathesis are submitted. Some restrictions for such constructions are studied as well.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku se podrobují diskusi distinkce uvnitř lokálního určení spojené s distribucí výrazů v předložkových konstrukcích v + Loc, na + Loc, u + Gen, po + 6 a s jejich sémantickými specifiky. Stať je stavěna na materiálu získaného pomocí nástroje ForFun 1.0.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The distribution of the Czech prepositional constructions connected with the semantics of space is studied and the proposal how to present their semantics within the grammatical description is submitted. The prepositional constructions v+6, 
na+6, u+2, and po+6 are included, the rich data for this analysis were obtained from PDT 3.0 using the tool ForFun 1.0</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek je věnován několika problémům, které vznikají při aplikaci valenční teorie FGP na valenci deverbativních substantiv a které demonstrují fakt, že aplikace valence slovesné na substantivní nemůže být přímočará a že substantivní valenční rámce nejsou vždy analogií rámců  jejich základových sloves. U substantiva přibývají formy typické pro volná doplnění uvozené zpravidla sekundárními předložkami, které se svou lexikální náplní obsahově blíží obligatorním aktantům (oznámení ze strany úřadu ohledně návštěvních hodin). Pokládáme je za stylové varianty, které se nezapisují do valenčních rámců, ale jsou ošetřeny speciálními pravidly. Jiné, uvozené primárními předložkami, mají marginální povahu; vyžadují však zjištění kontextových podmínek pro jejich užití (otázka nad stavem společnosti / po stavu společnosti), včetně podmínek jejich vzájemné zaměnitelnosti, popř. též úvah nad odstraněním jejich homonymie (otázka na psychologa.ADDR/PAT).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we demonstrate how to treat some difficult phenomena related to non-standard expression of valency patterns of Czech deverbal nouns. We introduce some new attributes to indicate (i) alternative, stylistically marked forms of actants expressed by secondary prepositions, and (ii) marginal forms of actants expressed by primary prepositions. We show that the number of forms of adnominal actants is often increased in comparison with the verbal ones, differentiating regular forms and alternative forms. We also discuss cases of ambiguous forms of two different actants and give criteria for their interpretation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kritický rozbor díla předního francouzského lingvisty J.-M. Zemba, především jeho prací o informační struktuře věty a negaci z pohledu autorčina vlastního přístupu k těmto základním otázkám syntaktické struktury. věty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A critical analysis of the writings of a prominent French linguist  J.-M. Zemb, especially those concerning the information structure and negation, from the point of view of the author's own approach to these basic issues.of the syntactic structure of the sentence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Korpusová studie lokální koherence textu založené na anaforických vztazích mezi elementy v tématické (Topic) a rématické (Focus) části věty v různých žánrech textů Pražského závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A corpus-based study of local coherence as established by anaphoric links between the elements in the thematic (Topic) and the rhematic (Focus) parts of sentences in different genres of texts in the Prague Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Analyzujeme data Pražského diskurzního korpusu 2.0 a na jejich základě studujeme tzv. tématické posloupnosti, tj. řetězce anaforických vztahů mezi větami vzhledem k jejich informační struktuře (topic–focus articulation).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We analyze the data of the Prague Discourse Treebank 2.0 as for the text coherence based on the so-called thematic progressions, that is chains of anaphoric links between sentences with regard to their topic–focus articulation (information structure).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zkoumáme dvě dichotomie, kterými se zabývá literatura o informační struktuře věty, konkrétně dichotomii topic/focus založenou na vztahu "býti o čem", a dichotomii mezi danou a novou informací. Zaměřujeme se obzvláště na otázku, zda dichotomie topic/focus může být založena na rozlišení mezi danou a novou informací, či zda vztah "býti o čem" tvoří přesnější obraz situace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We put under scrutiny two dichotomies discussed in the information structure literature, namely the dichotomy of topic and focus based on the relation of aboutness, and the dichotomy between given and new information. In particular, we examine whether the topic/focus dichotomy can be based on the distinction between given and new information, or whether the ‘aboutness’ relation is a more appropriate basis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek o slovníku anglicko-českých slovesných synonym na základě PCEDT. Synonyma jsou řazena do synonymních tříd, obsahujících anglické a české členy, které jsou zároveň propojeny s externími a interními lexikálními zdroji.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the present paper, we focus on synonymy of verbs in a bilingual, Czech-English setting. We introduce a new lexical resource called CzEngClass. Our research of semantic equivalence of verbs is based on the FGD theory on the syntactic side, and gets main inspiration from FrameNet on the semantic side. As the main evidence, we use a parallel dependency corpus (the Prague Czech-English Dependency Treebank 2.0). We consider this “bottom-up” approach a novel and appropriate approach to study verbal synonymy. Synonymous Czech and English verbs are being grouped into cross-lingual synonym classes and captured in the CzEngClass lexicon. This lexicon contains not only mappings of valency arguments to semantic roles for each member of the synonym group, but also links them to individual verb entries as captured in selected existing lexical resources, such as FrameNet, VerbNet, Vallex(es) and Czech and English WordNets. We believe such explicit description of verbal synonymy relations, collected in a richly interconnected lexicon,  contributes valuable knowledge to both traditional linguistics and various NLP tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Synonymní slovník CzEngClass je výsledkem projektu zkoumajícího sémantickou „ekvivalenci“ slovesných významů a jejich valenčního chování v paralelních zdrojích česko-anglického jazyka, tj. vztahujících se k slovním významům s ohledem na kontextuálně založená slovesná synonyma. Položky lexikonu jsou propojeny s PDT-Vallexem (http://hdl.handle.net/11858/00-097C-0000-0023-4338-F), EngVallexem (http://hdl.handle.net/11858/00-097C-0000-0023-4337-2), CzEngVallexem (http://hdl.handle.html.net/11234/1-1512), FrameNetem (https://framenet.icsi.berkeley.edu/verbverbands/), VerbNetem (http://coloredu.html.html Součástí datového souboru je soubor odrážející volby anotátorů pro přiřazení sloves do tříd.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The CzEngClass synonym verb lexicon is a result of a project investigating semantic ‘equivalence’ of verb senses and their valency behavior in parallel Czech-English language resources, i.e., relating verb meanings with respect to contextually-based verb synonymy. The lexicon entries are linked to PDT-Vallex (http://hdl.handle.net/11858/00-097C-0000-0023-4338-F), EngVallex (http://hdl.handle.net/11858/00-097C-0000-0023-4337-2), CzEngVallex (http://hdl.handle.net/11234/1-1512), FrameNet (https://framenet.icsi.berkeley.edu/fndrupal/), VerbNet (http://verbs.colorado.edu/verbnet/index.html), PropBank (http://verbs.colorado.edu/%7Empalmer/projects/ace.html), Ontonotes (http://verbs.colorado.edu/html_groupings/), and Czech (http://hdl.handle.net/11858/00-097C-0000-0001-4880-3) and English Wordnets (https://wordnet.princeton.edu/). Part of the dataset is a file reflecting annotators choices for assignment of verbs to classes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Synonymní slovník CzEngClass je výsledkem projektu zkoumajícího sémantickou „ekvivalenci“ slovesných významů a jejich valenčního chování v paralelních zdrojích česko-anglického jazyka, tj. vztahujících se k slovním významům s ohledem na kontextuálně založená slovesná synonyma. Položky lexikonu jsou propojeny s PDT-Vallexem (http://hdl.handle.net/11858/00-097C-0000-0023-4338-F), EngVallexem (http://hdl.handle.net/11858/00-097C-0000-0023-4337-2), CzEngVallexem (http://hdl.handle.html.net/11234/1-1512), FrameNetem (https://framenet.icsi.berkeley.edu/verbverbands/), VerbNetem (http://coloredu.html.html Součástí datového souboru je soubor odrážející volby anotátorů pro přiřazení sloves do tříd.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The CzEngClass synonym verb lexicon is a result of a project investigating semantic ‘equivalence’ of verb senses and their valency behavior in parallel Czech-English language resources, i.e., relating verb meanings with respect to contextually-based verb synonymy. The lexicon entries are linked to PDT-Vallex (http://hdl.handle.net/11858/00-097C-0000-0023-4338-F), EngVallex (http://hdl.handle.net/11858/00-097C-0000-0023-4337-2), CzEngVallex (http://hdl.handle.net/11234/1-1512), FrameNet (https://framenet.icsi.berkeley.edu/fndrupal/), VerbNet (http://verbs.colorado.edu/verbnet/index.html), PropBank (http://verbs.colorado.edu/%7Empalmer/projects/ace.html), Ontonotes (http://verbs.colorado.edu/html_groupings/), and Czech (http://hdl.handle.net/11858/00-097C-0000-0001-4880-3) and English Wordnets (https://wordnet.princeton.edu/). Part of the dataset are files reflecting annotators choices and agreement for assignment of verbs to classes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku se zaměřujeme na valency a synonymii sloves v bilingvním česko-anglickém kontextu. Náš výzkum sémantické ekvivalence sloves je z pohledu syntaktického (včetně valence) založen na FGP a z pohledu sémantického je inspirován především FrameNetem a VerbNetem. Jako hlavní zdroj korpusových dokladů používáme Pražský česko-anglický závislostní korpus 2.0. Postup výzkumu “od spoda nahoru” považujeme za nový a adekvátní přístup pro stadium slovesné synonymie. Synonymní česká a anglická slovesa jsou uskupeny do mezijazykových synonymních tříd v novém slovníku CzEngClass. Tento slovník pro každé synonymní sloveso dané třídy obsahuje jak mapování valenčních argumentů na sémantické role, tak propojení s jednotlivými slovesnými významy ve FrameNetu, VerbNetu, ve Vallexech a v anglickém WordNetu, což ze slovníku CzEngClass zároveň vytváří bohatě propojený slovník.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the present article, we focus on valency and synonymy of verbs in a bilingual, Czech-English setting. Our research of semantic equivalence of verbs is based on the FGD theory on the syntactic side (including valency), and gets main inspiration from FrameNet and VerbNet on the semantic side. As the main source of evidence, we use the Prague Czech-English Dependency Treebank 2.0. We consider this “bottom-up” approach a novel and appropriate approach to study verbal synonymy. Synonymous Czech and English verbs are being grouped into cross-lingual synonym classes and captured in the new CzEngClass lexicon. This lexicon contains not only mappings of valency arguments to semantic roles for each member of the synonym group, but also links them to individual verb entries in FrameNet, VerbNet, Vallex(es) and Czech and English WordNets, making CzEngClass also a richly interconnected lexicon.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme první výsledky našeho projektu o slovesné synonymii na základě paralelního česko-anglického korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the first findings of our recently started project of building a new lexical resource called CzEngClass, which consists
of bilingual verbal synonym groups. In order to create such a resource, we explore semantic ‘equivalence’ of verb senses of generally
different verbs in a bilingual (Czech-English) setting by using translational context of real-world texts in a parallel, richly annotated
dependency corpus. When grouping semantically equivalent verb senses into classes of synonyms, we focus on valency (arguments
as deep dependents with morphosyntactic features relevant for surface dependencies) and its mapping to a set of semantic “roles” for
verb arguments, common within one class. We argue that the existence of core argument mappings and certain adjunct mappings to a
common set of semantic roles is a suitable criterion for a reasonable verb synonymy definition, possibly accompanied with additional
contextual restrictions. By mid-2018, the first version of the lexicon called CzEngClass will be publicly available.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Při studiu synonymických vztahů jsme vzali v potaz vztah syntaxe a sémantiky, abychom mohli lépe definovat slovesnou valenci. Za pomocí anotačního  experimentu na datech UD a PUDs jsme zjišťovali koreleaci syntaxe a sémantiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>While studying verbal synonymy, we have investigated the relation between syntax and semantics
in hope that the exploration of this relationship will help us to get more insight into the question
of synonymy as the relationship relating (similar) meanings between different lexemes. Most
synonym lexicons (Wordnets and similar thesauri) are based on an intuition about the similarity
of word meanings, or on notions like “semantic roles.” In some cases, syntax is also taken into
account, but we have found no annotation and/or evaluation experiment to see how strongly
can syntax contribute to synonym specification. We have prepared an annotation experiment
for which we have used two treebanks (Czech and English) from the Universal Dependencies
(UD) set of parallel corpora (PUDs) in order to see how strong correlation exists between syntax
and the assignment of verbs in context to pre-determined (bilingual) classes of synonyms. The
resulting statistics confirmed that while syntax does support decisions about synonymy, such
support is not strong enough and that more semantic criteria are indeed necessary. The results of
the annotation will also help to further improve rules and specifications for creating synonymous
classes. Moreover, we have collected evidence that the annotation setup that we have used
can identify synonym classes to be merged, and the resulting data (which we plan to publish
openly) can possibly serve for the evaluation of automatic methods used in this area.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku popisujeme slovník bilingvní slovník anglických a českých slovesných synonym, CzEngClass. Ta jsou zachycena v synonymních třídách na základě kontextu. Zároveň je jejich valenční struktura vztažena k sémantickým rolím přiřazeným dané synonymní třídě.  Navíc jsou jednotlivé členy synonymní třídy propojeny s externími (FrameNet, VerbNet, PropbBank, WordNet) a interními zdroji (PDT-valency lexicons, Vallex and Czech WordNet). V čánku představujeme první verzi slovníku, obsahující 200 tříd (cca 1800 sloves) a evaluaci mezianotátorské shody.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes CzEngClass, a bilingual lexical resource being built to investigate verbal synonymy in bilingual context and to relate semantic roles common to one synonym class to verb arguments (verb valency). In addition, the resource is linked to existing resources with the same or a similar aim: English and Czech WordNet, FrameNet, PropBank, VerbNet (SemLink), and valency lexicons for Czech and English (PDT-Vallex, Vallex and EngVallex). There are several goals of this work and resource: (a) to provide gold standard data for automatic experiments in the future (such as automatic discovery of synonym classes, word sense disambiguation, assignment
of classes to occurrences of verbs in text, coreferential linking of verb and event arguments in text, etc.), (b) to build a core (bilingual) lexicon linked to existing resources, for comparative studies and possibly for training automatic tools, and (c) to enrich the annotation of a parallel treebank, the Prague Czech English Dependency Treebank, which so far contained valency annotation but has not linked synonymous senses of verbs together. The method used for extracting the synonym classes is a semi-automatic process with a substantial amount of manual work during filtering, role assignment to classes and individual members’ arguments, and linking to the external lexical resources. We present the first version with 200 classes (about 1800 verbs) and evaluate interannotator agreement using several metrics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek představuje editor pro tvorbu bilingvního slovesného synonymického slovníku, který propojuje jednotlivé slovesné třídy a jejich členy s dalšími lexikálními zdroji.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper (poster) presents a tool used for building a new cross-lingual verbal synonym lexical resource called CzEngClass, as well
as the structure of the lexicon. This lexicon captures interlingual synonyms, using valency behavior of synonymous verbs in relation
to semantic roles as one of the criteria for defining such interlingual synonymy. The tool, called Synonym Class Editor - SynEd, is a
user-friendly tool specifically customized to build and edit individual entries in the lexicon. It helps to keep the cross-lingual synonym
classes consistent and linked to internal as well as well-known external lexical resources. The structure of SynEd also allows to keep and
edit the appropriate syntactic and semantic information for each Synonym Class member. The editor makes it also possible to display
examples of class members usage in translational (parallel corpus) context. SynEd runs platform independently and may be used for
multiple languages. SynEd, CzEngClass and services based on them will be openly available.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předložkové skupiny představují druhou nejčastější formu valenčních doplnění českých substantiv. V této studii byla zkoumána na datech valenčního slovníku NomVallex, a to u tří sémantických tříd, Communication, Mental action a Psychological state. Tyto tři třídy adnominální předložkové skupiny ve velké míře sdílejí, liší se však míra jejich zastoupení. Srovnání valenčních vlastností substantiv z NomVallexu a jejich základových sloves z valenčního slovníku Vallex nám umožnilo upřesnit popis protějšků adnominálních i adverbálních předložkových skupin a ukázat, že tyto předložkové skupiny podléhají nejrůznějším posunům ve formách participantů. Díky tomuto srovnání byl také zjištěn vysoký podíl nově získaných adnominálních předložkových skupin v celkovém počtu předložkových skupin (minimálně 39%).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Prepositional groups represent the second most frequent form of valency complementations of Czech nouns (preceded only by prepositionless genitive). In this paper, we exploit the NomVallex lexicon which covers nouns belonging to three semantic classes, i.e. Communication, Mental action and Psychological state. We show that adnominal prepositional groups are shared among these semantic classes to a large extent but their frequency differs. A comparison between valency of nouns from NomVallex and valency of their base verbs from Vallex lexicon reveals that various non-systemic shifts in surface forms of participants exist between the verbal and adnominal prepositional groups. It also enables us to state a high ratio of non-systemic adnominal prepositional groups to all adnominal prepositional groups in the lexicon (not less than 39%).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>České valenční slovníky doposud pokrývaly především valenci slovesnou, slovníkové zpracování valence substantiv se tedy potýká s řadou teoretických i praktických otázek. V příspěvku představujeme východiska a cíle při budování valenčního slovníku českých deverbativních substantiv zvaného NomVallex, který vzniká v rámci projektu Valenční slovník substantiv založený na korpusu, s počátkem řešení v lednu 2016. Po teoretické stránce vycházíme z valenční teorie funkčního generativního popisu, při vlastním lexikografickém zpracování se opíráme o korpusový materiál (data Pražského závislostního korpusu a Českého národního korpusu – subkorpusů řady SYN). Při tvorbě slovníku budou částečně využita data valenčního slovníku PDT-Vallex, zpracování hesel však bude zdokonaleno v několika aspektech (například v pokrytí všech významů substantiva). Kritériem pro zařazení do slovníku bude především sémantická třída substantiva a složitost jeho valenčního chování, zejména specifické formy participantů. Valence substantiv bude zachycena pomocí valenčních rámců, výčtu všech možných kombinací adnominálních participantů a korpusových příkladů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The corpus-based valency lexicon of Czech nouns NomVallex is a starting project building upon the theory of valency developed within Functional Generative Description and extending the two lexicons developed within this tradition, PDT-Vallex and Vallex. The linguistic material is extracted from Czech linear and syntactically annotated corpora. In comparison with PDT-Vallex, the treatment of entries will be more exhaustive, for example, in the coverage of senses and in the semantic classification added to selected lexical units. The main criteria for including nouns in the lexicon will be semantic class membership and the complexity of valency patterns. Valency of nouns will be captured in the form of valency frames, enumeration of combinations of adnominal participants, and corpus examples.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Počítačový nástroj Evaluátor diskurzu pro cizince (EVALD 3.0 pro cizince) slouží k automatickému hodnocení povrchové koherence (koheze) textů v češtině psaných nerodilými mluvčími češtiny. Software hodnotí úroveň předloženého textu z hlediska jeho povrchové výstavby (zohledňuje např. frekvenci a rozmanitost užitých spojovacích prostředků, bohatost slovní zásoby, koreferenční vztahy apod.). Nová verze (3.0) přidává množinu rysů zaměřených na informační strukturu věty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Evaluator of Discourse for Foreigners (EVALD 3.0 for Foreigners) is a software that serves for automatic evaluation of surface coherence (cohesion) in Czech texts written by non-native speakers of Czech. Software evaluates the level of the given text from the perspective of its surface coherence (i.e. it takes into consideration, e.g., the frequency and diversity of connectives, richness of vocabulary, coreference relations etc.). The new version (3.0) adds a set of features related to the topic-focus articulation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Počítačový nástroj Evaluátor diskurzu (EVALD 3.0) slouží k automatickému hodnocení povrchové koherence (koheze) textů v češtině psaných rodilými mluvčími češtiny. Software tedy hodnotí (známkuje) úroveň předloženého textu z hlediska jeho povrchové výstavby (zohledňuje např. frekvenci a rozmanitost spojovacích prostředků, bohatost slovní zásoby, koreferenční vztahy apod.). Nová verze (3.0) přidává množinu rysů zaměřených na informační strukturu věty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Evaluator of Discourse (EVALD 3.0) is a software that serves for automatic evaluation of surface coherence (cohesion) in Czech texts written by native speakers of Czech. Software evaluates the level of the given text from the perspective of its surface coherence (i.e. it takes into consideration, e.g., the frequency and diversity of connectives, richness of vocabulary, coreference relations etc.). The new version (3.0) adds a set of features related to the topic-focus articulation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Podíváme se na jeden ze základních kamenů taxonomie vztahů v Universal Dependencies, na tzv. core arguments (základní aktanty) a na způsoby, jak je lze odlišit od tzv. oblique dependents (vedlejší aktanty a volná doplnění).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We will look at one of the cornerstones of Universal Dependencies relation taxonomy, the core arguments and their difference from oblique dependents.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato monografie představuje srovnávací studii přístupů k anotaci morfologie a syntaxe přirozených jazyků s důrazem na použitelnost v mnohojazyčném prostředí. Anotací se rozumí přidání lingvistických kategorií a vztahů do digitalizovaného textu v přirozeném jazyce. Výsledkem je anotovaný korpus; vzhledem k tomu, že syntaktické vztahy jsou často reprezentované jako závislostní stromy, tato monografie se soustředí na závislostní korpusy (treebanky).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This monograph presents a comparative study of annotation approaches to morphology and syntax of natural languages, with emphasis on applicability in a multilingual environment. Annotation is understood as adding linguistic categories and relations to digitally encoded natural language text, resulting in annotated corpus; as syntactic relations are often represented in the form of dependency trees, the annotated corpora covered by the monograph are dependency treebanks. Many treebanks exist and their annotation styles vary significantly, which hampers their usefulness for linguists and language engineers. We survey several harmonization efforts that tried to come up with cross-linguistically applicable annotation guidelines, including the most recent and broadest effort to date, Universal Dependencies. We examine language description on three levels: 1. tokenization and word segmentation, 2. morphology, and 3. surface dependency syntax. For each language phenomenon we provide a comparison of its analysis and annotation in various existing treebanks (or other corpora, for tokenization and morphology), pointing out advantages and disadvantages of the competing approaches. On the morphological layer, we go even beyond the currently available corpora and provide a typological survey of features that will be needed when less-resourced languages are covered by an annotation project. We conclude that no single approach is suitable for all purposes, but a good approach must not lose information, so that annotation can be converted to another style when necessary.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představím Universal Dependencies, komunitní projekt v počítačové lingvistice, který poskytuje morfologicky a syntakticky anotované korpusy v mnoha jazycích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I will present Universal Dependencies, a community project in computational linguistics that develops morphologically and syntactically annotated corpora for a large number of languages. UD started as an attempt to harmonize language resources used in various software tools for natural language processing. However, it quickly became a valuable resource also for corpus-based research. Three years after its first release, UD contains over 100 dependency treebanks in more than 60 languages. Despite the inevitable bias towards “big” languages, treebanks from less resourced language families are gradually added. In my talk, I will discuss some challenges of building a universal annotation scheme for all languages. In particular, how to make sure that comparable phenomena are annotated in a comparable fashion, without making all the languages look the same.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představím Universal Dependencies, celosvětový komunitní projekt zacílený na tvorbu mnohojazyčných korpusů anotovaných podle jednotných pravidel na morfologické a syntaktické rovině. Proberu koncept základních aktantů (core arguments), jednoho z pilířů, na kterých stojí anotační schéma UD. Ve druhé části přednášky se zaměřím na některé zajímavé problémy spojené s aplikací Universal Dependencies na slovanské jazyky. Na příkladech z 12 slovanských jazyků, které jsou v současné době reprezentované v UD, ukážu, že na mezijazykové konzistenci je ještě stále co zlepšovat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I will present Universal Dependencies, a worldwide community effort aimed at providing multilingual corpora, annotated at the morphological and syntactic levels following unified annotation guidelines. I will discuss the concept of core arguments, one of the cornerstones of the UD framework. In the second part of the talk I will focus on some interesting problems and challenges of applying Universal Dependencies to the Slavic languages. I will discuss examples from 12 Slavic languages that are currently represented in UD and show that cross-linguistic consistency can still be improved.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Testovací data rozebraná systémy, které se účastnily soutěže v syntaktické analýze Universal Dependencies při konferenci CoNLL 2018.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Test data parsed by systems submitted to the CoNLL 2018 shared task in parsing Universal Dependencies.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Konference o počítačovém učení přirozeného jazyka (CoNLL) každoročně zahrnuje tzv. společnou úlohu, tedy soutěž, jejíž účastníci trénují a testují své systémy strojového učení na jednotné sadě dat. Jedna z úloh roku 2018 byla věnována učení závislostních syntaktických analyzátorů (parserů) pro velké množství jazyků, v reálné situaci bez jakýchkoli ručních anotací na vstupu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Every year, the Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their learning systems on the same data sets. In 2018, one of two tasks was devoted to learning dependency parsers for a large number of languages, in a real-world setting without any gold-standard annotation on test input.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku představujeme první verzi Pražské databáze forem a funkcí, ForFun, jakožto neocenitelný zdroj jazykových dat pro lingvistický výzkum, zejména při popisu syntaktických funkcí a jejich formálních realizací v češtině. ForFun je vybudován na několika bohatě syntakticky anotovaných korpusecg z rodiny Pražských závislostních korpusů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>NOT FINAL VERSION.
In this paper, we introduce first version of ForFun, Prague Database of Syntactic Forms and Functions, as an invaluable resource for
profound linguistic research, particularly in describing syntactic functions and their formal realizations. ForFun takes advantage of sev-
eral richly syntactically annotated corpora, collectively called Prague Dependency Treebanks. The ForFun brings this rich and complex
annotation of Czech sentences closer to common researchers. We demonstrate that the ForFun 1.0 provides valuable and rich material
allowing to elaborate various syntactic issues in depth. We believe that nowadays when corpus linguistics differs from traditional linguis-
tics in its insistence on a systematic study of authentic examples of language in use, our database will contribute to the comprehensive
syntactic description.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládáme popis času a prostoru v českých větách na základě databáze ForFun. ForFun je nový lingvistický zdroj postavený na Pražskými závislostními korpusy češtiny. Umožňuje prohledávání tisíců skutečných příkladů tříděných podle užitých forem a také hloubkově syntaktických funkcí. Na základě databáze provedeme podrobný popis významů časových a prostorových určení včetně seznamu formálních prostředků. Příklady pocházejí z psaných i tištěných textů. Práce vychází z dat a neklade si za cíl nový teoretický popis.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a description of time and space modifications in Czech sentences based on the ForFun database. ForFun is a new resource built on annotated corpora of Czech—Prague Dependency Treebanks—for inspecting thousands of real examples categorized by their form as well as by their deep syntactic function. Based on the database, we perform a detailed description of meanings of time and space modifications including a list of formal means with real examples coming from both written and spoken texts. It should be emphasized that the study is data-oriented rather than theory-oriented.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme databázi forem a funkcí vybudovanou na podkladě ručně anotovaných víceúrovňových korpusů češtiny zvaných Pražské závislostní korpusy. Pražská databáze forem a funkcí (ForFun) byla vytvořena, aby usnadnila lingvistické badání o vztahu funkce a formy, což je jeden z hlavních úkolů, jak v teoretické lingvistice, tak v oblasti počítačového zpracování jazyka. V článku jsou představeny možnosti, jakým způsobem lze v databázi vztah funkce a formy zkoumat. 
Článek je z větší části založen na již dříve prezentovaném příspěvku na 16th International Workshop on Treebanks and Linguistic Theories in Prague (Bejček et al., 2017).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of the contribution is to introduce a database of linguistic forms and their functions
built with the use of the multi-layer annotated corpora of Czech, the Prague Dependency
Treebanks. The purpose of the Prague Database of Forms and Functions (ForFun) is to help the
linguists to study the form-function relation, which we assume to be one of the principal tasks
of both theoretical linguistics and natural language processing. We demonstrate possibilities
of the exploitation of the ForFun database.
This article is largely based on a paper presented at the 16th International Workshop on
Treebanks and Linguistic Theories in Prague (Bejček et al., 2017).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Na pozadí dvou skupin sloves, lexikálních a syntaktických reciprok v češtině, diskutujeme otázky homonymie a kombinovatelnosti reflexivity a reciprocity. Ukazujeme, že zdrojem homonymie i možnosti kombinace reflexivity a reciprocity je reflexivní zájmeno a že klíčovou úlohu sehrává jeho povrchověsyntaktická pozice. V případě homonymie je při reciprocitě a reflexivitě zasažena stejná dvojice situačních participantů / valenčních doplnění, reflexivní zájmeno tedy obsazuje totožnou povrchověsyntaktickou pozici a koreferuje s plurálovým subjektem. V případě kombinací těchto významů jde o rozdílné pozice a reflexivita a reciprocita zasahují různé dvojice participantů / doplnění. Uvedené dvě skupiny sloves se přitom vzhledem k homonymii a kombinovatelnosti těchto významů chovají do značné míry odlišně.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this article, we focus on two groups of Czech verbs, lexical and syntactic reciprocals. We provide an analysis of their syntactic properties with respect to reciprocity and reflexivity, their possible ambiguity and combination. We demonstrate that it is the reflexive pronoun and its surface expression that play a key role in the studied phenomena.  In case of ambiguity, both reciprocity and reflexivity affect the same pair of situational participants / valency complementations; the reflexive pronoun occupies the same syntactic position and it corefers with the plural subject. In contrast, when these phenomena combine, different pairs of participants / complementations are involved. We also show that lexical and syntactic reciprocals differ with respect to the studied phenomena.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zaměřil na polyfunkčnost reflexiv v češtině, vymezení jejich hlavní funckí, které v jazyce plní a důsledky pro jejich popis ve slovníku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk focused on multiple functions which the reflexives have in the Czech language and their representation in a lexicon.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valenční slovník českých sloves (též VALLEX 3.5) poskytuje informace o valenční struktuře českých sloves v jejich jednotlivých významech, které charakterizuje pomocí glos a příkladů; tyto základní údaje jsou doplněny o některé další syntaktické a sémantické charakteristiky. VALLEX 3.5 zachycuje téměř 4 600 českých sloves, která odpovídají více než 10 700 lexikálním jednotkám, tedy vždy danému slovesu v daném významu; tato verze je rozšířena o anotaci komplexních predikátů s kategoriálním slovesem (zpracováno celkem téměř 1 500 komplexních predikátů).
VALLEX je budován jako odpověď na potřebu rozsáhlého a kvalitního veřejně dostupného slovníku, který by obsahoval syntaktické a sémantické charakteristiky českých sloves a byl přitom široce uplatnitelný v aplikacích pro zpracování přirozeného jazyka. Při navrhování koncepce slovníku byl proto kladen důraz na všestranné využití jak pro lidského uživatele, tak pro automatické zpracování češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>VALLEX 3.5 provides information on the valency structure (combinatorial potential) of verbs in their particular senses, which are characterized by glosses and examples. VALLEX 3.5 describes almost 4 600 Czech verbs in more than 10 700 lexical units, i.e., given verbs in the given senses; this version is enriched with an annotation of light verb constructions -- almost 1 500 complex predicates have been added.
VALLEX is a is a collection of linguistically annotated data and documentation, resulting from an attempt at formal description of valency frames of Czech verbs. In order to satisfy different needs of different potential users, the lexicon is distributed (i) in a HTML version (the data allows for an easy and fast navigation through the lexicon) and (ii) in a machine-tractable form as a single XML file, so that the VALLEX data can be used in NLP applications.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Licenční smlouva a smlouva o poskytnutí služeb mezi Univerzitou Karlovou a Datlowe, s.r.o. (1. 1. 2018 -- 1. 1. 2020) obsahuje právo na užití dat PDT 2.5 a software pro textovou analytiku natrénovaného na PDT 2.5, a to pro komerční účely. Jedná se o upravené verze nekomerčního software a dat jinak dostupných z repozitáře LINDAT/CLARIN. Jmenovitě: software pro analýzu tvarosloví a lematizaci, syntaktickou analýzu a analýzu názvů pojmenovanýchch entit.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Licence and service agreement between Charles University and Datlowe Ltd.(January 1, 2018 -- January 1, 2020) covers the right to use the data of PDT 2.5 and the software for text analytics trained on PDT 2.5, for commercial purposes. The data and software (non-adapted) is otherwise available for free through the LINDAT/CLARIN repository for non-commercial use under the appropriate variant of the Creative Commons license. Namely, software for morphological analysis, lemmatization, syntactic analysis and named-entity recognition.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ačkoli treebanky anotované podle pravidel univerzálních závislostí (UD) dnes existují pro mnoho jazyků, cíl anotace stejných jevů křížově jazykově konzistentním způsobem není vždy splněn. V této studii zkoumáme jeden jev, u něhož se domníváme, že takový soulad chybí, a to expletiva. Takové prvky zaujímají pozici, která je strukturálně spojena s hlavním argumentem (nebo někdy s volnou závislostí), a přesto jsou nesouvisející a sémanticky prázdné. Mnoho UD stromů označuje alespoň některé prvky za expletiva, ale škála jevů se liší mezi stromy, a to i u úzce příbuzných jazyků, a někdy dokonce u různých stromů pro stejný jazyk. V tomto článku uvádíme kritéria pro určení výrazů, které jsou použitelná napříč jazyky a kompatibilní s cíli UD, poskytujeme přehled výrazů, které se nacházejí v současných UD stromech, a předkládáme doporučení pro anotaci expletiv, aby bylo možné v budoucích vydáních dosáhnout konzistentnější anotace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Although treebanks annotated according to the guidelines of Universal Dependencies (UD) now exist for many languages, the goal of annotating the same phenomena in a crosslinguistically consistent fashion is not always met. In this paper, we investigate one phenomenon where we believe such consistency is lacking, namely expletive elements. Such elements occupy a position that is structurally associated with a core argument (or sometimes an oblique dependent), yet are non-referential and semantically void. Many UD treebanks identify at least some elements as expletive, but the range of phenomena differs between treebanks, even for closely related languages, and sometimes even for different treebanks for the same language. In this paper, we present criteria for identifying expletives that are applicable across languages and compatible with the goals of UD, give an overview of expletives as found in current UD treebanks, and
present recommendations for the annotation of expletives so that more consistent annotation can be achieved in future releases.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje analýzu českých slovesných předpon, což je první krok projektu, který má konečný cíl automatickou morfematickou analýzu češtiny. Studovali jsme předpony, které se mohou vyskytnout v českých slovesech, zejména jejich možné a nemožné kombinace. Popisujeme postup rozpoznávání předpon a odvozujeme obecná pravidla pro výběr správného výsledku. Analýza "dvojitých" prefixů umožňuje vyvodit závěry o univerzálnosti první předpony. Připojili jsme také lingvistické komentáře k několika typům předpon.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents the analysis of Czech verbal prefixes, which is the first step of a project that has the ultimate goal an automatic
morphemic analysis of Czech. We studied prefixes that may occur in Czech verbs, especially their possible and impossible combinations. We
describe a procedure of prefix recognition and derive several general rules for selection of a correct result. The analysis of “double” prefixes enables to make conclusions about universality of the first prefix. We also added linguistic comments to several types of prefixes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prefixace je pro češtinu jedním ze základních slovotvorných prostředků. Zejména pro slovesa představuje velmi produktivní způsob, jakým lze modifikovat jejich význam a vid. V současném příspěvku ukážeme, jak lze korpusová data použít k analýze vícenásobných předpon vyskytujících se u českých sloves. Ve druhé části se zabýváme předponami, které se používají k modifikaci sloves přejatých z cizích jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Prefixation is one of the basic word formation means in Czech. Especially for verbs, it is a very productive way, which can modify their meaning and aspect. In the present contribution we show how the corpus data can be used to analyze multiple
prefixes occurring in Czech verbs. In the second part, we deal with the prefixes used to modify the loan verbs coming from foreign languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přinášíme pilotní studii strojového překladu vybraných gramatických konstrukcí, které se neshodují při překladu mezi češtinou a angličtinou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We  present  a  pilot  study  of  machine  transla-
tion of selected grammatical contrasts between
Czech and English in WMT18 News Transla-
tion  Task.   For  each  phenomenon,  we  run  a
dedicated  test  which  checks  if  the  candidate
translation  expresses  the  phenomenon  as  ex-
pected or not. The proposed type of analysis is
not an evaluation in the strict sense because the
phenomenon can be correctly translated in var-
ious ways and we anticipate only one. What is
nevertheless interesting are the differences be-
tween various MT systems and the single ref-
erence translation in their general tendency in
handling the given phenomenon.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek seznamuje s výsledky automatické analýzy pořádku slov ve 23 závislostních korpusech projektu HamleDT. Analýza se zaměřuje na základní vlasnosti pořádku slov, pořadí tří hlavních konstituentů - predikátu, subjektu a objektu. Kvantitativní analýza je provedena zvlášť pro hlavní a vedlejší věty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper gives an overview of results of automatic analysis of word order in 23 dependency treebanks of the HamleDT project. The analysis concentrates on basic characteristics of word order, the order of three main constituents, a predicate, a subject and an object. A quantitative analysis is performed separately for main clauses and subordinated clauses.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zaměřuje na deverbální substantiva tvořená nulovým sufixem a probírá jejich vlastnosti (hlásková skladba, lexikální význam, korpusová frekvence, valence) s cílem najít rysy, kterými se liší od nemotivovaných substantiv s nulovým sufixem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper focuses on action nouns with a zero suffix in Czech, which are described as derivation from verbs since the action meaning is primarily expressed by verbs. However, such nouns have a simpler morphemic structure than the corresponding verbs (cf. běh ‘run’ – běh-a-t ‘to run’) and thus differ from the majority of word-formation types in Czech in which the base word is formally simpler and the derivative more complex. In this paper, 100 top-frequent action nouns extracted from a representative corpus of Czech are analysed for features which could support the semantic evidence when determining the direction of motivation between a formally simpler noun and a more complex verb. First, we test whether action nouns derived from verbs do not undergo morphophonemic alternations in inflection (cf. běh.nom – běhu.gen) while in pairs of a base noun and a derived verb the noun is sensitive to alternations (cf. sníh‘snow’, which is the base word for sněžit ‘to snow’, has genitive sněhu; Millet 1958). Second, we verify the assumption that a derivative is less frequent than its base word. Third, the valency potential of both groups of nouns is compared.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce bude představena lexikální síť DeriNet, a to zvláště teoretické pozadí tohoto specializovaného zdroje jazykových dat, výběr lexémů a postup vytváření derivačních vztahů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the talk, the lexical network DeriNet will be presented. I focus on the theoretical background of this specialized language data resource, selection of lexemes and procedures used for identification of derivational links.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se věnuje grafémovým alternacím při odvozování slov v češtině z pohledu budování lexikální sítě DeriNet, která se specializuje na zachycování derivačních vztahů v českém lexikonu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper deals with morphographemic alternations in Czech derivation with regard to the build-up of a large-coverage lexical resource specialized in derivational morphology of contemporary Czech (DeriNet database). After a summary of available descriptions in the Czech linguistic literature and Natural Language Processing, an extensive list of alternations is provided in the first part of the paper with a focus on their manifestation in writing. Due to the significant frequency and limited predictability of alternations in Czech derivation, several bottom-up methods were used in order to adequately model the alternations in DeriNet. Suffix-substitution rules proved to be efficient for alternations in the final position of the stem, whereas a specialized approach of extracting alternations from inflectional paradigms was used for modelling alternations within the roots. Alternations connected with derivation of verbs were handled as a separate task. DeriNet data are expected to be helpful in developing a tool for morphemic segmentation and, once the segmentation is available, to become a reliable resource for data-based description of word formation including alternations in Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá změnami v kategorii slovesného vidu, k nimž dochází během odvozování sloves od sloves v češtině. Po stručném shrnutí základních bodů aspektologických diskuzí nad videm českého slovesa je tvoření vidových protějšků prezentováno jako integrální součást derivace českých sloves. Ve shodě s tímto pohledem je kategorie vidu využita jako důležitý rys při modelování slovesné derivace v databázi zachycující derivační morfologii češtiny. V příspěvku představujeme sadu kritérií, na jejichž základě byla slovesa v databázi organizována.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper deals with the changes of the category of grammatical aspect during derivation of verbs from verbs in Czech. After summarizing the main issues of the long-standing debate over aspect in Czech, it is argued that formation of aspectual pairs can be seen as an integral part of derivation of Czech verbs. In a language data resource capturing the derivational morphology of Czech, the category of aspect was employed as an important feature in modelling verb-to-verb derivation. A set of criteria used for organization of verbs in the database is presented in the paper.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek probírá postupně tři funkce, které jsou v odborné literatuře připisovány předponě po- u českých sloves. Na pozadí rozsáhlých korpusových dat se předponou po- zabýváme nejprve jako formantem odvozujícím předponová slovesa od sloves bezpředponových, dále jako morfémem podílejícím se na tvoření vidového protějšku a naposledy jako morfémem tvořícím syntetické nedokonavé futurum.
Cílem naší analýzy v jednotlivých oddílech je dosavadní poznatky jednak shrnout a syntetizovat, jednak je konfrontovat se zásadní otázkou, jak autentické korpusové doklady k jednotlivým funkcím přiřadit. Na závěr se na předponu po- u sloves díváme z hlediska konkurence s dalšími předponami a z hlediska, jaké místo předponová slovesa zaujímají v příslušných slovotvorných hnízdech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present case study deals with the functions of the Czech verbal prefix po-. Three functions of the prefix are analysed by contrasting the existing theoretical descriptions with corpus data. In its primary, word-formational function, po- modifies the meaning of the base verb (expressing one of the semantic features described as Aktionsart or other meanings; e.g. kreslit ‘to draw’ > pokreslit ‘to cover with drawings’). In its second function, po- derives perfective counterparts from the imperfective verb; here, the prefix is considered to be a grammatical means used for the formation of aspectual pairs of verbs (cf. kárat ‘to admonish.impf’ > pokárat ‘to admonish.pf’). The third function of po- is manifested in the class of determinate verbs; it is a part of the morphological form of these verbs in their (imperfective) future meaning (e.g. běžet ‘to run’ – poběží ‘(he) will run’). A group of verbs suspected of exhibiting similar behaviour as the pure determinate verbs is analysed and attested using the corpus data. Finally, the competition between the prefix po- and
several tens of prefixes in Czech verbs is commented upon and the position of the prefixed verbs within word-formation nests is sketched.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek informuje o novinkách ve vývoji lexikální databáze DeriNet, která je budována jako zdroj jazykových data specializovaný na derivaci v češtině a aktuálně modifikována tak, aby zde bylo možné zachycovat i slovotvornou kompozici. Možnosti využití těchto dat jsou ilustrovány nejnovějšími experimenty z oblasti zpracování přirozeného jazyka i z lingvistického výzkumu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper reports on recent progress in development of the lexical database DeriNet, which has been designed as a resource specialized in derivation of Czech but, recently, the structure of the database has been modified in order to allow  for  capturing  compounding  and  combined  word-formation  processes, too. The ambition is to cover a major part of the word-formation system of Czech in all its complexity.  The potential of the resource for both linguistic research and experiments  in  Natural  Language  Processing  are exemplified  by  recent  case studies based on the data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je deváté vydání treebanků UD, verze 2.3.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the ninth release of UD Treebanks, Version 2.3.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je osmé vydání treebanků UD, verze 2.2.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the eighth release of UD Treebanks, Version 2.2.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme naši stále probíhající práci na mezijazyčném přenosu syntaktických parserů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our ever ongoing work on cross-lingual syntactic parsing.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentuji, co jsem se dozvěděl o PhD studiu na University of Genova, a navrhuji několik změn pro PhD studium na ÚFALu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I present what I learned about the PhD studies at University of Genova, and suggest some changes to PhD studies at ÚFAL.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této dizertaci se zaměřujeme na problém automatického syntaktického rozboru jazyků, pro něž nejsou k dispozici žádná syntakticky anotovaná trénovací data. Zkoumáme několik metod mezijazyčného přenosu syntaktické i morfologické anotace, a nakonec docházíme k metodám založeným na využití dvojjazyčných či vícejazyčných korpů zarovnaných na úrovni vět, a strojového překladu. Zvláštní pozornost věnujeme automatickému odhadování vhodnosti zdrojového jazyka pro analýzu daného cílového jazyka, a navrhujeme novou míru založenou na podobnostech častých sledů slovních druhů. Účinnost představených postupů byla ověřena jak v našich pokusech, tak nezávisle v pracech uznávaných světových vědců.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this thesis, we focus on the problem of automatically syntactically analyzing a language for which there is no syntactically annotated training data. We explore several methods for cross-lingual transfer of syntactic as well as morphological annotation, ultimately based on utilization of bilingual or multilingual sentence-aligned corpora and machine translation approaches. We pay particular attention to automatic estimation of the appropriateness of a source language for the analysis of a given target language, devising a novel measure based on the similarity of part-of-speech sequences frequent in the languages. The effectiveness of the presented methods has been confirmed by experiments conducted both by us as well as independently by other respectable researchers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení projektu GA UK, zejména metody měření podobnosti jazyků, a osobní pohled na výhody a nevýhody GAUKu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presentation of the GAUK project, especially the language similarity measure, and a personal view of the advantages and disadvantages of GAUKs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Data v podobě prostého textu získaná z dumpů Wikipedie v únoru 2018.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Wikipedia plain text data obtained from Wikipedia dumps with WikiExtractor in February 2018.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje systém CUNI x-ling zaslaný do soutěže CoNLL 2018 UD Shared Task. Zaměřili jsme se na syntaktickou analýzu jazyků s nedostatkem zdrojů, které mají k dipozici malá nebo žádná trénovací data. Použili jsme široké spektrum přístupů, včetně jednoduchého překladu závislostních korpusů slovo po slově, kombinace delexikalizovaných parserů, a využití dostupných tvaroslovných slovníků. Náš příspěvek byl v oficiálním vyhodnocení označen za jasného vítěze v kategorii analýzy jazyků s nedostatkem zdrojů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This is a system description paper for the CUNI x-ling submission to the CoNLL 2018 UD Shared Task. We focused on parsing under-resourced languages, with no or little training data available. We employed a wide range of approaches, including simple word-based treebank translation, combination of delexicalized parsers, and exploitation of available morphological dictionaries, with a dedicated setup tailored to each of the languages. In the official evaluation, our submission was identified as the clear winner of the Low-resource languages category.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>SloNLP je workshop speciálně zaměřený na zpracování přirozeného jazyka a počítačovou lingvistiku. Jeho hlavním cílem je podpořit spolupráci mezi výzkumníky v oblasti NLP v Česku a na Slovensku.

Mezi témata workshopu patří automatické rozpoznávání mluvené řeči (ASR), automatická analýza a generování přirozeného jazyka (morfologie, syntax, sémantika...), dialogové systémy (DS), strojový překlad (MT), vyhledávání informací (IR), praktické aplikace NLP technologií, a další témata počítačové lingvistiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>SloNLP is a workshop focused on Natural Language Processing (NLP) and Computational Linguistics. Its primary aim is to promote cooperation among NLP researchers in Slovakia and Czech Republic.

The topics of the workshop include automatic speech recognition, automatic natural language analysis and generation (morphology, syntax, semantics, etc.), dialogue systems, machine translation, information retrieval, practical applications of NLP technologies, and other topics of computational linguistics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme svůj projekt LSD, ve kterém se snažíme zjistit, zda neuronové sítě pracují s reprezentacemi podobnými klasickým lingvistickým strukturám.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In recent years, deep neural networks have achieved and surpassed
state-of-the-art results in many tasks, including many natural language
processing problems, such as machine translation, sentiment analysis, image
captioning, etc.

Traditionally, solving these tasks relied on many intermediary
steps, internally using explicit linguistic annotations and representations,
such as part-of-speech tags, syntactic structures, semantic labels, etc. These
smaller substeps were thought of as useful or even necessary to solve the
larger and more complex tasks. 

However, deep neural networks have made it possible to use end-to-end
learning, where the network directly learns to produce the desired outputs
from the inputs, without any explicit internal intermediary representations.

Nevertheless, the networks are structured in such a way that we can still
think of them as using some intermediary representations of the inputs,
although these are learned only implicitly. Some of the representations can be
directly linked to certain parts of the input -- such as word embeddings
corresponding to individual words -- others are linked to the inputs more
vaguely, due to using recurrent units, attention, etc.

In our project, we are interested in investigating these internal representations,
trying to see what information they capture, how they are structured, and what
meaning we can assign to them. More specifically, we are currently trying to
reliably determine to what extent neural networks seem to capture some basic
linguistic notions, such as part of speech, in their various components --
encoder word embeddings, decoder word embeddings, encoder hidden states... We
are also interested in how this depends on the task for which the network is
trained -- language modelling (word2vec), machine translation, sentiment
analysis... Ultimately, we are interested in the somewhat philosophical
question of whether neural networks seem to understand language, or at least
capture the meanings of sentences.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme dialogový systém Politik, který imituje virtuálního politika. Systém zodpovídá otázky na politická témata. Otázky jsou analyzovány pomocí nástrojů počítačového zpracování jazyka pomocí systému Treex bez znalostní báze. Systém je navržen pro češtinu a angličtinu. Po zpracování morfologickém a syntaktickém zpracování otázky se vybírá vhodná šablona odpovědi z ručně vytvořených šablon. Následně je šablona transformována do gramiticky správné věty. Také jsme pilotně zkoumali rozdíly v doméně rozhovoru.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the question-answering system Politician, which
is a chatbot designed to imitate a fictional politician. The chatbot accepts questions on political issues and answers them accordingly. The questions are analyzed using natural language processing techniques, mainly using a custom scenario built on Treex, and no complex knowledge base is involved. There are two working versions so far, in Czech and in English.

Once morphological and syntactic annotations are available for
the question, an appropriate answer template is selected from the manually created set of answer templates based on the nouns, verbs and named entities present in the question. After that, the answer template is transformed into a grammatically correct reply.

We also briefly investigated the differences between the two languages and potential generalization of the approach to other topics. Apparently, morphological and syntactic information provides enough data for a very basic understanding of questions on a specific topic.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku provádíme experimenty s projekcí koreference na velkých datech a analyzujeme je napříč jemně zvolenými kategoriemi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We perform a fine-grained large-scale analysis
of coreference projection.  By projecting gold
coreference  from  Czech  to  English  and  vice
versa  on  Prague  Czech-English  Dependency
Treebank 2.0 Coref, we set an upper bound of
a proposed projection approach for these two
languages.  We undertake a detailed thorough
analysis that combines the analysis of projec-
tion’s  subtasks  with  analysis  of  performance
on individual mention types.  The findings are
accompanied with examples from the corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje experimenty s bilinguálně informovaným rozpoznáváním koreference na česko-anglických datech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Coreference is a basic means to retain coherence of a text that likely exists in every language. However, languages may differ in how a coreference relation is manifested on the surface. A possible way how to measure the extent and nature of such differences is to build a coreference resolution system that operates on a parallel corpus and extracts information from both language sides of the corpus. In this work, we build such a bilingually informed coreference resolution system and apply it on Czech-English data. We compare its performance with the system that learns only from a single language. Our results show that the cross-lingual approach outperforms the monolingual one. They also suggest that a system for Czech can exploit the additional English information more effectively than the other way round. The work concludes with a detailed analysis that tries to reveal the reasons behind these results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tématem této knihy je studium vlastností koreference s použitím mezijazykových přístupů. Navrhujeme dvě mezijazykové metody: rozpoznávání koreference s informací z druhého jazyka a projekci koreference. Výsledky našich experimentů s těmito metodami na česko-anglických datech naznačují, že s ohledem na koreferenci přináší angličtina více informací do češtiny než naopak.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The subject of this monograph is to study properties of coreference using cross-lingual approaches. We designed two cross-lingual methods: the bilingually informed coreference resolution and the coreference projection. The results of our experiments suggest that with respect to coreference English is more informative for cyech than vice versa.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tématem této práce je studium vlastností koreference s použitím mezijazykových přístupů. Motivací práce je výzkum lingvistické typologie založené na koreferenci. Další motivací je prozkoumání, jestli rozdíly ve způsobech, jak jazyky vyjadřují koreferenci, mohou být využity k natrénování lepších modelů pro rozpoznávání koreference. Navrhujeme dvě mezijazykové metody: rozpoznávání koreference s informací z druhého jazyka a projekci koreference. Výsledky našich experimentů s těmito metodami na česko-anglických datech naznačují, že s ohledem na koreferenci přináší angličtina více informací do češtiny než naopak. Rozpoznávání koreference s informací z druhého jazyka navíc dokázalo při aplikaci na paralelních datech překonat na obou jazycích výsledky jednojazykového systému na rozpoznávání. Při experimentech používáme jednojazykový rozpoznávač koreference a vylepšenou metodu na zarovnání koreferenčních výrazů, které jsme rovněž navrhli v rámci této práce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The subject of this thesis is to study properties of coreference using cross-lingual approaches. The work is motivated by the research on coreference-related linguistic typology. Another motivation is to explore whether differences in the ways how languages express coreference can be exploited to build better models for coreference resolution. We design two cross-lingual methods: the bilingually informed coreference resolution and the coreference projection. The results of our experiments with the methods carried out on Czech-English data suggest that with respect to coreference English is more informative for Czech than vice versa. Furthermore, the bilingually informed resolution applied on parallel texts has managed to outperform the monolingual resolver on both languages. In the experiments, we employ the monolingual coreference resolver and an improved method for alignment of coreferential expressions, both of which we also designed within the thesis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme systém pro automatické hodnocení textové koherence v českých esejích psaných nerodilými mluvčími. Systém EVALD pracuje s množinou rysů z oblasti pravopisu, slovní zásoby, morfologie, syntaxe, diskurzních vzahů a koreference. Nově nyní přidáváme rysy z oblasti informační struktury věty (aktuálního členění větného).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a system for automatic evaluation of surface text coherence in Czech essays written by native and non-native speakers. The features of the EVALD system cover spelling, vocabulary, morphology, syntax, discourse relations and coreference. Newly we add features targeting topic–focus articulation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje inovativní kombinaci recurrent neural-network based modelu na úrovni znaků a jazykového modelu aplikovanou na úlohu doplnění diakritiky do textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we describe a novel combination of a character-level recurrent neural-network based model and a language model applied to diacritics restoration. In many cases in the past and still at present, people often replace characters with diacritics with their ASCII counterparts. Despite the fact that the resulting text is usually easy to understand for humans, it is much harder for further computational processing. This paper opens with a discussion of applicability of restoration of diacritics in selected languages. Next, we present a neural network-based approach to diacritics generation. The core component of our model is a bidirectional recurrent neural network operating at a character level. We evaluate the model on two existing datasets consisting of four European languages. When combined with a language model, our model reduces the error of current best systems by 20% to 64%. Finally, we propose a pipeline for obtaining consistent diacritics restoration datasets for twelve languages and evaluate our model on it. All the code is available under open source license on https://github.com/arahusky/diacritics_restoration.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pokrok v kvalitě strojového překladu si žádá nové metody a metriky automatického vyhodnocování. V tomto příspěvku rozšiřujeme testovací sadu pro morfologické rysy (Burlot a Zvon, 2017) o další jazykové páry.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Progress in the quality of machine translation
output calls for new automatic evaluation pro-
cedures  and  metrics.    In  this  paper,  we  ex-
tend  the  Morpheval  protocol  introduced  by
Burlot  and  Yvon  (2017)  for  the  English-to-
Czech  and  English-to-Latvian  translation  di-
rections to three additional language pairs, and
report its use to analyze the results of WMT
2018’s  participants  for  these  language  pairs.
Considering  additional,  typologically  varied
source  and  target  languages  also  enables  us
to  draw  some  generalizations  regarding  this
morphology-oriented evaluation procedure.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje změny v proceduře, která se používá při převodu závislostního korpusu Index Thomisticus do anotačního stylu Universal Dependencies. Index Thomisticus je korpus středověkých latinských textů od Tomáše Akvinského.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the changes applied to the original process used to convert the Index Thomisticus Treebank, a corpus including texts in Medieval Latin by Thomas Aquinas, into the annotation style of Universal Dependencies. The changes are made both to harmonise the Universal Dependencies version of the Index Thomisticus Treebank with the two other available Latin treebanks and to fix errors and inconsistencies resulting from the original process. The paper details the treatment of different issues in PoS tagging, lemmatisation and assignment of dependency relations. Finally, it assesses the quality of the new conversion process by providing an evaluation against a gold standard.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Evaluace rozpoznávání notopisu (Optical Music Recognition, OMR) je dlouholetým trnem v patě oboru. Tento krátký position paper se pokouší vyjasnit, co přesně jsou překážky ve vyhodnocování OMR: detailnější pohled ukazuje, že hlavním problémem je nalézt způsob, jak vypočítat editační vzdálenost mezi prakticky použitelnými reprezentacemi notopisu.  Odhadovat tyto "ceny úprav" pro aplikaci OMR pro přímočarou digitalizaci not je obtížné, tvrdím však, že problémy s modelováním dalších faktorů ovlivňující náročnost lidského post-editování výstupu OMR je možné separovat od vyhodnocování sytémů OMR během jejich vývoje, pokud se použije intrinsní evaluace místo evaluace motivované aplikací, a načrtávám způsob, jak takovou evaluaci udělat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Evaluating Optical Music Recognition (OMR) has
long been an acknowledged sore spot of the field. This short
position paper attempts to bring some clarity to what are actually
open problems in OMR evaluation: a closer look reveals that the
main problem is finding an edit distance between some practical
representations of music scores. While estimating these editing
costs in the transcription use-case of OMR is difficult, I argue
that the problems with modeling the subsequent editing workflow
can be de-coupled from general OMR system development using
an intrinsic evaluation approach, and sketch out how to do this.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozpoznávání notopisu (Optical Music Recognition, OMR) je oblast výzkumu zaměřená na automatické dekódování hudební notace z dokumentů. Vzhledem k tomu, že většina hudebních skladeb v západní tradici existuje pouze v psané podobě, zapojení této modality do digitální domény může podstatně diversifikovat zdroje pro digitální muzikologii, hudební informatiku, aj. Doposud bylo OMR považováno za převážně nevyřešený problém, avšak tato situace se v posledních letech začla měnit: byly vydány rozsáhlé datové sady, metody založené na strojovém učení překonávají dříve nepřekonatelné překážky, a aplikace OMR migrují z motivačních úvodů jednotlivých článků spíše do sekcí Výsledky. Náš tutoriál prezentuje tento nový stav poznání: metody, výsledky, nástroje, a datasety. Po tutoriálu budou účastníci obeznámeni se stavem rozpoznávání notopisu, a měli by být schopní pomocí existujících nástrojů začít OMR integrovat do své práce, ať už hudebně-informatické či muzikologické; pro účastníky, kteří mají zájem na OMR přímo pracovat, pak tutoriál představuje efektivní úvod do problematiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Optical Music Recognition (OMR) is a field of research that investigates how to computationally decode music notation in documents. As most musical compositions in the Western tradition have been written rather than recorded, bringing this music into the digital domain can significantly diversify the sources for MIR, digital musicology, and more broadly lower the costs of introducing previously unheard works to audiences worldwide. While OMR has been regarded as a largely unsolved problem, this situation has recently shifted: new large-scale datasets and tools have been released, methods based on deep learning are successfully dealing with musical symbol detection and partial end-to-end recognition, and applications of OMR such as retrieval have started migrating from article introductions to the Results sections. Our tutorial will present this new and rather exciting state of the art in OMR. We will demonstrate recent methods and results, introduce the audience to the tools and datasets used to achieve them, and showcase the opportunities for using OMR. Finally, we will introduce the current challenges in OMR. After the tutorial, the participants should be familiar with state-of-the-art OMR research, and should be able to start using existing tools to integrate OMR into their own work, whether in MIR or (digital) musicology. For those interested in working on OMR themselves, the tutorial should provide a head start.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozpoznávání notopisu (optical music recognition, OMR) slibuje, že díky němu půjde prohledávat hudební "full-text" v rozsáhlých notových archivech. Otevřel by se tak nový způsob přístupu k obrovskému množství hudby, která byla napsána, avšak ne nahrána. OMR však toto už slibuje dlouho a povětšinou bezvýsledně: jeho výsledky nejsou dostatečně dobré, především pro hudební rukopisy či pro nedokonalé scany. V poslední době se však OMR zlepšilo, především díky pokrokům ve strojovém učení. V tomto příspěvku vezmeme OMR systém založený na tradičním několikakrokovém postupu a druhý systém založený na učení end-to-end, a ilustrujeme jejich možnosti v jednoduchých, prototypických vyhledávacích aplikacích. Na příkladu také ukazujeme, jak použití OMR může výrazně snížit náklady na studie kvantitativní muzikologie. Dohromady tyto výsledky interpretujeme tak, že v určitých úlohách již lze současné technologie OMR nasadit jako obecný nástroj pro obohacování digitálních knihoven.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Optical Music Recognition (OMR) promises to make large collections of sheet music searchable by their musical content. It would open up novel ways of accessing the vast amount of written music that has never been recorded before. For a long time, OMR was not living up to that promise, as its performance was simply not good enough, especially on handwritten music or under non-ideal image conditions. However, OMR has recently seen a number of improvements, mainly due to the advances in machine learning. In this work, we take an OMR system based on the traditional pipeline and an end-to-end system, which represent the current state of the art, and illustrate in proof-of-concept experiments their applicability in retrieval settings. We also provide an example of a musicological study that can be replicated with OMR outputs at much lower costs. Taken together, this indicates that in some settings, current OMR can be used as a general tool for enriching digital libraries.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Detekce notačních symbolů je nejpalčivější otevřený podproblém v rozpoznávání notopisu (Optical Music Recognition, OMR). Ukazujeme, že architektura U-Net pro sémantickou segmentaci spolu s triviálním detektorem představuje silnou baseline, a navrhujeme několik triků, které výsledky ještě zlepšují: trénování proti konvexním obalům notačních objektů, a vícekanálové výstupy které umožňují sdílet parametry sítě pro několik sémanticky příbuzných tříd objektů. Oba triky přináší výrazné zlepšení v detekci klíčů, což má zásadní následky pro výsledky OMR. Následně začleníme U-Nety do kompletního rozpoznávacího systému: přidáme model doplňující vztahy mezi rozpoznanými symboly, a dosáhneme tak výsledného f-score 0.81 pro extrakci výšek zapsaných tónů. Nad takto automaticky extrahovanými tóny provedeme pokusy pro vyhledávání rukopisných kopií stejné hudby, které přináší první empirické indikace, že využívání mocných modelů hlubokého učení pro OMR skutečně dle očekávání přibližuje full-textové vyhledávání ve velkých sbírkách hudebních rukopisů na dosah.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Detecting music notation symbols is the most immediate unsolved  subproblem  in  Optical  Music  Recognition  for musical manuscripts.  We show that a U-Net architecture for semantic segmentation combined with a trivial detector  already  establishes  a  high  baseline  for  this  task,  and we  propose  tricks  that  further  improve  detection  performance: training against convex hulls of symbol masks, and multichannel output models that enable feature sharing for semantically related symbols. The latter is helpful especially for clefs, which have severe impacts on the overall OMR result. We then integrate the networks into an OMR pipeline by applying a subsequent notation assembly stage, establishing a new baseline result for pitch inference in handwritten music at an f-score of 0.81.  Given the automatically inferred pitches we run retrieval experiments on handwritten scores, providing first empirical evidence that utilizing the powerful image processing models brings content-based search in large musical manuscript archives within reach.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Neuronový strojový překlad lze použít jako jedno z metod pro získání spojité větné reprezentace. V příspěvku ukazujeme, že pro takto získané reprezentace kvalita překladu negativně koreluje s kvalitou zachycení významu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>One of possible ways of obtaining continuous-space
sentence representations is by
training neural machine translation (NMT)
systems. The recent attention mechanism
however removes the single point in the
neural network from which the source sentence
representation can be extracted. We
propose several variations of the attentive
NMT architecture bringing this meeting
point back. Empirical evaluation suggests
that the better the translation quality, the
worse the learned sentence representations
serve in a wide range of classification and
similarity tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Český korpus právních textů 2.0 (CLTT 2.0) obsahuje texty z legislativní domény, které jsou manuálně syntakticky anotovány. Nová verze korpusu obsahuje vylepšenou syntaktickou anotaci a dvě nové vrstvy anotací - anotaci jmenných entit a sémantických vztahů. Celkově korpus obsahuje 2 právní dokumenty, 1121 vět a 40950 tokenů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Czech Legal Text Treebank 2.0 (CLTT 2.0) contains texts that come from the legal domain and are manually syntactically annotated. The syntactic annotation in CLTT 2.0 is more elaborate than in CLTT 1.0.  In addition, CLTT 2.0 contains two new annotation layers, namely the layer of entities and the layer of semantic entity relations. In total, CLTT 2.0 consists of two legal documents, 1,121 sentences and 40,950 tokens.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku se zabýváme jedním typem rozdílu v zachycení valenčních struktur v českém a anglickém valenčním slovníku a paralelním česko-anglickém syntakticky anotovaném korpusu. Jde o konstrukce tzv. alternace subjektu a instrumentu (Instrument-Subject Alternation) a jí podobné alternace subjektu a abstraktní příčiny (Abstract Cause-Subject Alternation), příp. alternace subjektu a materiálu (Locatum-Subject Alternation) (Levin, 1993), odpovídající zhruba třem sémantickým typům sloves. Tyto konstrukce představují specifický problém v rámci vzájemného mapování valenčních struktur v paralelním korpusu a jejich zachycení ve valenčních slovnících, neboť u nich dochází ke konkurenčnímu naplnění pozice povrchověsyntaktického subjektu a hloubkověsyntaktického aktoru dvěma sémanticky různými typy doplnění. Výsledkem jsou dvě interpretace, agentní a neagentní, které vstupují do konfliktu při zachycení v syntakticky anotovaných korpusových datech. Konflikt těchto dvou interpretací významně ovlivňuje sémantickou interpretaci „cíle evaluace“ v konstrukcích s evaluativními slovesy v rámci analýzy sentimentu.  Problém analyzujeme na základě vzájemného vztažení syntaktických, sémantických a situačních participantů, s ohledem na již publikovanou literaturu k tématu. Zaměřujeme se na společné rysy všech tří dotčených sémantických slovesných typů i na rozdíly mezi nimi. Zmíněné alternující konstrukce vztahujeme k dalším analogickým konstrukcím a studujeme z hlediska morfosyntaktického, lexikálněsémantického i propozičně obsahového.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper analyzes one type of valency structure difference in a bilingual, Czech-English valency lexicon and a parallel syntactically annotated treebank, namely the instances of Instrument-Subject Alternation, Abstract Cause-Subject Alternation and Locatum-Subject Alternation (Levin, 1993), roughly corresponding to three specific verb semantic classes. These alternations represent a problematic point of mutual alignment of valency structures in a parallel corpus and a bilingual valency lexicon because of the dual possible assignment of a semantic role to the position of a syntactic subject, and consequently, the dual interpretation of the deep-syntactic role of an actor.  As a result, two different interpretations arise, an agentive one and a non-agentive one, which collide in the syntactically annotated data. The conflict of the two intepretations influences substantially the semantic interpretation of the “target of evaluation” in constructions involving evaluative verbs in Sentiment Analysis tasks. We discuss the problem through linking individual syntactic, semantic and situational participants, focusing both on the similarities and differences between the three alternation types in question, providing analogies to other known constructions, and studying them from the morphosyntactic, semantic and propositional content point of view.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek prezentuje aplikaci pro laické, netrénované uživatele, která slouži pro generování kvalitních zarovnaných fonetických přepisů mluveného slova. Aplikace se již několik let používá a přepsalo se jí přes 600 tisíc slovních forem napříč dvěma verzemi webového rozhraní. Představujeme opatření pro kompenzaci nedostatku odborného tréninku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents an application for lay, untrained users to generate high-quality, aligned phonetic transcription of speech. The application has been in use for several years and has served to transcribe over 600 thousand word forms over two versions of a web interface. We present measures for compensating the lack of expert training.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek prezentuje webovou aplikaci nové generace, která umožňuje uživatelům přispívat opravami automaticky získaného přepisu dlouhých záznamů mluveného slova. Popisujeme rozdíly od podobných případů, porovnáváme svoje řešení s ostatními a pozastavujeme se nad vývojem oproti nyní 6 let staré aplikaci, z níž vycházíme, ve světle učiněného pokroku, získaného poučení a v prohlížeči nově dostupných technologií.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a next-generation web application that enables users to contribute corrections to automatically acquired transcription of long speech recordings. We describe differences from similar settings, compare our solution with others and reflect on the development from the now 6 years old work we build upon in the light of the progress made, lessons learned and the new technologies available in the browser.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozšířená anotace diskurzu v části Pražského diskurzního korpusu, přidávající implicitní vztahy, vztahy založené na koreferenci, vztahy mezi otázkou a odpovědí a další typy vztahů strukturujících výpověď.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Enriched discourse annotation of a subset of the Prague Discourse Treebank, adding implicit relations, entity based relations, question-answer relations and other types of discourse structuring relations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška představuje hlavní výsledky srovnávací analýzy věnované vágním diskurzním konektorům v angličtině, češtině, maďarštině, francouzštině a litevštině. Analýza je založena na textech z TED talks.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presentation describes general results of comparison how underspecified discourse connectives are used in English, Czech, Hungarian, French and Lithuanian, based on the texts of TED talks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V knize představujeme výzkum primárních a sekundárních diskurzních konektorů v češtině, provedený na datech Pražského diskurzního korpusu 2.0 (PDiT). Výsledky výzkumu jsou rozděleny do čtyř částí. První část přináší vymezení a charakteristiku diskurzních konektorů v češtině, druhá popisuje anotaci konektorů v PDiT. Ve třetí části jsou představeny výsledky diskurzní anotace PDiT a ve čtvrté obecné principy a tendence v užívání diskurzní konektorů v komunikaci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the monograph, we examine primary and secondary discourse connectives in Czech. The analysis is carried out on the data of the Prague Discourse Treebank 2.0 (PDiT). The results of the research are presented in four parts. The first one focuses on delimitation and characteristics of discourse connectives in Czech, the second one describes annotation of connectives in the PDiT and demonstrates its evaluation, the third one presents results of this annotation, and the fourth one discusses linguistic principles and tendencies in using discourse connectives.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá jazykovými faktory, které ovlivňují výběr diskurzních konektorů při tvorbě koherentního textu. Zaměřuje se na konkurenci tzv. primárních konektorů (gramatikalizovaných a většinou jednoslovných výrazů typu „protože“) a sekundárních konektorů (dosud ne plně gramatikalizovaných výrazů typu „z tohoto důvodu“). Analýza vychází z dat Pražského diskurzního korpusu 2.0 (Prague Discourse Treebank, PDiT), který obsahuje téměř 50 000 vět z českých novinových textů. Ukazuje se, že užívání diskurzních konektorů ovlivňuje tzv. jazyková ekonomie, tj. autoři textu usilují o dosažení maximálního výsledku s vynaložením minimálního úsilí. Nejčastěji proto volí krátké a sémanticky obecnější primární konektory. Naopak v případech, kdy by diskurzní vztahy mohly být interpretovány nesprávně, upřednostňují autoři složitější a specifické konektivní struktury.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we explore the linguistic factors that influence an author's choice of discourse connectives in the production of a coherent text. We focus on the competition between so-called primary connectives (grammaticalized and mostly one-word expressions such as therefore) and secondary connectives (not yet fully grammaticalized compositional discourse phrases such as for this reason). We attempt to describe the linguistic constraints on and preferences in connective selection. The analysis is based on manually annotated data from the Prague Discourse Treebank 2.0 (PDiT), which contains almost 50000 sentences from Czech newspaper texts. We demonstrate that discourse connectives are used in accordance with the economy principle in language, i.e. authors aim to achieve the maximal result with minimal effort. They most frequently choose short and semantically more generalized primary connectives. However, in cases where the discourse relations can be misunderstood, authors prefer more complex and specific structures.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje možnosti, jak zlepšovat dovednosti studentů psát koherentní text pomocí e-learningových metod. Zaměřuje se na téma automatického hodnocení povrchové koherence textu a představuje zlepšení softwaru, který toto hodnocení provádí pro texty v češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents possibilities of improving students’ writing skills through e-learning. It addresses the topic of automated evaluation of surface text coherence and introduces improvements of a system solving this task for Czech. With the implementation of new morpho-syntactic features, the system works with the reliability of 0.63 macro-average F-score. Except for the overall mark for coherence, the system newly provides also a detailed feedback for users. The user (student) learns stronger and weaker aspects of his or her text in the following language fields: spelling, vocabulary, morphology, syntax, and discourse (in terms of coreference and discourse relations). The student (a learner of Czech) is thus given a detailed evaluation of his or her text, which is available online and gives thus space for practicing writing skills easily through e-learning.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme pilotní verzi CzeDLexu, slovníku českých diskurzních konektorů. Ve své aktuální verzi CzeDLex obsahuje 205 lemmat konektorů, získaných z anotací Pražského diskurzního korpusu 2.0 (PDiT). 19 lemmat bylo plně ručně zpracováno, což zahrnuje více než dvě třetiny všech diskurzních vztahů anotovaných v PDiT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a pilot version of CzeDLex, a Lexicon of Czech Discourse Connectives. Currently, CzeDLex contains 205 lemmas of connectives coming from the annotation of the Prague Discourse Treebank 2.0 (PDiT). At this stage, 19 lemmas have been fully manually processed, which covers more than two thirds of all discourse relations annotated in the PDiT.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>UDPipe je trénovatelný nástroj, který provádí segmentaci vět, tokenizaci, morfologické značkování, lemmatizaci a syntaktickou analýzu. Představujeme prototyp UDPipe 2.0 a jeho vyhodnocení v Soutěži CoNLL 2018 UD: Multilingual Parsing from Raw Text to Universal Dependencies, která využívá tři míry pro hodnocení. Z 26 účastníků obsadil prototyp první místo dle míry MLAS, třetí dle míry LAS a třetí dle míry BLEX. V extrinsic hodnocení EPE 2018 se systém umístil na prvním místě v celkovém hodnocení.

Prototyp je založen na neuronovou síťi s jediným společným modelem pro současné morfologické značkování, lemmatizaci a syntaktickou analýzu a je trénován pouze pomocí trénovacích dat CoNLL-U a předtrénovaných slovních embeddingů, na rozdíl od obou systémů, které překonaly tento prototyp v LAS a BLEX mírách.

Open-source zdrojový kód prototypu je k dispozici na adrese http://github.com/CoNLL-UD-2018/UDPipe-Future.

Po soutěží CoNLL 2018 jsme mírně vylepšili modelovou architekturu, což vedlo k lepšímu výkonu jak v intrinsic hodnocení (odpovídající prvnímu, druhému a druhému místu dle metrik MLAS, LAS a BLEX), tak i v extrinsic hodnocení. Vylepšené modely budou brzy k dispozici v UDPipe na adrese http://ufal.mff.cuni.cz/udpipe.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>UDPipe is a trainable pipeline which performs sentence segmentation, tokenization, POS tagging, lemmatization and dependency parsing. We present a prototype for UDPipe 2.0 and evaluate it in the CoNLL 2018 UD Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, which employs three metrics for submission ranking. Out of 26 participants, the prototype placed first in the MLAS ranking, third in the LAS ranking and third in the BLEX ranking. In extrinsic parser evaluation EPE 2018, the system ranked first in the overall score.

The prototype utilizes an artificial neural network with a single joint model for POS tagging, lemmatization and dependency parsing, and is trained only using the CoNLL-U training data and pretrained word embeddings, contrary to both systems surpassing the prototype in the LAS and BLEX ranking in the shared task.

The open-source code of the prototype is available at http://github.com/CoNLL-UD-2018/UDPipe-Future.

After the shared task, we slightly refined the model architecture, resulting in better performance both in the intrinsic evaluation (corresponding to first, second and second rank in MLAS, LAS and BLEX shared task metrics) and the extrinsic evaluation. The improved models will be available shortly in UDPipe at http://ufal.mff.cuni.cz/udpipe.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Shrnutí dokumentu je dobře studovaným NLP úkolem. Se vznikem modelů umělé neuronové sítě se zvyšuje souhrnná výkonnost, stejně jako požadavky na výcvikové údaje. Pro Čechy je však k dispozici pouze několik datových souborů, z nichž žádný není zvlášť velký. Kromě toho bylo shrnutí vyhodnoceno převážně na angličtině, přičemž běžně používaná metrika ROUGE je specifická pro angličtinu. V tomto příspěvku se snažíme řešit obě otázky. Představujeme SumeCzech, český datový soubor pro sumarizaci zpráv. Obsahuje více než milion dokumentů, z nichž každá obsahuje nadpis, několik věty dlouhý abstrakt a úplný text. Sadu dat lze stáhnout pomocí dodaných skriptů, které jsou k dispozici na adrese http://hdl.handle.net/11234/1-2615. Vyhodnocujeme několik souhrnných základních dat na množině dat, včetně silného abstrakčního přístupu založeného na architektuře neuronových sítí Transformeru. Hodnocení se provádí jazykově-agnostickou variantou ROUGE.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Document summarization is a well-studied NLP task. With the emergence of artificial neural network models, the summarization performance is increasing, as are the requirements on training data. However, only a few datasets are available for Czech, none of them particularly large. Additionally, summarization has been evaluated predominantly on English, with the commonly used ROUGE metric being English-specific. In this paper, we try to address both issues. We present SumeCzech, a Czech news-based summarization dataset. It contains more than a million documents, each consisting of a headline, a several sentences long abstract and a full text. The dataset can be downloaded using the provided scripts available at http://hdl.handle.net/11234/1-2615. We evaluate several summarization baselines on the dataset, including a strong abstractive approach based on Transformer neural network architecture. The evaluation is performed using a language-agnostic variant of ROUGE.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje projekt, který sjednocuje jazykové infrastruktury v USA (LAPPSGrid) a Evropě (Clarin) po stránce workflow a z hlediska přístupu uživatelů z jiné oblasti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Andrew K. Mellon Foundation has funded a project to create a “trust network” between the Language Applications (LAPPS) Grid, a major framework for composing pipelines of natural language processing (NLP) tools, and the WebLicht workflow engine hosted by the CLARIN-D Center in Tuebingen. The project also includes integration of NLP services available from the LINDAT/CLARIN Center in Prague. The goal is to allow users on one side of the bridge to gain appropriately authenticated access to the other and enable seamless communication among tools and resources in both frameworks. The resulting “meta-framework” provides users across the globe with access to an unprecedented array of language processing facilities that cover multiple languages, tasks, and applications, all of which are fully interoperable.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zpráva se zaměřuje na existující morfologické zdroje obsahující derivační slovotvorné vztahy. Pro každý zdroj je ve zprávě popsána jeho historie, licence, formát, struktura dat a některé základní statistiky pro porovnání zdrojů. Zpráva představuje první krok v přezkoumání a harmonizaci těchto zdrojů podobným způsobem, jako tomu bylo již učiněno se zdroji syntaktických stromů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The report focuses on existing morphological resources containing derivational wordformation relations. For each resource, the report describes history, licence, format, data structure and some other basic statistics to compare. Therefore, it means the first step to review and harmonize these resources in a similar way as it has already been done with resources of syntactic trees.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme morfologický analyzátor shipiba-koniba, domorodého jazyka z oblasti peruánské Amazonie. Díky robustnosti konečněstavových systémů můžeme vytvořit model komplexní morfosyntaxe tohoto jazyka. Vyhodnocení na textových korpusech ukazuje slibné pokrytí gramatických jevů, omezením je pouze zatím malý slovník. Nástroj je volně k dispozici pro kohokoliv, čímž chceme podpořit další výzkum peruánských domorodých jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a morphological analyzer for Shipibo-Konibo, a low-resourced native language spoken in the Amazonian region of Peru. We resort to the robustness of finite-state systems in order to model the complex morpho-syntax of the language. Evaluation over raw corpora shows promising coverage of grammatical phenomena, limited only by the scarce lexicon. We make this tool freely available so as to aid the production of annotated corpora and impulse further research in native languages of Peru.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento nástroj je prvním morfologickým analyzátorem jazyka šipibo-konibo. Je to</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This tool is the first morphological analyzer ever for this language.
The analyzer is a FST that produces all possible segmentations and tagging sequences in a word-by-word fashion.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Paralelní dvojjazyčné korpusy v souladu s větou jsou hlavním a někdy jediným požadovaným zdrojem pro výuku systémů pro překlad statistických a neurálních strojů (SMT, NMT). Navrhujeme koncovou hlubokou neuronovou architekturu pro jazykové nezávislé zarovnání vět. Kromě zarovnání typu "one-to-one" může náš zarovnávač také provádět cross-a many-to-many alignment. Předkládáme také případovou studii, která ukazuje, jak může výrazná jazyková analýza výrazně zlepšit výkon čisté neuronové sítě. V souboru Europarl korpus (Koehn, 2005) a anglicko-perského korpusu (Pilevar et al., 2011) jsme použili tři páry jazyků pro vytvoření souhrnu dat. Pomocí této datové sady jsme testovali náš systém jednotlivě a v systému SMT. V obou nastaveních jsme dosáhli výrazně lepších výsledků ve srovnání s výchozími zdroji open source.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Sentence-aligned parallel bilingual corpora are the main and sometimes the only required resource for training Statistical and Neural Machine Translation systems (SMT, NMT). We propose an end-to-end deep neural architecture for language independent sentence alignment. In addition to one-to-one alignment, our aligner can perform cross- and many-to-many alignment as well. We also present a case study which shows how simple linguistic analysis can improve the performance of a pure neural network significantly. We used three language pairs from Europarl corpus (Koehn, 2005) and an English-Persian corpus (Pilevar et al., 2011) to generate an alignment dataset. Using this dataset, we tested our system individually and in an SMT system. In both settings, we obtained significantly better results compared to an open source baseline.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Wikipedia poskytuje neocenitelný zdroj paralelních vícejazyčných dat, které jsou ve vysoké poptávce pro různé druhy jazykových šetření, včetně teoretických i praktických studií. Zavedeme vede nový end-to-end neuronový model pro rozsáhlé paralelní sběr dat z Wikipedie. Náš model je nezávislý na jazyku, robustní a vysoce škálovatelný. Používáme náš systém pro shromažďování, francouzsko-anglické a perzština-anglické věty. Hodnocení člověka na konci ukazují silný výkon tohoto modelu při shromažďování vysoce kvalitních paralelních dat. My navrhnout také statistický rámec, který rozšiřuje výsledky našeho lidského hodnocení na jiné jazykové páry. Náš model také získal nejmodernější výsledek německo-anglické datové sady ze společného úkolu BUCC 2017 na paralelní extrakci vět z srovnatelných korpusů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Wikipedia provides an invaluable source of parallel multilingual data, which are in high demand
for various sorts of linguistic inquiry, including both theoretical and practical studies. We intro-
duce a novel end-to-end neural model for large-scale parallel data harvesting from Wikipedia.
Our model is language-independent, robust, and highly scalable. We use our system for collect-
ing parallel German-English, French-English and Persian-English sentences. Human evaluations
at the end show the strong performance of this model in collecting high-quality parallel data. We
also propose a statistical framework which extends the results of our human evaluation to other
language pairs. Our model also obtained a state-of-the-art result on the German-English dataset
of BUCC 2017 shared task on parallel sentence extraction from comparable corpora.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku navrhujeme nový jazykově založený přístup k odpovědi na otázky non-factoid otevřené domény z nestrukturovaných dat. Nejprve vypracujeme architekturu pro textové kódování, na jejímž základě zavedeme hluboký neuronový model. Tato architektura využívá mechanismus dvoustranné pozornosti, který pomáhá modelu soustředit se na otázku a větu odpovědi současně na extrakci frázové odpovědi. Za druhé, do modelu předáváme výstup analyzátoru volebních obvodů a integrujeme do sítě síť jazykových složek, abychom se mohli soustředit na kusy odpovědi spíše než na jeho jednotlivé slova pro vytvoření přírodnějšího výstupu. Díky optimalizaci této architektury se nám podařilo získat výsledky z hlediska výkonnosti, které jsou téměř shodné s lidskými parametry, a konkurenceschopné na nejmodernějších systémech datových souborů SQuAD a MS-MARCO.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we propose a new linguistically-based approach to answering non-factoid open-domain questions from unstructured data. First, we elaborate on an architecture for textual encoding based on which we introduce a deep end-to-end neural model. This architecture benefits from a bilateral attention mechanism which helps the model to focus on a question and the answer sentence at the same time for phrasal answer extraction. Second, we feed the output of a constituency parser into the model directly and integrate linguistic constituents into the network to help it concentrate on chunks of an answer rather than on its single words for generating more natural output. By optimizing this architecture, we managed to obtain near-to-human-performance results and competitive to a state-of-the-art system on SQuAD and MS-MARCO datasets respectively.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška přináší přehled vývoje slovníků textově-spojovacích prostředků napříč jazyky, od prvních z 90. let až po ty nejnovější, včetně projektu CzeDLex, slovníku českých textových konektorů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk is devoted to the development of dictionaries and lexicons of discourse-relational devices across languages, from the very first ones from 1990ies to the recent ones, including the CzeDLex, the Lexicon of Czech Discourse Connectives.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek představuje poloautomatickou metodu budování derivačních sítí. Metoda byla aplikována na polská a španělská jazyková data.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents a semi-automatic method for the construction of derivational networks. The proposed approach applies a sequential pattern mining technique in order to construct useful morphological features in an unsupervised manner.  The features take the form of regular expressions and later are used to feed a machine-learned ranking model.  The network is constructed by applying resulting model to sort the lists of possible base words and selecting the most probable ones.  This approach, besides relatively small training set and a lexicon, does not require any additional language resources such as a list of alternations groups, POS tags etc.  The proposed approach is applied to the lexeme sets of two languages, namely Polish and Spanish, which results in the establishment of two novel word-formation networks.  Finally, the network constructed for Polish is merged with the derivational connections extracted from the Polish WordNet and those resulting from the derivational rules developed by a linguist, resulting in the biggest word-formation network for that language. The presented approach is general enough to be adopted for other languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Neural Monkey je open-source toolkit pro sequence-to-sequence učení. Článek je zaměřen na prezentaci současného stavu toolkitu cílovým skupinám, mezi které patří studenti a výzkumníci, ať už aktivní v komunitě hlubokého učení či nováčci. Pro každou skupinu popisujeme nejvíce relevantní vlastnosti toolkitu spolu s jednoduchým schématem konfigurace, metodami analýzy modelů, které podporují užitečnou intuici, nebo modulární design umožňující snadné prototypování. Shrnujeme relevantní příspěvky vědecké komunity, které vznikly s využitím tohoto toolkitu a rozebíráme charakteristiky našeho toolkitu s ohledem na ostatní existující systémy. Článek uzavírá nástin budoucího vývoje toolkitu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Neural Monkey is an open-source toolkit for sequence-to-sequence learning. The focus of this paper is to present the current state of the toolkit to the intended audience, which includes students and researchers, both active in the deep learning community and newcomers. For each of these target groups, we describe the most relevant features of the toolkit, including the simple configuration scheme, methods of model inspection that promote useful intuitions, or a modular design for easy prototyping. We summarize relevant contributions to the research community which were made using this toolkit and discuss the characteristics of our toolkit with respect to other existing systems. We conclude with a set of proposals for future development.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Toto je česká verze datasetu Multi30k, který se používá při soutěžích v Multimodálním strojovém překladu. Dataset je založený ja datové sadě Flickr30k, která obsahuje přes 30 tisíc fotografií opatřených anglickými popisky. Pro soutěž na WMT16 a WMT17 byly tyto věty přeloženy do Němčiny a Francozštiny. Pro soutež v roce 2018 jsme obahatili tento dataset také o překlady do českého jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This is the Czech version of the Multi30k dataset that is used for WMT competitions in Multimodal Machine Translation. The dataset is based on the Flickr30k dataset with more 30,000 images accompanied by English captions. For the WTM16 and WMT17 German and French translation were added to these captions. For the WTM18 competition, we added also the translation into the Czech language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku představuje příspěvek do soutěže v multimodálním strjovém překladu na WMT18. V našem systému požíváme self-attentive neuronové sítě místo rekurentních. Evaluujeme dvě metody, jak lze zahrnout vizuální rysy do modelu: v prvním používáme vizuální informaci jako další vstup do dekodéruů v druhé metodě trénujeme enkodér tak, aby predikoval vizuální reprezentaci. Pro náš příspěvek jsem vytěžili dodatečná data. Obě navrhované metody přináší výrazné zlepšení oproti obdobným modelům využívajícím neuronové sítě.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our submission to the WMT18 Multimodal Translation Task. The main feature of our submission is applying a self-attentive network instead of a recurrent neural network. We evaluate two methods of incorporating the visual features in the model: first, we include the image representation as another input to the network; second, we train the model to predict the visual features and use it as an auxiliary objective. For our submission, we acquired both textual and multimodal additional data. Both of the proposed methods yield significant improvements over recurrent networks and self-attentive textual baselines.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V novinách poslední dobou můžeme číst, co zase kde udělala umělá inteligence, co dokázala, co umí, co bude umět, kolik lidí připraví o práci, kdy ovládne lidstvo a tak podobně. Za většinou těchto zpráv jsou úspěchy, kterých informatika dosáhla v posledních letech pomocí umělých neuronových sítí. Nejedná se však, jak by mohl název napovídat, o simulaci lidského mozku, ale o celkem jednoduché matematické modely, které se kdysi biologickými neurony vzdáleně inspirovaly. Na příkladu automatického překladu z jednoho jazyka do druhého, který právě díky neuronovým sítím dosáhl v poslední době velkého pokroku, si vysvětlíme, jak neuronové sítě fungují. Ukážeme si, jak lze na internetu sbírat příklady překladů a jak se s jejich pomocí neuronová síť dokáže naučit řešit tak komplexní úlohu jako je právě překlad.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Nowadays, newspapers often write what the artificial intelligence achieved, what other tasks it will master, and when it will conquer the wold. On the example of automatic translation from one language to another, which has recently made great progress due to neural networks, we will explain how neural networks work. We will show how to collect examples of translations on the Internet and how to use them with a neural network can learn how to solve a complex task like translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Automatické hodnocení kvality strojního překladu bylo zásadní pro rychlý vývoj systémů strojního překladu v posledních dvou desetiletích. Zatím se věnuje největší pozornost metrikám, které pracují s textem na úrovni vět, protože stejně tak fungují i překladové systémy. Kvalita překladu ale závisí i na diskurzních jevech, které se nemusí vůbec projevit, pokud se nacházejí uvnitř věty (např. koreference, diskurzní konektory, časová souslednost apod.). Navrhneme tedy několik metrik hodnocení strojového překladu na úrovni dokumentů: zobecnění exsitujících metrik a jazykově nezávislé metody měření lexikální soudržnosti a zachování koreference a morfologie zachování v cílovém jazyce. U těchto také měříme shodu s lidským úsudkem na nově vytvořené datové sadě, které obsahuje lidské hodnocení překladu pro čtyři jazykové páry.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Automatic machine translation evaluation was crucial for the rapid development of machine translation systems over the last two decades. So far, most attention has been paid to the evaluation metrics that work with text on the sentence level and so did the translation systems. Across-sentence translation quality depends on discourse phenomena that may not manifest at all when staying within sentence boundaries  (e.g. coreference, discourse connectives, verb tense sequence etc.). To tackle this, we propose several document-level MT evaluation metrics: generalizations of sentence-level metrics, language-(pair)-independent versions of lexical cohesion scores and coreference and morphology preservation in the target texts. We measure their agreement with human judgment on a newly created dataset of pairwise paragraph comparisons for four language pairs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vydali jsme natrénované modely pro nástroj Neural Monkey, které řeší 3 úlohy: strojový překlad, popisování obrázků, a analýzu sentimentu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This submission contains trained end-to-end models for the Neural Monkey toolkit for Czech and English, solving three NLP tasks: machine translation, image captioning, and sentiment analysis.
The models are trained on standard datasets and achieve state-of-the-art or near state-of-the-art performance in the tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci se zaměřujeme na tři různé úlohy NLP: popis obrázků, strojový překlad, a rozbor sentimentu. Reimplementujeme úspěšné postupy jiných autorů a přizpůsobujeme je českému jazyku. Nabízíme end-to-end architektury, které dosahují nejlepších či téměř nejlepších výsledků známých pro tyto úlohy, přičemž všechny jsou implementovány ve stejném nástroji pro sekvenční učení. Natrénované modely jsou k dispozici jak ke stažení, tak v podobě online dema.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this work, we focus on three different NLP tasks: image captioning, machine translation, and sentiment analysis. We reimplement successful approaches of other authors and adapt them to the Czech language. We provide end-to-end architectures that achieve state-of-the-art or nearly state-of-the-art results on all of the tasks within a single sequence learning toolkit. The trained models are available both for download as well as in an online demo.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Multimodální a abstraktní sumarizace videí bez doménového omezení vyžaduje shrnutí obsahu celého videa v několika krátkých větách a zároveň sloučení informací z více modalit, v našem případě videa a zvuku (nebo textu). Na rozdíl od obvyklé sumarizace žurnalistických textů není cílem pouze „komprimovat“ textové informace, ale poskytnout plynulé textové shrnutí informací, které byly shromážděny z různých vstupních modalit. V tomto příspěvku představujeme úlohu abstraktní sumarizace pro videa bez doménového omezení, ukazujeme, jak může model sekvenčního učení s hierarchickým mechanismem pozorností integrovat informace z různých modalit do uceleného výstupu. Dále prezentujeme pilotní experimenty na How2 korpusu instruktážních videí. Představujeme také novou evaluační metriku pro sumarizaci nazvanou Content F1, která měří spíše sémantickou přiměřenost než plynulost abstraktů, narozdíl od metrik jako jsou ROUGE a BLEU.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Multimodal and abstractive summarization of open-domain videos requires summarizing the contents of an entire video in a few short sentences, while fusing information from multiple modalities, in our case video and audio (or text). Different from traditional news summarization, the goal is less to “compress” text information only, but to provide a fluent textual summary of information that has been collected and fused from different source modalities. In this paper, we introduce the task of abstractive summarization for open-domain videos, we show how a sequence-to-sequence model with hierarchical attention can integrate information from different modalities into a coherent output, and present pilot experiments on the How2 corpus of instructional videos. We also present a new evaluation metric for this task called Content F1 that measures semantic adequacy rather than fluency of the summaries, which is covered by ROUGE and BLEU like metrics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Autoregresivní dekódování je jedinou součástí převádějících sekvence na sekvence, která bráňí masivní paralelizaci při inferenci. Neautoregresivní modely umožňují dekodéru generovat všechny výstupní symboly nezávisle a tedy paralelně. V článku představujeme novou neautoregresivní architekturu založenou na konekcionistické temporální klasifikaci (CTC). Na rozdíl od jiných neautoregresivních metod, které je nutné trénovat v několika krocích, představovaný systém se trénuje monoliticky. Experimentuje se strojovým překladem mezi angličinou a rumunštionou a angličtinou němčinou na standardních testovacích datech z WMT. Naše modely dosahují výrazného zrychlení oproti autoregresivním modelům, přičemž kvalita překladu je srovnatelná s jinými neautoregresivními modely.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Autoregressive decoding is the only part of sequence-to-sequence models that prevents them from massive parallelization at inference time. Non-autoregressive models enable the decoder to generate all output symbols independently in parallel. We present a novel non-autoregressive architecture based on connectionist temporal classification and evaluate it on the task of neural machine translation. Unlike other non-autoregressive methods which operate in several steps, our model can be trained end-to-end. We conduct experiments on the WMT English-Romanian and English-German datasets. Our models achieve a significant speedup over the autoregressive models, keeping the  translation quality comparable to other non-autoregressive models.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Při sekvenčním učením s více zrdoje informace, může být mechanismus pozornosti (attention) modelován různými způsoby. Toto téma bylo důkladně studováno na rekurentních neurnovoých sítích. V tomto článku se zabýváme tímto problém v architektuře Transormer. Navrhujeme čtyři různé strategie kombinace vstupů: sériové, paralelní, ploché a hierarchické. Navrhované metody vyhodnocujeme na úloze multimodálního překladu a překladu z více zdrojových jazyků současně. Z  výsledků experimentů vyplývá, že modely jsou schopny využívat více zdrojů a fungovat lépe než modely s pouze jedním zdrojem informace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In multi-source sequence-to-sequence tasks, the attention mechanism can be modeled in several ways. This topic has been thoroughly studied on recurrent architectures. In this paper, we extend the previous work to the encoder-decoder attention in the Transformer architecture. We propose four different input combination strategies for the encoder-decoder
attention: serial, parallel, flat, and hierarchical. We evaluate our methods on tasks of multimodal translation and translation with
multiple source languages. The experiments show that the models are able to use multiple sources and improve over single source baselines.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme náš neuronový překladač zaslaný do soutěže v překladu zpráv WMT2018. Náš systém je založen na modelu Transformer (Vaswani et al., 2017). Používáme vylepšenou techniku zpětného překladu, kdy opakujeme proces překládání jednojazyčných dat jedním směrem a trénujeme model pro opačný směr pomocí syntetických paralelních dat. Aplikujeme jednoduché, ale účinné filtrování syntetických dat.
Na vstupní věty aplikujeme rozpoznávač koreference za účelem doplnění vypuštěných osobních zájmen. Na přeložený výstup aplikujeme dvě jednoduché substituce.
Náš systém je výrazně (p < 0,05) lepší než všechny ostatní anglicko-české a česko-anglické systémy ve WMT2018.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe our NMT system submitted to
the WMT2018 shared task in news translation.
Our system is based on the Transformer model
(Vaswani et al., 2017). We use an improved
technique of backtranslation, where we iterate
the process of translating monolingual data in
one direction and training an NMT model for
the opposite direction using synthetic parallel
data. We apply a simple but effective filtering
of the synthetic data. We pre-process the input
sentences using coreference resolution in order
to disambiguate the gender of pro-dropped
personal pronouns. Finally, we apply two simple
post-processing substitutions on the translated output.
Our system is significantly (p < 0.05) better than all other English-Czech and Czech-English systems in WMT2018.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce popisuje zlepšení anglicko-českého a česko-anglického strojového překladu
pomocí metod, které lze použít i na další jazyky.
V první části je popsáno několik zlepšení hloubkově-syntaktického překladače TectoMT,
například rozšíření pro další jazyky a domény
nebo implementace nového typu překladových modelů využívajících kontext a různé metody strojového učení.
V druhé části je popsán neuronový překladač Transformer a jeho vylepšení.
Po detailní analýze vlivu různých hyperparametrů,
bylo optimalizováno trénování systému tak, že dosáhl o 1.0 BLEU lepšího překladu
než nejlepší systém v soutěži WMT2017.
Využitím jednojazyčných dat cílového jazyka pomocí nového typu zpětného překladu
bylo dosaženo dalšího zlepšení kvality překladu o 2.8 BLEU.
Využitím doménové adaptace zohledňující překladštinu (translationese)
-- tedy zohledněním toho, zda paralelní data jsou původně psána česky, nebo anglicky --
byl výsledný systém vylepšen o dalších 0.2 BLEU.
Tento výsledný neuronový překladač byl signifikantně lepší (p<0.05)
než všechny ostatní anglicko-české a česko-anglické překladače v soutěži WMT2018.
Podle výsledků ručního hodnocení byla kvalita tohoto strojového překladu dokonce vyšší
než kvalita lidského referenčního překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This thesis describes our improvement of machine translation (MT),
with a special focus on the English-Czech language pair, but using techniques
applicable also to other languages. First, we present multiple improvements
of the deep-syntactic system TectoMT. For instance, we implemented a novel
context-sensitive translation model, comparing several machine learning approaches.
We also adapted TectoMT to other domains and languages. Second,
we present Transformer – a state-of-the-art end-to-end neural MT system. We
analyzed in detail the effect of several training hyper-parameters. With our
optimized training, the system outperformed the best result on the WMT2017
test set by +1.0 BLEU. We further extended this system by utilization of monolingual
training data and by a new type of backtranslation (+2.8 BLEU compared
to the baseline system). In addition, we leveraged domain adaptation and the
effect of “translationese” (i.e which language in parallel data is the original
and which is the translation) to optimize MT systems for original-language and
translated-language data (gaining further +0.2 BLEU). Our improved neural MT
system significantly (p<0.05) outperformed all other systems in English-Czech
and Czech-English WMT2018 shared tasks, in terms of both automatic and
manual evaluation. It was even significantly better than the human reference
translation according to the manual evaluation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popularizační přednáška pro studenty gymnázia o tom, čím se zabývá ÚFAL: zpracování přirozeného jazyka, strojový překlad, umělá inteligence.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A popular lecture for high-school students about the research done at ÚFAL: natural language processing, machine translation, artificial intelligence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje naše experimenty s neuronovým strojovým překladem pomocí frameworku Tensor2Tensoru a modelu Transformer (Vaswani a kol., 2017).
Zkoumáme některé kritické parametry, které ovlivňují kvalitu překladu, paměťovou náročnost, stabilitu trénování a délku trénování. Každý experiment uzavíráme souborem doporučení. Krom jiného zkoumáme škálování na více GPU a poskytujeme praktické tipy pro vylepšené trénování týkající se velikosti dávky, rychlosti učení, počtu zahřívacích kroků, maximální délky věty a průměrování modelů. Doufáme, že naše pozorování umožní ostatním výzkumníkům dosáhnout lepších výsledků vzhledem k jejich specifickým hardwarovým a datovým omezením.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article describes our experiments in neural machine translation using the recent Tensor2Tensor
framework and the Transformer sequence-to-sequence model (Vaswani et al., 2017).
We examine some of the critical parameters that affect the final translation quality, memory
usage, training stability and training time, concluding each experiment with a set of recommendations
for fellow researchers. In addition to confirming the general mantra “more data
and larger models”, we address scaling to multiple GPUs and provide practical tips for improved
training regarding batch size, learning rate, warmup steps, maximum sentence length
and checkpoint averaging. We hope that our observations will allow others to get better results
given their particular hardware and data constraints.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku prezentujeme sadu vylepšení taggeru pro automatickou detekci slovesných víceslovných výrazů, MUMULS. Náš tagger se zúčastnil PARSEME shared tasku a jako jediný byl založen na neuronových sítích. Ukazujeme, že embeddingy slov na základě jejich znaků vedou k zlepšením, především díky redukci množství out-of-vocabulary slov. Dále nahrazením softmaxové vrstvy v dekodéru klasifikátorem založeným na conditional random fields dosahujeme dalšího zlepšení. Na závěr porovnáváme různé druhy reprezentací příznaků zohledňující okolní kontext slova za pomocí různých architektur enkodérů. Experimenty s češtinou ukazují, že kombinace embeddingů založených na konvoluci jednotlivých znaků, self-attentive architektura enkodéru a conditional random filed klasifikátor dosahují nejlepších empirických výsledků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present a set of improvements introduced to MUMULS, a tagger for the automatic detection of verbal multiword expressions. Our tagger participated in the PARSEME shared task and it was the only one based on neural networks. We show that character-level embeddings can improve the performance, mainly by reducing the out-of-vocabulary rate. Furthermore, replacing the softmax layer in the decoder by a conditional random field classifier brings additional improvements. Finally, we compare different context-aware feature representations of input tokens using various encoder architectures. The experiments on Czech show that the combination of character-level embeddings using a convolutional network, self-attentive encoding layer over the word representations and an output conditional random field classifier yields the best empirical results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvěk se zabýval popisem kohezních prostředků vyskytujících se v textech psaných nerodilými mluvčími češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presentation described cohesive means appearing in texts written by learners of Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem příspěvku je výzkum provázanosti textové koreference a aktuálního členění větného a jejich podíl na koherenci textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of the paper is to examine the interplay of text coreference and sentence
information structure and its role in text coherence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek shrnuje průběh 43. ročníku Olympiády v českém jazyce a stručně představuje zadané jazykové úkoly, jejich řešení, přístupy účastníků k úkolům (společně s hodnocením) a jména vítězů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article summarizes the course of the 43rd year of the Olympiad in the Czech language, presenting in brief its general settings as well as the tasks, their solutions, the approaches of the participants (jointly with their evaluation) and the names of winners.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku představujeme dvě softwarové aplikace na automatické hodnocení koherence textu v češtině s názvem EVALD – Evaluátor diskurzu. První aplikace "EVALD 1.0" hodnotí texty psané rodilými mluvčími češtiny (hodnotící škálou užívanou v českých školách: 1–5). Druhá aplikace "EVALD 1.0 pro cizince" hodnotí texty nerodilých mluvčích češtiny (na škále A1–C2 podle "Společného evropského referenčního rámce"). Obě aplikace jsou dostupné online na https://lindat.mff.cuni.cz/services/evald-foreign/.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In  the  paper, we  introduce  two  software applications  for  automatic  evaluation  of coherence in Czech texts called EVALD – Evaluator  of  Discourse.  The  first  one – EVALD 1.0 – evaluates texts  written by native  speakers  of  Czech on a  five - step scale  commonly  used  at  Czech  schools (grade 1 is the best, grade 5 is the worst). The second application is EVALD 
1.0 for Foreigners  assessing  texts  by  non-native speakers  of  Czech  using  six - step  scale (A1–C2) according to CEFR. Both applications are available online at https://lindat.mff.cuni.cz/services/evald-foreign/.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slovník afixů zpracovává předpony (prefixy) a přípony (sufixy), představuje však i četné části složených slov, jimž se někdy přiznává blízkost afixům. Je to tedy kompendium popisující všechny důležité odvozovací prvky slov užívaných v současných českých textech – včetně slov přejatých. Netradiční pohled na tvoření slov od afixů dává trochu odlišný obraz než běžný směr od derivátů: do ohniska pozornosti se leckdy dostanou doklady z různých příčin opomíjené. Výhodou je i korpus jako zdroj dat: materiál je dostatečně reprezentativní a poskytuje vesměs spolehlivý základ pro odborné závěry. Pomocí slovníku je možné studovat systém afixů v závislosti na jejich frekvenci či produktivitě. Lze z něho vyjít při řešení otázek synonymie afixů, návaznosti na povahu základu a mnoha dalších jevů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The dictionary of affixes presents not only prefixes and sufixes but also many word-parts of compound words.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kniha přináší nejnovější poznatky z oblasti strojového zpracování českých emocionálně laděných textů. Za prvé přináší analýzu jazykových prostředků, které společně formují emocionální význam psaných výpovědí v češtině. Za druhé využívá zjištění týkající se emocionálního jazyka v komputačních aplikacích.

Autorka podává systematický přehled lexikálních, morfosyntaktických, sémantických a pragmatických aspektů emocionálního významu v českých výpovědích a navrhuje formální reprezentaci emocionálních struktur v rámci Pražského závislostního korpusu a konstrukční gramatiky.

V oblasti komputačních aplikací se zaměřuje na témata sentiment analysis, tedy automatické extrakce emocí z textu. Popisuje tvorbu ručně anotovaných emocionálních zdrojů dat a řeší základní úlohy postojové analýzy, jako je např. klasifikace polarity a identifikace cíle hodnocení, a to za využití nejnovějších metod z oblasti automatického zpracování přirozeného jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The study introduces latest research findings from the area of machine processing of Czech emotional language data. First, it provides an analysis of language means which together form an emotional meaning of written utterances in Czech. Second, it employs the findings concerning emotional language in computational applications.

The author provides a systematic overview of lexical, morphosyntactic, semantic and pragmatic aspects of emotional meaning in Czech utterances. Also, she proposes two formal representations of emotional structures within the framework of the Prague Dependency Treebank and Construction Grammar.

Regarding the computational applications, the study focuses on sentiment analysis, i.e. automatic extraction of emotions from text. It describes a creation of manually annotated emotional data resources in Czech and performs main sentiment analysis tasks, such as e.g. polarity classification and opinion target identification on Czech data, employing the up-to-date methods of natural language processing.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku navrhujeme tři různé metody auotmatického vyhodnocování kvality strojového překladu. Dvě z těchto metrik jsou trénovatelné na skóre přímého vyhodnocení a dvě z nich používají závislostní struktury. Trénovatelná metrika AutoDA, která používá hloubkově syntaktické rysy, dosáhla lepší korelace s lidmi ve srovnání např. s metrikou chrF3.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this report paper we propose three different methods for automatic evaluation of the machine translation (MT) quality. Two of the metrics are trainable on direct-assessment scores and two of them use dependency structures. The trainable metric AutoDA, which uses deep-syntactic features, achieved better correlation with humans compared e.g. to the chrF3 metric.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento balíček obsahuje datové sady pro vývoj a testování modelů strojového překladu pro krátké vyhledávací dotazy v oblasti medicíny, a to pro čestinu, angličtinu, francouzštinu, němčinu, španělštinu, maďarštinu, polištinu a švédštinu. Dotazy obsažené v datech pochází jak od zdravotnických profesionálů, tak od laické veřejnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This package contains data sets for development and testing of machine translation of medical search short queries between Czech, English, French, German, Hungarian, Polish, Spanish and Swedish. The queries come from general public and medical experts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento balíček obsahuje sady paralelních vět pro vývoj a testování strojového překladu souhrnů vědeckých článků z oboru medicíny mezi češtinou, angličtinou, francouzštinou, němčinou, maďarštinou, polištinou, španělštinou a švédštinou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This package contains data sets for development (Section dev) and testing (Section test) of machine translation of sentences from summaries of medical articles between Czech, English, French, German, Hungarian, Polish, Spanish
and Swedish. Version 2.0 extends the previous version by adding Hungarian, Polish, Spanish, and Swedish translations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek popisuje utváření syntaktické struktury komplexních predikátů v češtině s ohledem jak na hloubkovou, tak povrchovou syntax.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the syntactic structure formation of Czech complex predicates with light verbs with respect to both deep and surface syntax.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku popisujeme česká deverbativní substantiva v substantivních i slovesných konstrukcích (tj. v komplexních predikátech s funkčním slovesem) a složité propojení slovníku a gramatiky při jejich tvoření. Naznačujeme, že komplexní predikáty s funkčním slovesem jsou výsledkem pravidelné sytaktické operace. Představujeme dva propojené valenční slovníky, NomVallex a VALLEX, s cílem ukázat jak minimalizovat počet slovníkových hesel při současném zachování možnosti generovat správně utvořené substantivní i slovesné konstrukce s deverbativními substantivy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we provide a well-founded description of Czech deverbal nouns in both nominal and verbal structures (light verb constructions), based on a complex interaction between the lexicon and the grammar. We show that light verb constructions result from a regular syntactic operation. We introduce two interlinked valency lexicons, NomVallex and VALLEX , demonstrating how to minimize the size of lexicon entries while allowing for the generation of well-formed nominal and verbal structures of deverbal nouns.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek shrnuje poznatky zjištěné při vytváření lexikografického modelu pro popis komplexních predikátů s kategoriálním slovesem v češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper summarizes theoretical findings on the lexicographic description of Czech complex predicates with light verbs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku se zabýváme koreferencí u českých komplexních predikátů s kategoriálním slovesem. Popisujeme jednotlivé typy koreference mezi valenčními doplněními slovesa a jména, zjištěné v rozsáhlé anotaci korpusových dat, a ukazujeme, jakou roli sehrává ve formování jejich syntaktické struktury.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we deal with coreference in Czech complex predicates with light verbs. In an extensive data annotation project, we identify,
delimit and thoroughly describe individual types of coreferential relations between valency
complementations of Czech complex predicates.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Obor rozpoznávání pojmenovaných entit v češtině (tj. úkol automaticky identifikovat a klasifikovat významné části textu, jako například jména lidí, míst a organizací) se významně rozvinul po vydání českého korpusu pojmenovaných entit, Czech Named Entity Corpus (CNEC). Tato doktorská práce předkládá autorské výsledky v oblasti rozpoznávání pojmenovaných entit, zejména v češtině. Publikuje práci a výzkum provedený v průběhu přípravy CNEC a později během jeho evaluace. Dále shrnuje autorské výsledky, které představují nejlepší známé výsledky v rozpoznávání českých pojmenovaných entit. Na základě jednoduché neuronové sítě s výstupní funkcí softmax a standardní sadou klasifikačních rysů je popsána metodologie a výsledky, ze kterých později vznikl otevřený software pro rozpoznávání pojmenovaných entit, NameTag. Doktorská práce je zakončena popisem rozpoznávače založeném na rekurentních neuronových sítích s embeddingy slov a embeddingy založenými na znacích, které představují výsledky současného výzkumu v oblasti neuronových sítí. Rozpoznávač nevyžaduje tvorbu klasifikačních rysů a dosahuje v současné době nejlepších známých výsledků v oblasti rozpoznávání pojmenovaných entit v češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Czech named entity recognition (the task of automatic identification and classification of proper names in text, such as names of people, locations and organizations) has become a well-established field since the publication of the Czech Named Entity Corpus (CNEC). This doctoral thesis presents the author's research of named entity recognition, mainly in the Czech language. It presents work and research carried out during CNEC publication and its evaluation. It further envelops the author's research results, which improved Czech state-of-the-art results in named entity recognition in recent years, with special focus on artificial neural network based solutions. Starting with a simple feed-forward neural network with softmax output layer, with a standard set of classification features for the task, the thesis presents methodology and results, which were later used in open-source software solution for named entity recognition, NameTag. The thesis finalizes with a recurrent neural network based recognizer with word embeddings and character-level word embeddings, based on recent advances in neural network research, which requires no classification features engineering and achieves excellent state-of-the-art results in Czech named entity recognition.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme korpus českých vět s ručně anotovanými pojmenovanými entitami, ve kterém byla použita bohatá dvouúrovňová hierarchie typů pojmenovaných entit. Korpus představuje první dostupný českým zdroj pro rozpoznávání pojmenovaných entit a od roku 2007 stimuloval výzkum v tomto oboru. Popisujeme dvouúrovňovou jemnou hierarchii s vnořenými entitami a motivace, které nás vedly k jejímu návrhu. Dále ukazujeme, jak byla tato data prakticky využita při návrhu a trénování rozpoznávače pojmenovaných entit a provádíme velké množství experimentů, abychom kriticky ohodnotili rozhodnutí, která jsme v průběhu návrhu korpusu provedli. Důkladně prodiskutujeme dopad zvoleného výběru vět, velikosti korpusu, způsobu morfologického zpracování, ale i výběr typů pojmenovaných entit a dalších vlastností korpusu na výkon rozpoznávače pojmenovaných entit z hlediska strojového učení s učitelem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a corpus of Czech sentences with manually annotated named entities, in which a rich two-level hierarchy of named entity types was used. The corpus was the first available large Czech named entity resource and since 2007, it has stimulated the research in this field for Czech. We describe the two-level fine-grained hierarchy allowing embedded entities and the motivations leading to its design. We further discuss the data selection and the annotation process. We then show how the data can be used for training a named entity recognizer and we perform a number of experiments to critically evaluate the impact of the decisions made in the process of annotation on the named entity recognizer performance. We thoroughly discuss the effect of sentence selection, corpus size, part-of-speech tagging and lemmatization, representativeness and bias of the named entity distribution, classification granularity and other corpus properties in terms of supervised machine learning.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V PARSEME Shared Task bylo úkolem určit slovesné VV v textu. Slovesné VV obsahují idiomy (let the cat out of the bag), analytické predikáty (make a decision), verb-particle constructions (give up) a inherentně reflexivní slovesa (se suicider 'spáchat sebevraždu' ve francouzštině). SVV byly anotovány podle univerzálních pravidel v 18 jazycích. Corpusy jsou k dispozici ve formátu parsemetsv inspirovaném formátem CONLL-U.

...

Obsahuje trénovací data, testovací data, nástroje a univerzální anotační pravidla.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The PARSEME shared task aims at identifying verbal MWEs in running texts. Verbal MWEs include idioms (let the cat out of the bag), light verb constructions (make a decision), verb-particle constructions (give up), and inherently reflexive verbs (se suicider 'to suicide' in French). VMWEs were annotated according to the universal guidelines in 18 languages. The corpora are provided in the parsemetsv format, inspired by the CONLL-U format.

For most languages, paired files in the CONLL-U format - not necessarily using UD tagsets - containing parts of speech, lemmas, morphological features and/or syntactic dependencies are also provided. Depending on the language, the information comes from treebanks (e.g., Universal Dependencies) or from automatic parsers trained on treebanks (e.g., UDPipe).

This item contains training and test data, tools and the universal guidelines file.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slovní vektory jsou rozhraním mezi světem diskrétníh jednotek slov a spojitým světem neuronových sítí. V této práci prozkoumáme různé druhy jejich inicializace na čtyrech úlohách z oblasti zpracování přirozeného jazyka a dvou různých neuronových architekturách. Potvrdíme, že předtrénované embeddingy rychleji konvergují k řešení a že pro náhodnou inicializaci nezávisí tolik na odchylce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Word embeddings are the interface between
the world of discrete units of text
processing and the continuous, differentiable
world of neural networks. In this
work, we examine various random and
pretrained initialization methods for embeddings
used in deep networks and their
effect on the performance on four NLP
tasks with both recurrent and convolutional
architectures. We confirm that pretrained
embeddings are a little better than
random initialization, especially considering
the speed of learning. On the other
hand, we do not see any significant difference
between various methods of random
initialization, as long as the variance
is kept reasonably low. High-variance initialization
prevents the network to use the
space of embeddings and forces it to use
other free parameters to accomplish the
task. We support this hypothesis by observing
the performance in learning lexical
relations and by the fact that the network
can learn to perform reasonably in its task
even with fixed random embeddings.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prozkoumali jsme vliv řazení vět pro online trénování neuronových sítí pro počítačová překlad. Práce má dvě části: řazení stejných příkladů v rámci minibatche a postupné zvyšování složitosti trénovacích dat (tzv. Curriculum learning). Výsledkem práce je, že homogenita minibatchí nemá na trénování velký vliv zato curriculum lepších větších výsledků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We examine the effects of particular orderings of sentence pairs on the on-line training of neural machine translation (NMT). We focus on two types of such orderings: (1) ensuring that each minibatch contains sentences similar in some aspect and (2) gradual inclusion of some sentence types as the training progresses
(so called “curriculum learning”). In our English-to-Czech experiments, the internal homogeneity of minibatches has no effect on the training but some of our “curricula” achieve a small improvement over the baseline.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V identifikaci jazyka, společný prvním krokem při zpracování přirozeného jazyka, chceme automaticky určit jazyk nějakého vstupního textu. Jednojazyčná identifikace jazyka předpokládá, že daný dokument je napsán v jednom jazyce. Ve vícejazyčné identifikaci jazyka, že dokument je obvykle ve dvou nebo ve třech jazycích a my jen chceme jejich jména. Naš cíl je ještě o krok dále a chceme navrhnout metodu pro identifikaci jazyků, kde se mohou jazyky libovolně měnit v textu a cílem je identifikovat rozpětí každého z jazyků. Naše metoda je založena na obousměrné rekurentních neuronových sítí, která funguje dobře v jednojazyčné a vícejazyčných identifikaci jazyka. Náš nástroj pokrývá 131 jazyků. Tato metoda zachovává přesnost i pro krátké dokumenty a napříč doménami, takže je ideální pro použití bez přípravy tréninkových dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In language identification, a common first step in natural language processing, we want to automatically determine the language of some input text. Monolingual language identification assumes that the given document is written in one language. In multilingual language identification, the document is usually in two or three languages and we just want their names. We aim one step further and propose a method for textual language identification where languages can change arbitrarily and the goal is to identify the spans of each of the languages. Our method is based on Bidirectional Recurrent Neural Networks and it performs well in monolingual and multilingual language identification tasks on six datasets covering 131 languages. The method keeps the accuracy also for short documents and across domains, so it is ideal for off-the-shelf use without preparation of training data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek prezentuje letošní submission CUNI na WAT 2017 Translation Task zaměřující se na japonsko-anglický překlad, jmenovitě překlad vědeckých článků, překlad patentů a novinových článků. Porovnali jsme dvě neuronové architektury, samostatnou standardní sequence-to-sequence s attentionem a architekturu používající konvoluční encoder. Také jsme experimentovali s mnoho druhy předzpracování dat. Navíc přidáváme výsledky našich out-of-domain experimentů získaných kombinací dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents this year's CUNI submissions to the WAT 2017 Translation Task focusing on the Japanese-English translation, namely Scientific papers subtask, Patents subtask and Newswire subtask. We compare two neural network architectures, the standard sequence-to-sequence with attention (Seq2Seq) (Bahdanau et al., 2014) and an architecture using convolutional sentence encoder (FBConv2Seq) described by Gehring et al. (2017), both implemented in the NMT framework Neural Monkey that we currently participate in developing.
We also compare various types of preprocessing of the source Japanese sentences and their impact on the overall results. Furthermore, we include the results of our experiments with out-of-domain data obtained by combining the corpora provided for each subtask.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V prezentácii popisujem nedávno publikovanú kolekciu pre viacjazyčné vyhľadávanie v českých nahrávkach korpusu Malach. Kolekcia obsahuje české nahrávky Archívu vizuálnej histórie, ktoré pozostávajú z rozhovorov s preživšími holokaustu. Archív pozostáva z audio nahrávok, štyroch typov automatických prepisov, manuálnych anotácií vybraných tém a ďalších metadát. Archív spolu obsahuje 353 nahrávok a 592 hodín rozhovorov.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk presents recently published Czech Malach Cross-lingual Speech Retrieval Test Collection.
The collection contains Czech recordings of the Visual History Archive which consists of the interviews with the Holocaust survivors. The archive consists of audio recordings, four types of automatic transcripts, manual annotations of selected topics and interviews' metadata. The archive totally contains 353 recordings and 592 hours of interviews.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V prednáške popisujem základy dolovania informácií z textu. Najprv sa zameriavam na rôzne aplikácie dolovania informácií. Následne popíšem základné a najčastejšie používané metódy dolovania informácií z textu. Nakoniec použijeme niekoľko online nástrojov na spracovanie textu a vytvoríme reprezentáciu textu pomocou word cloudu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the talk, I describe basics of text mining. 
First, I describe various applications of text mining. Then, I focus on basic methods frequently used in text mining applications. Last, we will use online tools to pre-process text and create a word cloud.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článok uvádza prehľad rozličných metód pre spracovanie obrazových dát a porovnáva efektívnosť ich využitia v úlohe vyhľadávania hyperlinkov v archíve videí. Vizuálna informácia, ktorá sa získa pomocou metód Feature Signatures, SIMILE deskriptorov a konvolučných neurónových sietí sa využíva pri výpočte podobnosti snímkov vo videu a umožňuje tak vyhľadávanie podobných tvárí, objektov a pozadia. Vizuálne deskriptory sa tiež používajú pri rozpoznávaní objektov v snímkoch a tento a textový popis je možné ďalej kombinovať s textovým vyhľadávaním na základe automatických prepisov a titulkov. Prezentované experimenty boli robené vrámci benchmarkov MediaEval a TRECVid.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we survey different state-of-the-art visual processing methods and utilize them in hyperlinking. Visual information, calculated using Features Signatures, SIMILE descriptors and convolutional neural networks (CNN), is utilized as similarity between video frames and used to find similar faces, objects and setting. Visual concepts in frames are also automatically recognized and textual output of the recognition is combined with search based on subtitles and transcripts. All presented experiments were performed in the Search and Hyperlinking 2014 MediaEval task and Video Hyperlinking 2015 TRECVid task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Archív obsahuje české nahrávky z Archívu vizuálnej histórie Malach, ktoré pozostávajú z rozhovorov so svedkami holokaustu. Archív pozostáva z audio nahrávok, štyroch typov automatických prepisov, manuálnych anotácií vybraných tém a ďalších metadát. Archív spolu obsahuje 353 nahrávok a 592 hodiín rozhovorov.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The package contains Czech recordings of the Visual History Archive which consists of the interviews with the Holocaust survivors. The archive consists of audio recordings, four types of automatic transcripts, manual annotations of selected topics and interviews' metadata. The archive totally contains 353 recordings and 592 hours of interviews.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem příspěvku je podat přehled anotace elipsy v korpusech Universal Dependencies (UD) 2.0. Z dlouhodobého hlediska je znalost typů a četností eliptických konstrukcí užitečná pro experimenty se syntaktickou analýzou zaměřené na elipsu; to byla také naše původní motivace. Nicméně se ukazuje, že současný stav anotace ještě zdaleka není dokonalý, a tudíž hlavním výstupem naší studie je přehled a popis anotačních chyb či nekonzistencí; doufáme, že tím přispějeme ke zlepšení budoucích verzí korpusů UD.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The goal of this paper is to survey annotation of ellipsis in Universal Dependencies (UD) 2.0 treebanks. In the long term, knowing the types and frequencies of elliptical constructions is important for parsing experiments focused on ellipsis, which was also our original motivation. However, the current state of annotation is still far from perfect, and thus the main outcome of the present study is a description of errors and inconsistencies; we hope that it will help improve the future releases.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Podrobný morfologický popis slovních tvarů v jakémkoli jazyce představuje jednu z podmínek úspěšného automatického zpracování jazykových dat. Cílem této práce je představit projekt nového popisu české morfologie, zejména plánovaných změn v morfologickém značkování. Klíčové změny jsou následující: 1) jednoznačný popis variant; 2) koncept vícenásobného lemmatu; 3) revize definic slovních druhů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A detailed morphological description of word forms in any language represents one of the necessary conditions of a successful automatic processing of linguistic data. 
The aim of this paper is to present the project of a new description of Czech morphology, especially planned changes in the tagset. The key changes are as follows: 1) unambiguous description of variants; 2) concept of a multiple lemma;3) revision of part-of-speech definitions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Text představuje možné analytické přístupy k orálněhistorickému interview s ohledem na jeho situovaný a interakční rozměr.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The oral history (OH) interview is a generally accepted method of obtaining verbal accounts of past events from eyewitnesses. Contemporary OH draws from several scientific disciplines and considers various philosophical and methodological issues. The original approach to OH as the “transparent” locus of information is no longer accepted, and researchers acknowledge that the interview unfolds in a specific time and place, and between particular people. Inevitably, OH interview is a speech exchange nested in situational and interactional context, with participants attempting (among other things) to collaboratively produce a comprehensible account of the past. One of the goals of the interview is to elicit storytelling, often for an imagined audience. Recordings of this specific type of interaction can be subjected to different kinds of analysis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Bezprecedentní možnosti archivace a sdílení informací vedou v naší současné společnosti k tomu, že se obraz minulosti zachycené ve vzpomínkách a vyprávění přímých pamětníků stává důležitým zdrojem našeho (laického i odborného) vědění o této minulosti. S ohledem na rostoucí počet archivovaných orálněhistorických interview (dále OHI), často dostupných na internetu či na specializovaných pracovištích, lze orální historii využít jako specifický zdroj dat, analyzovatelných (mimo jiné) i ze sociologického hlediska. Bližší představení jednoho z takových zdrojů, konkrétně Archivu vizuální historie USC Shoah Foundation, přístupného v Praze prostřednictvím Centra vizuální historie Malach, je hlavním cílem tohoto textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article describes the USC Shoah Foundation's Visual History Archive, and illustrates its utility for doing sociology through two case studies based on student's seminar works.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zabýváme se využitím velkého korpusu (Pražského diskurzního korpusu 2.0) s ručně anotovanými diskurzními vztahy pro vytvoření slovníku českých diskurzních konektorů (CzeDLex). Popisujeme teoretické aspekty projektu a technické řešení založené na Prague Markup Language, které umožňuje efektivní začlenění slovníku do rodiny pražských korpusů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We discuss a process of exploiting a large corpus
manually annotated with discourse relations – the Prague Discourse Treebank 2.0 – to create a lexicon of Czech discourse connectives (CzeDLex). We present theoretical aspects of the project and a technical solution based on the (XML-based) Prague Markup Language that allows for an efficient incorporation of the lexicon into the family of Prague treebanks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje dvě studie zaměřené na softwarové nástroje vyvinuté pro derivační sítě jako např. DeriNet; jde o nástroje určené pro vyhledávání a vizualizaci derivačních stromů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents two  studies on software tools developed for lexical derivationa databases such as  DeriNet; the tools under study serve for querying and visualizing derivational trees contained in the database.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>DeriNet je lexikální síť modelující derivační vztahy v češtině. Uzly odpovídají lexémům, hrany odpovídají slovotvorným derivacím. Současná verze, DeriNet 1.5, byla oproti předcházejícím verzím obohacena například o velké množství předponových i příponových derivací sloveso-sloveso a o vybrané typy příponových derivací substantivum-adjektivum. DeriNet 1.5 je první verzí, která obsahuje anotaci související s dalším slovotvorným typem - kompozicí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>DeriNet is a lexical network which models derivational relations in the lexicon of Czech. Nodes of the network correspond to Czech lexemes, while edges represent derivational relations between a derived word and its base word. The present version, DeriNet 1.5, contains 1,011,965 lexemes (sampled from the MorfFlex dictionary) connected by 785,543 derivational links. Besides several rather conservative updates (such as newly identified prefix and suffix verb-to-verb derivations as well as noun-to-adjective derivations manifested by most frequent adjectival suffixes), DeriNet 1.5 is the first version that contains annotations related to compounding (compound words are distinguished by a special mark in their part-of-speech labels).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje postup transformace latinského závislostního treebanku do dotazovatelné podoby, aby mohl být prohlížen online pomocí dotazovacího nástroje nad závislostními stromy. Nejdříve jsou představeny anotační vrstvy, poté architektura dotazovacího nástroje a nakonec postup konverze do relační databáze.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes how to turn a Latin dependency treebank into queryable information so that it can be browsed online using a tree query engine and its web interface. The annotation layers of the treebank are first introduced, then the query system architecture is detailed, and finally the way the treebank is converted into a relational database architecture is described.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá užíváním předpon v českém sylabickém trocheji.Testujeme hypotézu, kterou nastolili Miroslav Červenka, Květa Sgallová a Petr Kaiser, která uvádí, že někteří autoři v 19. století používali předpony ke zmírnění rytmických nepravidelností. V naší analýze - založené na automatickém rozpoznávání předpony ve velkém souboru poetických textů z Korpusu českého verše - pozorujeme jasnou tendenci v práci některých autorů používat předpony v takových kontextech s četností výrazně vyšší, než by se dalo očekávat pouze náhodou. Dále pozorujeme, že tato technika je velmi běžná v první polovině 19. století, ale v pozdějších pracích postupně vymizela.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article deals with the use of prefixes in the Czech accentual syllabic trochee.
We test a hypothesis raised by Miroslav Červenka, Květa Sgallová, and Petr Kaiser which states
that some authors in the 19 th century used prefixes to moderate rhythmical irregularities. In our
analysis – based on automatic prefix recognition in a large body of poetic texts from the Corpus
of Czech Verse – we observe a clear tendency in the work of some authors to employ prefixes in
such contexts with a frequency significantly higher than would be expected merely by chance.
Furthermore, we observe this technique to be very common in the first half of the 19 th century,
but to gradually disappear in later works.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V teto prednasce provadim srovnani korpusu anotovanych na koreferencni a diskurzni vztahy v ruznych jazycich. Popisuju diachronni a synchronni prechody mezi diskurzem a koreferenci, na prikladech z anglictiny, nemciny a cestiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Coreference and discourse markers are strong cohesive devices providing textual coherence. In my presentation, I discuss the interconnections between them. I present and briefly classify corpora annotated with coreference and discourse relations, with special focus on corpora that have both types of annotation, as well as linguistic research on the topic. Furthermore, I will describe some structural and diachronic coreference-to-discourse transitions and support them with examples from English, German and Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem přednášky je souhrnně představit výzkum výstavby textu, který probíhá v Ústavu formální a aplikované lingvistiky MFF UK.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of the lecture is to present the research in the field of text coherence in the Prague Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Srovnavame nemcinu, cestinu a anglictinu vzhledem k tomu, jak se v cestine a anglictine odrazi lokalni adverbia (typ "damit"). Vyzkum provadime na paralelnich TED-talks.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The purpose of the present study is to analyse the interplay between connectives or
discourse-relational devices (DRDs) and other discourse-related phenomena, i.e. coreference and
bridging in German, English and Czech. DRDs express logico-semantic relations between
propositions, such as contrast, time, addition and others). Coreference serves the task of linking
identical referents and events (i.e. complex anaphors), whereas bridging expresses near identical
relations between referents, linking them with semantic meanings. All these devices contribute to
the construction of meaningful discourse. These phenomena exist in all the three languages under
analysis. However, their realisations depend on the different preferences these languages have (both systemic and context-based). The knowledge of these preferences is important for translation, as
translators have to be aware of the full range of linguistic options that exist in both source and
target language. We aim at describing these preferences for German, English and Czech. For these,
different transfer patterns will be extracted from a parallel corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Srovnavame koreferencni vyrazy v paralelnich textech. Jde nam o vedlejsi vety vs. neosobni klauzy, korelativni vyrazy a pro-drop kvality v cestine, polstine, rustine a anglictine.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The subject of our analysis are cohesive devices important for discourse analysis in three Slavonic languages vs. English based on translated texts, particularly coreferential expressions such as finite and infinite constructs e.g. relative clauses, participial, possessive and correlative constructions with a demonstrative pronoun. A special point of our analysis is comparison of pro-drop qualities of three Slavonic languages in comparison with English.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Analýza jmenných přísudků v predikátech jmenných se sponou v pozici na začátku věty. V naší analýze hledáme pravidelné vzorky v konstrukcích s nezákladním slovosledem, které slouží jako prostředek koherence textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In our contribution we analyse second arguments in sentences with verbonominal predicate in the initial sentence position. Our material
provides evidence that the cohesion in these
cases is not given by semantics of these words only, but it is also determined by syntactic features, such as relational (valency) properties of nouns and adjectives and their initial position in the sentence. Operating together, these two factors represent strong means of text cohesion. In our contribution, we document and analyse this cohesive device in Czech on the data from the Prague Dependency Treebank, a large-scale newspaper corpus, annotated with morphological,
shallow and underlying syntactic information, as well as with coreference, discourse and bridging
relations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popularizační přednáška o výpočetní lingvistice, jejích podoblastech a řešených problémech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A popularizing talk about computational linguistics, its subtasks and problems which are being solved.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Detekce jednotlivých symbolů na stránce hudební notace je jedním ze zbývajících nevyřešených podproblémů rozpoznávání not. Navrhujeme použít plně konvoluční segmentační neuronovou síť, která produkuje vysoce kvalitní pravděpodobnostní masky přes pixely. Pokusy na detekci notových hlaviček nad těmito maskami dosahují f-score 0.98 i při použití pouze elementárních detektorů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Musical symbol detection on the page is an out- standing Optical Music Recognition (OMR) subproblem. We propose using a fully convolutional segmentation network to produce high-quality pixel-wise symbol probability masks. Experiments on notehead detection show a very promising detection f-score of 0.98 with elementary detection methods.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slovník jednoslovných parafrází slovesných idiomatických konstrukcí a víceslovných predikátů s lehkými slovesy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>ParaDi 2.0. is a dictionary of single verb paraphrases of Czech verbal multiword expressions - light verb constructions and idiomatic verb constructions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci prezentujeme nový, volně dostupný slovník parafrází českých komplexních predikátů s lehkými slovesy ParaDi. Kandidáti na jednoslovné predikativní parafráze byli vybráni automaticky z velkých jednojazyčných dat pomocí word2vecu. Dále byli manuálně ověřeni. V experimentu s vylepšováním kvality strojového překladu ukazujeme jednu z mnoha praktických aplikací ParaDi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a new freely available dictionary of paraphrases of Czech complex predicates with light verbs, ParaDi. Candidates for single predicative paraphrases of selected complex predicates have been extracted automatically from large mono-lingual data using word2vec. They have been manually verified and further refined. We demonstrate one of many possible applications of ParaDi in an experiment with improving machine translation quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slovník ParaDi poskytuje jednoslovné parafráze komplexních predikátů v češtině, které byly získány automaticky a dále manuálně ověřeny a obohaceny o sémantické a syntaktické rysy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The ParaDi dictionary provides single verb paraphrases of Czech light verb constructions obtained automatically and manually verified, refined and specified with syntactic and semantic properties.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zpráva o účasti týmu Univerzity Karlovy v soutěži vyhledávání zdravotních informací CLEF eHealth Evaluation Lab 2017.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we present our participation as the team of the Charles University at Task3 Patient-Centred Information Retrieval in CLEF eHealth Evaluation lab 2017</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek na interním workshopu DGT "Smart Select" o nástrojích pro neuronový strojový překlad i o potížích při trénování modelů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A talk at an in-house workshop of DGT "Smart Select" on neural MT toolkits and on the difficulties of NMT training.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek představil některé aspekty překotného vývoje na poli neuronového strojového překladu a naznačil, jak v prudce se měnícím prostředí udržet stabilní směr výzkumu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk has highlighted some aspects of the frantic development in the area of neural machine translation and suggested some ways of keeping the research directions stable in the quickly changing environments.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popularizující souhrn nejnovějšího vývoje ve strojovém překladu -- základy neuronového MT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A popularizing summary of the most recent developments of machine translation -- the basics of neural MT.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisující neuronový přístup ke strojovému překladu obecně a první experimenty s neuronovým překladem pro IBM.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A talk describing neural MT in general and our first experiments with neural MT for IBM.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Shrnutí stavu vývoje neuronových překladačů na UK MFF v rámci projektu QT21.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A summary of the development of neural MT at Charles University.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Seznámení zájemců o obor informatiky a počítačové lingvistiky s nejnovějším vývojem na poli strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A quick introduction to prospective students of Computer Science and Computational Linguistics with recent advances in machine translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popularizační přednáška shrnující stručnou historii strojového překladu: frázový, hloubkový a jejich kombinaci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A popularizing talk, a brief summary of the history of MT: phrase-based, deep-syntactic and their combination.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V posledních dvou letech strojový překlad prošel převratnou změnou technologie. Spolehlivý není stále, ale kvalitou se lidem vyrovná. Popíšeme si stav strojového překladu dnes (za půl roku to bude už zas jinak) a naznačíme, co dalšího může tzv. hluboké učení do zpracování textu přinést a jak to může ovlivnit budoucí roli wikipedistů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The last two years of machine translation have seen a tremendous change of the technology. MT is still not reliable but it is close to match human translators' quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce představím tzv. hluboké učení, tj. strojové učení realizované pomocí neuronových sítí. Hluboké učení spustilo revoluci v mnoha oblastech automatického zpracování signálu a zhruba od roku 2016 zcela změnilo techniky užívané ve strojovém překladu. Podrobně si projdeme, jak se dnes strojový překlad pomocí neuronových sítí modeluje, jak dobře překládá a jakých chyb se dopouští.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk presents deep learning, i.e. machine learning carried out by neural networks. Deep learning has started a revolution in many areas of automatic processing of signals, and fully changed the methods used in machine translation in 2016.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje neuronové systémy a systémy založené na frázovém překladu, kterými UK přispělo do anglicko-českého News Translation Task v WMT17. Experimentujeme se syntetickými daty pro trénování a zkoušíme několik technik na kombinaci systémů, jak neuronových, tak frázových. Náš hlavní příspěvek CU-CHIMERA využívá frázových překlad s využitím neuronových a hluboce-syntaktických navrhovaných překladů</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the neural and
phrase-based machine translation systems
submitted by CUNI to English-Czech
News Translation Task of WMT17. We
experiment with synthetic data for training
and try several system combination techniques,
both neural and phrase-based. Our
primary submission CU-CHIMERA ends
up being phrase-based backbone which incorporates
neural and deep-syntactic candidate
translations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek shrnuje výsledky soutěžních úloh konference WMT17: tři překladové úlohy (novinové texty, biomedicínské texty a multimodální překlad), dvě úlohy v automatickém hodnocení (metriky a odhad kvality MT), automatickou korekturu, úlohu v trénování neuronových MT systémů a učení metodou jednorukého bandity.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the
WMT17 shared tasks, which included
three machine translation (MT) tasks
(news, biomedical, and multimodal), two
evaluation tasks (metrics and run-time estimation
of MT quality), an automatic
post-editing task, a neural MT training
task, and a bandit learning task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje výsledky soutěže v automatickém hodnocení kvality překladu WMT17 Metrics Shared Task.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the
WMT17 Metrics Shared Task. We asked
participants of this task to score the outputs
of the MT systems involved in the
WMT17 news translation task and Neural
MT training task. We collected scores
of 14 metrics from 8 research groups. In
addition to that, we computed scores of
7 standard metrics (BLEU, SentBLEU,
NIST, WER, PER, TER and CDER) as
baselines. The collected scores were evaluated
in terms of system-level correlation
(how well each metric’s scores correlate
with WMT17 official manual ranking of
systems) and in terms of segment level
correlation (how often a metric agrees with
humans in judging the quality of a particular
sentence).
This year, we build upon two types of
manual judgements: direct assessment
(DA) and HUME manual semantic judgements.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje výsledky úlohy WMT17 pro trénink neuronového strojového překladu.

Cílem této úlohy je prozkoumat metody výcviku předem zvolené neuronové architektury, zaměřené především na nejlepší kvalitu překladu a jako sekundární cíl kratší čas trénování.

Účastníci měli k dispozici kompletní systém pro neuronový strojový překlad, výchozí parametry trénování a konfiguraci sítě.

Překlad byl proveden v anglicko-českém směru a úkol byl rozdělen na dvě podskupiny s různými konfiguracemi - jedna byla upravena tak, aby se vešla na 4GB a druhá na 8GB GPU kartu.

Obdrželi jsme 3 řešení pro variantu 4 GB a 1 řešení pro variantu 8 GB; také jsme poskytli výsledky nášeho běh pro každou velikost jako baseline.

Přeložili jsme zkušební sadu netrénovanými modely a výsledky vyhodnotili pomocí několika automatických metrik.

Uvádíme také výsledky lidského hodnocení předložených systémů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the WMT17 Neural MT Training Task.

The objective of this task is to explore the methods of training a fixed neural architecture, aiming primarily at the best translation quality and, as a secondary goal, shorter training time.

Task participants were provided with a complete neural machine translation system, fixed training data and the configuration of the network.

The translation was performed in the English-to-Czech direction and the task was divided into two subtasks of different configurations - one scaled to fit on a 4GB and another on an 8GB GPU card.

We received 3 submissions for the 4GB variant and 1 submission for the 8GB variant; we provided also our run for each of the sizes and two baselines.

We translated the test set with the trained models and evaluated the outputs using several automatic metrics.

We also report results of the human evaluation of the submitted systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek přestavil aktuální stav vývoje a experimentů s neuronovými překladači na UK MFF v rámci projektu QT21.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We presented the current state of development and experiments with neural MT at Charles University within the project QT21.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>UFAL Medical Corpus je sbírka paralelních korpusů,
které byly shromážděny pro účely EU projektů KConnect, Khresmoi a HimL s cílem dosáhnout spolehlivějšího strojového překladu medicínských textů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>UFAL Medical Corpus is a collection of parallel corpora assembled for the purposes of the EU projects KConnect, Khresmoi and HimL aiming at more reliable machine translation of medical texts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>MorphInd je robustní morfologický nástroj, který dané povrchové slovní formě přiřazuje její morfologickou značku a příslušné lemma, a umožňuje tak další automatické zpracování.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>MorphInd is a robust fínite state morphology tool for Indonesian, which handles
both morphological analysis and lemmatization for a given surface word form so that it is
suitable for further language processing.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kapitola popisuje historii, základní východiska a anotační schéma Pražského závislostního korpusu z formálně-lingvistického i technického hlediska.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The chapter describes the history and the linguistic theory behind the Prague Dependency Treebank. It also mentions technical solutions for the creation and use of the treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pro dva valenční slovníky slovanckých jazyků, PDT-Vallex pro češtinu a Walenty pro polštinu článek porovnává jejich frazeologickou část. Oba slovníky jsou založené na korpusu, i když se liší jak způsob propojení, tak technické řešení, ovšem oba jsou dostupné elektronicky ve standardnim formátu. V článku se porovnávají frazeologická hesla, jejich formální popis a možnosti a omezení. V závěru se doporučují rozšíření těchto komponent pro obecnější pokrytí i pro další jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Phraseological components of valency dictionaries for two West Slavic languages are presented, namely, of the PDT-Vallex dictionary for Czech and of the Walenty dictionary for Polish. Both dictionaries are corpus-based, albeit in different ways. Both are machine readable and employable by syntactic parsers and generators. The paper compares the expressive power of the phraseological subformalisms of these dictionaries, discusses their limitations and makes recommendations for their possible extensions, which can be possibly applied also to other valency dictionaries with rich phraseological components.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek obsahuje recenze všech referátů přednesených na konfrenci Komise pro gramatickou stavbu slovanských jazyků konané v listopadu 2011 v Sapporu a vydaných v prestižním nakladatelství Otto Sagner (Muenchen -Berlin - Washington).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this contribution the review of all contributions presented on the conference of The Committee of Grmmatical Structure of Slavonic Languages hold at Sapporo university in Japan and printed by the famous Otto Sagner Publishing House (Muenchen - Berlin - Washington)is included.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve stati je analyzován status některých netypických českých infinitivních konstrukcích (konstrukce s "dvojím" akuzativem), dále se slovesy mít a být (modální existenční konstrukce, tzv. absentiv a kauzální rezultát). Uvedené konstrukce se nepokládají za plně gramatikalizované morfologické kategorie.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the first section of the article a non-standard behaviour of several Czech control verbs is
analyzed and illustrated by examples from the Czech National Corpus. In the second part three
types of Czech syntactic constructions based on the verbs mít [to have] and být [to be] connected
with infinitive are presented and their position in the grammatical description of language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve stati se ověřuje platnost kritérií používaných pro stanovení valence při popisu valence substantiv a adjektiv. Poukazuje se na kritéria použitelná obecně a na specificitu valence u dalších slovních druhů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this article the application of the criteria used for the determination of valency of verbs is presented for the noun valency as well as for the adjective valency. The validity of the basic criteria is confirmed, the specificity of the valency of other parts of speech is exemplified.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Porozumění přirozenému jazyku vyžaduje reprezentaci hloubkové struktury věty. Argumentem může být zachycení aktuálního členění a povrchových elips.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Natural language understanding requires representation of the sentence on the deep syntactic layer.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Analýza aktuálního členění věty z hlediska její sémantické relevance a důležitosti pro strojový překlad.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Analysis of the information structure of the sentence from the point of view of its semantic relevance and importance for machine translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Soubor vybraných statí rozdělený do pěti oddílů, a to 1. Valence, 2. Aktuální členění, negace a presupozice, 3. Zachycení AČV v anotovaném  počítačovém korpusu češtiny, 4. Struktura a analýza diskurzu, 5. Rozbor některých zaharničních přístupů k uvedeným otázkám</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A collection of selected papers divided into five parts: 1. Valency, 2. Topic-Focus Articulation (TFA), Negation and Presupposition, 3. TFA in the annotated treebank of Czech, 4. Discourse structure and analysis, 5. Remarks on some related treatments of these issue in linguistic writings abroad</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Souhrn argumentů pro studium a odpovídající anotaci korpusu z hlediska hloubkové syntaktické struktury založené na závislostní gramatice</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A survey of arguments in favour of the study of the deep sentence structure on the basis of dependency grammar, illustrated by the annotation of the Prague Dependency Treebank</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kapitola o informační struktuře ("tématu") v prestižní encyklopedii jaykovědy vydané Oxford University Press v březnu 2017, pouze online.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the linguistic literature, the term theme has several interpretations, one of which relates to discourse analysis and two others to sentence structure. In a more general (or global) sense, one may speak about the theme or topic (or topics) of a text (or discourse), that is, to analyze relations going beyond the sentence boundary and try to identify some characteristic subject(s) for the text (discourse) as a whole. This analysis is mostly a matter of the domain of information retrieval and only partially takes into account linguistically based considerations. The main linguistically based usage of the term theme concerns relations within the sentence. Theme is understood to be one of the (syntactico-) semantic relations and is used as the label of one of the arguments of the verb; the whole network of these relations is called thematic relations or roles (or, in the terminology of Chomskyan generative theory, theta roles and theta grids). Alternatively, from the point of view of the communicative function of the language reflected in the information structure of the sentence, the theme (or topic) of a sentence is distinguished from the rest of it (rheme, or focus, as the case may be) and attention is paid to the semantic consequences of the dichotomy (especially in relation to presuppositions and negation) and its realization (morphological, syntactic, prosodic) in the surface shape of the sentence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kritický rozbor studií J.-M.Zemba o sémantice záporu a o informační struktuře věty z pohledu pražské školy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A critical analysis of the studies of J.-M. Zemb on the semantics of negation and on information structure of the sentence from the point of view of Praguian linguistic theory.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace cílů, průběhu a výsledků projektu KConnect na Dnech H2020 na UK.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presentation of the goals, progress, and results of the KConnect project at the Days of H2020 at CU.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Úspěchy v oblasti medicínského strojového překladu v projektu KConnect</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Achievements in Medical-domain Machine Translation within the KConnect project</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Shrnujeme zapojení našeho týmu CEMI do soutěže s úlohou rozpoznávání rodného jazyka autora, tzv. NLI Shared Task~2017, pro kterou byla k dispozici textová a mluvená data.

Představujeme výsledky, kterých jsme dosáhli použitím tří různých architektur, kde každá z nich kombinuje modely natrénované nad různými příznaky.

Jak jsme očekávali, lepších výsledků dosáhly systémy, které kombinují textové a mluvené příznaky. Dokonce bylo dosaženo dramatického zlepšení.

Naš nejlepší systém je založen na feed-forward neural networks, jejichž výstupy skryté vrstvy jsou kombinovány pomocí softmax. Dosáhli jsme úspěšnosti 0.9257 macro-averaged F1 na evaluační testovací sadě a náš tým spolu s dalšími třemi obsadil první místo v hlavní soutěži.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We summarize the involvement of our CEMI team in the Native Language Identification shared task, NLI Shared Task~2017, which deals with both textual and speech input data.

We submitted the results achieved by using three different system architectures; each of them combines multiple supervised learning models trained on various feature sets.

As expected, better results are achieved with the systems that use both the textual data and the spoken responses. Combining the input data of two different modalities led to a rather dramatic improvement in classification performance.

Our best performing method is based on a set of feed-forward neural networks whose hidden-layer outputs are combined together using a softmax layer. We achieved a macro-averaged F1 score of 0.9257 on the evaluation (unseen) test set and our team placed first in the main task together with other three teams.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme náš projekt o synonymii v bilingvním prostředí.Cílem projektu je prozkoumat sémantickou ekvivalenci slovesných významů v česko-anglickém prostředí. Zaměřujeme se na valenční chování sloves ve spojitosti se sémantickými rolemi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce our ongoing project about synonymy in bilingual context. This project aims at exploring semantic ‘equivalence’ of verb senses of generally different verbal lexemes in a bilingual (Czech-English) setting.  Specifically, it focuses on their valency behavior within such equivalence groups. We believe that using bilingual context (translation) as an important factor in the delimitation of classes of synonymous lexical units (verbs, in our case) may help to specify the verb senses, also with regard to the (semantic) roles relation to other verb senses and roles of their arguments more precisely than when using monolingual corpora. In our project, we work “bottom-up”, i.e., from an evidence as recorded in our corpora and not “top-down”, from a predefined set of semantic classes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek shrnuje první výsledky projektu o kontextové synonymii na základě bilingvního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper summarizes first findings of a three-year study (an ongoing research project) on verb synonymy based on both syntactic and
semantic criteria. Primary language resources used for the study are existing lexical and corpus resources, namely the Prague Dependency
Treebank-style valency lexicons, FrameNet, VerbNet, PropBank and Czech and English WordNets and the parallel Prague Czech-English
Dependency Treebank, which contains deep syntactic and partially semantic annotation of running texts. The resulting lexicon, called
CzEngClass, and all associated resources linked to the existing lexicons and corpora resulting from this project will be made publicly
and freely available. While the project proper assumes manual annotation work, we expect to use the resulting resource (together with
the existing ones) as a necessary resources for developing automatic methods for extending such a lexicon, or creating similar lexicons
for other languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek přináší srovnání valenčního chování českých deverbativních
substantiv zastupujících pět sémantických tříd zahrnutých do valenčního slovníku
NomVallex, konkrétně Mluvení, Výměna, Dotyk, Mentální činnost a Duševní projevy a
stavy. Na datech získaných z českých korpusů představujeme dva způsoby srovnání,
jednak projevy specifického valenčního chování (specifické formy adnominálních
doplnění a redukce počtu valenčních doplnění), jednak kvantitativní analýzu preferencí
v souvýskytu aktantů nebo ve výběru formy některých aktantů. Tato různá hlediska
srovnání umožňují vidět jak některé společné rysy, tak odlišnosti ve valenčním chování
vybraných sémantických tříd, včetně rozlišení centrálních a periferních jevů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper compares valency behavior of Czech deverbal nouns representing
five semantic classes, namely Communication, Exchange, Contact, Mental action and
Psychological nouns, included to a valency lexicon called NomVallex. Using data
extracted from Czech corpora we present two ways of the comparison, first
a manifestation of special valency behavior (special forms of adnominal participants
and reduction of the number of valency slots), second a quantitative analysis focusing
on relative frequencies of combinations of participants and their forms. These different
ways of view enable to see both common and different valency properties of the
semantic classes in question, including differentiation of central and peripheral
phenomena.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku analyzujeme relativní frekvence různých kombinací valenčních doplnění českých deverbativních substantiv v Pražském závislostním korpusu, s cílem optimalizovat korpusové vyhledávky při vytváření valenčního slovníku. Při analýze rozlišujeme produktivně a neproduktivně tvořená substantiva a jejich sémantickou třídu. Zkoumáme také kombinace forem valenčních doplnění a třídíme je s ohledem na jejich relativní frekvence.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In order to optimize corpus searches for valency lexicon production, we analyse the relative frequencies of different combinations of valency complementations of Czech deverbal nouns in the Prague Dependency Treebank, considering differences between productively and non-productively derived nouns and their semantic class. We also classify combinations of forms
of participants according to their frequency.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valenční slovník NomVallex vzniká v teoretickém rámci funkčního generativního popisu, přičemž navazuje na dva valenční slovníky budované v rámci stejné teorie, na Vallex a PDT-Vallex. NomVallex se zaměřuje na deverbativní substantiva zastupující pět sémantických tříd: Mluvení, Kontakt, Výměna, Mentální činnost a Duševní projevy a stavy. V přednášce slovník představíme na materiálu substantiv z třídy Mluvení. U jednotlivých lexémů zpracováváme všechny významy, a kde je to možné, mapujeme je na významy základových sloves ve Vallexu. NomVallex si klade za cíl popsat všechny formy valenčních doplnění u jednotlivých substantiv a také jejich kombinace doložené v Českém národním korpusu (v subkorpusech řady SYN) a v korpusu Araneum Bohemicum Maximum. Přibližný odhad frekvence forem a jejich kombinací získáváme pro jednotlivé sémantické třídy z dat pražských závislostních korpusů (PDT 3.0, PCEDT 2.0 a PDTSC).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presentation focused on the current work on the corpus-based valency lexicon of Czech nouns called NomVallex.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto rozšířeném abstraktu podáváme informaci o vznikajícím valenčním slovníku českých substantiv, nazvaném NomVallex. Analyzujeme rovněž relativní frekvence různých kombinací valenčních doplnění českých deverbativních substantiv v Pražském závislostním korpusu, s cílem optimalizovat korpusové vyhledávky při vytváření valenčního slovníku. Při analýze rozlišujeme produktivně a neproduktivně tvořená substantiva a jejich sémantickou třídu. Zkoumáme také kombinace forem valenčních doplnění a třídíme je s ohledem na jejich relativní frekvence.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The extended abstract summarises the current work on the corpus-based valency lexicon of Czech nouns called NomVallex. In order to optimize corpus searches for valency lexicon production, we analyse the relative frequencies of different combinations of valency complementations of Czech deverbal nouns in the Prague Dependency Treebank, considering differences between productively and non-productively derived nouns and their semantic class. We also classify combinations of forms of participants according to their frequency.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku se zaměřujeme na vyjádření propozice pomocí verbálních substantiv v dějovém užití (např. shánění materiálu, pití čaje) a zkoumáme způsob vyjádření jejich aktantů v psaném a mluveném jazyce. Pro tento účel byla vybrána česká část korpusu Prague Czech-English Dependency Treebank (PCEDT) a mluvený korpus Prague Dependency Treebank of Spoken Czech (PDTSC). Ukazujeme, že v psaném korpusu je nominální skupina syntakticky zhuštěnější a vyjádření adnominálních aktantů je explicitnější, zatímco v mluveném korpusu jsou častější elipsy adnominálních aktantů a častěji se vyskytuje exoforické odkazování. Z kvantitativní analýzy relativních četností kombinací adnominálních aktantů vyplývá, že tyto četnosti jsou sice v psaném korpusu vyšší, jejich pořadí je však stejné v obou korpusech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper extends understanding of differences in expressing actions by verbal
nouns in corpora of written vs. spoken Czech, namely in the Czech part of the Prague Czech-English Dependency Treebank and in the Prague Dependency Treebank of Spoken Czech.
We show that while the written corpus includes more complex noun phrases with more explicit
expression of adnominal participants, noun phrases in the spoken corpus contain more
deletions and more exophoric references. We also carried out a quantitative analysis focusing on relative frequencies of combinations of participants modifying verbal nouns; although the written corpus shows higher relative frequencies, the order of the relative frequencies of particular combinations is the same in both types of communication.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Na příkladu nově vznikajícího slovníku českých diskurzních konektorů CzeDLex představujeme obecné a efektivní technické řešení tvorby takového slovníku, založené na datovém a aplikačním balíku Prague Markup Language a na extrakci základního hrubého obsahu slovníku z velkého anotovaného korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a general and efficient technical solution for building a lexicon of discourse connectives (presented on the case of CzeDLex, a new Lexicon of Czech Discourse Connectives), based on the Prague Markup Language framework and extraction of the raw core version of the lexicon from a large treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jedná se o užitečnou a čtivě napsanou publikaci, která díky svému výrazně praktickému zaměření poslouží jak čtenáři, který chce „pouze“ vyhledávat v již existujících korpusech, tak čtenáři, který si chce vytvořit vlastní, byť třeba jen malý korpus.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>It is a useful and well written book. Thanks to its strong practical focus, it can serve both a user that "only" wants to search in already existing corpora, and also a user that wants to create his own, albeit small corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CzeDLex 0.5 je pilotní verze slovníku českých diskurzních konektorů. Slovník obsahuje konektory částečně automaticky extrahované z Pražského diskurzního korpusu 2.0 (PDiT 2.0), rozsáhlého korpusu s ručně anotovanými diskurzními vztahy. Nejfrekventovanější slovníková hesla (pokrývající více než 2/3 diskurzních vztahů anotovaných v PDiT 2.0) byla ručně zkontrolována, přeložena do angličtiny a doplněna dalšími lingvistickými informacemi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CzeDLex 0.5 is a pilot version of a lexicon of Czech discourse connectives. The lexicon contains connectives partially automatically extracted from the Prague Discourse Treebank 2.0 (PDiT 2.0), a large corpus annotated manually with discourse relations. The most frequent entries in the lexicon (covering more than 2/3 of the discourse relations annotated in the PDiT 2.0) have been manually checked, translated to English and supplemented with additional linguistic information.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CzeDLex je nový elektronický slovník českých diskurzních konektorů. Jeho formát a struktura jsou založeny na studiu obdobných existujících zdrojů a upraveny podle české syntaktické tradice a specifik pražského přístupu k anotaci diskurzních vztahů v textu. Nejprve uvádíme slovník do kontextu podobných zdrojů a probíráme teoretické otázky vytváření slovníku. Poté představujeme technické řešení založené na Prague Markup Language. Následně popisujeme proces získání dat pro slovník z velkého korpusu s ručně anotovanými diskurzními vztahy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CzeDLex is a new electronic lexicon of Czech discourse connectives. Its data format and structure are based on a study of similar existing resources, and adjusted to comply with the Czech syntactic tradition and specifics and with the Prague approach to the annotation of semantic discourse relations in text. We first put the lexicon in context of related resources and discuss theoretical aspects of building the lexicon. Second, we introduce the chosen technical solution based on the Prague Markup Language. Third, we describe the process of getting data for the lexicon by exploiting a large corpus manually annotated with discourse relations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zkoumáme, jakými strategiemi jsou označeny základní aktanty (core arguments) v indoevropských jazycích s pádovou morfologií. Koncept základních aktantů je v Universal Dependencies stěžejní, někdy je však obtížné ho namapovat na terminologie tradičně používané v jednotlivých jazycích. Přezkoumáváme metodologii popsanou Andrewsem (2007) a přidáváme stručné definice některých základních pojmů. Statistiky z 26 treebanků UD ukazují, že ne všichni poskytovatelé dat definují hranici mezi základními (core) a nepřímými (oblique) argumenty stejně. Proto navrhujeme úpravu a upřesnění anotačních pravidel, které zlepší konzistenci napříč treebanky na jedné straně a bude více slučitelné s tradiční gramatikou na straně druhé.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We investigate how core arguments are coded in case-marking Indo-European languages. Core arguments are a central concept in Universal Dependencies, yet it is sometimes difficult to match against terminologies traditionally used for individual languages. We review the methodology described in Andrews (2007), and include brief definitions of some basic terms. Statistics from 26 UD treebanks show that not all treebank providers define the core-oblique boundary the same way. Therefore we propose some refinement and particularization of the guidelines that would improve cross-treebank consistency on the one hand, and be more sensitive to the traditional grammar on the other.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Shrnu soutěž v parsingu, kterou jsme organizovali toto jaro. 11 let po první soutěžní úloze CoNLL v závislostním parsingu byla ta současná jednou z největších soutěží v historii CoNLL, a to jak co do velikosti a rozmanitosti testovacích dat (81 treebanků, 49 jazyků), tak co do počtu účastníků (přes 50 týmů, 32 odevzdaných systémů). Soutěž se vyznačovala několika novinkami: kompletní analýza od prostého textu až po závislostní stromy, jazyky s nedostatkem dat a předem neznámé jazyky, jednotné anotační schéma pro všechny jazyky, evaluace naslepo na vzdáleném serveru. Výsledky představují kvalitativně novou úroveň stavu poznání pro automatickou závislostní analýzu většiny jazyků včetně češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I will summarize the parsing shared task we organized this spring. 11 years after the first CoNLL shared task in dependency parsing, the present one has arguably been one of the largest CoNLL shared tasks ever, both in size and diversity of test data (81 treebanks, 49 languages) and in number of participants (over 50 teams, 32 submissions). There were several novel aspects: end-to-end parsing from raw text, low-resource and surprise languages, unified annotation across languages, blind evaluation on a remote server. The results set the new state of the art for dependency parsing of most languages, including Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Shrnu soutěž v parsingu, kterou jsme organizovali toto jaro. 11 let po první soutěžní úloze CoNLL v závislostním parsingu byla ta současná jednou z největších soutěží v historii CoNLL, a to jak co do velikosti a rozmanitosti testovacích dat (81 treebanků, 49 jazyků), tak co do počtu účastníků (přes 50 týmů, 32 odevzdaných systémů). Soutěž se vyznačovala několika novinkami: kompletní analýza od prostého textu až po závislostní stromy, jazyky s nedostatkem dat a předem neznámé jazyky, jednotné anotační schéma pro všechny jazyky, evaluace naslepo na vzdáleném serveru. Výsledky představují kvalitativně novou úroveň stavu poznání pro automatickou závislostní analýzu většiny jazyků včetně češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I will summarize the parsing shared task we organized this spring. 11 years after the first CoNLL shared task in dependency parsing, the present one has arguably been one of the largest CoNLL shared tasks ever, both in size and diversity of test data (81 treebanks, 49 languages) and in number of participants (over 50 teams, 32 submissions). There were several novel aspects: end-to-end parsing from raw text, low-resource and surprise languages, unified annotation across languages, blind evaluation on a remote server. The results set the new state of the art for dependency parsing of most languages, including Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme převod syntakticky anotované části Slovenského národního korpusu do anotačního schématu známého jako Universal Dependencies. Zatím byla převedena pouze malá část dat, nicméně jde o první slovenský treebank, který je volně přístupný pro výzkumné účely. Uvádíme řadu výzkumných projektů, ve kterých už tato data byla využita, včetně prvních výsledků automatické syntaktické analýzy (parsingu).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe a conversion of the syntactically annotated part of the Slovak National Corpus into the annotation scheme known as Universal Dependencies. Only a small subset of the data has been converted so far; yet it is the first Slovak treebank that is publicly available for research. We list a number of research projects in which the dataset has been used so far, including the first parsing results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme rodinu formátů korpusových formátů zvanou CoNLL, se zvláštním zřetelem na jejího nejnovějšího člena, formát CoNLL-U.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe the family of corpus file formats called CoNLL, with special focus on its newest member, the CoNLL-U format.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme Universal Dependencies (UD), komunitní projekt zaměřený na mezijazykově použitelné anotační schéma pro morfologii a syntaxi přirozených jazyků. Klíčovou myšlenkou UD je, že podobné gramatické konstrukce mají být analyzovány a anotovány podobně; strukturní reprezentace paralelních vět ve dvou jazycích mají být maximálně paralelní. Komunita UD je velmi rozmanitá, stejně jako předpokládané možnosti využití, které se UD snaží podporovat: modely pro počítačové zpracování přirozeného jazyka (zvláště morfologické značkování a syntaktická analýza); jazykovědné bádání a dotazy na korpus; studie z jazykové typologie. Kromě návrhu anotačních pravidel se projekt UD zabývá také sběrem samotných korpusů, jejich převodem do jednotné anotace a jejich zpřístupněním pro výzkum. Vzhledem k tomu, že UD je omezeno dostupností dat, má pochopitelně výrazně větší zastoupení velkých eurasijských jazyků bohatých na digitální zdroje; nicméně, přibývají i vzorky z menšinových jazyků a několika klasických jazyků. První část přednášky představí obecné principy UD. Ve druhé části se podíváme zblízka na treebanky klasických jazyků a probereme obtíže s harmonizací tradiční terminologie ze synchronního i diachronního hlediska. Předvedeme také nástroje, které lze využít k databázovým dotazům nad korpusy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present Universal Dependencies (UD), a community-driven project aimed at defining a cross-linguistically applicable annotation scheme for morphology and syntax of natural languages. The key idea of UD is that comparable constructions should be analyzed and annotated in comparable ways; structural representations of parallel sentences in two languages should be as parallel as possible. The community behind UD is very diverse and so are the use cases that UD tries to support: models for natural language processing (especially tagging and parsing); linguistic research and corpus queries; language-typological studies. Besides defining annotation guidelines, UD also collects actual corpora, converts them to the unified annotation and makes them freely available for research. Being driven by data availability, UD is obviously biased towards resource-rich Eurasian languages; however, there are also samples from minority languages, and several ancient languages, too. The first part of the talk will describe the main principles of UD and present the project in general. In the second part, we will look more closely at the treebanks of classical languages, and discuss some challenges of harmonizing traditional terminology both synchronically and diachronically. We will also demonstrate query tools that can be used to study the data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento balíček obsahuje výstupy 33 automatických syntaktických analyzátorů, které se účastnily společné úlohy (shared task) ve vícejazyčném parsingu z prostého textu do Universal Dependencies, v rámci konference CoNLL 2017.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This package contains the system outputs from the CoNLL 2017 Shared Task in Multilingual Parsing from Raw Text to Universal Dependencies.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Konference o počítačovém učení přirozeného jazyka (CoNLL) zahrnuje soutěž (společnou úlohu, shared task), ve které účastníci trénují a testují své učící systémy na stejných sadách dat. V roce 2017 byla jedna ze dvou soutěží věnována učení závislostních parserů (syntaktických analyzátorů) pro velké množství jazyků, v realistických podmínkách bez jakékoli ruční anotace na vstupu. Všechny testovací sady byly anotovány podle jednotného schématu zvaného Universal Dependencies. V tomto článku definujeme úlohu a vyhodnocovací metodiku, popisujeme přípravu dat, shrnujeme a rozebíráme hlavní výsledky a podáváme stručný přehled jednotlivých přístupů v účastnických systémech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their learning systems on the same data sets. In 2017, one of two tasks was devoted to learning dependency parsers for a large number of languages, in a real-world setting without any gold-standard annotation on input. All test sets followed a unified annotation scheme, namely that of Universal Dependencies. In this paper, we define the task and evaluation methodology, describe data preparation, report and analyze the main results, and provide a brief categorization of the different approaches of the participating systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška se zaměřila na vybrané vlastnosti českých příklonek: především na omezení na jejich umístění ve větě a haplologii reflexivních příklonek.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk evaluated several properties of Czech clitics: mainly constraints on their placement within the sentence and haplology of reflexive clitics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme pilotní studii věnovanou automatické syntaktické analýze žakovského korpusu CzeSL. Provedli jsme experimenty, které naznačily, že základní větné členy subjekt, predikát, objekt, mohou být určeny pomocí parseru natrénovaného na textech od rodilých mluvčích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a pilot study on parsing non-native texts written by learners of Czech. We performed experiments that have shown that at least  high-level syntactic functions, like subject, predicate, and object, can be assigned based on a parser trained on standard native language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>STYX 1.0 je korpus českých vět vybraných z Pražského závislostního korpusu (PDT, https://ufal.mff.cuni.cz/pdt2.0).
Kritériem pro začlenění vět do STYX 1.0 byla jejich vhodnost pro procvičování větných rozborů žáky základních škol.
Věty obsahují anotace z PDT a školní větné rozbory.
Školní rozbory vznikly transformací z anotací PDT pomocí ručně navržených pravidel, dále viz (Kučera, 2006) a (Hladká, Kučera, 2008).
Celkem je v korpusu STYX 1.0 11 655 vět.

Pražský závislostní korpus je pro vývoj a ladění nástrojů rozdělen do tři částí, a sice trénovací, testovací pro vývoj a testovací pro evaluaci (více informací https://ufal.mff.cuni.cz/pdt2.0/doc/pdt-guide/cz/html/ch03.html#a-data-purpose).
STYX 1.0 toto dělení zachovává (viz níže Data).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The STYX 1.0 corpus is a subset of the Prague Dependency Treebank (PDT, https://ufal.mff.cuni.cz/pdt2.0).
The criterion for including sentences into STYX 1.0 was their suitability for practicing Czech morphology and syntax in elementary schools.

The PDT data are divided into three groups: the training data, the development test data and the evaluation test data (see more info https://ufal.mff.cuni.cz/pdt2.0/doc/pdt-guide/cz/html/ch03.html#a-data-purpose).
The STYX 1.0 corpus keeps this division (see Data below). 

The sentences in STYX are annotated according to both the PDT and the Czech school annotation system (sentence diagramming).
The PDT annotation was transformed into the school annotation using manually designed rules, for more info see (Kucera, 2006) and (Hladka, Kucera, 2008).

In total, there are 11,655 sentences in the STYX corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pražský závislostní korpus mluvené češtiny 2.0 (PDTSC 2.0) je korpus mluveného jazyka o objemu 742 316 tokenů, 73 835 vět, což představuje 7 324 minut (více než 120 hodin) spontánních dialogů. Dialogy byly zaznamenány, přepsány a upraveny na několika vzájemně propojených rovinách: zvukový záznam, automatický a ruční přepis a ručně rekonstruovaný text. Tyto vrstvy byly součástí první verze korpusu (PDTSC 1.0). Verze 2.0 je rozšířena o automatickou analýzu závislostí (na analytické rovině) a především o manuální anotaci „hluboké“ syntaxe na tektogramatické rovině, která obsahuje anotaci hloubkových vztahů, valence i anotaci koreference.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Prague Dependency Treebank of Spoken Czech 2.0 (PDTSC 2.0) is a corpus of spoken language, consisting of 742,316 tokens and 73,835 sentences, representing 7,324 minutes (over 120 hours) of spontaneous dialogs. The dialogs have been recorded, transcribed and edited in several interlinked layers: audio recordings, automatic  and manual transcripts and manually reconstructed text. These layers were part of the first version of the corpus (PDTSC 1.0). Version 2.0 is extended by an automatic dependency parser at the analytical and by the manual annotation of “deep” syntax at the tectogrammatical layer, which contains semantic roles and relations as well as annotation of coreference.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme bohatě anotovaný zdroj mluveného jazyka: Pražský závislostní korpus mluvené češtiny, verze 2.0, který je primárně určen pro úlohy NLP orientované na zpracování mluvené řeči, ale najde využití i v nejrůznějších lingvistických studiích. Korpus představuje unikátní anotační schéma: audio, transkript, morfologická, syntaktická a sémantická anotace, které je obdobné jako v ostatních PDT korpusech. Navíc obsahuje novou anotaci: rekonstrukci řeči. Korpus je volně dostupný.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a richly annotated spoken language resource, the Prague Dependency Treebank of Spoken Czech 2.0, the primary purpose of which is to serve for speech-related NLP tasks. The treebank features several novel annotation schemas close to the audio and transcript, and the morphological, syntactic
and semantic annotation corresponds to the family of Prague Dependency Treebanks; it could thus be used also for linguistic studies, including comparative studies regarding text and speech. The most unique and novel feature is our approach to syntactic annotation, which differs from other similar corpora such as Treebank-3 [8] in that it does not attempt to impose syntactic structure over input, but it includes one more layer which edits the literal transcript to fluent Czech while keeping the original transcript explicitly aligned with the edited version. This allows the morphological, syntactic and semantic annotation to be deterministically and fully mapped back to the transcript and audio. It brings new possibilities for modeling morphology, syntax and semantics in spoken language – either at the original transcript with mapped annotation, or at the new layer after (automatic) editing. The corpus is publicly and freely available.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>ForFun je nástroj pro rozmanitý lingvistický výzkum, zejména v oblasti syntaxe pro popis syntaktických funkcí a jejich formálních realizací v českcýh větách. ForFun je založen na datech Pražských závislostních korpusů (PDTs), uspořádává jejich morfologickou a syntaktickou anotaci do nové databáze, ve které je možné rychle a snadno prohledávat autentické příklady užité v PDTs pro jednotlivé funkce (66 položek) a  též opačně lze zkoumat funkce vyjádřené zvolenou formou (téměř 1500 položek).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>ForFun is an interface for various linguistic research, particularly in describing syntactic functions and their formal realizations in Czech sentences. ForFun draws on the complex linguistic annotation of Prague Dependency Treebanks (PDTs) and arranges morphological and syntactical annotation into new tool which gives a possibility to search quickly and in a user-friendly way all forms (almost 1,500 items) used in PDTs for particular function and vice versa to look up all functions (66 items) expressed by the particular forms.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme popis adverbiálních významů v českých větách založený na korpusu (Pražské závislostní korpusy). Na příkladu prostorových určení popisujeme metologii, kterou chceme dosáhnout uceleného a koplexního popisu adverbiálních významů v češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a corpus based description of selected adverbial meanings in Czech sentences. Its basic repertory is one of a long lasting tradition in both scientific and school grammars. However, before the corpus era, researchers had to rely on their own excerption; but nowadays, current syntax has a vast material basis in the form of electronic corpora available. On the case of spatial adverbials, we describe our methodology which we used to acquire a detailed, comprehensive, well-arranged description of meanings of adverbials including a list of formal realizations with examples. Theoretical knowledge stemming from this work will lead into an improval of the annotation of the meanings in the Prague Dependency Treebanks which serve as the corpus sources for our research. The Prague Dependency Treebanks include data manually annotated on the layer of deep syntax and thus provide a large amount of valuable examples on the basis of which the meanings of adverbials can be defined more accurately and subcategorized more precisely. Both theoretical and practical results will subsequently be used in NLP, such as  machine translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Mezi gramatikou a slovníkem: Rodina elektronických valenčních slovníků VALLEX
Between Lexicon and Grammar: The family of valency distionaries VALLEX</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Between Lexicon and Grammar: The family of valency distionaries VALLEX
Mezi gramatikou a slovníkem: Rodina elektronických valenčních slovníků VALLEX</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Licenční smlouva a smlouva o poskytnutí služeb mezi Univerzitou Karlovou a Datlowe, s.r.o. (1. 1. 2015 -- 1. 1. 2018) obsahuje právo na užití dat PDT 2.5 a software pro textovou analytiku natrénovaného na PDT 2.5, a to pro komerční účely. Jedná se o upravené verze nekomerčního software a dat jinak dostupných z repozitáře LINDAT/CLARIN. Jmenovitě: software pro analýzu tvarosloví a lematizaci, syntaktickou analýzu a analýzu názvů pojmenovanýchch entit.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Licence and service agreement between Charles University and Datlowe Ltd.(January 1, 2015 -- January 1, 2018) covers the right to use the data of PDT 2.5 and the software for text analytics trained on PDT 2.5, for commercial purposes. The data and software (non-adapted) is otherwise available for free through the LINDAT/CLARIN repository for non-commercial use under the appropriate variant of the Creative Commons license. Namely, software for morphological analysis, lemmatization, syntactic analysis and named-entity recognition.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V mnoha jazycích lze některé slova psát více způsoby. Říkáme jim varianty. Hodnoty všech jejich morfologických kategorií jsou totožné, což vede k identické morfologické značce. Spolu s totožným lematem máme dva nebo více slovních tvarů se stejným morfologickým popisem. Tato nejednoznačnost může působit problémy v různých aplikacích automatického zpracování jazyka. Existují dva typy variant - ty, které ovlivňují celé paradigma (globální varianty), a ty, které mají vliv pouze na slovní tvary používající některé kombinace morfologických hodnot (inflexní varianty). V příspěvku navrhujeme prostředky, jak označit všechny slovní tvary, včetně jejich variant, jednoznačně. Tento požadavek nazýváme "Zlaté pravidlo morfologie". Práce se zabývá především češtinou, ale hlavní myšlenky lze uplatnit i v jiných jazycích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In many languages, some words can be written in several ways. We call them variants. Values of all their morphological categories are identical, which leads to an identical morphological tag. Together with the identical lemma, we have two or more wordforms with the same morphological description. This ambiguity may cause problems in various NLP applications. There are two types of variants – those affecting the whole paradigm (global variants) and those affecting only wordforms sharing some combinations of morphological values (inflectional variants). In the paper, we propose means
how to tag all wordforms, including their variants, unambiguously. We call this requirement ”Golden rule of morphology”. The paper deals mainly with Czech, but the ideas can be applied to other languages as well.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zkoumáme lidské úsudky o tom, jak dobře popisují jednotlivé vzory užívání (usage patterns) 29 cílových sloves z modelového slovníku anglických sloves jejich náhodné KWIC. Zaměřujeme se na případy, kdy je pro daný KWIC hodnoceno více než jeden model a snažme se odhadnout vliv účastníků události (argumenty), které jsou denotativně podobné ve dvou vzorcích, s ohledem na všechny kombinace párů v daném lemmatu. Tento efekt porovnáváme s účinkem několika kontextových rysů KWIC, účinkem spárovaných implicit PDEV, které se navzájem implikují, a účinkem příslušnosti k danému lemu. Ukazujeme, že lemmatický efekt je stále silnější než jakákoli vlastnost, která prochází napříč lemma, kterou jsme zatím prozkoumali, takže každé sloveso se zdá být malým vesmírem samo o sobě.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We explore human judgments on how well individual patterns of 29 target verbs from the Pattern Dictionary of English Verbs describe their random KWICs. We focus on cases where more than one pattern is judged as highly appropriate for a given KWIC and seek to estimate the effect of event participants (arguments) being denotatively similar in two patterns, considering all pair combinations in a given lemma. We compare this effect to the effect of several contextual features of the KWICs,  the effect of paired PDEV implicatures implying each other, and the effect of belonging to a given lemma. We show that the lemma effect is still stronger than any feature going across lemmas we have examined so far, so that each verb appears to be a little universe in its own right.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme anotační experiment, který kombinuje témata z lexikografie a disambiguace lexikálních významů. Zahrnuje lexikon (Pattern Dictionary of English Verbs, PDEV), existující datový soubor (VPS-GradeUp) a nepublikovanou sadu dat (RTE v PDEV Implications). Experiment měl dva cíle: pilotní anotace (RTE) na implikaturách PDEV (tj. slovníkových definicích) na jedné straně a na druhé straně analýzu efektu vzájemného textového vyplývání mezi slovníkovými definicemi na anotátorská rozhodování lexikální disambiguace ve srovnání s jinými prediktory, jako je finitnost cílového slovesa, explicitní přítomnost příslušných argumentů a sémantická vzdálenost mezi odpovídajícími syntaktickými argumenty ve dvou různých vzorcích (slovníkové významy).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We  describe  an  annotation  experiment  com-
bining topics from lexicography and Word Sense Disam-
biguation. It involves a lexicon (
Pattern Dictionary of En-
glish Verbs, PDEV
), an existing data set (
VPS-GradeUp
),
and an unpublished data set (
RTE in PDEV Implicatures
).
The aim of the experiment was twofold:  a pilot annota-
tion of Recognizing Textual Entailment (RTE) on PDEV
implicatures  (lexicon  glosses)  on  the  one  hand,  and,  on
the other hand, an analysis of the effect of Textual Entail-
ment between lexicon glosses on annotators’ Word-Sense-
Disambiguation decisions,  compared to other predictors,
such as finiteness of the target verb, the explicit presence
of  its  relevant  arguments,  and  the  semantic  distance  be-
tween corresponding syntactic arguments in two different
patterns (dictionary senses).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Velmi rychla implementace banky filtru pro rozdeleni vstupniho komplexniho signalu do N frekvencne ekvidistantnich kanalu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Very fast implementation of the filter bank for splitting the input complex signal into N equidistant frequency channels.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Soutěž PARSEME Shared Task v identifikaci slovesných víceslovných výrazů požaduje po účastnících vyhledávání výrazů v běžném textu.

V tomto článku ukazujeme, jak je možné česká trénovací data získat nikoli manuální anotací, nýbrž převodem informací z předchozích anotací uložených v Pražském závislostním korpusu na různých úrovních a různým způsobem. Prvním krokem je porovnání anotačních instrukcí a srovnání typologie.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The PARSEME Shared Task on automatic identification of verbal multiword expressions  aims  at  identifying  such  expressions  in  running  texts.   Typology of verbal multiword expressions, very detailed annotation guidelines and gold-standard data for as many languages as possible will be provided. Since the Prague Dependency Treebank includes Czech multiword expression annotation, it was natural to make an attempt to automatically convert the data into the Shared Task format. However, since the Czech treebank predates the Shared Task annotation guidelines, a prior examination was necessary to determine to which extent the conversion can be fully automatic and how much manual work remains.

In this paper, we show that information contained in the Prague Dependency Treebank is sufficient to extract all of the Shared Task categories of verbal multiword expressions relevant for Czech, even if these categories are originally annotated differently; nevertheless, some manual checking and annotation would still be necessary, e.g. for distinguishing borderline cases.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek představuje databázi lingvistických forem a funkcí postavenou nad Pražským závislostním korpusem. Účelem databáze ForFun je usnadnit lingivstům studium vztahu formy a funkce. Ukážeme možnosti využití databáze.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of our contribution is to introduce a database of linguistic forms and their functions built with the use of the multi-layer annotated corpora of Czech, the Prague Dependency Treebanks. The purpose of the Prague Database of Forms and Functions (ForFun) is to help the linguists to study the form-function relation, which we assume to be one of the principal tasks of both theoretical linguistics and natural language processing. We will also demonstrate possibilities of the exploitation of the ForFun database.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku informuji o pokrocích ve vývoji lexikální databáze DeriNet a rozebírám tři témata, kterým je v rámci projektu aktuálně věnována významná pozornost. Databáze DeriNet je jako jazykový zdroj specializovaný na derivační morfologii češtiny budována v Ústavu formální a aplikované lingvistiky MFF UK, mezi více než 1 milionem lexémů se zatím podařilo identifikovat 774 tisíc derivačních vztahů. Prvním z témat probíraných v příspěvku jsou hláskové alternace, které jsou pro poloautomatické metody identifikace derivačních vztahů zásadním problémem. Dále se zaměřím na kategorii slovesného vidu, tato flektivní kategorie českého slovesa je vyjadřována slovotvornými prostředky. Kategorie vidu byla spolu s dalšími rysy použita jako jedno z kritérií při uspořádávání příbuzných sloves do derivačního stromu. V závěru příspěvku - jako téma třetí - bude zachycování sloves odvozených od sloves představeno komplexně. Všechna tři témata jsou vzájemně provázána, ve všech jsou úzce propojeny aspekty teoretické a komputační lingvistiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk summarizes recent steps in development of the derivational network DeriNet. The following topics are taken under scrutiny: vowel and consonant alternations that occur during derivation, the role of the inflectional category of aspect in derivation, and the verb-to-verb derivation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku se zabýváme hranicí mezi přejímáním slov a slovotvorbou (zvláště derivací) na příkladu substantiv s příponou -ismus a -ita. Z hlediska české slovní zásoby jsou zkoumány formální (flektivní i derivační) vlastnosti a význam těchto slov.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the paper, the boundary between borrowing and word formation (particularly, derivation) is studied on the example of the suffixes -ismus and -ita, which occur in nouns in Czech. Formal (both inflectional and derivational) and semantic features of nouns with the suffixes -ismus and -ita are introduced.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se věnuje paradigmatu českého slovesa z hlediska flektivní a derivační morfologie, poukazuje na vzájemnou propojenost obou oblastí. Kategorie vidu je popisována jako flektivní kategorie slovesa, která je formálně vyjadřována derivačními prostředky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper deals with verbal paradigms with respect to inflectional and derivational morphology of Czech, and its aim is to contribute to the discussion on the absence of clear boundaries between inflection and derivation. The aspect is an inflectional category of Czech verbs which is formally expressed by derivation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek představuje poloautomatickou proceduru, jejímž cílem bylo v derivační databázi DeriNet identifikovat vidové dvojice sloves lišící se příponami. Na základě dat z valenčního slovníku Vallex jsme sestavili seznam párů sufixů, kterými se vidové protějšky liší, a tento seznam využili k vyhledání dalších dvojic v rozsáhlých datech sítě DeriNet. Nalezené dvojice byly potvrzeny ruční anotací.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper describes a semi-automatic method of adding derivational links to the lexical database DeriNet by identifying verbs which are derived by suffixation and constitute aspectual pairs. It briefly introduces the notion of aspect in Czech and discusses the account of aspect in the Czech linguistic literature and in existing data resources. As its main focus, it presents an approach to identifying aspectual pairs based on extraction of such pairs from the VALLEX valency dictionary, identification of suffix substitution rules and subsequent manual annotation, which resulted in the addition of almost 6,000 derivational links into the DeriNet database.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vícejazyčný výzkum syntaxe a syntaktické analýzy byl dlouhou dobu brzděn tím, že anotační styly používané v různých jazycích se výrazně lišily a bylo téměř nemožné provést metodologicky čisté srovnání a vyhodnocení vícejazyčných experimentů se strojovým učením.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Multilingual research on syntax and parsing has for a long time been hampered by 
the fact that annotation schemes vary enormously across languages, which has made
it virtually impossible to perform sound comparative evaluations and 
cross-lingual learning experiments. The Universal Dependencies (UD) project 
(www.universaldependencies.org) seeks to tackle this problem by developing 
cross-linguistically consistent treebank annotation for many languages, aiming to
capture similarities as well as idiosyncracies among typologically different
languages (e.g., morphologically rich languages, pro-drop languages, and 
languages featuring clitic doubling). The goal is not only to support comparative
evaluation and cross-lingual learning but also to facilitate multilingual natural
language processing and enable comparative linguistic studies. To serve all these
purposes, the framework needs to have a solid linguistic foundation and at the
same time be transparent and accessible to non-specialists.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je šesté vydání treebanků UD, verze 2.0.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the sixth release of UD Treebanks, Version 2.0.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je doplnění šestého vydání treebanků UD, verze 2.0, o testovací data.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the extension of the sixth release of UD Treebanks, Version 2.0, by test data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je sedmé vydání treebanků UD, verze 2.1.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the seventh release of UD Treebanks, Version 2.1.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek předkládá grafematické a morfologické rysy západoslovanských jazyků. Na základě těchto rysů lze predikovat současné podoby slov v češtině a polštině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article presents graphemic and morphological features of West Slavic languages. Based on these features, Czech and Polish word forms can be predicted.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška o mém výzkumu v oblasti mezijazyčného přenosu závislostních parserů, zahrnuje strojový překlad, segmentaci na podslova, harmonizaci dat, a další.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Cross-lingual transfer of NLP tools is motivated by the fact that sufficient training data and high-performance supervised NLP tools are available only for maybe 1% of world's languages; the remaining 99% of languages are under-resourced and therefore difficult to process automatically. In cross-lingual transfer methods, we try to find effective ways of utilizing supervised training data for resource-rich languages and various automatic transfer methods to be able to process the under-resourced languages (think of e.g. translating the training data with an MT system).
In my talk, I will review my research on cross-lingual transfer of dependency parsers (and taggers to some extent), including both positive and negative results. My work on this problem has involved several subproblems, which I will also address in the talk, including: Universal Dependencies and other annotation harmonization, morphological segmentation and other subword units, Giza++ and other word alignment, Moses and other machine translation systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>To je něco, co používá každý, přitom není úplně zřejmé, jak to funguje. Pro daný dotaz nejde prostě pro každý dotaz projít všechny existující miliardy webových stránek, a vrátit uživateli několik miliónů z nich, které obsahují hledaná slova. Ukážu, co je reverzní index (to zná každý pod názvem rejstřík), podle čeho se řadí výsledky, a přidám i pokročilejší vylepšení (TF.IDF, vektorový model, lematizace, synonyma, pojmenované entity).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This is something everyone uses, but it is not absolutely clear how it works. For a given query it is not possible to simply traverse all the existing billions of websites, and return the few millions that contain the sought words. I will show what a reverse index is, how results are sorted, and I will also add more advanced techniques (TF.IDF, vector model, lemmatization, synonyms, named entities).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme MonoTrans, systém statistického strojového překladu, který používá pouze jednojazyčná data ve zdrojovém a cílovém jazyce, bez použití paralelních korpů nebo pravidel specifických pro konkrétní jazyk. Systém překládá každé zdrojové slovo cílovém slovem, které je mu nejpodobnější na základě kombinace míry řetězcové podobnosti a podobnosti četností slov. Systém je určen pro překlad mezí blízkými jazyky v situaci kdy není k dispozici dostatek paralelních dat.
Přestože MonoTrans dosahuje nízkých skóre, významně překonává baseline.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present MonoTrans, a statistical machine translation system which only uses monolingual source language and target language data, without using any parallel corpora or language-specific rules. It translates each source word by the most similar target word, according to a combination of a string similarity measure and a word frequency similarity measure. It is designed for translation between very close languages, such as Czech and Slovak or Danish and Norwegian. 
It provides a low-quality translation in resource-poor scenarios where parallel data, required for training a high-quality translation system, may be scarce or unavailable. This is useful e.g. for cross-lingual NLP, where a trained model may be transferred from a resource-rich source language to a resource-poor target language via machine translation. 
We evaluate MonoTrans both intrinsically, using BLEU, and extrinsically, applying it to cross-lingual tagger and parser transfer. Although it achieves low scores, it does surpass the baselines by respectable margins.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Snadný způsob prohlížení souborů ve formátu CoNLL a CoNLLU ve vašem terminálu. Rychlý a textový.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A simple way of browsing CoNLL and CoNLLU format files in your terminal. Fast and text-based.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>SloNLP je workshop speciálně zaměřený na zpracování přirozeného jazyka a počítačovou lingvistiku. Jeho hlavním cílem je podpořit spolupráci mezi výzkumníky v oblasti NLP v Česku a na Slovensku.
Mezi témata workshopu patří automatické rozpoznávání mluvené řeči (ASR), automatická analýza a generování přirozeného jazyka (morfologie, syntax, sémantika...), dialogové systémy (DS), strojový překlad (MT), vyhledávání informací (IR), praktické aplikace NLP technologií, a další témata počítačové lingvistiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>SloNLP is a workshop focused on Natural Language Processing (NLP) and Computational Linguistics. Its primary aim is to promote cooperation among NLP researchers in Slovakia and Czech Republic.
The topics of the workshop include automatic speech recognition, automatic natural language analysis and generation (morphology, syntax, semantics, etc.), dialogue systems, machine translation, information retrieval, practical applications of NLP technologies, and other topics of computational linguistics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V několika rozměrech rozebíráme chyby v kroslingválním přenosu taggeru a parseru z angličtiny do 32 jazyků. Identifikujeme a vysvětlujeme silné a slabé stránky a nepravidelnosti a navrhujeme možná řešení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We thoroughly analyse the performance of cross-lingual tagger and parser transfer from English into 32 languages. We suggest potential remedies for identified issues and evaluate some of them.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>My měli korpus, anebo ten korpus měl nás.

Měl značky pěkný, jednoduchý, harmonický.

Když zvali nás řekli ať použijem všechno, co chcem.

Však k použití nikde nic není, ať hnem se kam hnem.

Tak cizí řeči jsme přeložili, využili.

Dva týdny práce, dřeli jsme fest, a je tu test.

Trénink běžel přes noc do rána, než přišel deadline.

My nespali, čekali, doufali, že dopadnem fajn.

Ráno bylo tu, hleďme na to, my měli zlato.

Tak popíšem papír, co zrodilo ho norský dřevo.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We once had a corp,

or should we say, it once had us

They showed us its tags,

isn’t it great, unified tags

They asked us to parse

and they told us to use everything

So we looked around

and we noticed there was near nothing

We took other langs,

bitext aligned: words one-to-one

We played for two weeks,

and then they said, here is the test

The parser kept training till morning,

just until deadline

So we had to wait and hope what we get

would be just fine

And, when we awoke,

the results were done, we saw we’d won

So, we wrote this paper,

isn’t it good, Norwegian wood.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Natrénované modely pro UDPipe, použité k vytvoření našeho příspěvku zaslaného na sdílenou úlohu VarDial 2017 (https://bitbucket.org/hy-crossNLP/vardial2017) a popsaných v článku stejných autorů s názvem Slavic Forest, Norwegian Wood.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Trained models for UDPipe used to produce our final submission to VarDial 2017 shared task (https://bitbucket.org/hy-crossNLP/vardial2017) and described in a paper by the same authors titled Slavic Forest, Norwegian Wood.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nástroje a skripty užité k vytvoření modelů mezijazyčných parserů, zaslaných na sdílenou úlohu VarDial 2017 (https://bitbucket.org/hy-crossNLP/vardial2017) a popsaných v článku stejných autorů s názvem Slavic Forest, Norwegian Wood.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Tools and scripts used to create the cross-lingual parsing models submitted to VarDial 2017 shared task (https://bitbucket.org/hy-crossNLP/vardial2017) and described in a paper by the same authors titled Slavic Forest, Norwegian Wood.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení Treex CR - systému na rozpoznávání koreference pro angličtinu a češtinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presenting Treex CR - the coreference resolution system for English and Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje Treex CR - systém na rozpoznávání koreference nejenom pro češtinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper introduces Treex CR, a coreference resolution (CR) system not only for Czech. As its name suggests, it has been implemented as an integral part of the Treex NLP framework. The main feature that distinguishes it from other CR systems is that it operates on the tectogrammatical  layer, a representation  of  deep syntax. This feature allows for natural handling of elided expressions, e.g. unexpressed subjects in Czech as well as generally ignored English anaphoric expression – relative pronouns and zeros. The system implements a sequence of mention ranking models specialized at particular types of coreferential expressions (relative, reflexive, personal pronouns etc.). It takes advantage of rich feature set extracted from the data linguistically preprocessed with Treex. We evaluated Treex CR on Czech and English datasets and compared it with other systems as well as with modules used in Treex so far.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška pojednává o experimentech s rozpoznáváním koreference na češtině a angličtině s použitím paralelních korpusů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk describes experiments on coreference resolution in Czech and English using parallel corpora.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Počítačový nástroj Evaluátor diskurzu (EVALD 2.0) slouží k automatickému hodnocení povrchové koherence (koheze) textů v češtině psaných rodilými mluvčími češtiny. Software tedy hodnotí (známkuje) úroveň předloženého textu z hlediska jeho povrchové výstavby (zohledňuje např. frekvenci a rozmanitost spojovacích prostředků, bohatost slovní zásoby, koreferenční vztahy apod.).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Evaluator of Discourse (EVALD 2.0) is a software that serves for automatic evaluation of surface coherence (cohesion) in Czech texts written by native speakers of Czech. Software evaluates the level of the given text from the perspective of its surface coherence (i.e. it takes into consideration, e.g., the frequency and diversity of connectives, richness of vocabulary, coreference realtions etc.).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Počítačový nástroj Evaluátor diskurzu pro cizince (EVALD 2.0 pro cizince) slouží k automatickému hodnocení povrchové koherence (koheze) textů v češtině psaných nerodilými mluvčími češtiny. Software hodnotí úroveň předloženého textu z hlediska jeho povrchové výstavby (zohledňuje např. frekvenci a rozmanitost užitých spojovacích prostředků, bohatost slovní zásoby, koreferenční vztahy apod.).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Evaluator of Discourse for Foreigners (EVALD 2.0 for Foreigners) is a software that serves for automatic evaluation of surface coherence (cohesion) in Czech texts written by non-native speakers of Czech. Software evaluates the level of the given text from the perspective of its surface coherence (i.e. it takes into consideration, e.g., the frequency and diversity of connectives, richness of vocabulary, coreference realtions etc.).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje rozšíření systémů EVALD, jejichž cílem je hodnocení úrovně koherence v slohových pracích, o rysy využívající informace o koreferenci a zájmenách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper contributes to the task of automated evaluation of surface coherence. It introduces a coreference-related extension to the EVALD applications, which aim at evaluating essays produced by native and non-native students learning Czech. Having successfully employed the coreference resolver and coreference-related features, our system outperforms the original EVALD approaches by up to 8 percentage points. The paper also introduces a dataset for non-native speakers' evaluation, which was collected from multiple corpora and the parts with missing annotation of coherence grade were manually judged. The resulting corpora contains sufficient number of examples for each of the grading levels.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje systém pro rozpoznávání koreference  v němčině a ruštině, trénovaný výlučně na koreferenčních vztazích projektovaných skrz paralelní korpus. Rozpoznávač operuje na tektogramatické vrstvě a používá vícero specializovných modelů. Měřeno metrikou CoNLL systém dosahuje 32 bodů pro Ruštinu a 22 bodov pro němčinu. Analýza výsledků ukazuje, že rozpoznávač pro ruštinu je schopen při projekci z angličtiny dosáhnout 66% kvality dosažené na angličtině. Systém byl poslán na CORBON 2017 Shared task.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes the system for coreference resolution in German and Russian, trained exclusively on coreference relations projected through a parallel corpus from English. The resolver operates on the level of deep syntax and makes use of multiple specialized models. It achieves 32 and 22 points in terms of CoNLL score for Russian and German, respectively. Analysis of the evaluation results show that the resolver for Russian is able to preserve 66\% of the English resolver's quality in terms of CoNLL score. The system was submitted to the Closed track of the CORBON 2017 Shared task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Automatická segmentace, tokenizace a morfologická a syntaktická anotace textů ve 45 jazycích, vygenerovaná pomocí UDPipe (http://ufal.mff.cuni.cz/udpipe), spolu se 100rozměrnými slovními embeddingy vypočítanými nad textem převedeným na malá písmena nástrojem word2vec (https://code.google.com/archive/p/word2vec/).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Automatic segmentation, tokenization and morphological and syntactic annotations of raw texts in 45 languages, generated by UDPipe (http://ufal.mff.cuni.cz/udpipe), together with word embeddings of dimension 100 computed from lowercased texts by word2vec (https://code.google.com/archive/p/word2vec/).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek shrnuje výsledky druhého ročníku Biomedical Translation Task konference WMT 2017, tj. strojového překladu biomedicínských textů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Automatic   translation   of   documents   is
an  important  task  in  many  domains,  in-
cluding  the  biological  and  clinical  do-
mains. The second edition of the Biomed-
ical Translation task in the Conference of
Machine  Translation  focused  on  the  au-
tomatic  translation  of  biomedical-related
documents  between  English  and  various
European  languages.   This  year,  we  ad-
dressed  ten  languages:   Czech,  German,
English,  French,  Hungarian,  Polish,  Por-
tuguese, Spanish, Romanian and Swedish.
Test sets included both scientific publica-
tions (from the Scielo and EDP Sciences
databases) and health-related news (from
the Cochrane and UK National Health Ser-
vice web sites).  Seven teams participated
in the task, submitting a total of 82 runs.
Herein we describe the test sets, participat-
ing systems and results of both the auto-
matic and manual evaluation of the trans-
lations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Strojový překlad do tvaroslovně bohatých jazyků představuje složitý problém. Prestože pokrytí na úrovni lemmat může být dostatečné, řada jejich tvaroslovných variant se z trénovacích dat nedá získat. Představujeme statistický překladový systém, který tyto tvaroslovné varinaty dokáže generovat. Na rozdíl od dřívějších prací nerozdělujeme modelování tvarosloví a lexikální volbu na dva navazující kroky. Náš postup je integrován přímo v dekódování a využívá informace z konextu jak na zdrojové, tak na cílové straně.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Translating into morphologically rich languages is  difficult. Although  the  coverage of lemmas may be reasonable,  many morphological variants cannot be learned from the training data. We present a statistical translation system that is able to produce these inflected word forms. Different from most previous work, we do not separate morphological prediction from lexical choice into two consecutive steps. Our approach  is  novel  in  that  it  is  integrated  in decoding  and  takes  advantage  of  context information from both the source language and the target language sides.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje vytváření volně dostupného závislostního korpusu maráthštiny, který odpovídá anotačnímu schématu Universal Dependencies (UD).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the creation of a free and open-source dependency treebank for Marathi,
the first open-source treebank for Marathi following the Universal Dependencies (UD) syntactic annotation scheme. In the paper, we describe some of the syntactic and morphological phenomena in the language that required special analysis, and how they fit into the UD guidelines. We also evaluate the parsing results for three popular dependency parsers on our treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje náš příspěvek do společné úlohy trénování systémů neuronového překladu (WMT2017 Neural MT Training Task).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This  paper  describes  our  submission  to
the WMT 2017 Neural MT Training Task.
We modified the provided NMT system in
order  to  allow  for  interrupting  and  con-
tinuing  the  training  of  models.   This  al-
lowed  mid-training  batch  size  decremen-
tation and incrementation at variable rates.
In  addition  to  the  models  with  variable
batch size,  we tried different setups with
pre-trained word2vec embeddings.  Aside
from batch size incrementation, all our ex-
periments performed below the baseline</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Bylo prokázáno, že zvyšující se hloubka modelu zlepšuje kvalitu neuronového strojového překladu.
Přes mnoho návrhů různých variant arhitektur pro zvýšení hloubky modelu doposud nebyla provedena žádná důkladná srovnávací studie.

V této práci popisujeme a vyhodnocujeme několik stávajících přístupů k zavedení hloubky v neuronovém strojovém překladu.
Navíc prozkoumáváme nové varianty architektur včetně hlubokých přechodových RNN a měníme, jak je hlubokém dekodéru použit mechanismus pozornosti ("attention").
Představujeme novou architekturu "BiDeep" RNN, která kombinuje hluboké přechodové RNN a skládané RNN.

Hodnocení provádíme na anglicko-německém datovém souboru WMT pro překlady novinových článků s využitím stroje s jednou GPU pro trénování i inferenci.
Zjistili jsme, že několik našich navrhovaných architektur zlepšuje stávající přístupy z hlediska rychlosti a kvality překladu.
Nejlepších výsledků jsme získali s BiDeep RNN kombinované hloubky 8, získáním průměrného zlepšení 1,5 BLEU nad silnou baseline.

Náš kód je pro snadný přístup zveřejněn.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>It has been shown that increasing model depth improves the quality of neural machine translation.
However, different architectural variants to increase model depth have been proposed, and so far, there has been no thorough comparative study.

In this work, we describe and evaluate several existing approaches to introduce depth in neural machine translation.
Additionally, we explore novel architectural variants, including deep transition RNNs, and we vary how attention is used in the deep decoder.
We introduce a novel "BiDeep" RNN architecture that combines deep transition RNNs and stacked RNNs.

Our evaluation is carried out on the English to German WMT news translation dataset, using a single-GPU machine for both training and inference.
We find that several of our proposed architectures improve upon existing approaches in terms of speed and translation quality.
We obtain best improvements with a BiDeep RNN of combined depth 8, obtaining an average improvement of 1.5 BLEU over a strong shallow baseline.

We release our code for ease of adoption.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Licenční smlouva pro Google Inc. (Mountain View, California 94043, USA) na užívání Tamilského závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>License agreement for Google Inc. (Mountain View, California 94043, USA) for using the Tamil Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozpoznávání notopisu je specializovaný pod-obor automatického zpracování dokumentů, který se zaměřuje na extrakci hudebního obsahu z obrazové informace (skenu či fotografie) notového zápisu. Vzhledem k tomu, že pravděpodobně existuje více hudebních děl v psané podobě než v podobě nahrávky či jiného záznamu, může automatická transkripce not přinést digitální muzikologii výrazně rozmanitější zdroje. Největší potenciál spočívá v masovém zpřístupnění rozsáhlých notových archivů, jako např. kroměřížská sbírka rodu Lichtenštejn-Kastelkornů, které pak bude možné zkoumat pomocí dalších metod digitální muzikologie, např. automatická detekce duplikátů či partů, vyhledávání pomocí melodií, či sledování genealogie hudebních motivů. 
V rámci zpracování dokumentů se jedná o úlohu výjimečně obtížnou, neboť moderní hudební notace je jedním z nejkomplikovanějších systémů psaní. Obzvlášť pro rukopisy, kde se její složitost násobí s variabilitou realizace zápisu, neexistují v současnosti ani vzdáleně uspokojivá řešení. Na problému také pracuje relativně málo lidí. Příspěvek nabídne vhled do současného stavu poznání a vývoje oboru, s důrazem na možné aplikace v digitální muzikologii.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Optical Music Recognition (OMR) is a sub-field of document analysis and recognition focused on extracting musical content from the image (e.g., scan or photo) of a musical score. Given that more compositions probably exist in written form than have been recorded, automatically transcribing written music can substantially diversify the sources available to digital musicology. The greatest potential of OMR lies in making accessible massive musical archives, such as the Lichtenstein-Castelcorn collection in Kroměříž, to further investigation using methods of digital musicology: automated duplicate or part detection, melody-based search, or tracking the genealogy of musical motifs.
Within document analysis, OMR occupies a particularly difficult niche, because modern music notation is one of the most complicated writing systems overall. Especially for manuscripts, where this difficulty compounds with the variability in handwriting, no remotely satisfactory solutions are available today. The problem also only has a small community dedicated to solving it. The talk will give an overview of the current state of the art and the developments in the field, with focus on potential applications for digital musicology.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Většina hudebních skladeb, které byly kdy vytvořeny, dnes existuje pouze v psané podobě, většinou v rámci archivů; konkrétně v ČR je více než 10 000 takových rukopisů. Pro zachování a šíření této části kulturního dědictví je vhodné jej digitalizovat, a další výhody přináší digitalizace hudebního obsahu těchto dokumentů. Ruční přepisování not v editorech jako Sibelius či MuseScore je však v tomto rozsahu příliš pomalé. Rozpoznávání notopisu (Optical Music Recognition, OMR), ekvivalent OCR pro hudební notaci, může být klíčovým nástrojem pro zpřístupnění obsahu hudebních archivů - pro široký muzikologický výzkum, pro lepší správu (např. detekce opisů), a pro zkrácení cesty, kterou skladby musí ujít od archiválie k živému provedení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Most compositions created throughout history exist today only in written form, usually residing in archive collections; specifically in the Czech Republic, there are many more than 10 000 such manuscripts. To preserve and disseminate this portion of cultural heritage, it is advantageous to digitize it; further usability would be brought by also digitizing the contents of these documents. However, transcribing music with notation editors such as Sibelius or MuseScore is too time-consuming. Optical Music Recognition (OMR), the equivalent of OCR for music notation, can be a key tool for opening the contents of musical archives to large-scale musicological research, better curation (e.g., duplicate search), and for making the way from an archival score to performance easier.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme MUSCIMarker, open-source nástroj pro vývoj systémů rozpoznávání notopisu (Optical Music Recognition, OMR). Nástroj je postavený okolo reprezentace notopisu jakožto grafu, definovaného v datasetu MUSCIMA++. Nástroj je transparentní a interaktivní, umožňuje uživateli vizualizovat, ověřit a upravovat výsledky jednotlivých kroků OMR.
Navíc je díky čistě Pythonové implementaci přenosný mezi operačními systémy, a umožňuje pracovat offline.
Dokládáme hodnotu MUSCIMarkeru skrze prototyp systému na plnohodnotné rozpoznávání notopisu, od předzpracování obrazu po přehrání výstupu a export do formátu MIDI. Publikum prezentace bude mít příležitost MUSCIMarker i rozpoznávací prototyp vyzkoušet.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present MUSCIMarker, an open-source workbench
for developing Optical Music Recognition (OMR) systems from image preprocessing to MIDI export. It is built
around the notation graph data model of the MUSCIMA++
dataset for full-pipeline OMR. The system is transparent
and interactive, enabling the user to visualize, validate
and edit results of individual OMR stages. It is platform-independent, written purely in Python, and can work offline. We demonstrate its value with a prototype OMR system for musical manuscripts that implements the recognition pipeline, up to playing the recognition outputs through
MIDI. The audience will interact with the program and can
test an integrated OMR system prototype.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Notové hlavičky představují rozhraní mezi zápisem hudby a hudbou samotnou. Každá hraná nota je kódována pomocí notové hlavičky, a detekovat hlavičky je tím pádem pro rozpoznávání not nevyhnutelné. V tištěné notaci jsou hlavičky jasně rozlišitelné, avšak různorodost rukopisů činí jejich identifikaci obtížnější. Představujeme jednoduchý detektor notových hlaviček používající konvoluční neuronové sítě pro klasifikaci pixelů a regresi na ohraničení, který dosahuje na detekci f-score 0.97 nad datasetem MUSCIMA++, nepotřebuje odstraňování osnov, a lze jej použít na různorodé rukopisné styly a úrovně složitosti zapsané hudby.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Noteheads are the interface between the written score and music. Each notehead on the page signifies one note to be played, and detecting noteheads is thus an unavoidable step for Optical Music Recognition. In printed notation, noteheads are clearly distinct objects; however, the variety of music notation handwriting makes noteheads harder to identify, and while handwritten music notation symbol classification is a well-studied task, symbol detection has usually been limited to heuristics and rule-based systems instead of machine learning methods better suited to deal with the uncertainties in handwriting. We present ongoing work on a simple notehead detector using convolutional neural networks for pixel classification and bounding box regression that achieves a detection f-score of 0.97 on binary score images in the MUSCIMA++ dataset, does not require staff removal, and is applicable to a variety of handwriting styles and levels of musical complexity.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tvorba datasetů pro ropzoznávání grafických znaků, především ručně psaných, je často drahá a časově náročná záležitost. Nástroj MUSCIMarker, kterým jsme vytvořili dataset MUSCIMA++ pro rozpoznávání not, pomohl omezené zdroje využít efektivně, a je dostatečně flexibilní na to, aby jej bylo možné použít na tvorbu datasetů pro další úlohy s podobnou reprezentací anotované "pravdy". Nejprve popíšeme tuto reprezentaci, aby byla zřejmé, na jaké úlohy je MUSCIMarker aplikovatelný; následně popisujeme samotný nástroj MUSCIMarker, jeho silné a slabé stránky, a praktické zkušenosti s tvorbou datasetu MUSCIMA++.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Dataset creation for graphics recognition, especially for hand-drawn inputs, is often an expensive and time-consuming undertaking. The MUSCIMarker tool used for creating the MUSCIMA++ dataset for Optical Music Recognition (OMR) led to efficient use of annotation resources, and it provides enough flexibility to be applicable to creating datasets for other graphics recognition tasks where the ground truth can be represented similarly. First, we describe the MUSCIMA++ ground truth to define the range of tasks for which using MUSCIMarker to annotate ground truth is applicable. We then describe the MUSCIMarker tool itself, discuss its strong and weak points, and share practical experience with the tool from creating the MUSCIMA++ dataset.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Podstatným problémem pro rozpoznávání not, především ručně psaných, je lokalizace symbolů ve vstupním obrázku. Řešení jsou stavěna jak odspodu, využívajíce nízkoúrovňové vizuální rysy, tak shora, kde se využívá skutečnost, že se hudební notace řídí silnými omezeními na syntakticky správné konfigurace symbolů. Oba přístupy se občas kombinují. V nedávné době se přístup odspodu výrazně zlepšil pomocí konvolučních neuronových sítí. Snížení nejistoty, které může notační syntax poskytnout, však ještě s těmito modely zkombinována nebyla. Tento rozšířený abstrakt diskutuje způsoby, jak neuronové sítě a notační syntax propojit, a analyzuje obtíže, se kterými by se jednotlivé přístupy měly potýkat. Doufáme, že náš příspěvek podnítí další diskusi o těchto možnostech, vyprovokuje výzkumníky v oboru k experimentálnímu prozkoumání navržených přístupů, a podnítí výzkumníky z příbuzných oblastí sdílet své zkušenosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A major roadblock for Optical Music Recognition, especially for handwritten music notation, is symbol detection: recovering the locations of musical symbols from the input page. This has been attempted both with bottom-up approaches exploiting visual features, and top-down approaches based on the strong constraints that music notation syntax imposes on possible symbol configurations; sometimes joined together at appropriate points in the recognition process. The bottom-up approach has recently greatly improved with the boom of neural networks. However, the reduction in uncertainty that music notation syntax can provide has not yet been married to the power of these neural network models. This extended abstract brainstorms ways in which this can be done, and analyzes the difficulties the various combined approaches will have to address. We hope our work will foster further discussion to clarify the issues involed, provoke OMR researchers to try some of these approaches experimentally, and entice researchers from other parts of the graphics recognition community to share relevant experience.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Od rozpoznávání notopisu (Optical Music Recognition, OMR) si lze slibovat zpřístupnění mnoha hudebních dokumentů, které jsou podstatnou součástí kulturního dědictví. OMR však nemá adekvátní data a příslušný formát anotace, který by umožnil porovnávání systémů OMR, což představuje výraznou překážku pro měřitelný pokrok. Řešení OMR využívající strojové učení navíc potřebují trénovací data. Navrhli jsme a sesbírali jsme nový OMR dataset MUSCIMA++. Poskytované anotace tvoří notační graf, jejž naše analýza odhalila jako nutný a postačující popis hudební notace. Stavíme nad daty CVC-MUSCIMA pro odstraňování notových osnov. MUSCIMA++ v1.0 obsahuje 140 stran hudebního rukopisu, s 91245 ručně vyznačenými symboly a 82247 vztahy mezi nimi. Dataset umožňuje trénovat a přímo evaluovat modely pro klasifikaci a lokalizaci symoblů, rekonstrukce logické struktury notace, a extrakce hudebního obsahu. Jsou poskytnuty open-source nástroje pro manipulaci s datasetem, vizualizaci a rozšiřování anotací, a data samotná jsou poskytnuta pod otevřenou licencí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Optical Music Recognition (OMR) promises to make accessible the content of large amounts of musical documents, an important component of cultural heritage. However, the field does not have an adequate dataset and ground truth for benchmarking OMR systems, which has been a major obstacle to measurable progress. Furthermore, machine learn- ing methods for OMR require training data. We design and collect MUSCIMA++, a new dataset for OMR. Ground truth in MUSCIMA++ is a notation graph, which our analysis shows to be a necessary and sufficient representation of music notation. Building on the CVC-MUSCIMA dataset for staffline removal, the MUSCIMA++ dataset v1.0 consists of 140 pages of hand- written music, with 91254 manually annotated notation symbols and 82247 explicitly marked relationships between symbol pairs. The dataset allows training and directly evaluating models for symbol classification, symbol localization, and notation graph assembly, and musical content extraction, both in isolation and jointly. Open-source tools are provided for manipulating the dataset, visualizing the data and annotating more, and the data is made available under an open license.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje přeměnu překladového systému pro příbuzné jazyky Česílko do formátu Open-source. Tento systém byl vytvořen pro účely rychlého a kvalitního překladu z jednoho zdrojového do více příbuzných cílových jazyků. Jeho architektura je založena na mělké analýze a transferu pomocí pravidel. Článek představuje architekturu systému a instalační instrukce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Macine translation system Česílko has been developed as an answer to a growing need of translation and localization from one source language to many target languages. The system
belongs to the shallow parse, shallow transfer RBMT paradigm and it is designed primarily for
translation of related languages. The paper presents the architecture, the development design
and the basic installation instructions of the translation system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku popisujeme vylepšení úlohy pro řízení robota neomezeným přirozeným jazykem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we describe an improvement
on the task of giving instructions to robots
in  a  simulated  block  world  using  unrestricted natural language commands</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>MorphoRuEval-2017 je hodnotící kampaň určená k povzbuzení rozvoje technologií automatického morfologického zpracování pro ruštinu, a to jak pro normativní texty (novinky, beletrie, fakta), tak pro méně formální povahu (blogy a další sociální média). Tento článek porovnává metody, které účastníci použili při řešení úlohy morfologické analýzy. Rovněž se zabývá problémem sjednocení různých stávajících výcvikových sbírek ruského jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>MorphoRuEval-2017 is an evaluation campaign designed to stimulate the development of the automatic morphological processing technologies for Russian, both for normative texts (news, fiction, nonfiction) and those of less formal nature (blogs and other social media). This article compares the methods participants used to solve the task of morphological analysis. It also discusses the problem of unification of various existing training collections for Russian language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Co dál je možné dělat s handlem, kromě "základního" oddělení id od lokace. Metadata handlu. Handle a content negotiation. Template handle.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>What else can be achieved with handles in addition to "basic" separation of resource id from its location. Handle metadata. Handles and content negotiation. Template handles.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Historie vzniku, vývoj a úvod do používání repozitáře clarin-dspace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>History, development and a introduction to usage of clarin-dspace.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku představujeme projekt anotace evaluativního významu v Pražském závislostním korpusu 2.0. Projekt navazuje na sérii anotací malých korpusů prostého textu. V projektu byla použita automatická identifikace potenciálně evaluativních uzlů prostřednictvím českého slovníku hodnotících výrazů Czech SubLex 1.0. V rámci anotačních prací byly odhaleny výhody i nevýhody zvoleného anotačního schématu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the paper, we present our efforts to annotate evaluative language in the Prague Dependency Treebank 2.0. The project is a follow-up of the series of annotations of small plaintext corpora. It uses automatic identification of potentially evaluative nodes through mapping a Czech subjectivity lexicon to syntactically annotated data. The annotations unveiled several advantages and disadvantages of the chosen framework.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku se zabýváme jedním typem strukturní odlišnosti mezi českými a anglickými paralelními větami v dvojjazyčném korpusu PCEDT, konkrétně vzájemným mapováním valenčních doplnění aktorového typu a (nevalenčních) lokačních doplnění. Analýzou korpusových příkladů ukazujeme, do jaké míry se na rozdílech ve valenci sloves v překladově ekvivalentních větách podílejí jazykově specifické syntakticko-sémantické preference konkrétních slov na pozicích subjektu a predikátu a jak je toto mapování ovlivněno vzájemnou souhrou těchto preferencí, větné diateze a sémantického principu potlačení agentu ve větné struktuře.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the paper we address one type of structural difference between Czech and English parallel sentences in a bilingual PCEDT treebank, the mutual mapping of actors and locations. Within the analysis of treebank examples, we show the effect of language-specific syntactic and semantic preferences of individual words in the positions of the subject and predicate on the differences in valency structure, as well as the effect of diathesis and the principal of agent demoting.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek sleduje chování valenčních doplnění sloves vzhledem k pozici evaluačního cíle mezi participanty valenčního rámce. Předkládáme klasifikaci typů evaluativnío významu vyjádřeného slovesy a popisujeme společné charakteristické rysy valenčních rámců evaluačních sloves. V analýze se zabýváme třemi problémy: sémantickou klasifikací evaluačních sloves a jejím vztahem k propagaci evaluačního významu jednotlivým participantům, různým valenčním pozicím evaluačního cíle u vzýjemně překladových sloves, tj. možným posunům evaluativního fokusu a dosahu a ztrátě evaluativního významu během překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper documents the behavior of verb valency complementations regarding the position of the target of evaluation within the valency frame. We classify the types of evaluative meaning expressed by the verbs and identify shared characteristic features considering the valency patterns of the verbs. In the analysis, we comment on three major issues of interest: the semantic classification of evaluative verbs and its relation to the propagation of sentiment value to the participants, the possible non-matching structural positions of the target of evaluation in the valency frame of a verb and its translation, i.e., the possible shift in evaluative focus and scope, and the possible loss of evaluative stance in the process of translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku se zabýváme možnostmi pojetí výuky slovních druhů v  konstruktivistickém  vyučování.  Problematiku  vágnosti  a  vzájemné  prostupnosti  jazykových  kategorií  analyzujeme  z  hlediska  lingvistické  teorie, která usiluje o konzistentnost a přístupnost pro školní praxi, z hlediska lexikografické práce, jejímž cílem je připravit jasnou oporu pro běžného uživatele  i  pedagoga,  i  z  hlediska  moderní pedagogiky,  jejímž  úkolem  je  zpřístupňovat žákům vědecké koncepty a teoretické konstrukty důvěryhodným a smysluplným způsobem.
Důraz  klademe  zejména  na  proměnu  cílů jazykového  vyučování  i  na  proměnu vnímání role učitele. Tvrdíme, že konstruktivistické vyučování je více v souladu s principy současné jazykové deskripce než vyučování transmisivní, neboť umožňuje žákovské budování pojmů vlastní manipulací s materiálem a vlastní logickou úvahou a přechod od nácviku jednoznačné kategorizace k prohlubování obecných analytických schopností.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The  paper  addresses  the  advantages  of teaching  word  class  categorization  in  Czech in  a  constructivist  manner.  The  problem  of  vagueness and blurriness of the linguistic category of word class is analyzed from the linguistic point of view, the lexicographer's point of view and the pedagogical point of view.
We argue that because of the overall change of the aims of the teaching process,  constructivism  is  more  in  compliance  with  the  principles  of modern linguistic approaches than the transmissive teaching. It allows pupils to build their own linguistic concepts by their logical thinking and direct manipulation  of  language,  shifting  the  attention  from  the  word  class  categorization  training to the analytical thinking.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme proces vzniku NUDAR, arabského treebanku ve stylu Universal Dependencies. Představujeme převod z Penn Arabic Treebanku do syntaktické reprezentace Universal Dependencies přes mezilehlou závislostní reprezentaci. Probíráme obtíže, se kterými je převod závislostních stromů spojen, řešení, která jsme použili, a hodnocení námi převedených dat. Dále představujeme prvotní výsledky parsingu na NUDARu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe the process of creating NUDAR, a Universal Dependency treebank for Arabic. We present the conversion from the Penn Arabic Treebank to the Universal Dependency syntactic representation through an intermediate dependency representation. We discuss the challenges faced in the conversion of the trees, the decisions we made to solve them, and the validation of our conversion. We also present initial parsing results on NUDAR.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Víceslovné výrazy představují problém pro všechny aplikace zpracování přirozeného jazyka. V článku představujeme výsledky experimentů s neuronovým strojovým překladem z angličtiny do češtiny a lotyšštiny s důrazem na víceslovné výrazy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Processing of multi-word expressions (MWEs) is a known problem for any natural language
processing task. Even neural machine translation (NMT) struggles to overcome it. This paper
presents results of experiments on investigating NMT attention allocation to the MWEs and
improving automated translation of sentences that contain MWEs in English→Latvian and
English→Czech NMT systems. Two improvement strategies were explored—(1) bilingual
pairs of automatically extracted MWE candidates were added to the parallel corpus used to
train the NMT system, and (2) full sentences containing the automatically extracted MWE
candidates were added to the parallel corpus. Both approaches allowed to increase automated
evaluation results. The best result—0.99 BLEU point increase—has been reached with the first
approach, while with the second approach minimal improvements achieved. We also provide
open-source software and tools used for MWE extraction and alignment inspection.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku popisuje nástroj pro vykreslování výstupu a pozornostních vah neuronového překladu a pro odhad spolehlivosti překladu vypočtený na základě pozornosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this article, we describe a tool for visualizing the output and attention weights of neural
machine translation systems and for estimating confidence about the output based on the
attention.
Our aim is to help researchers and developers better understand the behaviour of their
NMT systems without the need for any reference translations. Our tool includes command
line and web-based interfaces that allow to systematically evaluate translation outputs from
various engines and experiments. We also present a web demo of our tool with examples of
good and bad translations: http://ej.uz/nmt-attention</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá vedlejšími jazykovými signály, které mohou vyjadřovat sémantiku implicitních vztahů. K nim patří např. hodnotící výrazy ve fokusu, za nimiž následuje obvykle argument s významem explikace či specifikace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper deals with secondary language signals that can express the semantics of implicit discourse relations. These include, for example, evaluative expressions in focus, usually followed by an argument with the meaning of explication or specification.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zpráva o vědecké náplni mezinárodního projektu týkajícího se výstavby diskurzu a prostředků jeho konektivity a o workshopu pořádaném v Praze v rámci tohoto projektu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A report on the contents of an international research project on the structure of discourse and on the means of its connectivity and on a workshop organized in Prague.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kapitola se zaměřuje na popis a delimitaci diskurzních konektorů, tj. výrazů přispívajících ke kohereci textu a pomáhajících čtenáři lépe porozumět sémantickým vztahům v textu. V příspěvku je představen etymologický původ diskurzních konektorů z hlediska současné lingvistiky. Cílem je podat definici diskurzních konektorů s ohledem na jejich historický vývoj, díky němuž můžeme lépe nahlížet na  diskurzní vztahy v současném jazyce. V kapitole analyzujeme etymologický původ a vývoj deseti nejfrekventovanějších konektorů v češtině, angličtině a němčině (mezijazykový pohled může napomoci také přesnějšímu překladu konektorů) a poukazujeme na fakt, že tyto konektory prošly ve všech sledovaných jazycích velmi podobným vývojem, než se ustálily v roli současných diskurzních konektorů. Domníváme se proto, že způsob vzniku diskurzních konektorů může být jazykovou univerzálií. V závěru kapitoly ukazujeme, jak naše zjištění mohou pomoci při anotacích diskurzu ve velkých korpusech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper focuses on the description and delimitation of discourse connectives, i.e.
linguistic expressions significantly contributing to text coherence and generally
helping the reader to better understand semantic relations within a text. The paper
discusses the historical origin of discourse connectives viewed from the perspective
of present-day linguistics. Its aim is to define present-day discourse connectives
according to their historical origin through which we see what is happening in discourse in contemporary language. The paper analyzes the historical origin of the most frequent connectives in Czech, English and German (which could be useful for more accurate translations of connectives in these languages) and point out that they underwent a similar process to gain a status of present-day discourse connectives. The paper argues that this historical origin or process of rising discourse connectives might be language universal. Finally, the paper demonstrates how these observations may be helpful for annotations of discourse in large corpora.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Instalace a uživatelské nastavení odborných překladů z češtiny do angličtiny a z angličtiny do češtiny. Překládají se české odborné termíny směrnice INSPIRE a thesaury GEMET a AgroVoc (prostorová data). Pro účely projektu byl připraven balíček obsahující připravené modely v systému Moses a seznam příkazů pro provádění překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Installation and user setting for Czech-English and English-Czech translation of Czech technical terms of the INSPIRE Directive and thesauri GEMET and AgroVoc (spatial data). For the project a package containing trained Moses models and commands for translation was prepared.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme novou verzi UDPipe 1.0, což je trénovatelný nástroj provádějící větnou segmentaci, tokenizaci, morfologické značkování, lemmatizaci a syntaktickou analýzu. Poskytujeme modely pro všech 50 jazyků UD 2.0, a navíc lze jednoduše UDPipe natrénovat pomocí vlastních dat v CoNLL-U formátu.

Pro potřeby CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, upravená verze UDPipe 1.1 byla použita jako základový systém a umístila se na 13. místě z 33 účastníků. Nejnovější verze UDPipe 1.2, která se také účastnila, dosáhla na 8. místo, přičemž potřebuje jen malý čas na běh a středné velké modely.

Nástroj je k dispozici pod open-source licencí MPL a poskytuje rozhraní pro C++, Python (pomocí ufal.udpipe balíčku PyPI), Perl (pomocí UFAL::UDPipe balíčku CPAN), Javu a C#.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an update to UDPipe 1.0, a trainable pipeline which performs sentence segmentation, tokenization, POS tagging, lemmatization and dependency parsing. We provide models for all 50 languages of UD 2.0, and furthermore, the pipeline can be trained easily using data in CoNLL-U format.

For the purpose of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, the updated UDPipe 1.1 was used as one of the baseline systems, finishing as the 13th system of 33 participants. A further improved UDPipe 1.2 participated in the shared task, placing as the 8th best system, while achieving low running times and moderately sized models.

The tool is available under open-source Mozilla Public Licence (MPL) and provides bindings for C++, Python (through ufal.udpipe PyPI package), Perl (through UFAL::UDPipe CPAN package), Java and C#.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme náš příspěvek do First Shared Task on Extrinsic Parser Evaluation (EPE 2017). Náš systém, UDPipe, je trénovatelný nástroj provádějící tokenizaci, morfologickou analýzu, morfologické značkování, lemmatizaci a syntaktickou analýzu. Je nezávislý na jazyku a k dispozici jsou modely pro všech 50 jazyků UD 2.0. Použitím relativně omezeného množství trénovacích dat (200 tisíc tokenů z anglického korpusu UD) a bez nastavení specifického pro angličtinu získal systém celkové hodnocení 56.05 a umístil se mezi soutěžícími systémy jako 7.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our contribution to The First Shared Task on Extrinsic Parser Evaluation (EPE 2017). Our participant system, the UDPipe, is an open-source pipeline performing tokenization, morphological analysis, part-of-speech tagging, lemmatization and dependency parsing. It is trained in a language agnostic manner for 50 languages of the UD version 2. With a relatively limited amount of training data (200k tokens of English UD) and without any English specific tuning, the system achieves overall score 56.05, placing as the 7th participant system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popis architektury projektu clarin-dspace a dodrziavanie standardov a doporuceni RDA-DFT/FAIR/OAIS.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Describing clarin-dspace architecture and the compliance to RDA-DFT/FAIR/OAIS.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje společný systém projektu QT21 pro překlad z angličtiny do lotyšštiny na druhé konferenci WMT 2017.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This  paper  describes  the  joint  submis-
sion    of    the    QT21    projects    for    the
English
→
Latvian  translation  task  of  the
EMNLP 2017 Second Conference on Ma-
chine Translation
(WMT 2017).  The sub-
mission  is  a  system  combination  which
combines  seven  different  statistical  ma-
chine translation systems provided by the
different groups.
The  systems  are  combined  using  either
RWTH’s  system  combination  approach,
or
USFD’s
consensus-based
system-
selection approach.  The final submission
shows   an   improvement   of   0.5   B
LEU
compared  to  the  best  single  system  on
newstest2017.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Navrhujeme propuštěnou hlubokou neuronovou síť (CDNN) hluboký neuronový model pro výběr věty odpovědi v kontextu systémů QA (Question Answering). Pro vytvoření nejlepších předpovědí kombinuje CDNN neurální uvažování s určitým symbolickým omezením. Integruje techniku přizpůsobení vzoru do vektoru vět
učení se. Při výcviku za použití dostatečných vzorků převyšuje CDNN ostatní nejlepší modely pro výběr vět. Ukazujeme, jak se využívají další zdroje
Školení může zvýšit výkon CDNN. V dobře studovaném datovém souboru pro výběr věty odpovědi náš model výrazně zlepšuje nejmodernější technologii.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We propose Constrained Deep Neural Network (CDNN) a deep neural model for answer sentence selection in the context of
Question Answering (QA) systems. To
produce the best predictions, CDNN combines neural reasoning with a kind of
symbolic constraint. It integrates pattern
matching technique into sentence vector
learning. When trained using enough samples, CDNN outperforms the other best
models for sentence selection.
We show how the use of other sources of
training can enhance the performance of
CDNN. In a well-studied dataset for answer sentence selection, our model improves the state-of-the-art significantly</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto krátkém časopise oznamujeme postupnou práci na hybridním hlubokém nervovém systému, který odpovídá na faktoidní a nepravotní otázky otevřené oblasti. Tento systém doplňuje znalostní graf založený na zodpovězení dotazu s vyhledávacími technikami pro volné texty, které řeší problematiku sparsity ve znalostních grafech. Ospravedlňujeme účinnost navrhovaného systému na základě výsledků pilotního experimentu. Rovněž popisujeme nastavení probíhajícího projektu v kontextu tohoto systému.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this short paper, we report a progressing work on a hybrid deep neural system for answering open-domain factoid and non-factoid questions. This system supplements knowledge graph based Question Answering with free-texts searching techniques to address the sparsity issues in knowledge graphs. We justify the efficiency of the proposed system based on the results of a pilot experiment. We also describe the settings of an on-going project in the context of this system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Uvádíme zprávu o pokročilejších úkolech při sestavování datových sad otázka odpovědět Quora. Datová sada Quora se skládá z otázek, které jsou položeny na stránkách Quora answering site. Je to jediná datová sada, která poskytuje odpovědi současně na úrovni vět a slov. Otázky v datové sadě jsou navíc autentické, což je mnohem realističtější pro systémy odpovědí na otázky. Testujeme výkonnost nejmodernějšího záznamníku otázek na datové množině a porovnáváme ji s lidskou výkonností, abychom vytvořili horní hranici datové sady.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We report on a progressing work for compiling Quora question answer dataset. Quora dataset is composed of the questions which are posed in
Quora question answering site. It is the only dataset which provides answers in
sentence level and word level at the same time. Moreover, the questions in the
dataset are authentic which is much more realistic for question answering systems. We test the performance of a state-of-the-art question answering system on
the dataset and compare it with human performance to establish an upper bound
for the dataset</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace nabízí přehled a srovnání současných elektronických lexikonů textových konektorů v různých jazycích se zvláštním zřetelem na nový lexikon českých konektorů - CzeDLex.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presentation offers an overview and comparison of present-day electronic lexicons of discourse connectives in different languages, with a special regard on the recently developed lexicon of Czech connectives – CzeDLex.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek předkládá podrobné srovnání dvou proslulých přístupů k analýze imlicitních diskurzních vztahů, Penn Discourse Treebanku a databáze Rhetorical Structure Theory Signalling Corpus.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Describing implicit phenomena in discourse is known to be a problematic task, both from the
theoretical and from the empirical perspective. The present article contributes to this topic by
a novel comparative analysis of two prominent annotation approaches to discourse (coherence)
relations that were carried out on the same texts. We compare the annotation of implicit relations in the Penn Discourse Treebank 2.0, i.e. discourse relations not signalled by an explicit discourse connective, to the recently released analysis of signals of rhetorical relations in the RST Signalling Corpus. Our data transformation allows for a simultaneous depiction and detailed study of these
two resources.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Neuronový strojový překlad (NMT) se stal v posledních letech široce využívaným přístupem ke strojovému překladu.

V našem tutoriálu začneme úvodem do základů metod hloubkového učení používaných v NMT jako jsou rekurentní neuronové sítě a jejich pokročilé varianty (GRU nebo LSTM sítě) nebo algoritmy pro jejich optimalizaci.

Představujeme modely specifické pro NMT, jako je mechanizmus pozornosti, a popisujeme metody použité pro dekódování cílových vět, včetně ensemblování modelů a paprskového prohledávání.

Projdeme nedávné pokroky v této oblasti a budeme diskutovat o jejich dopadu na nejmodernější metody používané na letošní soutěži WMT (http://www.statmt.org/wmt17/).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Neural machine translation (NMT) has become a widely-adopted approach to machine translation in the past few years.

In our tutorial, we will start with the introduction to the basics of the deep learning methods used in NMT, such as recurrent neural networks and their advanced variants (GRU or LSTM networks), or the algorithms for their optimization.

We introduce the NMT-specific models, such as the attention mechanism, and describe the methods used for decoding the target sentences, including model ensembling and beam search.

We will go through the recent advancements in the field and discuss their impact on the state-of-the-art methods used in this year's WMT competition (http://www.statmt.org/wmt17/).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku popisujeme naše příspěvky do multimodální překladové úlohy na WMT17.
Pro úlohu 1 (multimodální překlad) byl nejlepším systémem čistě textový neuronový překlad titulku zdrojového obrázku do cílového jazyka.
Hlavním rysem našeho systému je využití dalších dat získaných výběrem podobných vět z paralelních korpusů a syntézou dat zpětným překladem.
Pro úlohu 2 (vícejazyčné generování popisu obrázků) náš nejlepší systém generuje anglický popis obrázku, který je poté přeložen podle nejlepšího systému používaného v úloze č. 1.
Také předkládáme negativní výsledky, které jsou založeny na myšlenkách, o kterých se domníváme, že mají potenciál překlad zlepšit, ale v našem konkrétním uspořádání 
 neprokázaly býti prospěšnými.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we describe our submissions to the WMT17 Multimodal
  Translation Task. For Task 1 (multimodal translation), our best scoring
  system is a purely textual neural translation  of the source image caption to
  the target language. The main feature of the system is the use of additional
  data that was acquired by selecting similar sentences from parallel corpora
  and by data synthesis with back-translation. For Task 2 (cross-lingual image
  captioning), our best submitted system generates an English caption which is
  then translated by the best system used in Task 1. We also present negative
  results, which are based on ideas that we believe have potential of making
  improvements, but did not prove to be useful in our particular setup.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku představujeme vývoj Neural Monkey — open source nástroj pro neuronový strojový překlad a sekvenční učení obecně, který je postavený na knihovně TensorFlow. Náš nástroj poskytuje svým uživatelům vysokoúrovňové rozhraní, které umožňuje rychlé vytváření prototypů komplexních modelů s několika enkodéry a dekodéry. Architektura modelů se vytváří pomocí snadno čitelných konfiguračních souborů. Dlouhodobým cílem Neural Monkey je vytvořit a udržovat kolekci moderních metod pro sekvenční učení. Tomu odpovídá i modulární snadno rozšiřitelný design. Natrénované modely je možné použít pro dávkové zpracování dat nebo spustit jako webovou službu. V předkládaném článku popisujeme základní design nástroje a postup, jak spustit trénování jednoduchého překladače.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we announce development of Neural Monkey — an open-source neural machine translation (NMT) and general sequence-to-sequence learning system built over TensorFlow machine learning library.
The system provides a high-level API with support for fast prototyping of complex architectures with multiple sequence encoders and decoders. These models’ overall architecture is specified in easy-to-read configuration files. The long-term goal of Neural Monkey project is to create and maintain a growing collection of implementations of recently proposed components or methods, and therefore it is designed to be easily extensible. The trained models can be deployed either for batch data processing or as a web service. In the presented paper, we describe the design of the system and introduce the reader to running experiments using Neural Monkey.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Účel tohoto labu je seznámit uživatele s toolkitem Neural Monkey, využívaným pro experimenty se sekvenčním učením.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of this lab is to familiarize the users with the Neural Monkey toolkit for sequence learning experiments.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Attention modely ve vícezdrojovém neuronovém  sekvenčním učení zůstávají poměrně neprobádanou oblastí, a to navzdory jeho užitečnosti v úkolech, které které využívají více zdrojových jazyků či modalit. Navrhujeme dvě nové strategie jak kombinovat výstupy attentiion modelu z různých vstupů, plochou a hierarchickou. Navrhované metody porovnáváme se stávajícími a výsledky vyhodnocujeme na datech pro multimodální překlad a automatické post-editování překladu z WMT16. Navrhované metody dosažení konkurenceschopných výsledků na obou úlohách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Modeling attention in neural multi-source sequence-to-sequence learning remains a relatively unexplored area, despite its usefulness in tasks that incorporate multiple source languages or modalities. We propose two novel approaches to combine the outputs of attention mechanisms over each source sequence, flat and hierarchical. We compare the proposed methods with existing techniques and present results of systematic evaluation of those methods on the WMT16 Multimodal Translation and
Automatic Post-editing tasks. We show the proposed methods achieve competitive results on both tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje jeden z možných způsobů, jak zobrazit a prohledávat data s anotací víceslovných jednotek. 
Využíváme mnohojazyčný korpus PARSEME s anotací verbálních víceslovných jednotek v 18 jazycích. Anotované jednotky zahrnují různé typy, jako např. idiomy, konstrukce s lehkými slovesy, inherentně reflexivní slovesa nebo konstrukce se slovesem a částicí. Korpus byl dosud využíván zejména pro trénování prediktivních modelů, ale nikoli k lingvistickému výzkumu per se.
Článek nabízí způsob, jak data zpřístupnit lingvistům skrze jednoduché vyhledávací prostředí a jazyk Corpus Query Language (CQL) známy například z často užívané platformy NoSke.
I přes omezené možnosti k zachycení komplexních jevů jakými jsou nespojité, koordinované nebo vnořené víceslovné predikáty, CQL může postačovat k základním vyhledávkám víceslovných jednotek pro korpusově založený výzkum.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper demonstrates one of the possible ways on how to represent and query corpora with multiword expression (MWE) annotation.
   We exploit the multilingual corpus of 18 languages created under the PARSEME project with verbal multiword expression (VMWE) annotation. VMWEs include categories such as idioms, light verb constructions, verb-particle constructions, inherently reflexive verbs, and others. The corpus was mainly used for the purposes of training predictive models, yet not much linguistic research was conducted based on this data.
We discuss how to allow linguists to query for MWEs in a simple user interface using the Corpus Query Language (CQL) within the NoSke corpus management and concordance system. 
Despite its limited abilities to represent challenging cases such as discontinuous, coordinated or embedded VMWEs, CQL can be sufficient to make basic analysis of the MWE-annotated data in corpus-based studies.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje systém MUMULS, který se účastnil 2017 Shared Task on Automatic Identification of Verbal Multiword expressions (VMWEs). Systém MUMULS byl implementován přístupem učení s učitelem pomocí rekurentních neuronových sítí v open-source knihovně TensorFlow. Model byl trénován na poskytnutých datech s VMWEs a také na morfologických a syntaktických anotacích. MUMULS provádí identifikaci VMWEs v patnácti jazycích, byl to jeden z mála systémů který dokázal kategorizovat VMWEs ve skoro všech jazycích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we describe the MUMULS system that participated to the 2017 shared task on automatic identification of verbal multiword expressions (VMWEs). The MUMULS system was implemented using a supervised approach based on recurrent neural networks using the open source library TensorFlow. The model was trained on a data set containing annotated VMWEs as well as morphological and syntactic information. The MUMULS system performed the identification of VMWEs in 15 languages, it was one of few systems that could categorize VMWEs type in nearly all languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje novou úlohu na využití dat od uživatelů dialogových systémů v konverzacích člověk-stroj. Tato úloha se zaměřuje na sběr denotací extrahováním z přirozených vět získaných v průběhu dialogu. Motivace spočívá v potřebě velkého monžství trénovacích dat pro vývoj Q&amp;A dialogových systémů, přičemž získání těchto dat je obvykle těžké a nákladné. Získávání denotací při interakcích s uživateli například umožňuje online vylepšování komponent pro porozumění přirozenému jazyku a zjednodušit sběr trénovacích dat. Tento článek také prezentuje výsledky evaluace několika přístupů k extrakci denotací zahrnující modely založené na neuronových sítích s attention architekturou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a novel task using real user data obtained in human-machine conversation. The task concerns with denotation extraction from answer hints collected interactively in a dialogue. The task is motivated by the need for large amounts of training data for question answering dialogue system development, where the data is often expensive and hard to collect. Being able to collect denotation interactively and directly from users, one could improve, for example, natural understanding components online and ease the collection of the training data. This paper also presents introductory results of evaluation of several denotation extraction models including attention-based neural network approaches.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje hybridní dialog state tracker, rozšířený o trénovatelnou jednotku zpracování přirozeného jazyka (SLU). Naše architektura je inspirována belief trackery založenými na neuronových sítích. Tento přístup navíc rozšiřujeme o derivovatelná prvidla, která umožní end-to-end trénink. Tato pravidla umožní našemu trackeru lépe generalizovat v porovnání s trackery založenými pouze na strojovém učení. Pro evaluaci používáme Dialog State Tracking Challenge (DSTC) 2 - populární dataset využívaný pro srovnání výkonnosti belief trackerů. Podle informací, které máme, náš tracker dosahuje state-of-the-art výsledků ve třech ze čtyř kategorií datasetu DSTC2.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a hybrid dialog state tracker enhanced by trainable Spoken Language Understanding (SLU) for slot-filling dialog systems. Our architecture is inspired by previously proposed neural-network-based belief-tracking systems. In addition, we extended some parts of our modular architecture with differentiable rules to allow end-to-end training. We hypothesize that these rules allow our tracker to generalize better than pure machine-learning based systems. For evaluation, we used the Dialog State Tracking Challenge (DSTC) 2 dataset - a popular belief tracking testbed with dialogs from restaurant information system. To our knowledge, our hybrid tracker sets a new state-of-the-art result in three out of four categories within the DSTC2.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Udapi je open-source framework poskytující APO pro zpracovávání dat z projektu Universal Dependencies.
Implementace Udapi je dostupné pro programovací jazyky: Python, Perl a Java. Udapi je vhodné jak pro plnohodnotné aplikace, tak pro rychlé vyváření prototypů: vizualizace stromů, konverze formátu, dotazování, editace, transformace, testy validity, závislostní parsing, vyhodnocování, atd.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Udapi is an open-source framework providing
an application programming interface
(API) for processing Universal Dependencies
data. Udapi is available in
Python, Perl and Java. It is suitable
both for full-fledged applications and fast
prototyping: visualization of dependency
trees, format conversions, querying, editing
and transformations, validity tests, dependency
parsing, evaluation etc.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V návaznosti na loňský systém pro automatickou post-editaci strojového překladu Karlovy Univerzity se soustředíme na využití potenciálu sequence-to-sequence neuronových modelů pro danou úlohu. V článku nejprve porovnáváme několik architektur typu enkodér-dekodér na modelech menšího měřítka a představujeme systém, který byl vybrán na základě těchto předběžných výsledků a odeslán na WMT 2017 Automatic Post-Editing shared task. V článku také ukazujeme jak jednoduchá inkluze umělých dat dokáže vylepšit úspěšnost modelu na základě automatických evaluačních metrik. V závěru uvádíme několik příkladů výstupů vygenerových našim post-editačním systémem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Following upon the last year's CUNI system for automatic post-editing of machine translation output,
we focus on exploiting the potential of sequence-to-sequence neural models for this task. In this system description paper, we compare several encoder-decoder architectures on a smaller-scale models and present the system we submitted to WMT 2017 Automatic Post-Editing shared task based on this preliminary comparison. We also show how simple inclusion of synthetic data can improve the overall performance as measured by an automatic evaluation metric. Lastly, we list few example outputs generated by our post-editing system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje frázové a syntaktické překladové systémy Univerzity v Edinburku účastnící se soutěžní překladového úlohy WMT16.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the University of Edinburgh’s
phrase-based and syntax-based
submissions to the shared translation tasks
of the ACL 2016 First Conference on Machine
Translation (WMT16). We submitted
five phrase-based and five syntaxbased
systems for the news task, plus one
phrase-based system for the biomedical
task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Analýza soutěžních úkolů z Olympiády v českém jazyce, hodnocení jazykových dovedností studentů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Analysis of competition tasks of the Olympiad in Czech language, evaluation of students' language skills.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Počítačový nástroj Evaluátor diskurzu (EVALD 1.0) slouží k automatickému hodnocení povrchové koherence (koheze) textů v češtině psaných rodilými mluvčími češtiny. Software tedy hodnotí (známkuje) úroveň předloženého textu z hlediska jeho povrchové výstavby (zohledňuje např. frekvenci a rozmanitost spojovacích prostředků, bohatost slovní zásoby apod.).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Evaluator of Discourse (EVALD 1.0) is a software that serves for automatic evaluation of surface coherence (cohesion) in Czech texts written by native speakers of Czech. Software evaluates the level of the given text from the perspective of its surface coherence (i.e. it takes into consideration, e.g., the frequency and diversity of connectives, richness of vocabulary etc.).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Počítačový nástroj Evaluátor diskurzu pro cizince (EVALD 1.0 pro cizince) slouží k automatickému hodnocení povrchové koherence (koheze) textů v češtině psaných nerodilými mluvčími češtiny. Software hodnotí úroveň předloženého textu z hlediska jeho povrchové výstavby (zohledňuje např. frekvenci a rozmanitost užitých spojovacích prostředků, bohatost slovní zásoby apod.).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Evaluator of Discourse for Foreigners (EVALD 1.0 for Foreigners) is a software that serves for automatic evaluation of surface coherence (cohesion) in Czech texts written by non-native speakers of Czech. Software evaluates the level of the given text from the perspective of its surface coherence (i.e. it takes into consideration, e.g., the frequency and diversity of connectives, richness of vocabulary etc.).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje, jakou roli hrají elidované větné participanty v koreferenčních řetězcích v textu, tj. zda a do jaké míry se participanty, které jsou v povrchové větné struktuře přítomné pouze implicitně, zapojují do vztahů textové a gramatické koreference. Článek zároveň představuje metody, jimiž je možné zkoumat vzájemné vztahy mezi různými jazykovými jevy, a to na datech Pražského závislostního korpusu, který obsahuje anotaci lingvistických jevů na několika jazykových rovinách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper focuses on which role is given to elided sentence participants in coreference chains in the text, i.e. whether (and to which degree) the participants that are present only implicitly in the surface layer are involved in relations of textual and grammatical coreference. Generally, the paper introduces  he methods how it is possible to examine the interplays of different language phenomena in corpus data of the Prague Dependency Treebank containing multilayer annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku jsou představeny možnosti automatické evaluace povrchové koherence (koheze) textů psaných nerodilými mluvčími češtiny během certifikovaných zkoušek. Na základě korpusové analýzy jsou vyhledávány a popisovány relevantní rozlišovací rysy (týkající se povrchové koherence textu) pro automatickou detekci úrovní textů nerodilých mluvčích A1–C1 (úrovně jsou ustanoveny Společným evropským referenčním rámcem pro jazyky). Úrovně A1–C1 byly hodnoceny nejprve lidmi (anotátory) – poté byly dělány strojové experimenty s cílem přiblížit se lidskému hodnocení automaticky, a to sledováním vybraných textových rysů, např. frekvence a různorodosti diskurzních konektorů nebo hustoty diskurzních vztahů v daném textu ap. V článku jsou představeny experimenty sledující vždy různé textové rysy při použití dvou algoritmů strojového učení. Úspěšnost automatického měření povrchové koherence (koheze) textu podle Společného evropského referenčního rámce pro jazyky je 73,2 % pro rozpoznávání úrovní A1–C1 a 74,9 % pro rozpoznávání úrovní A2–B2.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce possibilities of automatic evaluation of surface text coherence (cohesion) in texts written by learners of Czech during certified exams for non-native speakers. On
the basis of a corpus analysis, we focus on finding and describing relevant distinctive
features for automatic detection of A1–C1 levels (established by CEFR – the Common
European Framework of Reference for Languages) in terms of surface text coherence.
The CEFR levels are evaluated by human assessors and we try to reach this assessment
automatically by using several discourse features like frequency and diversity of discourse
connectives, density of discourse relations etc. We present experiments with various
features using two machine learning algorithms. Our results of automatic evaluation
of CEFR coherence/cohesion marks (compared to human assessment) achieved 73.2%
success rate for the detection of A1–C1 levels and 74.9% for the detection of A2–B2
levels.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Názvy logických spojek jsou často motivovány názvy spojek z přirozeného jazyka. V případě, že však chceme výroku v přirozeném jazyce (konkrétně češtině) přiřadit jednoznačně logické schéma, zjišt’ujeme, že to není možné dělat automaticky. V tomto článku se zabýváme tím, kdy je možné české spojky „a“ a „nebo“ zapsat pomocí konjunkce a disjunkce a kde je nutné zvolit jinou interpretaci – s ohledem na problematiku množného čísla a vyjádření vztahů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper studies the relationship between
conjunctions in a natural language (Czech) and their logical counterparts. It shows that the process of transformation of a natural language expression into its logical representation is not straightforward. The paper concentrates on the most frequently used logical conjunctions, AND and OR, and it analyzes the natural language phenomena which influence their transformation
into logical conjunction and disjunction.
The phenomena discussed in the paper are temporal
sequence, expressions describing mutual relationship and the consequences of using plural.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Adjektiva velmi často určují polaritu věty. Tento článek studuje roli spojek v analýze polarity adjektiv. Celá studie je prováděna pro češtinu modifikací algoritmu vyvinutého původně pro angličtinu. Modifikace berou v úvahu typologické rozdíly mezi oběma jazyky. Výsledky dosažené pro oba jazyky jsou porovnány a doplněny rozsáhlým rozborem výjimečných případů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Adjectives very often determine the polarity of
an utterance. This paper studies the role of conjunctions in the analysis of adjective polarity. The study is being performed on Czech language on the basis of an existing algorithm for English. The algorithm has been modified in order to reflect the differences between
the two typologically different languages. The results of the original and modified algorithm are being compared and discussed. The paper also contains a thorough discussion of exceptions and special cases supported by a number of examples from a large corpus of Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Značkování slovními druhy (POS tagging) se v počítačovém zpracování přirozeného jazyka někdy považuje za téměř vyřešený problém. Standardní řízené přístupy často dosahují úspěšnosti přes 95 %, pokud je k dispozici dostatek ručně anotovaných trénovacích dat (typicky několik set tisíc tokenů nebo více). My si nicméně myslíme, že je stále užitečné studovat polořízené a neřízené přístupy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Part-of-speech (POS) tagging is sometimes considered an almost solved problem in NLP. Standard supervised approaches often reach accuracy above 95% if sufficiently large hand-labeled training data are available (typically several hundred thousand tokens or more). However, we still believe that it makes sense to study semi-supervised and unsupervised approaches.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje systém hybridního strojového překladu (MT), který byl vytvořen pro překlad z angličtiny do němčiny v oblasti technické dokumentace.
Systém je založen na třech různých systémech MT (frázový, pravidlový a neuronový), které jsou spojeny výběrovým mechanismem, který používá hluboké jazykové rysy v procesu strojového učení.
Součástí je také podrobná manuální analýza chyb, kterou jsme provedli pomocí specializované "zkušební sady", která obsahuje vybrané příklady relevantních jevů.
Zatímco automatické výsledky ukazují obrovské rozdíly mezi systémy, celkový průměrný počet chyb, které (ne) dělají, je pro všechny systémy velmi podobný.
Podrobné rozdělení chyb však ukazuje, že systémy se chovají velmi odlišně, pokud jde o různé jevy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes a hybrid Machine Translation (MT) system built for translating from English
to German in the domain of technical documentation. The system is based on three different
MT engines (phrase-based SMT, RBMT, neural) that are joined by a selection mechanism
that uses deep linguistic features within a machine learning process. It also presents a detailed
source-driven manual error analysis we have performed using a dedicated “test suite” that contains
selected examples of relevant phenomena. While automatic scores show huge differences
between the engines, the overall average number or errors they (do not) make is very similar for
all systems. However, the detailed error breakdown shows that the systems behave very differently
concerning the various phenomena.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje VPS-GradeUp, sadu 11 400 manuálně anotovaných ohodnocení vzorů užití 29 anglických sloves z Pattern Dictionary of English Verbs Patricka Hankse.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present VPS-GradeUp ― a set of 11,400 graded human decisions on usage patterns of 29 English lexical verbs from the Pattern Dictionary of English Verbs by Patrick Hanks. The annotation contains, for each verb lemma, a batch of 50 concordances with the given lemma as KWIC, and for each of these concordances we provide a graded human decision on how well the individual PDEV patterns for this particular lemma illustrate the given concordance, indicated on a 7-point Likert scale for each PDEV pattern. With our annotation, we were pursuing a pilot investigation of the foundations of human clustering and disambiguation decisions with respect to usage patterns of verbs in context. The data set is publicly available at http://hdl.handle.net/11234/1-1585.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje metodu automatické identifikace deverbativ na základě slovníků a manuálně a automaticky anotovaných korpusů. Metoda je evaluována na novém testovacím korpusu, který je rovněž zveřejněn.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present an attempt to automatically identify Czech deverbative nouns using several methods that use large corpora as well as existing lexical resources. The motivation for the task is to extend a verbal valency (i.e., predicate-argument) lexicon by adding nouns that share the valency properties with the base verb, assuming their properties can be derived (even if not trivially) from the underlying verb by deterministic grammatical rules. At the same time, even in inflective languages, not all deverbatives are simply created from their underlying base verb by regular lexical derivation processes. We have thus developed hybrid techniques that use
both large parallel corpora and several standard lexical resources. Thanks to the use of parallel
corpora, the resulting sets contain also synonyms, which the lexical derivation rules cannot get. For evaluation, we have manually created a gold dataset of deverbative nouns linked to 100 frequent Czech verbs since no such dataset was initially available for Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje vyhledávací systém pro spojené vyhledávání ve dvojjazyčném slovníku CzEngVallex a v paralelním anglicko-českém korpusu PCEDT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper and the associated system demo, we present an advanced search system that allows
to perform a joint search over a (bilingual) valency lexicon and a correspondingly annotated
linked parallel corpus. This search tool has been developed on the basis of the Prague Czech-English Dependency Treebank, but its ideas are applicable in principle to any bilingual
parallel corpus that is annotated for ependencies and valency (i.e., predicate-argument structure), and where verbs are linked to appropriate entries in an associated valency lexicon. Our online search tool consolidates more search interfaces into one, providing expanded
structured search capability and a more efficient advanced way to search, allowing users to
search for verb pairs, verbal argument pairs, their surface realization as recorded in the lexicon, or for their surface form actually appearing in the linked parallel corpus. The search system is currently under development, and is replacing our current search tool available at
http://lindat.mff.cuni.cz/services/CzEngVallex, which could search the lexicon but the queries cannot take advantage of the underlying corpus nor use the additional surface form information from the lexicon(s). The system is available as open source.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Recognizing textual entailment is typically considered as a binary decision task – whether a text T entails a hypothesis H. Thus, in case of a negative answer, it is not possible to express that H is almost entailed by T. Partial textual entailment provides one possible approach to this issue. This paper presents an attempt to use word2vec model for recognizing partial (faceted) textual entailment. The proposed approach does not rely on language dependent NLP tools and other linguistic resources, therefore it can be easily implemented in different language environments where word2vec models are available.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Recognizing textual entailment is typically considered as a binary decision task – whether a text T entails a hypothesis H. Thus, in case of a negative answer, it is not possible to express that H is “almost entailed” by T. Partial textual entailment provides one possible approach to this issue. This paper presents an attempt to use word2vec model for recognizing partial (faceted) textual entailment. The proposed approach does not rely on language dependent NLP tools and other linguistic resources, therefore it can be easily implemented in different language environments where word2vec models are available.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Sentiment analysis neboli automatická extrakce subjektivních názorů z textu je v současnosti jedním z hlavních témat počítačového zpracovávání přirozeného jazyka. V příspěvku bude shrnut současný stav bádání v této oblasti na českých datech. Obšírněji představím základní metody vytěžování emocí z textu, datové zdroje pro češtinu i specifika českých dat a přiblížím konkrétní aplikace a use cases např. z oblasti internetového marketingu, monitoringu sociálních sítí apod.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Sentiment analysis or automatic extraction of subjective opinions from a written texts is currently one of the major topics in NLP. In this talk, I will introduce the state-of-the-art methods and approaches to sentiment analysis on Czech data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ignite talk na téma automatické detekce emocí, základní přístupy a datové zdroje.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Ignite talk on automatic detection of emotions, basic approaches, data resources overview.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pokud mají dostatek trénovacích dat, umějí stroje rozpoznat emoce v psaném textu. Je tomu skutečně tak? A jak se řeší ironie?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>If well trained, computers can count emotions in texts. Is that so? And what about irony?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Současná počítačová lingvistika zažívá masový zájem o postojovou analýzu jakožto nástroj zjišťování veřejného mínění. Je ale možné rozlišit dobro a zlo pomocí statistických metod? Jak zacházet s ironií, idiomy či vulgarismy? Jsou emoce jazykově nezávislé? V příspěvku zodpovím tyto otázky a popíšu současné přístupy k postojové analýze.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Current computational linguistics witnesses a massive increase of interest in sentiment analysis, as it is a powerful means of public opinion mining. However, is it even possible to distinguish between good and evil using statistical methods? How would you treat irony, idioms, innovative vulgarisms and other inherent elements of natural language? Are emotions language-independent? I will share my thoughts and opinions towards these issues, describing the state-of-the art approaches employed in sentiment analysis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Objem textových dat stále roste, rozvoj Webu 2.0 přináší množství textů generovaných samotnými uživateli Internetu. Jejich příspěvky nezřídka obsahují subjektivní názory, emoce, hodnocení… K čemu a jak můžeme tato data použít? Je možné emoce v textu spolehlivě strojově třídit? 
V přednášce si představíme základní prostředky emocionálního vyjadřování v češtině na všech rovinách popisu jazyka a popíšeme metody automatického zpracovávání emocionálně laděných textů i jejich praktické aplikace z oblasti internetového marketingu, monitoringu sociálních sítí apod.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The volume of text data is increasing constantly, the rise of Web 2.0 brings lots of data created by the Internet users themselves. How do we categorize these data automatically with respect to emotions they express? In this talk, I will introduce basic means of emotional language and methods we use to process it automatically.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce si představíme základní prostředky emocionálního vyjadřování v češtině na všech rovinách lingvistického popisu: zaměříme se na stránku lexikální (slovník emocionálních výrazů pro češtinu, evaluativní idiomy, vulgarismy), gramatickou (význam jednotlivých slovních druhů, typické syntaktické vzorce hodnotících vět) a především sémantickou a pragmatickou (ironie, sarkasmus). Popíšeme také základní metody automatického zpracovávání emocionálně laděných textů a jejich praktické aplikace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this talk, I will introduce basic means of Czech emotional language on all the levels of linguistic description. I will focus on both grammar and lexicon and semantics and pragmatics of evaluative utterances.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Může mít umělá inteligence vlastní úsudek? A jak dokážeme počítačově odhalit objektivní či subjektivní názor? Přednáška shrne základní přístupy k objektivitě a subjektivitě v textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Can artificial intelligence have an opinion? And how do we distinguish between subjective and objective stances using computer linguistic? This talk introduces basic approaches to these problems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek představuje projekt Universal Dependencie a konkrétní případ jeho využití v oblasti sentiment analysis.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper is to present a unique project of parallel treebank annotation and a case study of its practical use. We describe the project of Universal Dependencies, explain the methodological approach, illustrate the basic principles of parallel annotation and introduce the universal annotation scheme. Also, we provide a contrastive syntactic analysis of evaluative sentences in typologically different languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce si představíme základní prostředky emocionálního vyjadřování v češtině na všech rovinách lingvistického popisu: zaměříme se na stránku lexikální (slovník emocionálních výrazů pro češtinu, evaluativní idiomy, vulgarismy), gramatickou (význam jednotlivých slovních druhů, typické syntaktické vzorce hodnotících vět) a především sémantickou a pragmatickou (ironie, sarkasmus). Popíšeme také základní metody automatického zpracovávání emocionálně laděných textů a jejich praktické aplikace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this talk, we introduce the basic means of expressing emotions in Czech on all levels of the linguistic description.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Poster představuje tvorbu pravidel pro určení cílů hodnocení založenou na evaluativních strukturách automaticky vyhledaných v Pražském závislostním korpusu. Tato pravidla jsme zkombinovali s metodami strojového učení, konkrétně s conditional random fields.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This poster shows how we created the rules for  opinion  target  identification  based  on  evaluative  structures automatically found in Prague Dependency Treebank and  combined  them  with  machine  learning  methods, namely linear-chain conditional random fields.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Česko-anglické ruční zarovnání slov. Česko-anglické ruční zarovnání slov.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Czech-English Manual Word Alignment. Czech-English Manual Word Alignment.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku budeme porovnávat delexicalized přenos a minimálně pod dohledem rozebrat techniky na 32 různých jazyků od Universal závislostí korpusu sbírky. Minimální dohled při přidávání pouštět univerzální gramatická pravidla pro POS tagy. Pravidla jsou začleněny do nekontrolované závislost parseru ve formách externích dřívějších pravděpodobnostech. Také jsme experimentovat s učit se toto pravděpodobností z jiných stromových korpusů. Průměrná připevnění skóre našeho parseru je o něco nižší než v delexicalized přenosu analyzátor, nicméně, to funguje lépe pro jazyky z méně zdroji jazykových rodin (non-Indo-Evropan), a proto je vhodný pro ty, pro které stromových korpusů často neexistují.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we compare delexicalized transfer and minimally supervised parsing techniques on 32 different languages from Universal Dependencies treebank collection. The minimal supervision is in adding handcrafted universal grammatical rules for POS tags. The rules are incorporated into the unsupervised dependency parser in forms of external prior probabilities. We also experiment with learning this probabilities from other treebanks. The average attachment score of our parser is slightly lower then the delexicalized transfer parser, however, it performs better for languages from less resourced language families (non-Indo-European) and is therefore suitable for those, for which the treebanks often do not exist.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku představujeme náš nový experimentální systém sloučení závislost
reprezentace dvou rovnoběžných vět
do jednoho strom závislostí. Všechny vnitřní uzly v závislosti stromu představují zdroj-cílové páry slovy, další slova jsou ve formě koncové uzly. Používáme Univerzální Závislosti anotace styl, ve kterém funkční slova, jejichž použití se často liší mezi jazyky, jsou zaznamenány jako listy.
Paralelní korpus je analyzován v minimálně dohlíží způsobem. Nezarovnaný slova jsou zde automaticky tlačil na povrch listů. Představujeme jednoduchý systém překladu vyškoleného na takových sloučených stromech a vyhodnocovat jej WMT 2016 anglicko-to-český a česko-to-anglický překlad úloh. I přesto, že model je doposud velmi jednoduché a byl používán žádný jazykový model a model word-li řazení varianta Český k angličtině dosáhl podobného Bleu skóre jako další zavedeného systému stromu bázi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In  this  paper,  we  present  our  new  experimental system of merging dependency
representations  of  two  parallel  sentences
into  one  dependency  tree. All  the  inner  nodes  in  dependency  tree  represent source-target  pairs  of  words,   the  extra words are in form of leaf nodes.  We use Universal Dependencies annotation style, in  which  the  function  words,  whose  usage  often  differs  between  languages,  are annotated  as  leaves.
The  parallel  treebank  is  parsed  in minimally  supervised way. Unaligned  words  are  there  automatically  pushed  to  leaves. We  present a  simple  translation  system  trained  on such   merged   trees   and   evaluate   it   in WMT 2016 English-to-Czech and Czech-to-English  translation  task. Even though the model is so far very simple and no language  model  and  word-reordering  model were  used,  the  Czech-to-English  variant reached similar BLEU score as another established tree-based system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V posledních 12 letech došlo k velkým pokroku v oblasti bez dozoru závislostní parsování. Různé přístupy však někdy liší v motivaci a ve vymezení problému. Některé z nich umožňují využití zdrojů, které jsou zakázány jinými, neboť se s nimi zachází jako druh dohledu. Cílem tohoto příspěvku je definovat všechny varianty bez dozoru závislost rozebrat problém a ukázat jejich motivace, pokrok, a nejlepší výsledky. Také jsme diskutovali o užitečnosti látky jako součásti bez dozoru analýze obecně, a to jak pro formální lingvistiky a pro aplikace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the last 12 years, there has been a big progress in the field of unsupervised dependency parsing. Different approaches however sometimes differ in motivation and definition of the problem. Some of them allow using resources that are forbidden by others, since they are treated
as a kind of supervision. The goal of this paper is to define all the variants of unsupervised dependency parsing problem and show their motivation, progress, and the best results. We also discuss the usefulness of the unsupervised
parsing generally, both for the formal linguistics and for the applications.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Texty ve 107 jazycích z korpusu W2C (http://hdl.handle.net/11858/00-097C-0000-0022-6133-9), první 1000000 tokenů pro každý jazyk, označkované delexikalizovaným taggerem popsaným v Yu et al. (2016, LREC, Portorož, Slovenia).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Texts in 107 languages from the W2C corpus (http://hdl.handle.net/11858/00-097C-0000-0022-6133-9), first 1,000,000 tokens per language, tagged by the delexicalized tagger described in Yu et al. (2016, LREC, Portorož, Slovenia).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nedokončené zaměřená na těžbu překladové páry zdrojové a cílové závislost treelets které mají být použity v systému strojového překládání závislostí na bázi. Představíme nový voze metodu pro paralelní segmentaci stromu na základě vzorků Gibbs. S použitím dat z české, anglické paralelním korpusu, ukážeme, že postup konverguje ke slovníku, který obsahuje poměrně velké treelets; V některých případech se zdá, že segmentace mít zajímavé lingvistické výklady.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a work in progress aimed at extracting translation pairs of source and target dependency treelets to be used in a dependency-based machine translation system. We introduce a novel unsupervised method for parallel tree segmentation based on Gibbs sampling. Using the data from a Czech-English parallel treebank, we show that the procedure converges to a dictionary containing reasonably sized treelets; in some cases, the segmentation seems to have interesting linguistic interpretations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme novou datovou sadu pro generování jazyka v hlasových dialogových systémech, která spolu s každou odpovědí systému k vygenerování (pár zdrojová sémantická reprezentace – cílová věta v přirozeném jazyce) uvádí i předcházející kontext (uživatelský dotaz). Očekáváme, že tento kontext dovolí generátorům jazyka adaptovat se na způsob vyjadřování uživatele a tím docílit přirozenějších a potenciálně úspěšnějších odpovědí. Datová sada byla vytvořena za pomoci crowdsourcingu v několika fázích, aby bylo možno získat přirozené uživatelské dotazy a odpovídající přirozené, relevantní a kontextově zapojené odpovědi systému. Datová sada je dostupná online pod otevřenou licencí Creative Commons 4.0 BY-SA.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a novel dataset for natural language generation (NLG) in spoken dialogue systems which includes preceding context (user utterance) along with each system response to be generated, i.e., each pair of source meaning representation and target natural language paraphrase. We expect this to allow an NLG system to adapt (entrain) to the user’s way of speaking, thus creating more natural and potentially more successful responses. The dataset has been collected using crowdsourcing, with several stages to obtain natural user utterances and corresponding relevant, natural, and contextually bound system responses. The dataset is available for download under the Creative Commons 4.0 BY-SA license.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nový generátor přirozeného jazyka pro hlasové dialogové systémy, který je schopný přizpůsobit se způsobu, jakým mluví uživatel, a poskytovat odpovědi přiměřené kontextu dialogu. Generátor je založen na neuronových sítích a přístupu sequence-to-sequence. Je plně trénovatelný z dat, která spolu s trénovacími výstupy generátoru obsahují také předchozí kontext. Ukazujeme, že kontextový generátor přináší signifikantní zlepšení oproti základnímu generátoru, a to jak z pohledu automatických metrik, tak v preferenčním testu lidského hodnocení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a novel natural language generation system for spoken dialogue systems capable of entraining (adapting) to users' way of speaking, providing contextually appropriate responses. The generator is based on recurrent neural networks and the sequence-to-sequence approach. It is fully trainable from data which include preceding context along with responses to be generated. We show that the context-aware generator yields significant improvements over the baseline in both automatic metrics and a human pairwise preference test.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Datová sada pro plně trénovatelné generátory jazyka v hlasových dialogových systémech, která pokrývá doménu anglických informací o veřejné dopravě. 
Spolu s každou datovou položkou (pár zdrojové reprezentace významu a věty v přirozeném jazyce jako cíl generování) obsahuje i předcházející kontext (uživatelův dotaz, který má systém egnerovanou větou zodpovědět). Zohlednění formy předchozího dotazu pro generování umožní generátorům natrénovaným na této datové sadě adaptovat se na předchozí dotazy, tj. používat stejné shodné výrazy a syntaktické konstrukce jako uživatel dialogového systému. Předpokládáme, vygenerované věty tak budou vnímány jako přirozenější, což může vést i k úspěšnějším dialogům.
Pro získání přirozených uživatelských dotazů i odpovědí systému byla použita metoda crowdsourcingu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A dataset intended for fully trainable natural language generation (NLG) systems in task-oriented spoken dialogue systems (SDS), covering the English public transport information domain. 
It includes preceding context (user utterance) along with each data instance (pair of source meaning representation and target natural language paraphrase to be generated). Taking the form of the previous user utterance into account for generating the system response allows NLG systems trained on this dataset to entrain (adapt) to the preceding utterance, i.e., reuse wording and syntactic structure. This should presumably improve the perceived naturalness of the output, and may even lead to a higher task success rate.
Crowdsourcing has been used to obtain natural context user utterances as well as natural system responses to be generated.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme generátor přirozeného jazyka založený na metodě sequence-to-sequence, který lze natrénovat tak, aby z dialogových aktů produkoval buď věty v přirozeném jazyce, nebo hloubkově syntaktické závislostní stromy. Na generátoru potom přímo porovnáváme dvoufázové generování, které používá oddělené větné plánování a povrchovou realizaci, s jednofázovým přístupem.

Obě nastavení generátoru jsme byli schopni natrénovat s použitím velmi malého množství trénovacích dat. Jednofázové generování dosahuje lepších výsledků; překonává nejlepší předchozí výsledek podle n-gramových automatických metrik a zároveň nabízí relevantnější výstupy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a natural language generator based on the sequence-to-sequence approach that can be trained to produce natural language strings as well as deep syntax dependency trees from input dialogue acts, and we use it to directly compare two-step generation with separate sentence planning and surface realization stages to a joint, one-step approach.

We were able to train both setups successfully using very little training data. The joint setup offers better performance, surpassing state-of-the-art with regards to n-gram-based scores while providing more relevant outputs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Diateze představují vztahy mezi různými povrchověsyntaktickými strukturami sloves. Jsou podmíněny změnou morfologické charakteristiky slovesného rodu a jsou spojeny se změnami v povrchověsyntaktickém vyjádření valenčních doplnění sloves. V příspěvku jsme se zaměřili na specifické změny, kterým podléhají valenční doplnění českých funkčních sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Diatheses represent relations between different surface syntactic structures of verbs. They are conditioned by changes in morphological meaning of a verb and they are associated with specific changes in its valency structure. In this contribution, I discussed changes in the surface syntactic expressions of Czech complex predicates.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá distribucí Konatele v konstrukcích komplexních predikátů s kategoriálními slovesy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The contribution discusses the distribution of ACTors in light verb constructions in Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kapitola popisuje syntaktickou strukturu českých komplexních predikátů s funkčním slovesem v aktivu. Vymezuje čtyři typy komplexních predikátů. Navrhuje pravidlový popis daných jazykových jevů a jejich zachycení ve slovníku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This chapter describes the syntactic structure of Czech complex predicates with light verbs in active voice. Four types of Czech complex predicates are distinguished. Their rules based description and representation in the valency lexicon are proposed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku navrhujeme lexikografickou reprezentaci komplexivních predikátů v češtině. Zvláštní oddíl je věnován výběru kolokací funkčních sloves a predikativních jmen a anotaci jejich syntaktické struktury.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this contribution, we propose a lexicographic representation of Czech complex predicates. Special attention is devoted to the selection of collocations of function verbs and predicative nouns and to the annotation of their syntactic structure.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku se zaměřujeme na české složené predikáty, které jsou tvořené lehkým slovesem a predikativním jménem vyjádřeným jako přímý objekt. Ačkoli čeština -- jakožto inflekční jazyk -- poskytuje výbornou příležitost pro studium distribuce valenčních doplnění složených predikátů, tato distribuce dosud nebyla poplsána. Na základě manuální analýzy bohatě anotovaných dat PDT formulujeme poučky řídicí tuto distribuci. V automatickém experimentu tyto poučky ověřujeme na korektních syntaktických strukturách PDT a PCEDT s velmi uspokojivými výsledky: distribuce 97% valenčních doplnění se řídí navrženými poučkami. Tyto výsledky dokládají, že vytváření povrchové struktury složených predikátů je pravidelný proces.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we focus on Czech complex predicates formed by a light verb and a predicative noun expressed as the direct object. Although Czech – as an inflectional language encoding syntactic relations via morphological cases – provides an excellent opportunity to study the distribution of valency complements in the syntactic structure with complex predicates, this distribution has not been described so far. On the basis of a manual analysis of the richly annotated data from the Prague Dependency Treebank, we thus formulate principles governing this distribution. In an automatic experiment, we verify these principles on well-formed syntactic structures from the Prague Dependency Treebank and the Prague Czech-English Dependency Treebank with very satisfactory results: the distribution of 97% of valency complements in the surface structure is governed by the proposed principles. These results corroborate that the surface structure formation of complex predicates is a regular process.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zvýšený zájem o ‚porozumění‘ přirozeným jazykům způsobil, že do centra pozornosti současného výzkumu se dostávají různé postupy, často popisované jako ‚sémantická analýza‘.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Increased interest in natural language ‘understanding’ has brought into the focus of much current work a variety of techniques often described as ‘semantic parsing’.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládáme systém pro rozpoznávání pojmenovaných entit, který je jazykově nezávislý a nepotřebuje klasifikační rysy pro strojové učení. Systém využívá současných výsledků v oblasti umělých neuronových sítí, jako jsou parametric rectified linear units (PReLU), embeddingy slov a embeddingy charakterů ve slovech založené na gated linear units (GRU). Systém nepotřebuje vyhledávání vhodné sady klasifikačních rysů (feature engineering) a pouze s využitích povrchových forem, lemmat a slovních druhů na vstupu dosahuje vynikajících výsledků v rozpoznávání pojmenovaných entit v češtině a překonává stávající výsledky dříve publikovaných prací, které využívají ručně vytvořené klasifikační rysy založené na ortografické podobnosti slov. Navíc tato síť podává robustní výkon i v případě, kdy jsou na vstupu pouze povrchové formy. Síť dovede využít navíc i kombinaci ručně vytvořených klasifikačních rysů a v tom případě překonává stávající výsledky s markantním rozdílem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a completely featureless, language agnostic named entity recognition system. Following recent advances in artificial neural network research, the recognizer employs parametric rectified linear units (PReLU), word embeddings and character-level embeddings based on
gated linear units (GRU). Without any feature engineering, only with surface forms, lemmas and tags as input, the network achieves excellent results in Czech NER and surpasses the current state of the art of previously published Czech NER systems, which use manually designed rule-based orthographic classification features. Furthermore, the neural network achieves robust results even when only surface forms are available as input. In addition, the proposed neural network can use the manually designed rule-based orthographic classification features and in such combination, it exceeds the current state of the art by a wide margin.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Závislostní parsery jsou obvykle vyhodnocovány pomocí podílu správně zavěšených uzlů, toto skóre ale neříká nic o užitečnosti daného parseru pro koncové aplikace, v nichž slouží jako komponenta. V této kapitole se zabývá tím, jak rozdílné parsery a jejich různé kombinace přispívají ke kvalitě strojového překladu založeného na syntaxi. Překládáme výsledky pro několik základních typů parserů a pro různé metody jejich kombinace. Ukazujeme korelace se standardními metrikami pro vyhodnocování kvality strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Dependency parsers are almost ubiquitously evaluated on their accuracy
scores, these scores say nothing of the complexity and usefulness of the
resulting structures. As dependency parses are basic structures in which
other systems are built upon, it would seem more reasonable to judge
these parsers down the NLP pipeline. In this chapter, we will discuss
how different forms and different hybrid combinations of dependency
parses effect the overall output of Syntax-Based machine translation both
through automatic and manual evaluation. We show results from a variety
of individual parsers, including dependency and constituent parsers, and
describe multiple ensemble parsing techniques with their overall effect
on the Machine Translation system. We show that parsers’ UAS scores
are more correlated to the NIST evaluation metric than to the BLEU
Metric, however we see increases in both metrics. To truly see the effect
of hybrid dependency parsers on machine translation, we will describe
and evaluate a combined resource we have released, that contains gold
standard dependency trees along with gold standard translations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme SubGram, rozšíření Skip-gram modelu, které používá podřetězce slov během trénování reprezentace slov.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Skip-gram (word2vec) is a recent method for creating vector representations of words (“distributed word representations”) using a neural network. The representation gained popularity in various areas of natural language processing, because it seems to capture syntactic and semantic information about words without any explicit supervision in this respect.
We propose SubGram, a refinement of the Skip-gram model to consider also the word structure during the training process, achieving large gains on the Skip-gram original test set.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nové anotační schéma závislostní syntaxe pro ruštinu, založené na formalismu Universal Dependencies a aplikované na korpus SynTagRus.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a new annotation schema of dependency syntax for Russian. The schema is based on the Universal Dependencies formalism. It has been applied to the SynTagRus corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku popisujeme náš postup aplikovaný v benchmarku TRECVID 2015 v úlohe Video Hyperlinking. Náš prístup kombinuje podobnosť textov vypočítanú z titulkov, vizuálnu podobnosť medzi zábermi vypočítanú pomocou Feature Signatures a informáciou o tom, či query segment a vyhľadaný segment pochádzajú z toho istého televízneho programu. Všetky experimenty boli odladené a otestované na kolekcii 2500 hodín televíznych programov poskytnutých BBC.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present our approach used in the TRECVID 2015 Video Hyperlinking Task. Our approach combines text-based similarity calculated on subtitles, visual similarity between keyframes calculated using Feature Signatures, and preference whether the query
and retrieved answer come from the same TV series. All experiments were tuned and tested on about 2500 hours of BBC TV programmes.
Our Baseline run exploits fixed-length segmentation, text-based retrieval of subtitles, and query expansion which utilizes metadata, context, in-formation about music and artist contained in the query segment and visual concepts. The Series run combines the Baseline run with weighting based on information whether the query and data segment come from the same TV series. The FS run combines the Baseline run with the similarity between query and data keyframes calculated using Feature Signatures. The FSSeriesRerank run is based on the FS run on which we applied reranking which, again, uses information about the TV series. The Series run significantly outperforms the FSSeriesRerank run. Both these runs are significantly inferior to our Baseline run in terms of all our reported measures. The FS run outperforms the Baseline run in terms of all measures but it is significantly better than the Baseline run only in terms of the MAP score. Our test results confirm that employment of visual similarity can improve video retrieval based on information contained in subtitles but information about TV series which was most helpful in our training experiments did not lead to further improvements.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článok popisuje systém SHAMUS určený na jednoduché vyhľadávanie a navigáciu v multimediálnych archívoch. Systém pozostáva z troch komponentov. Search poskytuje textové vyhľadávanie v mutlimediálnej kolekcii, Anchoring automaticky detekuje najvýznamnejšie segmenty videa a sémanticky súvisiace segmenty sú vyhľadané v komponente Hyperlinking. V článku popisujeme jednotlivé komponenty systému, ako aj online demo rozhranie, ktoré pracuje s kolekciou videí z konferencie TED.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we describe SHAMUS, our system for an easy search and navigation in multimedia archives. The system consists of three components. The Search component provides a text-based search in a multimedia collection, the Anchoring component determines the most important segments of videos, and segments topically related to the anchoring ones are retrieved by the Hyperlinking component. In the paper, we describe each component of the system as well as the online demo interface which currently works with a collection of TED talks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje práci na strojovém překladu provedenou v rámci projektu KConnect, financovaném v rámci programu H2020 a zaměřeném na vývoj a komercializaci cloudových služeb pro vícejazyčnou sémantickou anotaci, sémantické vyhledávání a strojový překlad elektronických zdravotních záznamů a medicínských publikací. Nejprve prezentujeme hlavní cíl a úlohu strojového překladu v projektu, následně stručně popisujeme hlavní metody a komponenty vyvinuté v rámci projektu, mj. získání dat, metody doménové adaptace, nasazení MT jako cloudové webové služby a nástroj pro trénování překladových systémů, který umožňuje snadnou adaptaci překladové služby na nové jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the work on Machine Translation (MT) that has been conducted within the KConnect project, funded under the H2020 programme and focused on development and commercialization of cloud-based services for multilingual Semantic Annotation, Semantic Search and Machine Translation of electronic health records and medical publications. We first present the main goal and role of MT in the project and then briefly describe the main methods and components developed in the project, including training data acquisition, methods of domain adaptation, deployment of MT as cloud-based web-service, and the training toolkit allowing easy adaptation of the MT service to new languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Na základě objednávky komerčního partnera ACREA CR byl implementován software pro binární klasifikaci dokumentů v ruštině na dokumenty relevantní nebo nerelevantní vůči tématu zadanému příklady relevantních dokumentů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Given the specification of ACREA CR, a commercial partner of UFAL, a software for binary classification of documents written in the Russian language was implemented. New documents are classified as relevant or non-relevant with respect to a given topic, while the topic is specified using a training dataset of documents that are relevant for the topic.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>The aim of our talk is to present results of quantitative analysis of dependency characteristics of particular analytical functions. For each word in a syntactically annotated corpus (Bejček et al. 2013), a dependency frame is derived first. The dependency frame consists of all analytical functions assigned to its directly dependent words.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Cílem prezentace je kvantitativní analýza syntaktických závislostí v češtině. Pro každé slovo syntakticky anotovaného korpusu (Bejček et al. 2013) odvozujeme závislostní rámec , který je tvořen analytickými funkcemi přiřazenými každému bezprostředně závislému slovu.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem je podat snadno použitelné webové aplikace speciálně vyvinuté pro anotaci ruských textů s morfologickou a syntaktickou informaci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of this poster is to present an easy-to-use web application specifically developed for annotating Russian texts with morphological and syntactic information.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato zpráva představuje ruský korpus anotovaný podle standardu Universal Dependencies a popisuje postup, kterým byl existující ruský závislostní korpus SynTagRus transformován do stylu UD.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This report presents the Universal Dependencies (UD) annotated corpus for Russian and a conversion process which was developed to transform the SynTagRus dependency treebank of Russion into a UD-style annotated corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zaměřuje na generování pádových příznaků ve strojovém překladu do jazyků s volným slovosledem. Navrhujeme několik pravidel identifikujících vhodná místa pro takové příznaky při překladu z angličtiny do urdštiny. Výsledky ukazují zlepšení až 1 bod BLEU.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper focuses on the generation of case markers for free word order languages that use case markers as phrasal clitics for marking the relationship between the dependent- noun and its head. The generation of such clitics becomes essential task especially when translating from fixed word order languages where syntactic relations are identified by the positions of the dependent-nouns. To address the problem of missing markers on source-side, artificial markers are added in source to improve alignments with its target counterparts. Up to 1 BLEU point increase is observed over the baseline on different test sets for English-to-Urdu.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek shrnuje výsledky soutěže v optimalizaci parametrů pevně daného překladového systému. V letošní variantě soutěže jde o systém opírající se o velká trénovací data.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the
WMT16 Tuning Shared Task. We provided
the participants of this task with a
complete machine translation system and
asked them to tune its internal parameters
(feature weights). The tuned systems were
used to translate the test set and the outputs
were manually ranked for translation
quality. We received 4 submissions in the
Czech-English and 8 in the English-Czech
translation direction. In addition, we ran
2 baseline setups, tuning the parameters
with standard optimizers for BLEU score.
In contrast to previous years, the tuned
systems in 2016 rely on large data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Hlavní témata přednášky: (1) užití orální historie jako zdroje a tématu výzkumu; (2) rozmanité bariéry v tomto kontextu (epistemologické, metodologické, technické); (3) možnosti a budoucí příležitosti, nové výzkumné směřování.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Key themes of the talk were: (1) using oral history as topic and resource; (2) different barriers in doing so (epistemological, methodological, technical); (3) imagined possibilities, future opportunities and 
new research directions. Some of the current barriers seem to result from the fragmentary nature of social/human sciences, which is hard to overcome. At the same time, facilitating the use of ICT provides enviroment for interdisciplinary cooperation, which often leads to interesting and ground-breaking research.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kapitola popisuje Archiv vizuální historie USC Shoah Foundation, jeho obsah relevantní s ohledem na tematické zaměření knihy, a některé metodologické souvislosti sekundární analýzy orálněhistorických nahrávek.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The chapter describes USC Shoah Foundation's Visual History Archive, its relevant content for the thematic focus of the book, and some methodological issues of secondary analysis of oral history recordings.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V práci popisujeme syntaktický přístup zpracovaní asociační anafory. Představujeme nově vytvořený korpus RuGenBridge  s anotací tzv. genitivní asociační anafory v ruštině. Srovnáváme výsledky anotace se statistyky a typy anotovaných vztahů v PDT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present a syntactic approach to the annotation of bridging relations, so-called genitive bridging. We introduce the RuGenBridge corpus for Russian annotated with genitive bridging and compare it to the semantic approach that was applied in the Prague Dependency Treebank for Czech. We discuss some special aspects of bridging resolution for Russian and specifics of bridging annotation for languages where definite nominal groups are not as frequent as e.g. in Romance and Germanic languages. To verify the consistency of our method, we carry out two
comparative experiments: the annotation of a small portion of our corpus with bridging relations according to both approaches and finding for all relations from the RuGenBridge their semantic interpretation that would be annotated for Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jedna z našich čtyř anglických reprezentací (DM) a kompletní česká data (v reprezentaci PSD) nejsou odvozené od anotací licencovaných LDC, a tudíž mohou být poskytnuta k přímému stažení (Open SDP; verze 1.1; duben 2016) pod svobodnější licencí, konkrétně Creative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA). Tento balíček také obsahuje některé bohatší reprezentace významu, ze kterých jsou anglické bilexikální DM grafy odvozeny, totiž logické formy a abstraktnější nelexikalizované sémantické sítě. Ty jsou formálně (když už ne lingvisticky) podobné abstraktní reprezentaci významu (AMR) a jsou k dispozici v různých formátech, včetně syntaxe podobné AMR.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>One of the four English target representations (viz. DM) and the entire Czech data (in the PSD target representation) are not derivative of LDC-licensed annotations and, thus, can be made available for direct download (Open SDP; version 1.1; April 2016) under a more permissive licensing scheme, viz. the Creative Common Attribution-NonCommercial-ShareAlike scheme (CC BY-NC-SA 2.0). This package also includes some ‘richer’ meaning representations from which the English bi-lexical DM graphs derive, viz. scope-underspecified logical forms and more abstract, non-lexicalized ‘semantic networks’. The latter of these are formally (if not linguistically) similar to Abstract Meaning Representation (AMR) and are available in a range of serializations, including in AMR-like syntax.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>SDP 2014 &amp; 2015: Sémantická závislostní analýza se širokým pokrytím sestává z dat, nástrojů, systémových výstupů a publikací spojených se soutěžemi v sémantické závislostní analýze (SDP) v letech 2014 a 2015, které byly pořádány spolu s mezinárodním workshopem SemEval. Tento balíček dat a softwarových nástrojů byl připraven organizátory soutěží SDP.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>SDP 2014 &amp; 2015: Broad Coverage Semantic Dependency Parsing consists of data, tools, system results, and publications associated with the 2014 and 2015 tasks on Broad-Coverage Semantic Dependency Parsing (SDP) conducted in conjunction with the International Workshop on Semantic Evaluation (SemEval) and was developed by the SDP task organizers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek zkoumá podobnost aplikace atributových gramatik ve dvou zdánlivě odlišných vědeckých oblastech, jmenovitě ve formálním popisu pracovních postupů a v kontrole syntaktické správnosti přirozených jazyků. Jsou použity existující modely a formalismy a hledá se společný jmenovatel, který by umožnil využít zkušenosti nabyté v obou oblastech. Zároveň také ukazuje nutnost mírné adaptace formalismu pro kontrolu gramatiky pro jazyky s vysokým stupněm volnosti slovosledu, která by mohla vést k vytvoření nástroje použitelného pro kontrolu pracovních postupů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper investigates the similarities in the application of attribute grammars to two seemingly different research areas, namely the area of formal description of workflows and the area of checking the syntactic correctness
of natural languages. It uses existing models
and formalisms and tries to find a common ground
which would enable to exploit mutually the experience gained in both individual fields. It shows how a slight adaptation of a grammar formalism used for grammar checking of languages with a high degree of word-order freedom may lead to a tool useful for a workflow verification.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nedávné výsledky ukázaly, že vhodným nástrojem popisu procesních toků jsou atributové gramatiky. Ty v minulosti posloužily také jako vhodný prostředek implementace kontroly gramatické správnosti přirozených jazyků. Článek popisuje způsob, jak by bylo možné validovat, zda daný procesní tok je správně sestaven podobně jako se to dělá pro věty přirozeného jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Grammar checking has been used for a long time to validate if a given sentence - a sequence of words - complies with grammar rules. Recently attribute grammars have been proposed as a formal model to describe workflows. A workflow specifies valid processes, which are sequences of actions. In this paper we show how a grammar checker developed for natural languages can be used to validate whether or not a given process complies with the workflow model expressed using an attribute grammar. The checker can also suggest possible corrections of the process to become a valid process.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve světě akademických federací identit je častým problémem neuvolňování atributů potřebných pro chod služeb.
Naše služba umožňuje sbírat statistiky o uvolňovaných atributech mezi různými poskytovateli služeb (SP) a poskytovateli identit (IDP).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the world of academic identity (inter-)federations, the release attribute problem is often mentioned. The problem is that Service Providers (SPs) do not get the attributes from Identity Providers (IdPs) they need. This service makes it possible to gather statistics about released attributes names between various SPs and IDPs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Služba pro snadnou a trvalou citaci dat vzešlých z výzkumu. Tuto službu poskytujeme formou zkracovače URL. Reprodukovatelnost výsledků, důležitá součást vědeckého výzkumu, přímo závisí na dostupnosti dat, nad kterými výzkum probíhal. Rozvoj webových technologií velmi usnadnil sdílení dat. Kvůli dynamické povaze webu se ale obsah často přesouvá z jednoho místa na druhé. Běžné URL, které může výzkumník použít k citaci svých dat, nemá prostředky, kterými by se s tímto přesouváním dalo vyrovnat. Čtenář/uživatel tak často zjistí, že data se na dané URL již nenalézají, nebo dostane k dispozici novější verzi. Námi navrhované řešení, ve kterém je zkrácené URL perzistentním identifikátorem, poskytuje spolehlivý mechanismus pro stálou dostupnost dat a může tak zlepšit dopad výzkumu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Easy-to-cite and persistent infrastructure (shortref.org) for research and data citation in the form a URL shortener service. Reproducibility of results is very important for the extension of research and is directly depends on the availability of the research data. The advancements in the web technologies made redistribution of the data much more easy nowadays, however, due to the dynamic nature of the web, the content is consistently on the move from one destination to another. The URLs researchers use for the citation of their contents do not directly account for these changes and many times when the users try to access the cited URLs, the data is either not available or moved to a newer version. In our proposed solution, the shortened URLs are not simple URLs but use persistent identifiers and provide a reliable mechanism to make the data always accessible that can directly improve the impact of research.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CLARIN centrum sa môže nazývať CLARIN-B centrum iba ak spĺňa špecifické kritériá. Digitálny repozitár LINDAT/CLARIN postavený na DSpace spĺňa väčšinu týchto požiadaviek.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This talk presents the steps required for becoming a CLARIN-B centre using LINDAT/CLARIN digital library based on DSpace.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>ConFarm je webová služba, věnovaná extrakci povrchových reprezentaci slovesnych a jmennych tvaru ze závislostních anotovaných korpusů ruských textů. V současné době je  k dispozici extrakce konstrukci s konkrétním lemmatem z korpusu SynTagRus a ruského národního korpusu. Nas systém poskytuje flexibilní rozhraní, které umožňuje uživatelům vyladit výstup. Extrahované konstrukce jsou seskupeny podle jejich obsahu, aby byla možna kompaktní reprezentace, a skupiny jsou zobrazeny ve formě grafu. ConFarm se liší od podobných existujících nástrojů pro rustinu v tom, že nabízí kompletní konstrukce, v protikladu k samostatni extrakci zavislostnich clenu nebo práci s slovních spojení, a umožňuje uživatelům objevit nečekané konstrukce, na rozdíl od vyhledavani příkladu podle formy zadane uživatelem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>ConFarm is a web service dedicated to extraction of surface representations of verb and noun constructions from dependency annotated corpora of Russian texts. Currently, the extraction of constructions with a specific lemma from SynTagRus and Russian National Corpus is available. The system provides flexible interface that allows users to fine-tune the output. Extracted constructions are grouped by their contents to allow for compact representation, and the groups are visualized as a graph in order to help navigating the extraction results. ConFarm differs from similar existing tools for Russian language in that it offers full constructions, as opposed to extracting separate dependents of search word or working with collocations, and allows users to discover unexpected constructions as opposed to searching for examples of a user-defined construction.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce se zabývá otázkou budování svobodného NLP potrubí pro zpracování ruské texty z prostého textu na morfologicky a syntakticky anotovaný struktury ve formátu CONLL. Potrubí je napsán v python3. Segmentace je zajišťována vlastní modul. Mystem s četnými postprocesních oprav se používá pro lemmatizace a morfologie značkování.
A konečně, syntaktická anotace se získá MaltParser využitím naší vlastní model vyškolený na SynTagRus, který byl převeden do formátu CONLL pro tento účel, s jeho morfologické tagset převádí do Mystem / ruského národního korpusu tagset</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This work addresses the issue of building a free NLP pipeline for processing Russian texts from plain text to morphologically and syntactically annotated structures in CONLL format. The pipeline is written in python3. Segmentation is provided by our own module. Mystem with numerous postprocessing fixes is used for lemmatization and morphology tagging.
Finally, syntactical annotation is obtained with MaltParser utilizing our own model trained on SynTagRus, which was converted into CONLL format for this purpose, with its morphological tagset being converted into Mystem/Russian National Corpus tagset</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Lexikální síť DeriNet verze 1.2 obsahuje 1.003.590 lexémů (z morfologického slovníku MorfFlex), mezi nimiž bylo poloautomatickými metodami vytvořeno 740.750 hran odpovídajících vztahu odvození (mezi slovem odvozeným a slovem základovým). V této verzi dat jsou nově se základovým slovem spojena i slova, při jejichž odvozování dochází k hláskovým alternacím (př. boží - bůh).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Lexical network DeriNet version 1.2 contains 1,003,590 lexemes (from the morphological dictionary MorfFlex) among which 740.750 edges were created according to derivational relations (between a derived word and its base word). In the current version, words containg consonant and/or vowel alternations were connected to their base word.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá klasifikací českých sloves s genitivním doplněním z hlediska tvoření pasivní a deagentní diateze. Slovesa jsou klasifikována na ta, která se chovají podobně jako slovesa s akuzativním doplněním,,
slovesa, u nichž je genitivní vazba v příznakových členech diateze zachována, a
slovesa, jejichž genitivní doplnění může být v diatezi vyjádřeno nominativem i genitivem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This contribution dealt with passive and deagentive diatheses of Czech verbs with genitive complementation. According to their changes in valency structure, these verbs are classified into three groups: (1) verbs that behave in the same way as verbs with accusative complementation, (2) verbs that preserve genitive complementations and (3) verbs complementations of which can be expressed in the nominative, or in the genitive.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje konverzi katalánského a španělského treebanku AnCora do formalismu Universal Dependencies (univerzální závislosti). Popisujeme proces konverze a odhadujeme kvalitu výsledného treebanku nepřímo pomocí automatické syntaktické analýzy v jednojazykovém, mezijazykovém a mezidoménovém testu. Převedené treebanky vykazují interní konzistenci srovnatelnou s původní distribucí AnCora pro CoNLL09. Od jiného, dříve vydaného španělského UD treebanku se liší zejména v repertoáru vyznačených víceslovných výrazů. Tyto dva nově převedené treebanky budou vydány v Universal Dependencies verzi 1.3.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article describes the conversion of the Catalan and Spanish AnCora treebanks to the Universal Dependencies formalism. We describe the conversion process and assess the quality of the resulting treebank in terms of parsing accuracy by means of monolingual, cross-lingual and cross-domain parsing evaluation. The converted treebanks show an internal consistency comparable to the one shown by the original CoNLL09 distribution of AnCora, and indicate some differences in terms of multiword expression inventory with regards to the already existing UD Spanish treebank. The two new converted treebanks will be released in version 1.3 of Universal Dependencies.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Provedli jsme přehled VV v existujících treebancích pomocí online průzkumu. Výsledky ukazují významné odlišnosti. Srovnání se zaměřuje na anotaci analytických predikátů a slovesných idiomů.
Na základě průzkumu navrhujeme všeobecné směrnice pro anotaci VV v treebancích. Doporučení se týkají následujících potřeb: rozlišovat VV od podobných, avšak kompozicionálních konstrukcí; vyhledávat rozličné druhy VV v treebanku; rozlišení doslovného a přeneseného významu a normalizace reprezentace VV. Průzkum vedený napříč jazyky a teoriemi je míněn jako pomůcka pro zpracování a další práci s anotovanými treebanky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>By means of an online survey, we have investigated ways in which various types of multiword expressions are annotated in existing treebanks. The results indicate that there is considerable variation in treatments across treebanks and thereby also, to some extent, across languages and across theoretical frameworks. The comparison is focused on the annotation of light verb constructions and verbal idioms. The survey shows that the light verb constructions either get special annotations as such, or are treated as ordinary verbs, while VP idioms are handled through different strategies. Based on insights from our investigation, we propose some general guidelines for annotating multiword expressions in treebanks. The recommendations address the following application-based needs: distinguishing MWEs from similar but compositional constructions; searching distinct types of MWEs in treebanks; awareness of literal and nonliteral meanings; and normalization of the MWE representation. The cross-lingually and cross-theoretically focused survey is intended as an aid to accessing treebanks and an aid for further work in treebank annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Práce se zabývá automatickou kontrolou pravopisu arabštiny a ukazuje, jak vylepšení jednotlivých komponent (slovníku, jazykového modelu, chybového modelu) vede ke kumulativnímu zlepšení celého systému.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A spelling error detection and correction application is typically based on three main components: a dictionary (or reference word list), an error model and a language model. While most of the attention in the literature has been directed to the language model, we show how improvements in any of the three components can lead to significant cumulative improvements in the overall performance of the system. We develop our dictionary of 9.2 million fully-inflected Arabic words (types) from a morphological transducer and a large corpus, validated and manually revised. We improve the error model by analyzing error types and creating an edit distance re-ranker. We also improve the language model by analyzing the level of noise in different data sources and selecting an optimal subset to train the system on. Testing and evaluation experiments show that our system significantly outperforms Microsoft Word 2013, OpenOffice Ayaspell 3.4 and Google Docs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kontrastivní popis “svůj” v češtině a “svoj” v ruštině. Korpusová data ve třijazyčném paralelním korpusu jsou analyzována se zřetelem k existujícím popisům reflexiv v každém jazyce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents the contrastive description of reflexive possessive pronouns “svůj” in Czech and “svoj” in Russian. The research concerns syntactic, semantic and pragmatic aspects. With our analysis, we shed a new light on the already investigated issue, which comes from the detailed comparison of the phenomenon of possessive reflexivization in two typologically and genetically similar languages. We show that whereas in Czech, the possessive reflexivization is mostly limited to syntactic functions and does not go beyond the grammar, in Russian it gets additional semantic meanings and moves substantially towards the lexicon. The obtained knowledge lets us explain heretofore unclear marginal uses of
reflexives in each language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáska je zaměřena na anotační nástroj, které se používají pro zaznamenávání diskurzních jevů (TrEd, PDTB, Brat, PDTB-tool, MMAX, etc.)</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This session focuses on discussing tools applied to the annotation of discourse phenomena in different languages. Following the general session on “Annotation theories and tools”, trainees will be invited to use several annotation tools, such as  TrEd, PDTB, Brat, PDTB-tool and MMAX.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Shrnutí čeho jsme dosáhli v rozborech, anotacích a analýze češtiny (PDT) a němčiny v rámci korpusu GECCo.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the talk, I summariye common and different features in the annotation approaches to Czech (PDT-like) and German (applied in GECCo).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V rámci přednásky byly představeny existující teorie anotace diskurních jevů a nástroje, které se pro tento typ anotací používají.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Discourse coherence is a complex natural language phenomenon which is achieved by different linguistic means (e.g., anaphoricity, information structure, discourse markers and connectives, rhetorical structure of text, etc.). Many approaches in computational linguistics are used to capture discourse relations and Tind practical applications. This session focuses on discussing theories and approaches applied to the annotation of discourse phenomena in different languages, such as Rhetorical Structure Theory, Penn Discourse Treebank, Segmented Discourse Representation Theory and so on. Participants will have the opportunity to compare these approaches to see which phenomena are central to them and which ones are less prominent. We will also introduce the tools of discourse annotation and demonstrate esp. TrEd, PDTB and MMAX.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>K zajímavým lingvistickým poznatkům lze dojít různými cestami: analýzou korpusových dat, srovnáním typologicky blízkých a vzdálenějších jazyků mezi sebou, použitím dotazníků nebo podnětů z odborné literatury. V plánovaném příspěvku chceme kombinací těchto přístupů  dospět  k důkladnějšímu popisu chování posesívních reflexivních zájmen v češtině a ruštině.

Rozbor anglo-česko-ruského paralelního korpusu (PCEDT-R) poukázal na značné statistické rozdíly v použití posesiv v těchto třech jazycích. Zatímco v angličtině se často používají ve funkci determinátoru při jmenné frázi, v češtině a ruštině je jejich frekvence výrazně nižší. Avšak i mezi češtinou a ruštinou se najdou značné – jak frekvenční tak funkční – rozdíly. Chování osobních a reflexivních přivlastňovacích zájmen a jejich konkurence byly dosud popisovány pro každý jazyk zvlášť. Pro češtinu jsou normativní pravidla použití reflexivního posesiva „svůj“ formulována již v Trávníčkově mluvnici (Trávníček, 1951). Pravidlům jejich skutečné distribuce v současné češtině se podrobně věnovaly např. práce F. Daneše a K. Hausenblase (1962), J. Panevové (1986) a S. Čmejrkové (1998, 2011). Konkurenci „svoj“ a „jego“ v ruštině se věnovala hlavně E. Padučevová (1983, 1985), která nabídla formálně syntaktická, sémantická a pragmatická pravidla distribuce těchto zájmen. Každá z těchto prací se však soustředí na analýzu tohoto jevu v jednom jazyce. Zde se zaměříme na porovnání výsledků, k nimž jmenovaní autoři došli a budeme hledat inspiraci pro lepší pochopení použití těchto zájmen v češtině a ruštině zvlášť.

Na první pohled se zdá, že se pravidla distribuce osobních posesiv a „svůj“ se v ruštině a češtině do značné míry shodují. Avšak dotazníky vytvořené na základě použité literatury tuto podobnost neprokazují. Zjišťujeme například, že Padučevovou navržený distributivní význam reflexivního posesiva, který je pro ruštinu úplně běžný, je v češtině zcela marginální (U každogo učenogho jesť svoja biblioteka – Každý vědec má *svou/vlastní knihovnu). Podobné je to u významu, který je nazýván 'ovladání'. Ten je pro užití ruského „svoj“ natolik produktivní, že se již vymyká pojetí frazému (Svoja kvartira lučše čem sjemnaja - *Svůj (=vlastní) byt je lepší než pronajatý.) Ruské „svoj“ má také řadu sekundárních významů, které v češtině chybí.

Analýza paralelních korpusových dat (Intercorp, PCEDT-R) ukazuje, že česká posesiva se dají častěji vynechat, jejich výskyt je tedy často fakultativní, tím se čeština od ruštiny liší. Zjistili jsme to na textech, které byly přeloženy z angličtiny, ale ověřili jsme to statisticky i na originálních textech přiměřené velikosti a tematiky (z PDT a RTB).

V referátu sjednotíme teoretické poznatky s výsledky korpusové analýzy a vymezíme společné a odlišné rysy použití osobních a reflexivních přivlastňovacích zájmen v češtině a ruštině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Comparative analysis of personal and possessive reflexives in Czech and Russian. Analysis of secondary literature.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme anotaci koreference na paralelních česko-anglických textech Pražského česko-anglického závislostního treebanku (PCEDT).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present coreference annotation on parallel Czech-English texts of the Prague Czech-English Dependency Treebank (PCEDT). The paper describes innovations made to PCEDT 2.0 concerning coreference, as well as coreference information already present there. We characterize the coreference annotation scheme, give the statistics and compare our annotation with the coreference annotation in Ontonotes and Prague Dependency Treebank for Czech. We also present the experiments made using this corpus to improve the alignment of coreferential expressions, which helps us to collect better statistics of correspondences between types of coreferential relations in Czech and English. The corpus released as PCEDT 2.0 Coref is publicly available.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vydání Pražského česko-anglického závislostního treebanku 2.0 (PCEDT), obsahující všechny anotace koreference (původní z PCEDT 2.0 i nové) a vylepšené mezijazyčné zarovnání koreferenčních výrazǔ.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an extended version of the Prague Czech-English Dependency Treebank 2.0 (PCEDT 2.0). It includes all annotation of coreference (the original one from PCEDT 2.0 as well as the new one) and improved cross-lingual alignment of coreferential expressions. The corpus released as PCEDT 2.0 Coref is publicly available.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Srovnávácí analýzy koreferenčních výrazů poukazujících k abstraktním entitám a propozicím v češtině a němčině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper aims at a cross-lingual analysis of coreference to abstract entities in Czech and German, two languages that are typologically not very close, since they belong to two
different language groups – Slavic and Germanic. We will specifically focus on coreference chains to abstract entities, i.e. verbal phrases, clauses, sentences or even longer text
passages. To our knowledge, this type of relation is underinvestigated in the current stateof-the-art literature.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V clanku venujeme koreferencnim retezcum v ceskych a nemeckych textech. Za zaklad bereme jiz anotovane originalni texty v techto jazycich, aplikujeme spolecne prunikove schema a analyzujeme vysledky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we analyse coreference patterns in Czech and German. We specifically
focus on different types of coreference chains and their properties,
for instance, their length, number and functional subtypes of the elements
inside these chains. We use two datasets annotated within different annotation
frameworks, showing that this approach is possible if an interoperable
analysis scheme is applied.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Korpusová analýza použití posesivních zájmen v nově vytvořeném anglicko-česko-ruském paralelním korpusu PCEDT-R. V článku představujeme korpus, popisujeme jeho anotaci a prezentujeme statistiky na něm spočtené.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a corpus-based analysis of the use of possessive and reflexive possessive pronouns in a newly created English-Czech-Russian parallel corpus (PCEDT-R). Automatic word-alignment was applied to the texts, which were subsequently manually corrected. In the word-aligned data, we have manually annotated all correspondences of possessive and possessive reflexive pronouns from the perspective of each analysed language. The collected statistics and the analysis of the annotated data allowed us to formulate assumptions about language differences. Our data confirm the relative frequency of possessive pronouns in English as compared to Czech and Russian, and we explain it by the category of definiteness in English. To confirm some of our hypotheses, we used other corpora and questionnaires. We compared the translated texts in Czech and Russian from our corpus to the original texts from other corpora, in order to find out to what degree the translation factor might influence the frequency of possessives.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Titul předkládané publikace napovídá, že navazuje na Encyklopedický slovník češtiny (2002) a cíle, které si kladl. Rozsahem, obsahem a formou jednotlivých hesel i slovníku jako celku jej však přesahuje.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The title of the present publication suggests that it follows up the Encyclopedic Dictionary of Czech (2002) and its goals. However, it exceeds the previous dictionary in the extent, contents and form of the individual entries as well as the dictionary as a whole.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek se zabývá metodou překladu vyhledávacích dotazů, která přeuspořádávává překladové hypotézy produkované překladovým systémem, a to s ohledem na kvalitu výsledného vyhledávání. V článku je prezentován způsob adaptace této metody na nové zdrojové jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We investigate adaptation of a supervised  machine learning model for reranking of query translations to new languages in the context of cross-lingual information retrieval. The model is trained to rerank multiple translations produced by a statistical machine translation system and optimize retrieval quality. The model features do not depend on the source language and thus allow the model to be trained on query translations coming from multiple languages. In this paper, we explore how this affects the final retrieval quality. The experiments are conducted on medical-domain test collection in English and multilingual queries (in Czech, German, French) from the CLEF eHealth Lab series 2013--2015.
We adapt our method to allow reranking of query translations for four new languages (Spanish, Hungarian, Polish, Swedish). The baseline approach, where a single model is trained for each source language on query translations from that language, is compared with a model co-trained on translations from the three original languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Strojový překlad dotazů pro vícejazyčné vyhledávání informací je obvykle cílen na maximální překladovou kvalitu. To ovšem nemusí být optimální s ohledem na kvalitu vyhledávání a jiné překladové hypotézy mohou vést k lepším výsledkům vyhledávání. V tomto článku zkoumáme metodu využívající více možných překladů, které jsou přeuspořádávány pomocí metody založené na strojovém učení, která je optimalizována přímo na kvalitu překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Machine Translation (MT) systems employed to translate queries for Cross-Lingual Information Retrieval typically produce single translation with maximum translation quality. This, however, might not be optimal with respect to retrieval quality and other translation variants might lead to better retrieval results. In this paper, we explore a method exploiting multiple translations produced by an MT system, which are reranked using a supervised machine-learning method trained to directly optimize the retrieval quality. We experiment with various types of features and the results obtained on the medical-domain test collection from the CLEF eHealth Lab series show significant improvement of retrieval quality compared to a system using single translation provided by MT.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zpráva o účasti týmu Univerzity Karlovy v soutěži vyhledávání zdravotních informací CLEF eHealth Evaluation Lab 2016.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we present our participation as the team of
the Charles University at Task3 Patient-Centred Information Retrieval. In the monolingual task and its subtasks, we submitted two runs: one is based on language model approach and the second one is based on vector space model. For the multilingual task, Khresmoi translator, a Statistical Machine Translation (SMT) system, is used to translate the queries into English and get the n-best-list. For the baseline system, we take 1-best-list translation and use it for the retrieval, while for other runs, we use a machine learning model to rerank the n-best-list translations and predict the translation that gives the best CLIR performance in terms of P@10. We present set of features to train the model, these features are generated from the SMT verbose output, different resources like UMLS Metathesaurus, MetaMap, document collection and from the Wikipedia
articles. Experiments on previous CLEF eHealth IR tasks test set show significant improvement brought by the reranker over the baseline system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vystadial 2016 je databáze telefonních hovorů v češtině, vyvinuté pro trénování akustických modelů pro automatické rozpoznávání řeči v dialogových systémech. Data obsahují více než 75 hodin v českém jazyce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Vystadial 2016 is a dataset of telephone conversations in Czech, developed for training acoustic models for automatic speech recognition in spoken dialogue systems. The data comprise over over 75 hours in Czech, plus orthographic transcriptions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek se zabývá modely pro sledování stavu dialogu pomocí rekurentních neuronových sítí (RNN). Představujeme pokusy na datové sadě, DSTC2. Na jedné straně, RNN modely dosahují vynikajících výsledků. Na druhou stranu většina state-of-the-art modelů jsou "turn-based" a vyžadují specifické předzpracování (např pro data z DSTC2) k dosažení vynikajících výsledků. Představili jsme dvě architektury, které mohou být použity v inkrementálních nastavení a nevyžadují téměř žádnou předzpracování. V članku porovnáváme jejich výkonnost na referenčních hodnotách pro DSTC2 a diskutujeme jejich vlastnosti. S pouze triviální předzpracováním se výkon našich modelů blíží k výsledkům state-of-the-art.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper discusses models for dialogue state tracking using recurrent neural networks (RNN). We present experiments on the standard dialogue state tracking (DST) dataset, DSTC2. On the one hand, RNN models became the state of the art models in DST, on the other hand, most state-of-the-art models are only turn-based and require dataset-specific preprocessing (e.g. DSTC2-specific) in order to achieve such results. We implemented two architectures which can be used in incremental settings and require almost no preprocessing. We compare their performance to the benchmarks on DSTC2 and discuss their properties. With only trivial preprocessing, the performance of our models is close to the state-of- the-art results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek prezentuje novou datovou sadu pro výcvik end-to-end úkol orientovaně konverzační agentů. Obsahuje rozhovory mezi operátorem - odborníkem na danou doménu, a klientem, který hledá informace o úloze. Spolu s konverzační přepisy zaznamenáme databázová volání prováděné operátorem, které zachycují význam dotazu uživatele. Očekáváme, že se snadno získatelné databázová volání nám umožní trénovat end-to-end dialog agenty se s výrazně méně tréninkových dat. Datová sada je sbírána pomocí crowdsourcing a rozhovory pokrývají dobře známé restaurace doménu. Kvalita dat je vynucováno vzájemné kontroly mezi přispěvateli. Datový soubor je k dispozici ke stažení pod licencí Creative Commons 4.0 BY-SA licencí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a novel dataset for training end-to-end task oriented conversational agents. The dataset contains conversations between an operator – a task expert, and a client who seeks information about the task. Along with the conversation transcriptions, we record database API calls performed by the operator, which capture a distilled meaning of the user query. We expect that the easy-to-get supervision of database calls will allow us to train end-to-end dialogue agents with significantly less training data. The dataset is collected using crowdsourcing and the conversations cover the well-known restaurant domain. Quality of the data is enforced by mutual control among contributors. The dataset is available for download under the Creative Commons 4.0 BY-SA license.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Navrhujeme dva příspěvky k diskriminativnímu výběru pravidel v hierarchickém strojovém překladu. Ověřujeme předchozí přístupy na dvou úlohách francouzsko-anglického překladu na doménách s omezenými zdroji a ukazujeme, že nedokážou zlepšit překladovou kvalitu. Navrhujeme model pro výběr pravidel, který je (i) globální a využívá bohatou sadu rysů a (ii) je trénován s využitím všech dostupných negativních příkladů. Náš globální model přináší významné zlepšení až 1 bod BLEU oproti předchozím modelům pro výběr pravidel.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We propose two contributions to discriminative rule selection in hierarchical machine translation. First, we test  previous  approaches on two French-English translation tasks in domains for which only limited resources are  available  and  show  that  they  fail  to improve  translation  quality. To improve on  such  tasks,  we  propose  a  rule  selection  model  that  is  (i)  global  with  rich label-dependent  features  (ii)  trained  with all available negative samples.  Our global model yields significant improvements, up to 1  BLEU  point,  over  previously  proposed rule selection models.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení technik strojového překladu pro pracovníky státní správy a veřejných institucí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An introduction to machine translation for employees of governmental and other public institutions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přehled o metodách získávání paralelních dat a jejich přípravě pro trénování modelů strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An overview of method for acquiring parallel data and for their preparation for the purposes of MT training.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek shrnul deset let konání soutěže v automatickém hodnocení kvality překladu: představil úspěšné minulé metriky, současné experimenty a poukázal na potřebné další směry vývoje.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A summary of ten years of automatic MT evaluation, highlighting past successful metrics, current evaluation challenges and topics that need to be tackled in the future.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nové vydání česko-anglického paralelního korpusu CzEng. CzEng 1.6 obsahuje kolem půl miliardy slov v každém z jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A new release of the Czech-English parallel corpus CzEng. CzEng 1.6 consists of about 0.5 billion words (“gigaword”) in each language. The corpus is equipped with automatic annotation at a deep syntactic level of representation and alternatively in Universal Dependencies.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Lingvistická anotace angličtiny a češtiny použitá pro CzEng 1.6 je nyní k dispozici pro jakýkoliv text ve formě Docker kontejneru.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A pipeline of linguistic analysis used for CzEng 1.6 packaged in a Docker container. Any English or Czech text can be processed by this pipeline. The processing pipeline includes part-of-speech tagging, parsing, named entity recognition, semantic role labelling, coreference resolution etc.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nové vydání česko-anglického paralelního korpusu CzEng. CzEng 1.6 obsahuje kolem půl miliardy slov v každém z jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a new release of the Czech-English parallel corpus CzEng. CzEng 1.6 consists of about 0.5 billion words (“gigaword”) in each language. The corpus is equipped with automatic annotation at a deep syntactic level of representation and alternatively in Universal Dependencies. Additionally, we release the complete annotation pipeline as a virtual machine in the Docker virtualization toolkit.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje výsledky společných úloh WMT16, které zahrnovaly 5 úloh strojového překladu (novinové texty, IT doména, biomedicína, multimodální překlad a překlad zájmen), 3 úlohy vyhodnocovací (metriky, tuning, odhadu kvality překladu) a automatické posteditace a bilingvní párování dokumentů. Letos se WMT zůčastnilo 102 překladových systémů z 24 institucí (plus 36 anonymizovaných online systémů).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the
WMT16 shared tasks, which included five
machine translation (MT) tasks (standard
news, IT-domain, biomedical, multimodal,
pronoun), three evaluation tasks (metrics,
tuning, run-time estimation of MT quality),
and an automatic post-editing task
and bilingual document alignment task.
This year, 102 MT systems from 24 institutions
(plus 36 anonymized online systems)
were submitted to the 12 translation
directions in the news translation task. The
IT-domain task received 31 submissions
from 12 institutions in 7 directions and the
Biomedical task received 15 submissions
systems from 5 institutions. Evaluation
was both automatic and manual (relative
ranking and 100-point scale assessments).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek shrnuje výsledky soutěže v hodnocení kvality strojového překladu. Letos je soutěž rozšířena o několik novinek: větší počet jazykových párů, data z více domén, tři způsoby ručního vyhodnocení, jemuž se mají automatické metody v soutěži přiblížit.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the
WMT16 Metrics Shared Task. We asked
participants of this task to score the outputs
of the MT systems involved in the
WMT16 Shared Translation Task. We
collected scores of 16 metrics from 9 research
groups. In addition to that, we computed
scores of 9 standard metrics (BLEU,
SentBLEU, NIST, WER, PER, TER and
CDER) as baselines. The collected scores
were evaluated in terms of system-level
correlation (how well each metric’s scores
correlate with WMT16 official manual
ranking of systems) and in terms of segment
level correlation (how often a metric
agrees with humans in comparing two
translations of a particular sentence).
This year there are several additions to
the setup: large number of language pairs
(18 in total), datasets from different domains
(news, IT and medical), and different
kinds of judgments: relative ranking
(RR), direct assessment (DA) and HUME
manual semantic judgments. Finally, generation
of large number of hybrid systems
was trialed for provision of more conclusive
system-level metric rankings.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme naše systémy v překladové soutěži IWSLT 2016. Jedná se o naše první systémy pro překlad filmových titulků a současně o jedny z prvních experimentů s neuronovým překladem, které provádíme.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our submissions to the IWSLT 2016 machine
translation task, as our first attempt to translate subtitles and
one of our early experiments with neural machine translation
(NMT). We focus primarily on English→Czech translation
direction but perform also basic adaptation experiments for
NMT with German and also the reverse direction. Three MT
systems are tested: (1) our Chimera, a tight combination of
phrase-based MT and deep linguistic processing, (2) Neural
Monkey, our implementation of a NMT system in TensorFlow
and (3) Nematus, an established NMT system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek shrnuje deset ročníků soutěží ve strojovém překladu a ve vyhodnocování jeho kvality: WMT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The WMT evaluation campaign (http://www.statmt.org/wmt16) has been run annually since 2006. It is a collection of shared
tasks related to machine translation, in which researchers compare their techniques against those of others in the field. The longest
running task in the campaign is the translation task, where participants translate a common test set with their MT systems. In addition
to the translation task, we have also included shared tasks on evaluation: both on automatic metrics (since 2008), which compare the
reference to the MT system output, and on quality estimation (since 2012), where system output is evaluated without a reference. An
important component of WMT has always been the manual evaluation, wherein human annotators are used to produce the official ranking
of the systems in each translation task. This reflects the belief of theWMTorganizers that human judgement should be the ultimate arbiter
of MT quality. Over the years, we have experimented with different methods of improving the reliability, efficiency and discriminatory
power of these judgements. In this paper we report on our experiences in running this evaluation campaign, the current state of the art in
MT evaluation (both human and automatic), and our plans for future editions of WMT.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Soutěž ve strojovém překladu WMT používá od roku 2007 ruční hodnocení, kdy anotátor vidí najednou pět kandidátských překladů. Přes relativně dlouhou dobu používání nemáme dosud jasnou představu, co přesně anotátoři dělají. Tento článek představuje pilotní studii s osmi anotátory, jejichž oční pohyby jsme během hodnocení zaznamenávali na eyetrackeru a následně vyhodnotili.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The shared translation task of the Workshop of Statistical Machine Translation (WMT) is one of the key annual events of the field. Participating
machine translation systems in WMT translation task are manually evaluated by relatively ranking five candidate translations
of a given sentence. This style of evaluation has been used since 2007 with some discussion on interpreting the collected judgements but
virtually no insight into what the annotators are actually doing. The scoring task is relatively cognitively demanding and many scoring
strategies are possible, influencing the reliability of the final judgements. In this paper, we describe our first steps towards explaining the
scoring task: we run the scoring under an eye-tracker and monitor what the annotators do. At the current stage, our results are more of a
proof-of-concept, testing the feasibility of eye tracking for the analysis of such a complex MT evaluation setup.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace vítězného systému ze soutěže (Shared Task) WMT 2013-2015. Byla prezentována architektura, zásadní moduly a principy, a statistické komponenty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The TectoMT system is a result of long-term development which began in the pre-statistical era at Charles University in Prague and continued to include state-of-the-art tools for POS tagging, morphological feature disambiguation, lemmatization parsing, and some aspects of semantic analysis. It follows the usual Analysis – Transfer – Generation workflow, with transfer trained on a large parallel corpus using Hidden Markov Tree Model. Generation is partly rule-based (at the syntax level) and partly statistical (at the inflection/morphology level). Chimera is a hybrid system that uses a specific combination of TectoMT and a standard Phrase-based SMT (Moses), complemented by a “Depfix” automatic post-editing system, which as a whole improves on the individual systems, as documented in the results of the recent WMT Shared tasks. The system has been originally developed for English-Czech and recently transferred to several other languages within the EU QTLeap project (qtleap.eu), where it has been successfully used in the IT domain for both question and answer translation in a Q&amp;A context. Both the TectoMT and Chimera systems will be presented together with a discussion about language (in)dependence of such a hybrid solution.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku popisujeme CzEngVallex, bilingvní česko-anglický valenční slovník, zachycující propojení slovesné valence na základě překladových textů. Předkládáme zde základní statistiky, které podchycují mapování argumentů propojených sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe CzEngVallex, a bilingual Czech-English valency lexicon which aligns verbal valency frames and their arguments. It is based on a parallel Czech-English corpus, the Prague Czech-English Dependency Treebank, where for each occurrence of a verb a reference to the underlying Czech and English valency lexicons is explicitly recorded. CzEngVallex lexicon pairs the entries (verb senses) of these two lexicons, and allows for detailed studies of verb valency and argument structure in translation. While some related studies have already been published on certain phenomena, we concentrate here on basic statistics, showing that the variability of verb argument mapping between verbs in the two languages is richer than it might seem and than the perception from the studies published so far might have been.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Případová studie založená na zkušenosti autorů s lingvistickým výzkumem opírajícím se o anotovaný monolingvální I paralelní korpus. Příspěvek zahrnuje popis jevů patřících k různým jazykovým rovinám (morfologie, syntax povrchová I hloubková a diskurs). Podkladem je  Pražský závislostní korpus , soubor českých textů komplexně anotovaných na hloubkové syntaktické rovině včetně diskurzních vztahů a informační struktury věty. Autoři ukazují, že anotování (značkování) textových korpusů, pokud je založeno na spolehlivé lingvistické teorii,  není  účelem samo o sobě, ale že může především sloužit jako dobrý test pro danou teorii i pro jazykovědný výzkum obecně.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A case study based on experience in linguistic investigations using annotated monolingual and multilingual text corpora; the “cases” include a description of language phenomena belonging to different layers of the language system: morphology, surface and underlying syntax, and discourse; the analysis is based on a complex annotation of syntax, semantic functions, information structure and discourse relations of the Prague Dependency Treebank, a collection of annotated Czech texts. We want to demonstrate that annotation of corpus is not a self-contained goal: in order to be consistent, it should be based on some linguistic theory, and, at the same time, it should serve as a test bed for the given linguistic theory in particular and for linguistic research in general.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pro dva valenční slovníky slovanckých jazyků, PDT-Vallex pro češtinu a Walenty pro polštinu článek porovnává jejich frazeologickou část. Oba slovníky jsou založené na korpusu, i když se liší jak způsob propojení, tak technické řešení, ovšem oba jsou dostupné elektronicky ve standardnim formátu. V článku se porovnávají frazeologická hesla, jejich formální popis a možnosti a omezení. V závěru se doporučují rozšíření těchto komponent pro obecnější pokrytí i pro další jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Phraseological components of valency dictionaries for two West Slavic languages are presented, namely, of the PDT-Vallex dictionary for Czech and of the Walenty dictionary for Polish. Both dictionaries are corpus-based, albeit in different ways. Both are machine readable and employable by syntactic parsers and generators. The paper compares the expressive power of the phraseological subformalisms of these dictionaries, discusses  their limitations and makes recommendations for their possible extensions, which can be possibly applied also to other valency dictionaries with rich phraseological components.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku jsou analyzovány výhrady vůči dichotomii arguemtu (valenčního členu) a volného doplnění, které jsou předloženy ve stati A. Przepiórkovského ve stejné publikaci. Obhajují se kritéria opakovatelnosti, specifičnosti a podmínky užití dialogového testu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The arguments against the Argument-Adjunct distinction presented by A. Przepiórkowski in this volume are analyzed. The criteria iterability, specificity and dialogue test conditions are analyzed here and their usefulness for the framework of FGD is demonstrated.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve stati jsou charakterizovány přínosy skladby Vladimíra Šmilauera v kontextu jejího vzniku (1947) a její trvalé přínosy pro současnou českou syntax. Analyzují se její základní rysy odrážející se v nových syntaxích do dnešních dnů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this article the contribution of Šmialuer´s syntax in the period when it was published (1947) as well as its permanent influence on the contemporary books and studies on the syntax of contemporary Czech are analyzed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V kapitole jsou rozebrána kritéria používaná pro stanovení valenčních členů a způsob jejich zacycení ve slovníkové hesle. Zvláštní pozornost je věnována analýze možností vypuštění valenčního členu v povrchové struktuře.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this chapter the criteria used for the determination of valency members and the shape of the valency frames in lexicon are presented. The possibilities of the surface deletion of obligatory valency member are analyzed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce se budeme zabývat stromovými strukturami, které jsou využívány v počítačovém zpracování přirozeného jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We will present tree-shaped structures that are used in Natural Languages Processing for representing sentence syntactical structure, and discuss the benefits and drawbacks of such representations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá propojením dvou existujících, vzájemně se doplňujících datových zdrojů pro morfologii češtiny, a to flektivního slovníku MorfFlex CZ a derinační sítě DeriNet. MorfFlex CZ pokrývá několik milionů slovních forem v češtině. DeriNet obsahuje několik set tisíc českých lemmat, která jsou propojena explicitními slovotvornými relacemi odpovídajícímu. Výsledný zdroj je zpřístupněn pod licencí CC-BY-NC-SA a lze k němu rovněž přistupovat pomocí několika webových uživatelských rozhraní.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper deals with merging two complementary resources of morphological data previously existing for Czech, namely the inflectional dictionary MorfFlex CZ and the recently developed lexical network DeriNet. The MorfFlex CZ dictionary has been used by a morphological analyzer capable of analyzing/generating several million Czech word forms according to the rules of Czech inflection. The DeriNet network contains several hundred thousand Czech lemmas interconnected with links corresponding to derivational relations (relations between base words and words derived from them). After summarizing basic characteristics of both resources, the process of merging is described, focusing on both rather technical aspects (growth of the data, measuring the quality of newly added derivational relations) and linguistic issues (treating lexical homonymy and vowel/consonant alternations). The resulting resource contains 970 thousand lemmas connected with 715 thousand derivational relations and is publicly available on the web under the CC-BY-NC-SA license. The data were incorporated in the MorphoDiTa library version 2.0 (which provides morphological analysis, generation, tagging and lemmatization for Czech) and can be browsed and searched by two web tools (DeriNet Viewer and DeriNet Search tool).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vedle diskurzních vztahů vyjádřených primárně tzv. diskurzními konektory (nebo jsou případně dány implicitně) je třeba při analýze diskurzu brát v úvahu též další vztahy. To se týká především aktuálního členění větného a vztahů koreference. Všechny tři tyto aspekty jsou začleněny do anotačního scénáře Pražského závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Apart to discourse relations expressed primarily by discourse connectives (and partly also being implicit), there are other relations in the discourse that contribute to the text connectedness and have to be taken into consideration. This concerns first of all the information  structure of the sentence and the relations of coreference. All these three aspects are included in the annotation scheme of the Prague Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zaměřuje na popis neprojektivních konstrukcí z hlediska valence sloves. Pro tuto studii jsme použili data PDT a PCEDT a zameřili jsme se na kombinaci povrchové hloubkové syntaxe. V článku navrhujeme novou definici projektivity a klasifikujeme neprojektivni konstrukce z hlediska predikatove struktury. 
Porovnáváme češtinu a angličinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe results of investigation of a
specific type of discontinuous constructions,
namely non-projective constructions concerning
verbs and their arguments. This topic is
especially important for languages with a relatively
free word order, such as Czech, which is
the language we have primarily worked with.
For comparison, we have included some results
for English. The corpora used for both
languages are the Prague Czech-English Dependency
Treebank and the Prague Dependency
Treebank, which are both annotated at
a dependency syntax level as well as a deep
(semantic) level, including verbs and their valency
(arguments). We are using traditionally
defined non-projectivity on trees with full linear
ordering, but the two levels of annotation
are innovatively combined to determine if a
particular (deep) verb -argument structure is
non-projective. As a result, we have identi-
fied several types of discontinuities, which we
classify either by the verb class or structurally
in terms of the verb, its arguments and their
dependents. In addition, we have quantitatively
compared selected phenomena found in
Czech translated texts (in the PCEDT) to the
native Czech as found in the original Prague
Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje nový dvojjazyčný česko-anglický valenční slovník nazvaný CzEng-Vallex. Elektronická podoba slovníku je  umístěna v repozitáři Centra jazykové výzkumné infrastruktury LINDAT/CLARIN v XML formátu a je zde k dispozici také v prohledávatelné podobě. Slovník je propojen s českým valenčním slovníkem PDT-Vallex, s anglickýmm valenčním slovníkem EngVallex a rovněž je propojen s příklady z PCEDT korpusu (PCEDT 2.0). CzEngVallex  obsahuje 20835 propojených párů valenčních rámců (slovesných významů) překladových ekvivalentů a obsahuje i propojení jejich argumentů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper introduces a new bilingual Czech-English verbal valency lexicon (called CzEng-Vallex) representing a relatively large empirical database. It includes 20,835 aligned valency frame pairs (i.e., verb senses which are translations of each other) and their aligned arguments.
This new lexicon uses data from the Prague Czech-English Dependency Treebank and also takes advantage of the existing valency lexicons for both languages: the PDT-Vallex for Czech and the EngVallex for English. The CzEngVallex is available for viewing in LINDAT/CLARIN repository.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>PARSEME Shared Task (PST) se zabývá automatickou identifikací víceslovných výrazů (VV) v textu.
Jeho organizátoři připravili základní anotační pokyny se čtyřmi základními skupinami slovesných VV.
Jedním z dvaceti vybraných jazyků je i čeština.
Článek popisuje konversi dat Pražského závislostního korpusu (PDT), prozatím výhradně inherentně zájmenných sloves (IPronV) -- současnou anotaci v PDT, porovnává ji s anotačními pokyny PST. Závěrem je, že PDT a přidružený slovník obsahuje pro konversi dostatek údajů (ačkoli specifické jevy budou muset být kontrolovány ručně). Vedlejším efektem je, že jsme odhalili některé drobné chyby v anotaci PDT, které teď mohou být opraveny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes results of a study related to the PARSEME Shared Task on automatic detection of verbal Multi-Word Expressions (MWEs) which focuses on their identification in running texts in many languages. The Shared Task’s organizers have provided basic annotation guidelines where four basic types of verbal MWEs are defined including some specific subtypes. Czech is among the twenty languages selected for the task. We will contribute to the Shared Task dataset, a multilingual open resource, by converting data from the Prague Dependency Treebank (PDT) to the Shared Task format. The question to answer is to which extent this can be done automatically. In this paper, we concentrate on one of the relevant MWE categories, namely on the quasi-universal category called “Inherently Pronominal Verbs” (IPronV) and describe its annotation in the Prague Dependency Treebank. After comparing it to the Shared Task guidelines, we can conclude that the PDT and the associated valency lexicon, PDT-Vallex, contain sufficient information for the conversion, even if some specific instances will have to be checked. As a side effect, we have identified certain errors in PDT annotation which can now be automatically corrected.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme návrh nového slovníku českých diskurzních konektorů. Formát dat i anotační schéma vycházejí z podobných existujících elektronických zdrojů; zabýváme se důvody pro volbu struktury dat a pro výběr vlasností jednotlivých záznamů ve slovníku. Pozornost věnujeme obzvláště konzistentnímu zachycení primárních i sekundárních konektorů. Samotná data pocházejí z Pražského závislostního korpusu, rozsáhlého korpusu s manuálně anotovanými diskurzními vztahy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a design for a new electronic lexicon of Czech discourse connectives. The data format and the annotation scheme are based on a study of similar existing resources, and we discuss arguments for choosing the data structure and
selecting features of the lexicon entries. A special attention is paid to a consistent encoding
of both primary and secondary connectives. The data itself comes from exploiting the Prague Dependency Treebank, a large treebank manually annotated with discourse relations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>PML-Tree Query je mocný a uživatelsky přítulný vyhledávací nástroj pro vyhledávání v bohatě lingvisticky anotovaných datech. Článek ukazuje, jak může být PML-TQ využito k vyhledávání diskurzních vztahů Penn Discourse Treebanku 2.0 namapovaných na syntaktické stromy Penn Treebanku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The PML-Tree Query is a general, powerful and user-friendly system for querying richly linguistically annotated treebanks. The paper shows how the PML-Tree Query can be used for searching for discourse relations in the Penn Discourse Treebank 2.0 mapped onto the syntactic annotation of the Penn Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies (UD) je projekt, který hledá morfologické a syntaktické anotační schéma aplikovatelné na mnoho jazyků, a vydává data anotovaná podle tohoto schématu. Stručně motivujeme a uvedeme tento projekt, jeho historii a principy, na kterých stojí. Poté popíšeme návrh nové (druhé) verze anotačních pravidel UD. Nakonec, pokud zbude čas, popíšeme několik zajímavých problémů, které se týkají aplikace UD na slovanské jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies (UD) is a project that seeks to find a cross-linguistically applicable morphological and syntactic annotation scheme, and to publish data conforming to that scheme in a wide range of languages. We will briefly motivate and introduce the project, its history and underlying principles. Then we will present the new proposal of version 2 UD guidelines. Finally, time permitting, we will describe several interesting problems related to the application of UD to Slavic languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek je pokusem navrhnout aplikaci podmnožiny standardu Universal Dependencies (UD) na skupinu slovanských jazyků. Dotyčnou podmnožinou jsou morfosyntaktické rysy jednotlivých slovesných tvarů. Systematicky dokumentujeme kategorie, které lze pozorovat u slovanských sloves, a přinášíme množství příkladů z 10 jazyků. Ukazujeme, že terminologie známá z literatury se často liší, i když podstata zkoumaných jevů je stejná. Náš cíl je praktický. Rozhodně nemáme v úmyslu přehodnocovat výsledky mnohaletého bádání slovanské srovnávací jazykovědy. Spíše chceme zasadit vlastnosti slovanských sloves do kontextu UD a navrhnout jednotný (všeslovanský) způsob, jak mají být prostředky UD aplikovány na tyto jazyky. Věříme, že náš návrh je kompromisem, který bude přijatelný pro korpusové lingvisty pracující se všemi slovanskými jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article is an attempt to propose application of a subset of the Universal Dependencies (UD) standard to the group of Slavic languages. The subset in question comprises morphosyntactic features of various verb forms. We systematically document the inventory of features observable with Slavic verbs, giving numerous examples from 10 languages. We demonstrate that terminology in literature may differ, yet the substance remains the same. Our goal is practical. We definitely do not intend to overturn the many decades of research in Slavic comparative linguistics. Instead, we want to put the properties of Slavic verbs in the context of UD, and to propose a unified (Slavic-wide) application of UD features and values to them. We believe that our proposal is a compromise that could be accepted by corpus linguists working on all Slavic languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies (UD) je mezinárodní projekt, který se snaží definovat pravidla morfologické a syntaktické anotace, aplikovatelná na všechny přirozené jazyky. Motivací projektu je najít společnou lingua franku pro lidi a nástroje, které pracují s lingvisticky anotovanými daty. Kromě pravidel UD také vydává závislostní korpusy převedené do anotačního stylu UD; z těchto dat se stává významný zdroj pro výzkum a aplikace ve vícejazyčném zpracování přirozených jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies (UD) is an international project that seeks to define guidelines for annotation of morphology and syntax, applicable to all natural languages. Its motivation is to find a common lingua franca for people and tools that deal with annotated linguistic data. Besides guidelines, UD also releases dependency treebanks converted to the UD annotation style; this data is becoming an important resource for multilingual NLP research and applications. In my talk, I will give a brief introduction to UD in general, and then I will focus on phenomena specific to Slavic languages. The UD release 1.2 includes six Slavic languages and others are being worked on. This gives us plenty of material for comparative studies, that in turn can (and should) further contribute to improved cross-linguistic consistency and fine-grained UD guidelines for Slavic languages. I will present observations from the UD data, as well as some results of dependency parsers trained on the data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pro morfologické značkování a syntaktickou analýzu neznámých jazyků byla navržena řada metod. My zkoumáme delexikalizovaný parsing, navržený Zemanem a Resnikem (2008), a delexikalizované značkování, navržené Yu et al. (2016). V obou případech předkládáme podrobné vyhodnocení na datech z Universal Dependencies (Nivre et al., 2016), de-facto standardu pro vícejazyčné morfosyntaktické zpracování (předchozí práce pracovaly s jinými daty). Naše výsledky potvrzují, že každá z uvedených delexikalizovaných metod samostatně má určitý omezený potenciál v případech, kdy není k dispozici žádná ruční anotace cílového jazyka. Nicméně, pokud obě metody zkombinujeme, jejich chyby se vzájemně zmnožují nad přijatelnou mez. Ukazujeme, že i sebemenší střípek expertní anotace cílového jazyka může významně zvýšit úspěšnost a měl by být použit, jestliže ho lze získat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Various unsupervised and semi-supervised
methods have been proposed to tag and parse
an unseen language. We explore delexicalized
parsing, proposed by (Zeman and Resnik,
2008), and delexicalized tagging, proposed
by (Yu et al., 2016). For both approaches
we provide a detailed evaluation on Universal
Dependencies data (Nivre et al., 2016), a de-facto standard for multi-lingual morphosyntactic processing (while the previous work used other datasets). Our results confirm that in separation, each of the two delexicalized techniques has some limited potential when no annotation of the target language is available. However, if used in combination, their errors multiply beyond acceptable limits. We demonstrate that even the tiniest bit of expert annotation in the target language may contain significant potential and should be used if available.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku na příkladu reciprocity představujeme popis gramatikalizovaných alternací v češtině, který je založen na rozdělení informace do slovníku a gramatiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we introduce the representation of grammaticalized alternations in Czech based on the division of the information between grammar and lexicon. This representation is exemplified by reciprocity.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek představil rodinu elektronických valenčních slovníků, které jsou rozvíjeny v Ústavu formální a aplikované lingvistiky, VALLEX, PDT-Vallex, EngVallex a CzEngVallex, jejichž teoretické zázemí tvoří funkční generativní popis.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this contribution, the family of valency lexicons whose theoretical background represents the Functional Generative Description is introduced, VALLEX, PDT-Vallex, EngVallex and CzEngVallex.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku jsme představili valenční slovník českých sloves, VALLEX, a jeho nejnovější verzi 3.0, která je založena na dvou komponentech: na slovníkové a gramatické komponentě.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this talk, a new version of the valency lexicon of Czech verbs, VALLEX, was introduced. This version is based on the division of the lexicon between two components: a data component and a grammar component.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valenční slovník českých sloves (dále též VALLEX 3.0) poskytuje informace o valenční struktuře českých sloves v jejich jednotlivých významech, které charakterizuje pomocí glos a příkladů; tyto základní údaje jsou doplněny o některé další syntaktické a sémantické charakteristiky. VALLEX 3.0 zachycuje 4586 českých sloves náležejících 2722 lexémům, které odpovídají 6710 lexikálním jednotkám, tedy daným slovesům v daném významu (počítáme-li vidové protějšky zvlášť, jde -- bez iterativ, jež nejsou detailně zpracována -- o 10816 lexikálních jednotek).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Valency Lexicon of Czech Verbs (henceforth VALLEX 3.0) provides information on the valency structure of Czech verbs in their respective senses. Each sense is characterized by a gloss and an example; further syntactic and semantic characteristics are provided as relevant. VALLEX 3.0 covers 4586 Czech verbs in 2722 lexemes with a total of 6710 lexical units (i.e. a given verb in a given meaning; if aspectual counterparts are counted separately, we get a total of 10816 lexical units, excluding iteratives).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valenční slovník českých sloves (též VALLEX 3.0) poskytuje informace o valenční struktuře českých sloves v jejich jednotlivých významech,
které charakterizuje pomocí glos a příkladů;
tyto základní údaje jsou doplněny o některé další syntaktické a sémantické charakteristiky.
VALLEX 3.0 zachycuje téměř 4 600 českých sloves, která odpovídají více než 10 700 lexikálním jednotkám, tedy vždy danému slovesu v daném  významu.

VALLEX je  budován jako odpověď na potřebu rozsáhlého a kvalitního veřejně dostupného slovníku, který by obsahoval
syntaktické a sémantické charakteristiky českých sloves a byl přitom široce uplatnitelný v aplikacích pro zpracování
přirozeného jazyka. Při navrhování koncepce slovníku byl proto kladen důraz
na všestranné využití jak pro lidského uživatele, tak pro automatické zpracování češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>VALLEX 3.0 provides information on the valency structure (combinatorial potential) of verbs in their particular senses, which are characterized by glosses and examples. VALLEX 3.0 describes almost 4 600 Czech verbs in more than 10 700 lexical units, i.e., given verbs in the given senses.

VALLEX 3.0 is a is a collection of linguistically annotated data and documentation, resulting from an attempt at formal description of valency frames of Czech verbs.  In order to satisfy different needs of different potential users, the lexicon is distributed (i) in a HTML version (the data allows for an easy and fast navigation through the lexicon) and (ii) in a machine-tractable form as a single XML file, so that the VALLEX data can be used in NLP applications.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této kapitole je popsáno lexikografické zpracování diatezí ve valenčním slovníku VALLEX. Dále je představen poloautomatický postup při identifikaci diatezí lexikálních jednotek sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this chapter, the representation of Czech diatheses in the valency lexicon VALLEX is described. A semiautomatic method identifying the possibility of lexical units of verbs to create diatheses is introduced.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Licenční smlouva a smlouva o poskytnutí služeb mezi Univerzitou Karlovou a Datlowe, s.r.o. (1. 1. 2015 -- 1. 1. 2018) obsahuje právo na užití dat PDT 2.5 a software pro textovou analytiku natrénovaného na PDT 2.5, a to pro komerční účely. Jedná se o upravené verze nekomerčního software a dat jinak dostupných z repozitáře LINDAT/CLARIN. Jmenovitě: software pro analýzu tvarosloví a lematizaci, syntaktickou analýzu a analýzu názvů pojmenovanýchch entit.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Licence and service agreement between Charles University and Datlowe Ltd.(January 1, 2015 -- January 1, 2018) covers the right to use the data of PDT 2.5 and the software for text analytics trained on PDT 2.5, for commercial purposes. The data and software (non-adapted) is otherwise available for free through the LINDAT/CLARIN repository for non-commercial use under the appropriate variant of the Creative Commons license. Namely, software for morphological analysis, lemmatization, syntactic analysis and named-entity recognition.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slovník valence substantiv založený na korpusu je začínající projekt navazující na předchozí výzkumy valence substantiv v češtině. Tento příspěvek se zabývá zachycením valence substantiv v moderním slovníku založeném na korpusu, dostupném jak pro lidské uživatele, tak ve formě strojově čitelných dat. Referujeme také o omezeních, která pro výzkum jmenné valence představují v současnosti nejčastěji užívané korpusové vyhledávače.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Corpus-based Valency Lexicon of Czech Nouns is a starting project picking up the threads of our previous work on nominal valency. It builds upon solid theoretical foundations of the theory of valency developed within the Functional Generative Description. In this paper, we describe the ways of treating valency of nouns in a modern corpus-based lexicon, available as machine readable data in a format suitable for NLP applications, and report on the limitations that the most commonly used corpus interfaces provide to the research of nominal valency. The linguistic material is extracted from the Prague Dependency Treebank, the synchronic written part of the Czech National Corpus, and Araneum Bohemicum. We will utilize lexicographic software and partially also data developed for the valency lexicon PDT-Vallex but the treatment of entries will be more exhaustive, for example, in the coverage of senses and in the semantic classification added to selected lexical units (meanings). The main criteria for including nouns in the lexicon will be semantic class membership and the complexity of valency patterns. Valency of nouns will be captured in the form of valency frames, enumeration of all possible combinations of adnominal participants, and corpus examples.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Na příkladu českého morfologického slovníku MorfFlex CZ upozorňujeme na problém homonymie a polysémie. V morfologickém slovníku není nutné rozlišovat význam slov, pokud takové rozlišení nemá důsledky ve slovotvorbě nebo v syntaxi. Příspěvek navrhuje několik důležitých pravidel a principů pro dosažení konzistence.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We focus on a problem of homonymy and polysemy in morphological dictionaries on the example of the Czech morphological dictionary MorfFlex CZ. It is not necessary to distinguish meanings in morphological dictionaries unless the distinction has consequencies in word formation or syntax. The contribution proposes several  important rules and principles for achieving consistency.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje vznik nového lexikálního zdroje pro češtinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Human judgments of lexical similarity/relatedness are used as evaluation data for Vector Space Models, helping to judge how the distributional similarity captured by a given Vecotr Space Model correlates with human intuitions. A well establishde data set for the evaluation of lexical similarity/relatedness id WordSim353, alnog with its translations into sefveral other languages. Thiospaper presents its Czech translation and annotation, which is publicly available via the Lindat-Clarin repository.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme pilotní analýzu nového lingvistického zdroje, VPS-GradeUp, který je dostupný z http://hdl.handle.net/11234/1-1585.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a pilot analysis of a new linguistic resource, VPS-GradeUp (available at http://hdl.handle.net/11234/1-1585). The resource contains 11,400 graded human decisions on usage patterns of 29 English lexical verbs, randomly selected from the Pattern Dictionary of English Verbs (Hanks, 2000 2014) based on their frequency and the number of senses their lemmas have in PDEV. This data set has been created to observe the interannotator agreement on PDEV patterns produced using the Corpus Pattern Analysis (Hanks, 2013). Apart from the graded decisions, the data set also contains traditional Word-Sense-Disambiguation (WSD) labels. We analyze the associations between the graded annotation and WSD annotation. The results of the respective annotations do not correlate with the size of the usage pattern inventory for the respective verbs lemmas, which makes the data set worth further linguistic analysis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek pojednává o škálovaných rozhodnutích anotátorů v CPA analýze anglických sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We work with 1450 concordances of 29 English verbs (50 concordances per lemma) and their
corresponding entries in the Pattern Dictionary of English Verbs (PDEV). Three human annotators
working independently but in parallel judged how well each lexical unit of the corresponding PDEV
entry illustrates the given concordance. Thereafter they selected one best-fitting lexical unit for each concordance – while the former setup allowed for ties (equally good matches), the latter did not. We measure the interannotator agreement/correlation in both setups and show that our results are not worse (in fact, slightly better) than in an already published graded-decision annotation performed on a traditional dictionary. We also manually examine the cases where several PDEV lexical units were classified as good matches and how this fact affected the interannotator agreement in the bestfit
setup. The main causes of overlap between lexical units include semantic coercion and regular
polysemy, as well as occasionally insufficient abstraction from regular syntactic alternations, and eventually also arguments defined as optional and scattered across different lexical units despite not being mutually exclusive.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Toto je dataset k evaluaci vzájemných sémantických vztahů mezi slovy (podobnost a souvislost). Vytvořeno podle již klasického anglického zdroje WordSim353</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Human judgments of lexical similarity/relatedness are used as evaluation data for Vector Space Models, helping to judge how the distributional similarity captured by a given Vector Space Model correlates with human intuitions. A well established data set for the evaluation of lexical similarity/relatedness is WordSim353, along with its translations into several other languages. This paper presents its Czech translation and annotation, which is publicly available via the LINDAT-CLARIN repository at hdl.​handle.​net/​11234/​1-1713.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Poster představuje přehled výsledků automatické analýzy slovosledu ve 23 treebancích. Tyto treebanky byly vytvořeny v rámci projektu HamleDT, jehož cílem je poskytnout univerzální anotaci závislostních korpusů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This poster gives an overview of the results of automatic analysis of word order in 23 dependency treebanks. These treebanks have been collected in the frame of in the HamleDT project, whose main goal is to provide universal annotation for dependency corpora.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek porovnává několik způsobů měření stupně volnosti slovosledu na datech 23 jazyků, získaných z jejich syntakticky anotovaných korpusů. Tyto korpusy jsou součástí projektu HamleDT. Měření bere v úvahu vzájemné postavení podmětu, přísudku a předmětu v hlavních a vedlejších větách a měří jej pomocí euklidovské vzdálenosti, vzdálenosti max-min, entropie a kosinové podobnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper compares various means of measur-
ing of word order freedom applied to data from syntactically annotated corpora for 23 languages. The corpora are part of the HamleDT project, the word order statistics are relative frequencies of all word order combinations of subject, predicate and object both in main and subordinated clauses. The measures include Euclidean distance, max-min distance, entropy and cosine similarity. The dif-
ferences among the measures are discussed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení mých projektů týkajících se víceslovných výrazů na PARSEME Training school v La Rochelle.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Overview of my project concerning MWEs at Second PARSEME Training School in La Rochelle.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Analyzujeme vnitřní strukturu komplexních adresních datových údajů v textu, kategorizujeme je, anotujeme a vyhodnocujeme výsledky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We analyze inner structure of complex address data
given in text, categorize it, annotate
it and evaluate the annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ukážeme, že slovesné víceslovné výrazy (VV) specifikované v Shared Tasku PARSEME jsou anotovány v PDT a jednotlivé kategorie VV je možné z něj extrahovat a v Shared Tasku použít.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We show that verbal MWEs similar to those specified in PARSEME Shared Task have already been annotated in Prague Dependency Treebank and all their individual categories can be extracted and used within the Shared Task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek prezentuje dva problémy, při jejichž řešení bylo využíváno vztahů mezi derivačními a flektivními paradigmaty. Za prvé, tyto vztahy byly využity při identifikaci nových vztahů mezi odvozenými slovy a slovy základovými, a to především v případech, kdy během odovzení dochází k hláskové alternaci. Za druhé, flektivní rysy je možné využívat rovněž jako důležitou vstupní informaci při poloautomatickém značkování významových vztahů mezi deriváty a slovy základovými.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper  deals  with  two  main  issues,  each  approaching  the  relation between  derivational  (sub-)paradigms (as discussed by Štekauer 2014) and inflectional paradigms  from  a  different perspective.  First,  a  recent  enrichment  of  the  DeriNet network  is  described  which  focused  on  words  the  derivation  of  which  is  accompanied by consonant and/or vowel alternations. Second,  inflectional  features  have  been  used  as  an  important  input  for  the  semi-automatic task of semantic labelling of derivational relations in the DeriNet network.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje lexikální databázi DeriNet, která obsahuje téměř 1 milion českých slov propojených více než 700 tisíci hranami odpovídajícími vztahu mezi slovem odvozeným a základovým. V textu je popsán proces budování této databáze, lingvistická rozhodnutí, která bylo nutné během tohoto procesu učinit, i možné využití a výhledy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the present paper, the lexical database DeriNet is introduced which includes more than 969 thousand Czech word interconnected with 718 thousand links corresponding to derivational relations (relations between a base word and a word derived from it). Derivational relations were identified by semi-automatic procedures and manual annotation. As the DeriNet network is fully compatible with a big inflectional dictionary of Czech (MorfFlex CZ), it can be used as a resource for an integrating approach to derivational and inflectional morphology of Czech both in linguistic research and in natural language processing.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje zdroje dosud vytvořené v projektu Universal Dependencies (UD), který se snaží vypořádat s chybějící závislostní reprezentací pro zpracování přirozených jazyků aplikovatelnou na více jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the resources so far created in the Universal Dependencies (UD) project, which attempts to address a lack of cross-linguistically adequate dependency representations for natural language processing.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je páté vydání treebanků UD, verze 1.4.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the fifth release of UD Treebanks, Version 1.4.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je čtvrté vydání treebanků UD, verze 1.3.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the fourth release of UD Treebanks, Version 1.3.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Toto je dokument-alignovaný paralelní korpus anglických a českých abstraktů vědeckých článků, které publikovali autoři z Institutu formální a aplikované lingvistiky Univerzity Karlovy v Praze, jak byly reportovány v systému institutu Biblio. Autoři každé publikace jsou povinni poskytnout jak originální abstrakt v češtině nebo angličtině, tak jeho překlad do angličtiny  respektive češtiny. Žádné filterování nebylo provedeno, kromě odstranění záznamů, kterým chybí český nebo anglický abstrakt a nahrazení nových řádků a tabulátorů mezerami.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This is a document-aligned parallel corpus of English and Czech abstracts of scientific papers published by authors from the Institute of Formal and Applied Linguistics, Charles University in Prague, as reported in the institute's system Biblio. For each publication, the authors are obliged to provide both the original abstract in Czech or English, and its translation into English or Czech, respectively. No filtering was performed, except for removing entries missing the Czech or English abstract, and replacing newline and tabulator characters by spaces.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme lexikon-lesový rule-bazovaný systém machín translace od Engliše Čecha, který bazoval na verově limitované amountu rulů transformace. Jeho kor je novelový modul translace, implementovalo jako komponent systému translace tektomtu a dependuje masivně na extensivní pipelínu lingvistické preprocesování a postprocesovat v Tektomtu. Jeho skop je naturálně limitovaná, ale pro specifické texty z například scientifické nebo marketování doménu okasionálně producuje sensibilní resulty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a lexicon-less rule-based machine translation system from English to Czech, based on a very limited amount of transformation rules. Its core is a novel translation module, implemented as a component of the TectoMT translation system, and depends massively on the extensive pipeline of linguistic preprocessing and postprocessing within TectoMT. Its scope is naturally limited, but for specific texts, e.g. from the scientific or marketing domain, it occasionally produces sensible results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme naši práci v oblasti částečně řízené syntaktické analýzy vět přirozeného jazyka, se zaměřením na mezijazyčný přenos delexikalizovaných závislostních parserů s více zdroji. Představujeme KLcpos3, empirickou míru podobnosti jazyků, navrženou a vyladěnou pro vážení zdrojových parserů při přenosu delexicalizovaných parserů s více zdroji. Nakonec představíme novou metodu kombinace zdrojů, založenou na interpolaci natrénovaných modelů parserů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our work on semi-supervised parsing of natural language sentences, focusing on multi-source crosslingual transfer of delexicalized dependency parsers. We present KLcpos3, an empirical language similarity measure, designed and tuned for source parser weighting in multi-source delexicalized parser transfer. And finally, we introduce a novel resource combination method, based on interpolation of trained parser models.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>To je něco, co používá každý, přitom není úplně zřejmé, jak to funguje. Pro daný dotaz nejde prostě pro každý dotaz projít všechny existující miliardy webových stránek, a vrátit uživateli několik miliónů z nich, které obsahují hledaná slova. Ukážu, co je reverzní index (to zná každý pod názvem rejstřík), podle čeho se řadí výsledky, a přidám i pokročilejší vylepšení (TF.IDF, lematizace, synonyma, pojmenované entity).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This is something everyone uses, but it is not absolutely clear how it works. For a given query it is not possible to simply traverse all the existing billions of websites, and return the few millions that contain the sought words. I will show what a reverse index is, how results are sorted, and I will also add more advanced techniques (TF.IDF, lemmatization, synonyms, named entities).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>SloNLP je workshop speciálně zaměřený na zpracování přirozeného jazyka a počítačovou lingvistiku. Jeho hlavním cílem je podpořit spolupráci mezi výzkumníky v oblasti NLP v Česku a na Slovensku.
Mezi témata workshopu patří automatické rozpoznávání mluvené řeči (ASR), automatická analýza a generování přirozeného jazyka (morfologie, syntax, sémantika...), dialogové systémy (DS), strojový překlad (MT), vyhledávání informací (IR), praktické aplikace NLP technologií, a další témata počítačové lingvistiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>SloNLP is a workshop focused on Natural Language Processing (NLP) and Computational Linguistics. Its primary aim is to promote cooperation among NLP researchers in Slovakia and Czech Republic.
The topics of the workshop include automatic speech recognition, automatic natural language analysis and generation (morphology, syntax, semantics, etc.), dialogue systems, machine translation, information retrieval, practical applications of NLP technologies, and other topics of computational linguistics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisuje náš příspěvek do úlohy překladu v doméně IT na WMT16.
Provádíme doménovou adaptaci již natrénovaných překladových systémů pomocí slovníkových dat bez dalšího trénování.
Náš postup aplikujeme na dva konceptuálně odlišné překladače, vyvíjené v rámci projektu QTLeap -- TectoMT a Moses -- a na jejich kombinaci -- Chiméru.
Všechny naše metody zlepšují ve všech experimentech kvalitu překladu.
Základní varianta naší metody je navíc použitelná na libovolný překladový systém, včetně takového, který je pro uživatele černou skříňkou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe our submission to the IT-domain translation task of WMT 2016.
We perform domain adaptation with dictionary data on already trained MT systems with no further retraining.
We apply our approach to two conceptually different systems developed within the QTLeap project: TectoMT and Moses, as well as Chimera, their combination.
In all settings, our method improves the translation quality.
Moreover, the basic variant of our approach is applicable to any MT system, including a black-box one.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Moses je dobře známý reprezentant rodiny frázových systémů statistického strojového překladu, který je známý tím, že je extrémně chudý na explicitní lingvistické znalosti, a operuje na plochých reprezentacích jazyka, složených jen z tokenů a frází. Na druhé straně, Treex NLP toolkit, který je vysoce lingvisticky motivovaný, operuje na několika vrstvách reprezentace jazyka, bohatých na lingvistické anotace. Jeho hlavní aplikace je TectoMT, hybridní systém strojového překladu s hloubkovým transfrem syntaxe. Nabízíme přehled velkého počtu systémů strojového překladu, sestavených v minulých letech, které různými způsoby kombinují Mosese a Treex/TectoMT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Moses is a well-known representative of the phrase-based statistical machine translation systems family, which are known to be extremely poor in explicit linguistic knowledge, operating on flat language representations, consisting only of tokens and phrases. Treex, on the other hand, is a highly linguistically motivated NLP toolkit, operating on several layers of language representation, rich in linguistic annotations. Its main application is TectoMT, a hybrid machine translation system with deep syntax transfer. We review a large number of machine translation systems that have been built over the past years by combining Moses and Treex/TectoMT in various ways.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme dialogový systém Politik. Politik je navržen jako chatovací robot, který imituje politika. Odpovídá na otázky týkající se politických témat. Otázky jsou zpracovány nástroji počítačového zpracování jazyka bez použití externí znalostní báze. Jazykem dialogu je čeština.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a question-answering system
Politician designed as a chatbot imitating a
politician. It answers questions on political
issues. The questions are analyzed using
natural language processing techniques and
no complex knowledge base is involved.
The language used for the interviews is
Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme systém na mezijazyčnou predikci zájmen, konkrétně mezi angličtinou a němčinou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a system submitted to the WMT16 shared task in cross-lingual pronoun prediction, in particular, to the English-to-German and German-to-English sub-tasks. The system is based on a linear classifier making use of features both from the target language model and from linguistically analyzed source and target texts. Furthermore, we apply example weighing in classifier learning, which proved to be beneficial for recall in less frequent pronoun classes. Compared to other shared task participants, our best English-to-German system is able to rank just below the top performing submissions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento projekt rozšiřuje část PCEDT 2.0 o ruské překlady a anotaci zaměřenou na koreferenci a mezijazyčné zarovnání koreferenčních výrazǔ. Ve verzi 0.5 korpus obsahuje překlad více než 1000 anglických vět s automatickými anotacemi na úrovni morfologie a ručním mezijazyčným zarovnáním českých, anglických a ruských posesivních zájmen.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Prague Czech-English Dependency Treebank - Russian translation (PCEDT-R) is a project of translating a subset of Prague Czech-English Dependency Treebank 2.0 (PCEDT 2.0) to Russian and  linguistically annotating the Russian translations with emphasis on coreference and cross-lingual alignment of coreferential expressions. Cross-lingual comparison of coreference means is currently the purpose that drives development of this corpus. In version 0.5, it contains Russian translations of more than 1000 English sentences with automatic morphological analysis and manual cross-lingual alignment of Czech, English and Russian possessive pronouns.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozšířená a přepracovaná verze Lapshinova et al. (2016). V článku provádíme srovnávací analýzu diskurzních jevů v němčině a češtině na základě dvou korpusových zdrojů pro tyto jazyky. Korpusy jsou anotovány v rámci různých teoretických přístupů: FGP (Sgall et al. 1986) pro češtinu a teorie textové koheze (Halliday &amp; Hasan 1976) pro němčinu. Hledáme společné rysy, automaticky je identifikujeme v textech a tím se dostáváme k srovnatelným korpusům anotovaných stejnými typy vztahů, které dále rozebíráme. (viz hlavně angl. abstrakt a text)</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the present paper, we analyse variation of discourse phenomena in two typologically different languages, i.e. in German and Czech.
The novelty of our approach lies in the nature of the resources we are using. Advantage is taken of existing resources, which are,
however, annotated on the basis of two different frameworks. We use an interoperable scheme unifying discourse phenomena in both
frameworks into more abstract categories and considering only those phenomena that have a direct match in German and Czech. The
discourse properties we focus on are relations of identity, semantic similarity, ellipsis and discourse relations. Our study shows that the
application of interoperable schemes allows an exploitation of discourse-related phenomena analysed in different projects and on the
basis of different frameworks. As corpus compilation and annotation is a time-consuming task, positive results of this experiment open
up new paths for contrastive linguistics, translation studies and NLP, including machine translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku provádíme srovnávací analýzu diskurzních jevů v němčině a češtině na základě dvou korpusových zdrojů pro tyto jazyky. Korpusy jsou anotovány v rámci různých teoretických přístupů: FGP (Sgall et al. 1986) pro češtinu a teorie textové koheze (Halliday &amp; Hasan 1976) pro němčinu. Hledáme společné rysy, automaticky je identifikujeme v textech a tím se dostáváme k srovnatelným korpusům anotovaných stejnými typy vztahů, které dále rozebíráme. (viz hlavně angl. abstrakt a text)</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>in this paper, we perform a cross-lingual analysis of discourse phenomena in German and Czech, using two corpus resources annotated monolingually within two different frameworks: functional Generative description (Sgall et al. 1986) for Czech, and textual cohesion (Halliday &amp; Hasan 1976) for German.
We take advantage of the existing resources reflecting systemic peculiarities and realisational options of the languages under analysis. in our previous work (Lapshinova et al. 2015), we have shown that the annotations of the involved resources are comparable if abstract categories are used and only the phenomena with a direct match in German and Czech are taken into consideration. Our
analysis is a first step towards unifying separate analyses of discourse relations in Germanic and Slavic languages. at the same time, it demonstrates that the application of ’theoretically’ different resources is possible in one contrastive analysis. This is especially valuable for nlp, which uses annotated resources to train language models for various tools.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje metodu hledání dvojic dokumentů, které jsou si překladem, na základě podobného umístění klíčových slov v kandidátech a na základě podobnosti měřené n-gramovým jazykovým modelem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The WMT Bilingual Document Alignment
Task requires systems to assign
source pages to their “translations”, in a
big space of possible pairs. We present
four methods: The first one uses the term
position similarity between candidate document
pairs. The second method requires
automatically translated versions of the
target text, and matches them with the candidates.
The third and fourth methods try
to overcome some of the challenges presented
by the nature of the corpus, by
considering the string similarity of source
URL and candidate URL, and combining
the first two approaches.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>MUSCIMarker 1.1 je open-source nástroj pro anotaci vizuálních objektů a jejich vztahů v obrázkách. Byl navržen pro anotování hudební notace, avšak uživatel může definovat libovolnou sadu objektů. Poskytuje specializované funkce pro zrychlení anotace binárních obrázků (tj. obrázků, kde je jsou jasně vymezené oblasti pozadí, ve kterých se žádný anotovaný objekt nevyskytuje). MUSCIMarker je samostatná aplikace s grafickým rozhraním pro osobní počítače a nevyžaduje připojení na internet; je implementována v jazyce Python a otestována na všech třech hlavních operačních systémech (Windows, Linux a OS X).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>MUSCIMarker 1.1 is an open-source tool for annotating visual objects and their relationships in images. It was designed with music notation in mind, but it can annotate any user-defined object set.
Specialized functionality is provided to speed up annotation of binary images (where the background, defined as an area where no object of interest resides, is known). MUSCIMarker is an offline standalone graphical user interface desktop application implemented in Python and known to run on all major desktop operating systems (Windows, Linux and OS X).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Evaluace rozpoznávání notopisu (Optical Music Recognition, OMR) je notoricky obtížná a automatické metriky pro evaluaci konečného výstupu OMR nejsou k dispozici. V "Towards a Standard Testbed for Optical Music Recognition: Definitions, Metris and Page Images", Byrd a Simonsen nedávno zdůraznili, že benchmarkovací standard je v komunitě OMR nutný: jak evaluační data, tak metriky. Navazujeme na jejich analýzu a předkládáme prototyp benchmarku pro OMR. Náš příspěvek není úplné řešení komplexního problému evaluace OMR; je to (a) snaha definovat víceúrovňový testovací dataset pro OMR a implementace jeho prototypu pro tištěná a rukopisná data, (b) na korpusu založená metodologie pro vyhodnocování automatických evaluačních metrik, a příslušný korpus více než 1000 expertních posouzení relativní obtížnosti opravy různých druhů chyb. Na (b) pak navazujeme vyhodnocením několika přímočarých evaluačních metrik pro MusicXML a nastavujeme tak baseline, kterou další metriky budou zlepšovat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Evaluating Optical Music Recognition (OMR) is notoriously difficult and automated end-to-end OMR evaluation metrics are not available to guide development. In “Towards a Standard Testbed for Optical Music Recognition: Definitions, Metrics, and Page Images”, Byrd and Simonsen recently stress that a benchmarking standard is needed in the OMR community, both with regards to data and evaluation metrics. We build on their analysis and definitions and present a prototype of an OMR benchmark. We do not, however, presume to present a complete solution to the complex problem of OMR benchmarking. Our contributions are: (a) an attempt to define a multi- level OMR benchmark dataset and a practical prototype implementation for both printed and handwritten scores, (b) a corpus-based methodology for assessing automated evaluation metrics, and an underlying corpus of over 1000 qualified relative cost-to-correct judgments. We then assess several straightforward automated MusicXML evaluation metrics against this corpus to establish a baseline over which further metrics can improve.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje pokus o automatizaci všech procesů vytváření dat u systému strojového překladu založeného na pravidlovém přístupu s mělkým transferem. Uváděné metody byly vyzkoušeny na čtyřech plně funkčních překladových systémech pokrývajících tyto jazykové páry: slovinštinu do srbštiny, češtiny, angličtiny a estonštiny. Aplikovatelnost použitých metod byla testována rozsáhlou řadou evalučních testů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents an attempt to automate all data creation processes of a rule-based shallow-transfer machine translation system. The presented methods were tested on four fully functional translation systems covering language pairs: Slovenian paired with Serbian, Czech, English and Estonian language. An extensive range of evaluation tests was performed to assess the applicability of the methods.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Medical and healthcare study programmes are quite complicated in terms of branched structure and heterogeneous content. In logical sequence a lot of requirements and demands placed on students appear there. This paper focuses on an innovative way how to discover and understand complex curricula using modern information and communication technologies. We introduce an algorithm for curriculum metadata automatic processing - automatic keyword extraction based on unsupervised approaches, and we demonstrate a real application during a process of innovation and optimization of medical education. The outputs of our pilot analysis represent systematic description of medical curriculum by three different approaches (centrality measures) used for relevant keywords extraction. Further evaluation by senior curriculum designers and guarantors is required to obtain an objective benchmark.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Medical and healthcare study programmes are quite complicated in terms of branched structure and heterogeneous content. In logical sequence a lot of requirements and demands placed on students appear there. This paper focuses on an innovative way how to discover and understand complex curricula using modern information and communication technologies. We introduce an algorithm for curriculum metadata automatic processing -- automatic keyword extraction based on unsupervised approaches, and we demonstrate a real application during a process of innovation and optimization of medical education. The outputs of our pilot analysis represent systematic description of medical curriculum by three different approaches (centrality measures) used for relevant keywords extraction. Further evaluation by senior curriculum designers and guarantors is required to obtain an objective benchmark.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Navrhujeme metodu, která zlepšuje závislostní parsing složených vět. Metoda předpokládá segmentaci věty do klauzí a nevyžaduje přetrénování parseru. Klauzální strukturu věty reprezentujeme pomocí klauzálních grafů, které poskytují informaci o vnoření každé klauze. Navrhujeme postup, ve kterém parsujeme klauze nezávisle a vzniklé závislostní stromy vkládáme jako podstromy do finálního stromu pro celou větu. Metodu aplikujeme na češtinu a experimentujeme s MST parserem natrénovaným na PDT 2.0. Dosahujeme zvýšení UAS o 0.97%.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We propose a method for improving the dependency parsing of complex sentences. This method assumes segmentation of input sentences into clauses and does not require to re-train a parser of one's choice. We represent a sentence clause structure using clause charts that provide a layer of embedding for each clause in the sentence. Then we formulate a parsing strategy as a two-stage process where (i) coordinaed and subordinated clauses of the sentence are parsed separately with respect to the sentence clause chart and (ii) their dependency trees become subtrees of the final tree of the sentence. The object language is Czech and the parser used is a maximum spanning tree parser trained on the Prague Dependency Treebank. We have achieved an average 0.97% improvement in the unlabeled attachment score. Although the method has been designed for the dependency parsing of Czech, it is useful for other parsing techniques and languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>RExtractor je systém pro extrakci informací. Vstupní dokumenty jsou zpracovány NLP nástroji. Extrakce následně probíhá pomocí dotazů nad závislostními stromy. Výsledkem je znalostní báze dokumentu, definována jako množina entit a vztahů mezi nima. Dotazy nad stromy jsou definovány manuálně. Architektura systému je navržena doménově a jazykově nezávisle. Systém demonstrujeme na českých a anglických právních dokumentech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The RExtractor system is an information extractor that processes input documents by natural language processing tools and consequently queries the parsed sentences to extract a knowledge base of entities and their relations. The extraction queries are designed manually using a tool that enables natural graphical representation of queries over dependency trees. A workflow of the system is designed to be language and domain independent. We demonstrate RExtractor on Czech and English legal documents.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>RExtractor je systém pro extrakci informací. Vstupní dokumenty jsou zpracovány NLP nástroji. Extrakce následně probíhá pomocí dotazů nad závislostními stromy. Výsledkem je znalostní báze dokumentu, definována jako množina entit a vztahů mezi nima. Dotazy nad stromy jsou definovány manuálně. Architektura systému je navržena doménově a jazykově nezávisle. Systém demonstrujeme na českých a anglických právních dokumentech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The RExtractor system is an information extractor that processes input documents by natural language processing tools and consequently queries the parsed sentences to extract a knowledge base of entities and their relations. The extraction queries are designed manually using a tool that enables natural graphical representation of queries over dependency trees. A workflow of the system is designed to be language and domain independent. We demonstrate RExtractor on Czech and English legal documents.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Navrhujeme metodu, která zlepšuje závislostní parsing složených vět. Metoda předpokládá segmentaci věty do klauzí a nevyžaduje přetrénování parseru. Klauzální strukturu věty reprezentujeme pomocí klauzálních grafů, které poskytují informaci o vnoření každé klauze. Navrhujeme postup, ve kterém parsujeme klauze nezávisle a vzniklé závislostní stromy vkládáme jako podstromy do finálního stromu pro celou větu. Metodu aplikujeme na češtinu a experimentujeme s MST parserem natrénovaným na PDT 2.0. Dosahujeme zvýšení UAS o 0.97%.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We propose a method for improving the dependency parsing of complex sentences. This method assumes segmentation of input sentences into clauses and does not require to re-train a parser of one's choice. We represent a sentence clause structure using clause charts that provide a layer of embedding for each clause in the sentence. Then we formulate a parsing strategy as a two-stage process where (i)  coordinaed and subordinated clauses of the sentence are parsed separately with respect to the sentence clause chart and (ii) their dependency trees become subtrees of the final tree of the sentence. The object language is Czech and the parser used is a maximum spanning tree parser trained on the Prague Dependency Treebank. We have achieved an average 0.97% improvement in the unlabeled attachment score. Although the method has been designed for the dependency parsing of Czech, it is useful for other parsing techniques and languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nového člena rodiny pražských závislostních korpusů. Český korpus právních textů je morfologicky a syntakticky anotovaný korpus 1128 vět, který obsahuje texty z právní domény, konkrétně dokumenty ze Sbírky zákonů České republiky. Právní texty se odlišují od jiných domén v několika jazykových jevech vyplývajících z vysoké četnosti velmi dlouhých vět. Manuální anotace takových vět představuje novou výzvu. Popisujeme strategii a nástroje pro tento úkol. Korpus je dostupný několika způsoby, a sice z repozitáře LINDAT/CLARIN a on-line pomocí aplikací KonText a TreeQuery.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a new member of the family of Prague dependency treebanks. The Czech Legal Text Treebank 1.0 is a morphologically and syntactically annotated corpus of 1,128 sentences. The treebank contains texts from the legal domain, namely the documents from the Collection of Laws of the Czech Republic. Legal texts differ from other domains in several language phenomena influenced by rather high frequency of very long sentences. A manual annotation of such sentences presents a new challenge. We describe a strategy and tools for this task. The resulting treebank can be explored in various ways. It can be downloaded from the LINDAT/CLARIN repository and viewed locally using the TrEd editor or it can be accessed on-line using the KonText and TreeQuery tools.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>VIADAT-REPO je součást projektu VIADAT, tj. část "virtuálního asistenta" pro zpracování, anotaci, obohacení a přístup k audio a video nahrávkám. Základem tohoto softwaru je repozitář lindat-dspace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>VIDATA-REPO is a modification to lindat-dspace platform; it's a part of the VIADAT project and as such will be a part of a "virtual assistant" for processing, annotation, enrichment and accessing of audio and video recordings.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Souhrnný článek o přípravě, průbehu a výsledcích soutěže vyhledávání zdravotních informací CLEF eHealth Evaluation Lab 2016.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper details the collection, systems and evaluation methods used in the IR Task of the CLEF 2016 eHealth Evaluation Lab. This task investigates the effectiveness of web search engines in providing access to medical information for common people that have no or little medical knowledge. The task aims to foster advances in the development of search technologies for consumer health search by providing resources and evaluation methods to test and validate search systems. The problem considered in this year’s task was to retrieve web pages to support the information needs of health consumers that are faced by a medical condition and that want to seek relevant health information online through a search engine. As part of the evaluation exercise, we gathered 300 queries users posed with respect to 50 search task scenarios. The scenarios were developed from real cases of people seeking health information through posting requests of help on a web forum. The presence of query variations for a single scenario helped us capturing the variable quality at which queries are posed. Queries were created in English and then translated into other languages. A total of 49 runs by 10 different teams were submitted for the English query topics; 2 teams submitted 29 runs for the multilingual topics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>nástroj, který pomůže uživateli vybrat vhodnou veřejnou licenci jeo jeho data nebo software</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Customizable tool that will help user select the right public license for his data or software</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CLARIN Concept Registry (CCR, Rejstřík konceptů CLARIN) je součást Infrastruktury CLARIN a místo, kde se definuje společná a sdílená sémantika. To je důležité pro dosažení sémantické interoperability a pro překonání různosti datových struktur v metadatech a lingvistických zdrojích, se kterou se v rámci infrastruktury setkáváme.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The CLARIN Concept Registry (CCR) is the place in the CLARIN Infrastructure where common
and shared semantics are defined. This is important to achieve semantic interoperability, and to overcome to a degree the diversity in data structures, either in metadata or linguistic resources, encountered within the infrastructure. Whereas in the past, CLARIN has been using the ISOcat registry for these purposes, nowadays this new CCR registry is being used, as ISOcat turned out to have some serious drawbacks as far as its use in the CLARIN community is concerned. The main difference between both semantic registries is that the new CCR registry is a concept
registry whereas ISOcat is a data category registry. In this paper we describe why this choice has been made. We also describe the most important other characteristics of the new (Open)SKOSbased registry, as well as the management procedures used to prevent a recurrent proliferation of entries, as was the case with ISOcat.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku se zabýváme jedním typem strukturní odlišnosti mezi českými a anglickými paralelními větami v dvojjazyčném korpusu PCEDT, konkrétně vzájemným mapováním valenčních doplnění aktorového typu a (nevalenčních) lokačních doplnění. Analýzou korpusových příkladů ukazujeme, do jaké míry se na rozdílech ve valenci sloves v překladově ekvivalentních větách podílejí jazykově specifické- syntakticko-sémantické preference konkrétních slov na pozicích subjektu a predikátu</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the talk, we discuss one particular type of structural difference between Czech and English parallel sentences in a bilingual dependency treebank PCEDT, i.e., the mapping of valency complementation of teh ACT type and the free modification of the LOC type. In the analysis of corpus sentences we comment on the impact of the language specific syntacto-semantic preferences of concrete words in the position of subject and predicate on the differences in valency patterns of verbs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>PDiT 2.0 je novou verzí Pražského diskurzního korpusu. Přináší komplexní anotaci diskurzních jevů, obohacenou o anotaci sekundárních konektorů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>PDiT 2.0 is a new version of the Prague Discourse Treebank. It contains a complex annotation of discourse phenomena enriched by the annotation of secondary connectives.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme experimenty s použitím rozlišení významu slov (WSD) ve strojovém překladu. Zaměřujeme se na WSD u sloves a používáme dva různé přístupy -- slovesné vzorce založené na metodě corpus pattern analysis a významy sloves z valenčního slovníku. Vyhodnocujeme několik způsobů, jak použít významy sloves jako dodatečný faktor ve strojovém překladači Moses. Výsledky ukazují statisticky signifikantní zlepšení kvality překladu z pohledu metriky BLEU pro přístup s valenčním slovníkem, ale v ruční evaluaci vycházejí obě metody WSD jako přínos.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe experiments in Machine Translation using word sense disambiguation (WSD) information. This work focuses on WSD in verbs, based on two different approaches -- verbal patterns based on corpus pattern analysis and verbal word senses from valency frames. We evaluate several options of using verb senses in the source-language sentences as an additional factor for the Moses statistical machine translation system.
Our results show a statistically significant translation quality improvement in terms of the BLEU metric for the valency frames approach, but in manual evaluation, both WSD methods bring improvements.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento představuje nástroj MT-ComparEval pro kvalitativní vyhodnocení strojového překladu. MT-ComparEval je opensourcový nástroj, který byl navržen tak, aby pomáhal vývojářům strojových překladačů díky grafickému rozhraní, které umožňuje porovnávat a vyhodnocovat různé překladače či experimenty a nastavení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper showcases the MT-ComparEval tool for qualitative evaluation of machine translation (MT). MT-ComparEval is an opensource
tool that has been designed in order to help MT developers by providing a graphical user interface that allows the comparison
and evaluation of different MT engines/experiments and settings.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek reaguje na potřebu podpory strojového překladu pomocí vývojových cyklů s integrovanými evaluačními metodami. Naším cílem je zhodnotit, porovnat a vylepšit různé varianty strojových překladačů. Článek pojednává o nových nástrojích a praktikách, které podporují informovaný přístup k vývoji strojových překladačů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This work addresses the need to aid Machine Translation (MT) development cycles with a complete workflow of MT evaluation methods. Our aim is to assess, compare and improve MT system variants. We hereby report on novel tools and practices that support various measures, developed in order to support a principled and informed approach of MT development</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje náš systém pro aspektovou analýzu postojů (ABSA). Účastníme se podúkolu 1 (ABSA na úrovni vět), v jeho rámci se pak zaměřujeme na detekci kategorií aspektů. Trénujeme binární klasifikátory pro každou kategorii. Letošní rozšíření o další jazyky motivuje jazykově nezávislé přístupy. Navrhujeme využít neuronové sítě, které by měly automaticky detekovat v datech jazykové konstrukce, a tak omezit nutnost využívat jazykově závislé nástroje a ruční návrh rysů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This  paper  describes  our  system  for  aspect-based sentiment analysis (ABSA). We participate in Subtask 1 (sentence-level ABSA), focusing specifically on aspect category detection. We  train  a  binary  classifier  for  each category. This  year’s  addition  of  multiple languages  makes  language-independent  approaches  attractive. We  propose  to  utilize neural  networks  which  should  be  capable  of
discovering linguistic patterns in the data automatically,   thereby  reducing  the  need  for language-specific tools and feature engineering.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Diskriminativní překladové modely, které využívají zdrojový kontext, zlepšují kvalitu statistického strojového překladu. V tomto článku navrhujeme nové rozšíření, které navíc využívá i informace z cílového kontextu. Ukazujeme, že i takový model lze efektivně integrovat přímo do procesu dekódování. Náš přístup lze uplatnit i na velká trénovací data a jeho využití konzistentně zlepšuje kvalitu překladu u čtyř jazykových párů. Analyzujeme také zvlášť přínos zdrojového a cílového kontextu a ukazujeme, že toto rozšíření lépe zachycuje morfologickou shodu. Model je volně dostupný v rámci softwaru Moses.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Discriminative translation models utilizing source context have been shown to help statistical machine translation performance. We propose a novel extension of this work using target context information. Surprisingly, we show that this model can be efficiently integrated directly in the decoding process. Our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs. We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and target-context model and we show that our extension allows us to better capture morphological coherence. Our work is freely available as part of Moses.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Již v minulosti se ukázalo, že parafrázování referenčních překladů zlepšuje korelaci s lidským hodnocením při automatickém vyhodnocování strojového překladu. V této práci představujeme novou sadu dat k vyhodnocování anglicko-českého překladu založenou na automatických parafrázích. Tuto sadu porovnáváme s existující sadou ručně tvořených parafrází a zjišťujeme, že i automatické parafráze mohou hodnocení překladu zlepšit. Navrhujeme a vyhodnocujeme také několik kritérií pro výběr vhodných referenčních překladů z větší sady.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Paraphrasing of reference translations has been shown to improve the correlation with human judgements in automatic evaluation of
machine translation (MT) outputs. In this work, we present a new dataset for evaluating English-Czech translation based on automatic
paraphrases. We compare this dataset with an existing set of manually created paraphrases and find that even automatic paraphrases can
improve MT evaluation. We have also propose and evaluate several criteria for selecting suitable reference translations from a larger set.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje společnou praci UK a LMU nad anglické-českého a anglický-rumunského systémy frázových překladu pro WMT16. V porovnání s minulými pokusy, jsme přísně omezili trénovací daty na constrained, aby bylo možné spolehlivé porovnat naší systémy s jiné výzkumné systémy. My jsme zkoušeli využiti několika dalších modelů v naších systémech, včetně diskriminačního modelu frázových překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the phrase-based systems
jointly submitted by CUNI and LMU
to English-Czech and English-Romanian
News translation tasks of WMT16. In contrast
to previous years, we strictly limited
our training data to the constraint datasets,
to allow for a reliable comparison with
other research systems. We experiment
with using several additional models in our
system, including a feature-rich discriminative
model of phrasal translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>UDPipe je trénovatelný nástroj pro tokenizaci, tagging, lemmatizaci a závislostní parsing CoNLL-U souborů. UDPipe je jazykově nezávislý a pro natrénování jazykového modelu stačí označkovaná data v CoNLL-U formátu. Předtrénované jazykové modely jsou k dispozici pro téměř všechny UD korpusy. UDPipe je k dispozici jako spustitelný soubor, jako knihovna pro C++, Python, Perl, Java, C#, a také jako webová služba.

UDPipe je svobodný software licencovaný pod Mozilla Public License 2.0 a jazykové modely jsou k dispozici pro nekomerční použití pod licencí CC BY-NC-SA, nicméně původní data použitá k vytvoření modelů mohou v některých případech ukládat další licenční omezení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>UDPipe is an trainable pipeline for tokenization, tagging, lemmatization and dependency parsing of CoNLL-U files. UDPipe is language-agnostic and can be trained given only annotated data in CoNLL-U format. Trained models are provided for nearly all UD treebanks. UDPipe is available as a binary, as a library for C++, Python, Perl, Java, C#, and as a web service.

UDPipe is a free software under Mozilla Public License 2.0 and the linguistic models are free for non-commercial use and distributed under CC BY-NC-SA license, although for some models the original data used to create the model may impose additional licensing conditions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme naše softwarové nástroje pro NLP: UDPipe, open-source nástroj pro zpracování CoNLL-U souborů, který provádí tokenizaci, morfologickou analýzu, určování slovních druhů a závislostní parsing pro 32 jazyků; a NameTag, rozpoznávač pojmenovaých entit pro češtinu a angličtinu. Oba nástroje dosahují vynikajících výsledků, jsou dostupné s předtrénovanými modely a mají minimální nároky na čas a paměť počítače. UDPipe a NameTag lze také trénovat s použitím vlastních dat. Oba nástroje jsou open-source a jsou dostupné v licenci Mozilla Public License 2.0 (software) a CC BY-NC-SA (data). Program, C++ knihovna s vazbami na Python, Perl, Java a C# a také online web service a demo jsou dostupné na odkazech http://ufal.mff.cuni.cz/udpipe a http://ufal.mff.cuni.cz/nametag.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our in-house software tools for NLP: UDPipe, an open-source tool for processing CoNLL-U files which performs tokenization, morphological analysis, POS tagging and dependency parsing for 32 languages; and NameTag, a named entity recognizer for Czech and English. Both tools achieve excellent performance and are distributed with pretrained models, while running with minimal time and memory requirements. UDPipe and NameTag are also trainable with your own data. Both tools are open-source and distributed under Mozilla Public License 2.0 (software) and CC BY-NC-SA (data). The binary, C++ library with Python, Perl, Java and C# bindings along with online web service and demo are available at http://ufal.mff.cuni.cz/udpipe and http://ufal.mff.cuni.cz/nametag.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Při automatickém zpracování rozsáhlých textů v přirozeném jazyce se často opakují podobné úkoly v několika jazycích: i při zpracování obtížných úloh jsou texty vždy zpracovávány obvyklými základními kroky od tokenizace k parsingu. Představujeme mimořádně jednoduchý a použitelný nástroj pro základní zpracování přirozeného jazyka, který sestává pouze z jednoho programu a jednoho modelu (pro každý jazyk). Tento nástroj provádí tyto úkoly pro mnoho jazyků, aniž by vyžadoval dodatečná data. UDPipe je tedy nástroj, který zpracovává soubory ve formátu CoNLL-U a provádí tokenizaci, morfologickou analýzu, rozpoznávání slovních druhů, lematizaci a závislostní parsing pro téměř všechny jazyky korpusu Universtal Dependencies 1.2 (konkrétně je nástroj dostupný pro 32 jazyků). Navíc je celý nástroj snadno trénovatelný při použití vlastních trénovacích dat v CoNLL-U formátu a vyžaduje minimální znalost lingvistiky. Kód pro trénovaní nástroje je také dostupný.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Automatic natural language processing of large texts often presents recurring challenges in multiple languages: even for most advanced tasks, the texts are first processed by basic processing steps – from tokenization to parsing. We present an extremely simple-to-use tool consisting of one binary and one model (per language), which performs these tasks for multiple languages without the need for any other external data. UDPipe, a pipeline processing CoNLL-U-formatted files, performs tokenization, morphological analysis, part-of-speech tagging, lemmatization and dependency parsing for nearly all treebanks of Universal Dependencies 1.2 (namely, the whole pipeline is currently available for 32 out of 37 treebanks). In addition, the pipeline is easily trainable with training data in CoNLL-U format (and in some cases also with additional raw corpora) and requires minimal linguistic knowledge on the users’ part. The training code is also released.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jedním z cílů infrastruktury LINDAT/CLARIN je poskytovat technické prostředky pro sdílení nástrojů a dat pro výzkumné účely a v souladu s technickými požadavky CLARIN ERIC. Proto byl vybudován a je rozvíjen tento silně adaptovaný repozitářový systém založený na platformě DSpace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>One of the goals of LINDAT/CLARIN Centre for Language Research Infrastructure is to provide technical background to institutions or researchers who wants to share their tools and data used for research in linguistics or related research fields. The digital repository is built on a highly customised DSpace platform.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CLARIN SPF není jediná inter-federace, ale je první, která se systematicky zabývá problémy, které působí neuvolnění povinných atributů poskytovateli identit. Například, jsou-li data zveřejněná pod restriktivní licencí, musí být uživatel jednoznačně identifikovatelný v průběhu času. Pokud poskytovatel identity nezpřístupní potřebnou informaci, služba nemůže uživateli dovolit, aby data získal.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CLARIN Service Provider Federation is not the only inter-federation but it is the first one to systematically address the issue when Identity Providers do not release mandatory attributes to a service. For instance, if a data set is licensed under a restrictive license the user must be uniquely identifiable over time. However, if the Identity Provider does not release such information, the service cannot let the user download the data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku představujeme shortref.org - službu pro snadnou a trvalou citaci
dat vzešlých z výzkumu. Tuto službu poskytujeme formou zkracovače URL.
Reprodukovatelnost výsledků, důležitá součást vědeckého výzkumu, přímo závisí
na dostupnosti dat, nad kterými výzkum probíhal.
Rozvoj webových technologií velmi usnadnil sdílení dat. Kvůli
dynamické povaze webu se ale obsah často přesouvá z jednoho místa na druhé.
Běžné URL, které může výzkumník použít k citaci svých dat, nemá prostředky,
kterými by se s tímto přesouváním dalo vyrovnat. Čtenář/uživatel tak často
zjistí, že data se na dané URL již nenalézají, nebo dostane k dispozici novější
verzi.
Námi navrhované řešení, ve kterém je zkrácené URL perzistentním
identifikátorem, poskytuje spolehlivý mechanismus pro stálou dostupnost dat a
může tak zlepšit dopad výzkumu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present a easy-to-cite and persistent infrastructure (shortref.org) for research and data citation in the form a URL shortener service. Reproducibility of results is very important for the extension of research and is directly depends on the availability of the research data. The advancements in the web technologies made redistribution of the data much more easy nowadays, however, due to the dynamic nature of the web, the content is consistently on the move from one destination to another. The URLs researchers use for the citation of their contents do not directly account for these changes and many times when the users try to access the cited URLs, the data is either not available or moved to a newer version. In our proposed solution, the shortened URLs are not simple URLs but use persistent identifiers and provide a reliable mechanism to make the data always accessible that can directly improve the impact of research.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje společný systém projektů QT21 a HimL pro překlad z angličtiny do rumunštiny. Systém je automatickou kombinací 12 základních systémů připravených několika skupinami.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the joint submission
of the QT21 and HimL projects for
the English→Romanian translation task of
the ACL 2016 First Conference on Machine
Translation (WMT 2016). The submission
is a system combination which
combines twelve different statistical machine
translation systems provided by the
different groups (RWTH Aachen University,
LMU Munich, Charles University in
Prague, University of Edinburgh, University
of Sheffield, Karlsruhe Institute of
Technology, LIMSI, University of Amsterdam,
Tilde). The systems are combined
using RWTH’s system combination
approach. The final submission shows an
improvement of 1.0 BLEU compared to the
best single system on newstest2016.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme vysoce škálovatelný přístup k otevřenému question answeringu,
který nezávisí na datasetu mapujícím z logické formy na povrchovou, ani
na jakýchkoliv lingvistických nástrojích pro analýzu jako např. POS tagger nebo
rozpoznavání pojmenovaných entit. Popisovaný přístup je definován v rámci
frameworku Constrained Conditional Models, který umožnuje škálovat
znalostní grafy bez jakéhokoliv omezení velikosti. Ve standardním benchmarku jsme dosáhli
výsledky srovnatelné se state-of-the-art v otevřeném question answeringu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a highly scalable approach for open-domain question answering with no dependence on any logical form to surface form mapping data set or any linguistic analytic tool such as POS tagger or named entity recognizer. We define our approach under the Constrained Conditional Models framework which lets us scale to a full knowledge graph with no limitation on the size. On a standard benchmark, we obtained competitive results to state-of-the-art in open-domain question answering task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme vysoce škálovatelný přístup k otevřenému question answeringu,
který nezávisí na datasetu mapujícím z logické formy na povrchovou, ani
na jakýchkoliv lingvistických nástrojích pro analýzu, jako např. POS tagger nebo
rozpoznavání pojmenovaných entit. Popisovaný přístup je definován v rámci
frameworku Constrained Conditional Models, který umožnuje škálovat
znalostní grafy bez omezení velikosti. Ve standardním benchmarku jsme dosáhli
4% zlepšení výsledků oproti state-of-the-art v otevřeném question answeringu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a highly scalable approach for open-domain question answering with no dependence on any data set for surface form to logical form mapping or any linguistic analytic tool such as POS tagger or named entity recognizer. We define our approach under the Constrained Conditional Models framework which lets us scale up to a full knowledge graph with no limitation on the size. On a standard benchmark, we obtained near 4 percent improvement over the state-of-the-art in open-domain question answering task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje 12 systémů zaslaných do úlohy strojového překladu textů z IT domény v rámci WMT16. Systémy překládají z angličtiny do 6 jazyků: baskičtiny, bulharštiny, holandštiny, češtiny, portugalštiny a španělštiny. Všechny systémy byly vyvinuty v rámci projektu QTLeap. Pro každý jazykový pár byly zaslány dva systémy: frázový (Moses) a hloubkově-syntaktický (TectoMT, krom bulharštiny). Pro 4 z 6 jazyků dosáhlo TectoMT lepších výsledků než Moses.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the description of 12
systems submitted to the WMT16 IT-task,
covering six different languages, namely
Basque, Bulgarian, Dutch, Czech, Portuguese
and Spanish. All these systems
were developed under the scope of the
QTLeap project, presenting a common
strategy. For each language two different
systems were submitted, namely a phrase-based
MT system built using Moses, and
a system exploiting deep language engineering
approaches, that in all the languages
but Bulgarian was implemented
using TectoMT. For 4 of the 6 languages,
the TectoMT-based system performs better
than the Moses-based one.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje náš systém UFAL_MULTIVEC v soutěži WMT16 Quality Estimation. Systém odhaduje kvalitu strojového překladu z angličtiny do němčiny. Náš přínos spočívá v rozšíření zavedené metody o dvojjazyčné vektorové reprezentace slov a o slovní zarovnání.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes our submission
UFAL MULTIVEC to the WMT16 Quality
Estimation Shared Task, for EnglishGerman
sentence-level post-editing effort
prediction and ranking. Our approach exploits
the power of bilingual distributed
representations, word alignments and also
manual post-edits to boost the performance
of the baseline QuEst++ set of
features. Our model outperforms the
baseline, as well as the winning system
in WMT15, Referential Translation Machines
(RTM), in both scoring and ranking
sub-tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá studiem diskurzních konektorů umístěných mimo oba argumenty, které jsou jimi spojeny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>During annotation of discourse connectives in Czech written texts, externally placed connectives were found (outside both their arguments and non-adjacent to them). These cases occur in vast majority in connection with attribution (reporting).
In these structures, a connective expressing contrast between two reported contents moves left in order to stand closer to its left (first in the linear order) argument. We call this phenomenon connective movement.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V rámci přednásky byly představeny existující teorie anotace diskurních jevů a nástroje, které se pro tento typ anotací používají.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Discourse coherence is a complex natural language phenomenon which is achieved by different linguistic means (e.g., anaphoricity, information structure, discourse markers and connectives, rhetorical structure of text, etc.). Many approaches in computational linguistics are used to capture discourse relations and Tind practical applications. This session focuses on discussing theories and approaches applied to the annotation of discourse phenomena in different languages, such as Rhetorical Structure Theory, Penn Discourse Treebank, Segmented Discourse Representation Theory and so on. Participants will have the opportunity to compare these approaches to see which phenomena are central to them and which ones are less prominent. We will also introduce the tools of discourse annotation and demonstrate esp. TrEd, PDTB and MMAX.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace software, který pomáhá autorovi vybrat pro jeho dílo nejvhodnější (nejsvobodnější) veřejnou licenci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presenting a software solution that helps authors choose the best public license for their works.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek prezentuje nový online interaktivní nástroj vydaná pod OpenSource licensí MIT, který usnadňuje uživateli pomocí sekvence otázek a jejich vysvětlení výběr tzv. veřejné licence, pokud je ji možno jeho výsledku (datům nebo software) přiřadit. Cílem nástroje je podpořit nejotevřenější licencování vědeckých výsledků, které je v souladu se zákony možné, a vůbec podpora autorů v tom, aby své výsledky zvěřejňovali a dokázali jim přiřadit vhodnou licenci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Researchers  in  Natural  Language  Processing rely  on  availability  of  data  and  software, ideally under open licenses, but little is done to actively encourage it. In fact, the current Copyright framework grants exclusive rights to authors to copy their works, make them available to the public and make derivative works (such as annotated language corpora). Moreover, in the EU databases are protected against unauthorized extraction and re-utilization of their contents. Therefore,  proper  public  licensing  plays  a  crucial  role  in providing access to research data.  A public license is a license that grants certain rights not to one particular user, but to the general public (everybody). Our article presents a tool that we developed and whose purpose is to assist the user in the licensing process. As software and data should be licensed under different licenses, the tool is composed of two separate parts: Data and Software. The underlying logic as well as elements of the graphic interface are presented.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Spojité distribuované slovní reprezentace prokázaly svoji užitečnost v mnoha úlohách zpracování přirozeného jazyka. V tomto článku používáme vektorové reprezentace slov jako další vstupy pro MST parser. Použití distribuovaných reprezentací umožňuje snížit dimenzionalitu lexikálních rysů a neuronovou sítí odpadá nutnost ručního ladění kombinací rysů. Přestože jeho úspěšnost je nižší, než u klasického MST Parseru, výsledný model je výrazně menší než modely využívající klasickou sadu rysů. Navíc funguje velice dobře pro jazyky, pro které je dostupný pouze relativně malý treebank a výsledky vypadají velice slibně i pro delexikalizovaný parsing.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Continuous word representations appeared to be a useful feature in many natural language processing tasks.  Using fixed-dimension
pre-trained word embeddings allows avoiding sparse bag-of-words representation and to train models with fewer parameters.  In this
paper,  we  use  fixed  pre-trained  word  embeddings  as  additional  features  for  a  neural  scoring  function  in  the  MST  parser.   With  the
multi-layer architecture of the scoring function we can avoid handcrafting feature conjunctions. The continuous word representations on
the input also allow us to reduce the number of lexical features, make the parser more robust to out-of-vocabulary words, and reduce the
total number of parameters of the model.  Although its accuracy stays below the state of the art, the model size is substantially smaller
than with the standard features set.  Moreover, it performs well for languages where only a smaller treebank is available and the results
promise to be useful in cross-lingual parsing.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku se zabýváme získáváním textových informací z fotografií. Dosud se v této oblasti pracovalo pouze s rozpoznáváním izolovaných slov a krátká slova se pro zjednodušení vynechávala. Tento přístup není vhodný pro další lingvistické zpracování rozpoznaného textu. Proto se pokoušíme a lepší definice úlohy, která zahrnuje i souvislost rozpoznaného textu. Rozšířili jsme anotaci stávajících datastetů a vyvinuli evaluační metriku, které bude sloužit k hodnocení rozpoznávání souvislého textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we deal with extraction of textual information from scene images.  So far, the task of Scene Text Recognition (STR) has only been focusing on recognition of isolated words and, for simplicity, it omits words which are too short.  Such an approach is not suitable for further processing of the extracted text.  We define a new task which aims at extracting coherent blocks of text from scene images with regards to their future use in natural language processing tasks, mainly machine translation.  For this task, we enriched the annotation of existing STR benchmarks in English and Czech and propose a string-based evaluation measure that highly correlates with human judgment.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Neuronové sekvenční modely jsou velmi slibným novým paradigmatem ve strojovém překladu, které dosahuje srovnatelných výsledků s frázovým strojovým překladem. Tento článek popisuje systém, se kterým se Univerzita Karlova účastnila soutěže při WMT16 v úkolech automatické post-editace strojového překladu a multimodálního strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Neural sequence to sequence learning recently became a very promising paradigm in machine translation, achieving competitive results with statistical phrase-based systems. In this system description paper, we attempt to utilize several recently published methods used for neural sequential learning in order to build systems for WMT 2016 shared tasks of Automatic Post-Editing and Multimodal Machine Translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V prezentaci jsem ukázala jak se dá vyhledávat v korpusech z repozitáře Lindat v KonTextu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I described describe how KonText – a corpus query interface for Czech National Corpus is adopted for handling various corpora from Lindat. The repository contains corpora with different types of annotation – like syntactic, shallow semantic, sentiment and other types. I showed how to search for this information within the KonText environment using CQL (Corpus Query Language) on the example of two corpora. First, I focused on the Universal Dependencies – syntactically annotated treebanks for several languages. Secondly, I demonstrated the queries over the Prague Dependency Treebank that are related both to syntactic (analytical) and deep syntactic (tectogrammatical) layers. I also showed several query examples from other Lindat corpora.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V posteru jsem popsala jak víceslovné výrazy se dají reprezentovat a vyhledávít v systému KonText.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the poster, I demonstrated how multiword expressions can be represented and quired in a corpus query system KonText</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku popisujeme jak se dá vyhledávat v syntakticky anotovaných korpusech v KonTextu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we describe an addition to the corpus query system Kontext that enables to enhance the search using syntactic attributes in addition to the existing features, mainly lemmas and morphological categories. We present the enhancements of the corpus query system itself, the attributes we use to represent syntactic structures in data, and some examples of querying 
the syntactically annotated corpora, such as treebanks in various languages as well as an automatically parsed large corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vztah mwe se v současném vydání Universal Dependencies nepoužívá konzistentně. To může být zčásti způsobeno rozdíly mezi jazyky, nicméně v našem příspěvku ukazujeme na blízce příbuzných jazycích, že tomu tak není vždy; i doslovně ekvivalentní výrazy jsou v některých případech anotovány rozdílně.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The UD relation mwe is not used consistently in the current release of Universal Dependencies. While part of the issue may be caused by true linguistic differences, we demonstrate on closely related languages that it is not always the case; even literally equivalent expressions do not always receive the same analysis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se soustřeďuje na včlenění valenčního slovníku do systému TectoMT pro jazykový pár čeština-ruština. Ukazuje na chyby ve valenci na výstupu ze systému a popisuje, jak začlenění slovníku ovlivnilo výsledky. Ačkoli podle BLEU skóre nebylo zaznamenáno zlepšení, ruční inspekce prokázala zlepšení v některých konkrétních případech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we focus on the incorporation of a valency lexicon into TectoMT system for Czech-Russian language pair.  We demonstrate valency errors in MT output and describe how the introduction of a lexicon influenced the translation results.  Though there was no impact on BLEU score, the manual inspection of concrete cases showed some improvement.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje paralelní korpusy s automatickou anotací pomocí několika nástrojů pro zpracování přirozeného jazyka, včetně lemmatizace, taggingu, rozpoznání a klasifikace pojmenovaných entit a jejich disambiguace, word-sense disambiguation  a koreference.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This work presents parallel corpora automatically annotated with several NLP tools, including lemma and part-of-speech tagging,
named-entity recognition and classification, named-entity disambiguation, word-sense disambiguation, and coreference.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje datovou sadu vytvořenou z přirozených dialogů, která umožňuje testovat schopnost dialogových systémů učit se nová fakta od uživatelů v průběhu dialogu. Toto interaktivní učení pomůže s jedním z nejpodstatnějších problémů dialogových systémů v otevřených doménách, kterým  je omezenost dat, se kterými je dialogový systém schopen pracovat. Představovaná datová sada, skládající se z 1900 dialogů, umožňuje simulaci interaktivního získávání rad od uživatelů v podobě odpovědí a podrobných vysvětlení otázek, které mohou být použity pro interaktivní učení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a dataset collected from natural dialogs which enables to test the ability of dialog systems to learn new facts from user utterances throughout the dialog. This interactive learning will help with one of the most prevailing problems of open domain dialog system, which is the sparsity of facts a dialog system can reason about. The proposed dataset, consisting of 1900 collected dialogs, allows simulation of an interactive gaining of denotations and questions explanations from users which can be used for the interactive learning.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dataset sesbíraný z přirozených dialogů umožňující testovat schopnost dialogových systémů učit se nová fakta z uživatelských výpovědí v průběhu dialogu. Dataset, skládající se z 1900 dialogů umožňuje simulovat interaktivní získávání denotací a vysvětlení k otázkám od uživatelů ve formě, která je vhodná pro interaktivní učení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Dataset collected from natural dialogs which enable to test the ability of dialog systems to interactively learn new facts from user utterances throughout the dialog. The dataset, consisting of 1900 dialogs, allows simulation of an interactive gaining of denotations and questions explanations from users which can be used for the interactive learning.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Chimera systém strojovéo překladu, který kombinuje hluboce lingvistické jádro TectoMT s frázovým strojovým překladačem Moses. Pro anglicko-český překlad také používá post-editovací systém Depfix. Všechny komponenty běží na platformě Unix/Linux a jsou open-source (dostupné z Perlového repozitáře CPAN a repozitáře LINDAT/CLARIN). Hlavní webová stránka je https://ufal.mff.cuni.cz/tectomt. Vývoj je momentálně podporován projektem QTLeap ze 7th FP (http://qtleap.eu).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Chimera is a machine translation system that combines the TectoMT deep-linguistic core with phrase-based MT system Moses. For English–Czech pair it also uses the Depfix post-correction system. All the components run on Unix/Linux platform and are open source (available from Perl repository CPAN and the LINDAT/CLARIN repository). The main website is https://ufal.mff.cuni.cz/tectomt. The development is currently supported by the QTLeap 7th FP project (http://qtleap.eu).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme MLFix, systém pro automatickou statistickou post-editaci, který je duchovním
následníkem pravidlového systému, Depfixu. Cílem této práce bylo prozkoumat možné postupy
automatické identifikace nejčastějších morfologických chyb tvořených současnými systémy pro strojový
překlad a natrénovat vhodné statistické modely, které by byly postaveny na získaných znalostech.
Provedli jsme automatickou i ruční evaluaci našeho systému a výsledky porovnali s Depfixem. Systém
byl vyvíjen především na výstupech anglicko-českého strojového překladu, cílem
ale bylo zobecnit post-editační proces tak, aby byl aplikovatelný na další jazykové páry. Upravili jsme
původní pipeline, aby post-editovala výstupy anglicko-německého strojového překladu, a provedli
dodatečnou evaluaci této modifikace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present MLFix, an automatic statistical post-editing system, which is a spiritual successor to the rulebased
system, Depfix. The aim of this
thesis was to investigate the possible approaches to automatic identification of the most common
morphological errors produced by the state-of-the-art machine translation systems and to train sufficient
statistical models built on the acquired knowledge. We performed both automatic and manual evaluation
of the system and compared the results with Depfix. The system was mainly developed on the English-toCzech
machine translation output, however, the aim was to generalize the post-editing process so it can
be applied to other language pairs. We modified the original pipeline to post-edit English-German
machine translation output and performed additional evaluation of this modification.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje náš příspěvek do optimalizační soutěže WMT16 Tuning Task. Grid search ve standardní implementaci MERT nahrazujeme optimalizací pomocí metody reje částic.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes our submission to the
Tuning Task of WMT16. We replace the
grid search implemented as part of standard
minimum-error rate training (MERT)
in the Moses toolkit with a search based
on particle swarm optimization (PSO). An
older variant of PSO has been previously
successfully applied and we now test it
in optimizing the Tuning Task model for
English-to-Czech translation. We also
adapt the method in some aspects to allow
for even easier parallelization of the
search.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek analyzuje úkoly 41. ročníku lingvistické soutěže pro žáky ZŠ a studenty SŠ.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper analyzes the tasks of the 41st year of the linguistic competition for primary and secondary school pupils.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem této práce je ukázat na datech Pražského závislostního korpusu, že domnělý volný slovosled v češtině ve skutečnosti volný není, a že je řízen jistými pravidly, odlišnými od pravidel gramatiky, které určují slovosled v jiných evropských jazycích, jako např. v angličtině, němčině nebo francouzštině. Na nově anotovaných datech je rovněž testována implementace algoritmu pro rozdělení věty na základ a ohnisko na základě kontextové zapojenosti uzlů na tektogramatické rovině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of the present contribution is to document, on the material of the Prague Dependency Treebank, that the assumed freedom of Czech word order is not really a freedom but that it is guided by certain principles, different from the grammatically given principles determining the word order in some other European languages such as English, German or French. On newly annotated data, an implementation of the algorithm for the division of the sentence into topic and focus based on contextual boundness of nodes at the tectogrammatical layer is tested.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Výzkum korpusových dat se zabývá interakcí vztahů asociativní anafory a aktuálního členění větného.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Corpus-based research demonstrates an existence of a mutual interaction of bridging anaphoric relations in the text and sentence information
structure.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve zprávě přestavujeme koncept aktuálního členění větného na základě funkčního generativního popisu. V první části zprávy prezentujeme základní pojmy spojené s aktuálním členěním větným – zejména kontextovou zapojenost a výpovědní dynamičnost a popisujeme operativní kritéria na rozlišení základu a ohniska: tzv. otázkový test a test s negací. V další části představujeme principy pro anotaci aktuálního členění větného v Pražském česko-anglickém závislostním korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the report, we introduce the concept of topic–focus articulation on the basis of Functional Generative Description. Firstly, we present the crucial terms connected with topic–focus articulation – mainly contextual boundness and communicative dynamism and we describe operational criteria how to detect topic and focus: the so-called question test and test by negation. In the next part, we present the annotation principles for annotation of topic–focus articulation in the Prague Czech-English Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Výroba a evaluace subjectivity lexikonu pro indonéštinu. Výsledný slovník byl pořízen za využití automatického strojového překladu a jeho spolehlivost byla ověřena v rámci trénování pravděpodobnostního klasifikátoru na ručně označkovaných datech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present subjectivity lexicon of positive and negative words for Indonesian language created by automatically translating English lexicons. We compare the lexicons in the task of predicting sentence polarity.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje přehled moderních trendů ve studiu jazykové morfologie z neurolingvistické perspektivy. Neurologický výzkum morfologie započal diskusí, zda jsou morfologicky komplexní slova reprezentována v mozku rozloženým způsobem, nebo uložena jako celé jednotky. Ani jeden z těchto pohledů neodpovídá zcela exterimentálním zjištěním, proto byl přijat kompromisní dvoucestný přístup. V článku je diskutovány faktory související s reprezentací slova jako transparence, četnost konstituentů nebo typ afixace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents an overview of recent trends in studying morphology
from the neurolinguistic perspective. Neurolinguistic investigations of morphology
started by discussing whether morphologically complex words are represented in the
brain in a decomposed manner or stored as whole forms. None of these could fully
account for available experimental findings, thus a compromise dual-route approach
was adopted. While nowadays there is a wide agreement that representations
of complex words have at least partial morphological structure, it is still not clear
what precisely determines whether such structure emerges for a particular word
or not; commonly mentioned factors are transparency, constituent frequency or
type of affixation. These factors are discussed in the paper, and an experiment is
proposed that can contribute to the current debate about the topic.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato studie popisuje první úkol v rámci SemEvalu, který je zaměřen na užití NLP systémů pro automatické generování slovníkových hesel podle metody Corpus Pattern Analysis.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the first SemEval task to
explore the use of Natural Language Processing
systems for building dictionary entries,
in the framework of Corpus Pattern Analysis.
CPA is a corpus-driven technique which provides
tools and resources to identify and represent
unambiguously the main semantic patterns
in which words are used. Task 15 draws
on the Pattern Dictionary of English Verbs
(www.pdev.org.uk), for the targeted lexical
entries, and on the British National Corpus
for the input text.
Dictionary entry building is split into three
subtasks which all start from the same concordance
sample: 1) CPA parsing, where arguments
and their syntactic and semantic categories
have to be identified, 2) CPA clustering,
in which sentences with similar patterns have
to be clustered and 3) CPA automatic lexicography
where the structure of patterns have to
be constructed automatically.
Subtask 1 attracted 3 teams, though none
could beat the baseline (rule-based system).
Subtask 2 attracted 2 teams, one of which beat
the baseline (majority-class classifier). Subtask
3 did not attract any participant.
The task has produced a major semantic multidataset
resource which includes data for 121
verbs and about 17,000 annotated sentences,
and which is freely accessible.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>VPS-GradeUp je kolekce trojnásobných manuálních anotací na 29 slovesných lemmatech popsaných v Pattern Dictionary of English Verbs (PDEV): abolish, act, adjust, advance, answer, approve, bid, cancel, conceive, cultivate, cure, distinguish, embrace, execute, hire, last, manage, murder, need, pack, plan, point, praise, prescribe, sail, seal, see, talk, urge. 
Anotace obsahuje dvě úlohy:
1) odstupňované hodnocení vhodnosti každého slovníkového významu (přesněji syntakticko-sémantického vzorce užívání) definovaného PDEV na každé z padesáti vět náhodně vybraných z BNC pro každé zkoumané sloveso
2) klasickou značkovací úlohu, kdy anotátor pro každou větu vybere nejvhodnější slovníkový význam. Tato úloha je obohacena o detaily tzv. exploitations -- čím se slovesné užití v daném kontextu odchyluje od přiřazeného vzorce užití.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>VPS-GradeUp is a collection of triple manual annotations of 29 English verbs based on the Pattern Dictionary of English Verbs (PDEV) and comprising the following lemmas: abolish, act, adjust, advance, answer, approve, bid, cancel, conceive, cultivate, cure, distinguish, embrace, execute, hire, last, manage, murder, need, pack, plan, point, praise, prescribe, sail, seal, see, talk, urge . It contains results from two different tasks:

    Graded decisions
    Best-fit pattern (WSD) .

In both tasks, the annotators were matching verb senses defined by the PDEV patterns with 50 actual uses of each verb (using concordances from the BNC). The verbs were randomly selected from a list of completed PDEV lemmas with at least 3 patterns and at least 100 BNC concordances not previously annotated by PDEV’s own annotators. Also, the selection excluded verbs contained in VPS-30-En, a data set we developed earlier. This data set was built within the project Reviving Zellig S. Harris: more linguistic information for distributional lexical analysis of English and Czech and in connection with SEMEVAL 2015.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku je popsán CzEngVallex, dvojjazyčný valenční slovník, který páruje valenční rámce a jejich členy. Je založen na parelelním česko-anglickém korpusu, konkrétně na Pražském česko-anglickém závislostním korpusu. V tomto korpusu je pro každý výskyt slovesa uveden odkaz na rámec v příslušném českém nebo anglickém valenčním slovníku. CzEngVallex pak přidává párování slovesných rámců a argumentů v nich. Umožňuje tak studium valence v kombinaci s překladem mezi češtinou a angličtinou. CzEngVallex je k dispozici veřejně online, a umožňuje prohlížení a základní prohledávání a zobrazení párovaných rámců a zároveň i konrétních příkladů z korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe CzEngVallex, a bilingual English-Czech valency lexicon which aligns verbal valency frames and their arguments. It is based on a parallel Czech-English corpus, the Prague Czech-English Dependency Treebank, where for each occurrence of a verb, a reference to the underlying Czech and English valency lexicons is recorded. CzEngVallex then pairs the entries (verb senses) of the two lexicons, and allows for detailed studies of verb valency and argument structure in translation.  The CzEngVallex lexicon is now accessible online, and the demo will show all the possibilities for browsing and searching the lexicon and their examples from the PCEDT corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Objem nestrukturovaných dat stále roste, rozvoj Webu 2.0 přináší množství textů generovaných samotnými uživateli Internetu. Jejich příspěvky nezřídka obsahují subjektivní názory, emoce, hodnocení… K čemu a jak můžeme tato data použít? Je možné emoce v textu spolehlivě strojově třídit? Příspěvek z oblasti sentiment analysis představí metody a úspěchy automatické extrakce emocí z textu s důrazem na česká data a aplikace pro byznys.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The volume of unstructured data is constantly increasing, the rise of Web 2.0 brings lots of texts created by the Internet users. Their comments are usually full of subjective opinions, emotions, evaluation... Why and how to use this data? Is it possible to categorize emotions automatically? A contribution from the area of sentiment analysis introduce methods and outputs of automatic emotion extraction with the emphasis on Czech data and business applications.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dizertační práce má dva hlavní cíle. Za prvé přináší analýzu jazykových prostředků,
které společně formují emocionální význam psaných výpovědí v češtině. Za druhé
využívá zjištění týkající se emocionálního jazyka v komputačních aplikacích.
Podáváme systematický přehled lexikálních, morfosyntaktických, sémantických
a pragmatických aspektů emocionálního významu v českých výpovědích
a navrhujeme formální reprezentaci emocionálních struktur v rámci Pražského
závislostního korpusu a konstrukční gramatiky.
V oblasti komputačních aplikací se zaměřujeme na témata postojové analýzy,
tedy automatické extrakce emocí z textu. Popisujeme tvorbu ručně anotovaných
emocionálních zdrojů dat a řešíme dvě základní úlohy postojové analýzy, klasifikaci
polarity a identifikaci cíle hodnocení. V obou těchto úlohách dosahujeme
uspokojivých výsledků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This thesis has two main goals. First, we provide an analysis of language means
which together form an emotional meaning of written utterances in Czech. Second,
we employ the findings concerning emotional language in computational
applications.
We provide a systematic overview of lexical, morphosyntactic, semantic and
pragmatic aspects of emotional meaning in Czech utterances. Also, we propose
two formal representations of emotional structures within the framework of the
Prague Dependency Treebank and Construction Grammar.
Regarding the computational applications, we focus on sentiment analysis, i.e.
automatic extraction of emotions from text. We describe a creation of manually
annotated emotional data resources in Czech and perform two main sentiment
analysis tasks, polarity classification and opinion target identification on Czech
data. In both of these tasks, we reach the state-of-the-art results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška má dva hlavní cíle. Za prvé předložíme analýzu lexikálních, morfosyntaktických, sémantických a pragmatických jazykových prostředků, které společně formují emocionální význam psaných výpovědí v češtině, a podáme návrh formální reprezentace emocionálních struktur. Za druhé se zaměříme na komputační aplikace, které zjištění týkající se emocionálního jazyka využívají. Budeme se věnovat tématům z oblasti postojové analýzy, tedy automatické extrakce emocí z textu. Popíšeme tvorbu ručně anotovaných emocionálních zdrojů dat a řešení dvou základních úloh postojové analýzy, klasifikace polarity a identifikace cíle hodnocení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk has two main goals. First, we provide an analysis of language means which together form an emotional meaning of written utterances in Czech. Second, we employ the findings concerning emotional language in computational
applications.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku popisujeme experimenty s neřízeným závislostním parsingem bez použití jakéhokoliv slovnědruhových kategorií naučených z manuálně označkovaných korpusů. Používáme pouze neřízeně naučené slovní třídy a prezentujeme tedy plně neřízenou syntaktickou analýzu věty z prostého textu Ukazuje se, že výsledky nejsou o mnoho horší, než ty, které využívají anotovaná data.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present experiments with unsupervised dependency parser without using any part-of-speech tags learned from manually annotated data. We use only unsupervised word-classes and therefore propose fully unsupervised approach of sentence structure induction from a raw text. We show that the results are not much worse than the results with supervised part-of-speech tags.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>LiStr - Sada nástrojů pro odvození lingvistické struktury. Obsahuje nástroje a podpůrné skripty pro
indukci závislostních stromů z textu s přiřazenými tagy. V této verzi je navíc automatický odhadovač slovních druhů. Nástroj UDPC byl vylepšen, nyní umí pracovat i s neřízenými slovními třídami a s apriorními pravděpodobnostmi z kolekce korpusů "universal dependencies".</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>LiStr - a toolkit for induction of linguistic structures without annotated corpora. This is the second version of the toolkit, released at December 2015. It comprises the tools and supporting scripts for induction of dependency trees from texts with already assigned part-of-speech tags. In this version, the delexicalized part-of-speech tag guesser is comprised. The UDPC was improved, now it may work with unsupervised word-classes and with universal-dependencies prior probabilities.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje systém rozlišování významu sloves na základě paralelního korpusu a paralelního valenčího slovníku, který rovněž obsahuje informaci o párování významu sloves a jejich argumentů. Jedná se o rozšíření předchozí metody publikované v roce 2014. Metoda byla testována na angličtině i češtině a v obou případech dala signifikantně lepší výsledky než metoda předchozí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a system for verbal Word Sense Disambiguation (WSD) that is able to exploit additional information from parallel texts and lexicons. It is an extension of our previous WSD method, which gave promising results but used only monolingual features. In the follow-up work described here, we have explored two additional ideas: using English-Czech bilingual resources (as features only - the task itself remains a monolingual WSD task), and using a 'hybrid' approach, adding features extracted both from a parallel corpus and from manually aligned bilingual valency lexicon entries, which contain subcategorization information. Albeit not all types of features proved useful, both ideas and additions have led to significant improvements for both languages explored.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nový systém pro generování přirozeného jazyka založený na sytaxi, který je možné trénovat z nezarovnaných párů vstupních reprezentací významu a výstupních vět. Dělí se na větný plánovač, který inkrementálně staví hloubkově syntaktické závislostní stromy, a povrchový realizátor. Větný plánovač je založen na A* vyhledávání s perceptronovým rankerem, který používá nové updaty na základě odlišných podstromů a jednoduchý odhad budoucího potenciálu stromů; povrchová realizace je zajištěna pravidlovým systémem z prostředí Treex.
První výsledky ukazují, že trénování z nezarovnaných dat je možné, výstupy našeho generátoru jsou většinou plynulé a relevantní.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a novel syntax-based natural language generation system that is trainable from unaligned pairs of input meaning representations and output sentences. It is divided into sentence planning, which incrementally builds deep-syntactic dependency trees, and surface realization. Sentence planner is based on A* search with a perceptron ranker that uses novel differing subtree updates and a simple future promise estimation; surface realization uses a rule-based pipeline from the Treex NLP toolkit.
Our first results show that training from unaligned data is feasible, the outputs of our generator are mostly fluent and relevant.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Strojový překladač TectoMT byl během posledního roku vylepšen tak, aby umožňoval snadnější přidávání nových překladových jazykových párů.
K tomu se používají multilinguální standardy pro morfologickou a syntaktickou anotaci a také jazykově nezávislé moduly s pravidly.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The TectoMT tree-to-tree machine translation system has been updated this year to support easier retraining for more translation directions. We use multilingual standards for morphology and syntax annotation and language-independent base rules. We include a simple, non-parametric way of combining TectoMT’s transfer model outputs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto přípsěvku jsme se zabývali distribucí slovesných a jmenných valenčních doplnění v konstrucích s tzv. funkčními slovesy. Dále jsme vymezili principy, podle kterých je utvářena povrchověsyntaktická struktura věty s těmito slovesy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this talk, the distribution of verbal and nominal valency complementations in the syntactic structure of light verb constructions is debated. Esp. principles governing the surface form of these constructions are proposed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Komplexní predikáty s funkčními slovesy představují z hlediska syntaktického popisu problematický jev. Příspevěk se zabývá především distribucí valenčních doplnění funkčního slovesa a predikativního jména v syntaktické struktuře věty. Na základě syntaktické analýzy pak navrhuje teoreticky adekvátní a přitom ekonomické zachycení komplexních predikátů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Complex predicates with light verbs have
proven to be very challenging for syntactic
theories, particularly due to the tricky distribution of valency complementations of
light verbs and predicative nouns in their syntactic structure. We propose a theoretically adequate
and economical representation of complex
predicates  with  Czech  light  verbs  based
on a division of their description between
the lexicon and the grammar.  We demonstrate that a close interplay between these
two components makes the analysis of the
deep  and  surface  syntactic  structures  of
complex predicates reliable and efficient.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku se zabýváme vlastní (syntaktickou) reflexivitou – tedy případy, kdy reflexiva se/si jsou klitickými tvary reflexivního zájmena, vyjadřujícího referenční totožnost aktantu v subjektové pozici (typicky Konatele děje) a aktantu vyjádřeného daným zájmenem. K diskusím o tom, zda reflexivní klitika se/si a po řadě jejich dlouhé tvary sebe/sobě u tzv. vlastních reflexiv plní ve větě stejnou funkci, chceme přispět analýzou jejich vlivu na kongruenci s povrchověsyntaktickým doplňkem uvozeným (zejména) výrazem jako. Předmětem naší argumentace jsou významové distinkce mezi reflexivními konstrukcemi vyplývající z rozdílů ve shodě doplňku a dále ověření, či vyvrácení obvyklého popisu, podle kterého rozdíl klitické a dlouhé formy reflexiva má vliv na udělování pádu doplňku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we deal with syntactic reflexivity. In this case, the reflexives represent the clitic forms of the reflexive pronoun expressing referential identity of the valency complementation realized in subject (usually of ACTor) and of the valency complementation lexically expressed by the given pronoun. This linguistic phenomenon is studied here especially in relation to the agreement of a surface syntactic complement (with the function of EFF or COMPL). We aim to demonstrate that the case of complement is rather determined by the meaning of the reflexive construction (as in non-reflexive structures) than by the distinction of the clitic vs. long form of the reflexives.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Sdílení vědeckých dat se stalo pro data driven science základní službou a může vědecký výzkum zasadně zlepšit tím, že zpřístupní spolehlivá a důvěryhodná data. Aby byly takovéto služby pro sdílení vědeckých dat ve vědeckých procech použitelné a užitečné, musejí splnit řadu požadavků na vyhledatelnost a přístupnost dat. Služba B2SHARE vyvinutá v projektu EUDAT tyto požadavky plní pro řadu vědeckých oborů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Scientific data sharing is becoming an essential service for data driven science and can significantly improve the scientific process by making reliable, and trustworthy data available. Thereby reducing redundant work, and providing insights on related research and recent advancements. For data sharing services to be useful in the scientific process, they need to fulfill a number of requirements that cover not only discovery, and access to data. But to ensure the integrity, and reliability of published data as well. B2SHARE, developed by the EUDAT project, provides such a data sharing service to scientific communities. For communities that wish to download, install and maintain their own service, it is also available as software. B2SHARE is developed with a focus on user-friendliness, reliability, and trustworthiness, and can be customized for different organizations and use-cases. In this paper we discuss the design, architecture, and implementation of B2SHARE. We show its usefulness in the scientific process with some case studies in the biodiversity field.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Úloha 18 na SemEval 2015 definuje sémantickou závislostní analýzu (SDP) se širokým pokrytím jako problém nalezení vztahů mezi predikátem a jeho argumenty ve větě pro všechna plnovýznamová slova, tj. sémantické struktury, která představuje relační jádro významu věty. V tomto článku, který úlohu 18 představuje, zasazujeme tento problém do rámce dalších podúloh analýzy jazyka, představujeme a srovnáváme použité cílové reprezentace sémantických závislostí a shrnujeme zadání úlohy, informace o zúčastněných systémech, jakož i hlavní výsledky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Task 18 at SemEval 2015 defines Broad-Coverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicate-argument relationships for all content words, i.e. the semantic structure constituting the relational core of sentence meaning. In this task description, we position the problem in comparison to other language analysis sub-tasks, introduce and compare the semantic dependency target representations used, and summarize the task setup, participating systems, and main results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>SHAMUS (ÚFAL Multimediálny vyhľadávací systém) je systém na jednoduché vyhľadávania a navigáciu v multimediálnych archívoch. Systém pozostáva z textového vyhľadávania, automatického výberu dôležitých segmentov a prepájania súvisiacich video segmentov.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>SHAMUS (UFAL Search and Hyperlinking Multimedia System) is a system for an easy search and navigation in multimedia archives. The system consists of a text-based Search, an automatic selection of Anchoring segments and video-based retrieval of Hyperlinks.

The segments of the videos in a collection relevant to the user-typed query are provided by the Search component. The Anchoring component determines the most important segments of the videos. The beginnings of the most informative segments are marked in each video and the transcription of the beginning of each Anchoring segment is displayed. The segments topically related to each Anchoring segments (the Hyperlinks) are then retrieved. The list of the segments is generated on the fly for each Anchoring segment.  All three system components use Terrier IR Platform.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku skúmame využitie zvukovej informácie pri vyhľadávaní multimediálneho obsahu. Konkrétne sa zaoberáme prepájaním príbuzných segmentov v kolekcii 4000 hodín televíznych programov BBC. V článku popisujeme náš systém použitý v úlohe Search and Hyperlinking v rámci Benchmarku MediaEval 2014, v ktorom dosiahol najlepšie výsledky. V článku ďalej skúmame tri automatické prepisy, porovnávame ich s titulkami a potvrdzujeme vzťah medzi kvalitou prepisov a kvalitou vyhľadávania. Kvalita vyhľadávania je ďalej vylepšená pomocou rozšírenia prepisov o metadáta a kontext, kombináciou rôznych prepisov, využitím najspoľahlivejších slov z prepisov a využitím akustickej podobnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we explore the use of audio information in the retrieval of multimedia content. Specifically, we focus on linking similar segments in a collection consisting of 4,000 hours of BBC TV programmes. We provide a description of our system submitted to the Hyperlinking Sub-task of the Search and Hyperlinking Task in the MediaEval 2014
Benchmark, in which it scored best. We explore three automatic transcripts and compare them to available subtitles. We confirm the relationship between retrieval performance and transcript quality. The performance of the retrieval
is further improved by extending transcripts by metadata and context, by combining different transcripts, using the highest confident words of the transcripts, and by utilizing acoustic similarity.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku sa venujeme automatickej detekcii anchoring segmentov v kolekcii televíznych programov. Nájdené anchoring segmenty by mali byť ďalej použité pri následnom vyhľadávaní príbuzných segmentov v kolekcii. Anchoring segmenty by mali byť preto pútavé pre používateľov video kolekcie. S využitím linkov odkazujúcich z anchoring segmentov môžu užívatelia prechádzať kolekciu a nájsť ďalšie informácie, ktoré ich zaujímajú. V článku popisujeme dva prístupy - jeden založený na metadatách, druhý na frekvencii vlastných mien a čísloviek nachádzajúcich sa v segmente.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the paper we deal with automatic detection of anchoring segments in a collection of TV programmes. The anchoring segments are intended to be further used as a basis for subsequent hyperlinking to another related video segments.
The anchoring segments are therefore supposed to be fetching for the users of the collection.  Using the hyperlinks, the users can easily navigate through the collection and find
more information about the topic of their interest.
We present two approaches, one based on metadata, the second one based on frequencies of proper names and numbers contained in the segments.  Both approaches proved to be helpful for different aspects of anchoring problem: the
segments which contain a large number of proper names and numbers are interesting for the users, while the segments most similar to the video description are highly informative.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku je zkoumán vztah mezi
dvěma důležitými semantickými vlastnostmi jazyka (polysémií a synonymií) a jednou ze základních vlastností syntaktických sítí (stupňem uzlu). Na základě synergické teorie jazyka je předpokládáno, že slovo, které se objevuje ve více syntaktických kontextech, tzn. má větší stupeň,  by mělo být více polysémní a mít více synonym než slovo, které se objevuje v méně kontextech. Pro testování hypotézy je použito šest jazyků. Analýza syntakticky-závislostních sítí předložená v této studii přináší novou interpretaci známého vztahu mezi frekvencí a polysémii (nebo synonymií).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The relationship between two important semantic properties (polysemy and synonymy) of language and one of the most fundamental syntactic network properties (a degree of the node) is observed. Based on the synergetic theory of language, it is hypothesized that a word which occurs in more syntactic contexts, i.e. it has a higher degree, should be more polysemous and have more synonyms than a word which occurs in less syntactic contexts, i.e. it has a lesser degree. Six languages are used for hypotheses testing and, tentatively, the hypotheses are corroborated. The analysis of syntactic dependency networks presented in this study brings a new interpretation of the well-known relationship between frequency and polysemy (or synonymy).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje sérii experimentů na vybudování závislostního parsovácího
modelu s použitím MaltParser, korpusu ruštiny SynTagRus a morfologického
Tagger Mystem. Experimenty mají dva účely. První je natrénovat model s rozumnou rovnováhu mezi kvalitou a parsovácího času. Druhý účel je vytvořit uživatelsky přívětivý software, který by byl praktický
pro získání rychlych výsledku bez nutnosti technických znalostí (programovácí jazyky, jazykové nástroje, atd.)</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes a series of experiments on building a dependency parsing
model using MaltParser, the SynTagRus treebank of Russian, and the morphological
tagger Mystem. The experiments have two purposes. The first
one is to train a model with a reasonable balance of quality and parsing time.
The second one is to produce user-friendly software which would be practical
for obtaining quick results without any technical knowledge (programming
languages, linguistic tools, etc.).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Poster představuje CVH Malach a dostupné archivy orální historie, stejně jako některé výzkumné a vzdělávací aktivity realizované s využitím dostupných dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Malach Center for Visual History (Malach CVH) at the Faculty of Mathematics and Physics of the Charles University in Prague provides local access to the extensive digital archives of the USC Shoah Foundation – the Institute for Visual history and Education, the Refugee Voices archive of the Association of Jewish Refugees, and other archives of oral histories. The Visual History Archive of USC Shoah Foundation contains over 53 000 witness testimonies covering the history of entire 20th century. The Refugee Voices archive complements this collection with additional 150 interviews, conducted in English language by Association of Jewish Refugees in United Kingdom. Generally speaking, oral history interviews are valuable information resource for students and researchers in many disciplines. Secondary analysis of the testimonies is also applicable to many different scientific fields besides historiography and genocide studies. This paper provides a brief overview and description of available data, and it presents some of the more recent research and educational activities facilitated by the existence of Malach CVH in the Czech Republic.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje digitální orálněhistorické sbírky dostupné uživatelům CVH Malach a příklady badatelského a vzdělávacího využití těchto materiálů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Malach Center for Visual History (Malach CVH) at the Faculty of Mathematics and Physics of the Charles University in Prague provides local access to the extensive digital archives of the USC Shoah Foundation – the Institute for Visual history and Education, the Refugee Voices archive of the Association of Jewish Refugees, and other archives of oral histories. The Visual History Archive of USC Shoah Foundation contains over 53 000 witness testimonies covering the history of entire 20th century. The Refugee Voices archive complements this collection with additional 150 interviews, conducted in English language by Association of Jewish Refugees in United Kingdom. Generally speaking, oral history interviews are valuable information resource for students and researchers in many disciplines. Secondary analysis of the testimonies is also applicable to many different scientific fields besides historiography and genocide studies. This paper provides a brief overview and description of available data, and it presents some of the more recent research and educational activities facilitated by the existence of Malach CVH in the Czech Republic.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zpráva popisuje poslední přírůstek v Archivu vizuální historie USC Shoah Foundation, kolekci rozhovorů se svědky a přeživšími genocidy Arménů, která je dostupná od dubna 2015 i v CVH Malach. Pojednáno je o charakteristikách kolekce i o jejích specificích v rámci celého Archivu USC SF.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article describes the latest addition into USC Shoah Foundation's Visual History Archive, which is the collection of interviews with witnesses and survivors of Armenian genocide. The text deals with the characteristics of the collection as well as its specific features in the context of VHA.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>DeriNet je databáze lexikálních derivátů vyvíjená v Ústavu formální a aplikované lingvistiky. Takovou síť vztahů s velkým počtem derivačních stromů je těžké vizualizovat. V tomto článku popisujeme softwarový nástroj, který byl vyvinut za účelem zobrazování a prohledávání derivačních stromů. Nástroj má formu webové aplikace a využívá nově vyvinutý dotazovací jazyk podobný jazyku CQL, který se používá k prohledávání Českého národního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>DeriNet, a new database of lexical derivates,
is being developed at the Institute of Formal and Applied Linguistics.  Since it is a wordnet containing large amounts of trees, it is hard to visualize, search and extend without specialized tools. This paper describes a program which has been developed to display the trees and enable finding certain types of errors, and its query language. The application is web-based for ease of use and access. The language has been inspired by CQL, the language used for searching the Czech National Corpus, and uses similar key-words and syntax extended to facilitate searching in tree structures.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>DeriNet 1.0 je lexikální síť zahrnující informace o lemmatech obsažených v morfologickém slovníku Morflex.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The lexical network DeriNet 1.0 captures core word-formation relations on the set of lemmas stored in the morphological lexicon Morflex.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme metologii a výsledky průzkumu anotace víceslovných výrazů v treebancích. Průzkum probíhal pomocí webu na principu wiki, kam přispívali lidé obeznámení s treebanky. Výsledky průzkumu jsme studovali zejména z hlediska porovnání přístupů k předložkovým víceslovným výrazům, slovesným konstrukcím s předložkou a víceslovným pojmenovaným entitám.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the methodology and results of a survey on the annotation of multiword expressions in treebanks. The survey was conducted using a wiki-like website filled out by people knowledgeable about various treebanks. The survey results were studied with a comparative focus on prepositional MWEs, verb-particle constructions and multiword named entities.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace popisuje textove jevy zachycene v tektogramaticke anotace PDT - koreference, elipsy, aktualni cleneni a diskurz.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the presentation, the general overview of textual phenomena annotated in the Prague Dependency treebank is presented. We discuss coreference,  reconstruction of ellises, topic-focus articulation and discourse structure.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V teto prednasce se zamerime na detailnejsi popis koreference v cestine. predstavim proces anotace, vysledky automatickeho rozreseni koreference a srovname to s vysledky mezianotatorske shody. Zamerime se taky na oreferencni retezce a jejich reprezentace v cestine a jinych jazycich.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk presents manual annotation of coreference relations in more detail: coreference resolution experiments, manual annotation of English (Ontonotes vs. PCEDT, problematic cases (generics, premodifiers, etc.)), (dis)agreements, coreference chains in Czech, English and Russian.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce jsme představili anotaci koreference a diskurzu v PDT s přihlednutím ke konkrétním problémům, jako je mezianotátorská shoda a příčiny neshod, koreference generických jmenných frází atd.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk presents manual annotation of coreference and discourse relations in PDT in more detail: coreference resolution experiments, manual annotation (generics, premodifiers, etc.)), (dis)agreements, etc.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V prezengtaci představujeme několik přístupu k analýze a anotace textových jevů, např. ke koreference, elipsám, aktualnímu cleneni a diskurzním konektorům a argumentům.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the presentation, the general overview of textual phenomena annotated is presented. We discuss coreference,  reconstruction of ellises, topic-focus articulation and discourse structure.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V prezentaci jsme analyzovali výskyty kontextově zapojených výrazů ve větě, které nejsou propojeny žádným z anaforických vztahů (koreference, asociační anafora) anotatovaném v Pražském závislostním korpusu. Ukázalo se, že takových případů je v PDT cca. 30%. Vyčlenili jsme tři základní příčiny této absence: (i) kontextově zapojeny je sémanticky či pragmaticky spojen s s předchozím textem nebo s extralingvisticou situací; (ii) sekundární okolnosti, kulisy a (iii) výrazy s nízkým referenčním potenciálem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the presentation, we analyzed contextually bound nominal expressions explicitly expressed in the sentence, that lack an anaphoric (bridging, coreference or segment) link to a previous context. The statistics collected from the PDT annotated data has shown that in almost one third of contextually bound expressions there is no anaphoric link to the previous context. 
Disregarding different types of more or less technical reasons evoked by the tectogrammatical structure of the PDT sentences and annotation errors, we can claim that there are three groups of reasons why contextually bound nominal groups are not linked by any anaphoric link. These are (i) contextually bound nominal groups semantically or pragmatically related to previous textual or extralinguistic context but not specified as bridging relations within the PDT; (ii) secondary circumstances (temporal, local, etc.) and (iii) nominal groups with low referential potential.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Daný článek popisuje pokus o vytvoření obecného anotačního schématu pomocí existujících anotací textových vztahů v češtině, němčině a angličtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper describes an attempt to create
an interoperable scheme using existing annotations
of textual phenomena across languages
and genres including non-canonical
ones. Such a kind of analysis requires annotated
multilingual resources which are costly.
Therefore, we make use of annotations already
available in the resources for English,
German and Czech. As the annotations in
these corpora are based on different conceptual
and methodological backgrounds, we
need an interoperable scheme that covers existing
categories and at the same time allows a
comparison of the resources. In this paper, we
describe how this interoperable scheme was
created and which problematic cases we had
to consider. The resulting scheme is supposed
to be applied in the future to explore contrasts
between the three languages under analysis,
for which we expect the greatest differences in
the degree of variation between non-canonical
and canonical language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem výzkumu je statistické srovnání použití diskurzních konektorů v mluvené češtině, angličtině a němčině. Analýza se opírá na srovnatelné texty ve třech zkoumaných jazycích, na které byla promítnuta automatická anotace diskurzu vytvořena na základě srovnání anotačních schémat pro tyto jazyky, primárně provedena ručně v rámci různých vědeckých paradigmat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of the present paper is to analyse contrasts in Czech, English and German in terms of discourse-relational devices (DRDs). The novelty of our approach lies in the nature of the resources we are using. Advantage is taken of existing resources, which are, however, annotated on the basis of two different frameworks. We use an interoperable scheme unifying DRDs of both frameworks in more abstract categories and considering only those phenomena that have a direct match in German, English and Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této prezentaci jsme představili první výsledky srovnání dvou anotačních schémat, PDiT a GECCo v oblasti zpracování diskurzních konektorů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the presentation, the two approaches to discourse-structuring devices have been compared and analysed: PDiT and GECCo</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>This paper is a pilot comparative study on coreference chaining in three languages,
namely, Czech, English and Russian. We have analyzed 16 parallel English-Czech newspaper texts and 16 texts in Russian (similar to the English-Czech ones in length and topics). Our motivation was to find out what the linguistic structure of coreference chains in different languages is and what types of distinctions we should take into account for advancing the development of systems for coreference resolution. Taking into account
theoretical approaches to the phenomenon of coreference we based our research on the following assumption: the recognition of coreference links for different structural types of noun phrases is regulated by different language mechanisms. The other starting point was that different languages
allow pronominal chaining of different length and that coreference chains properties differ for the languages with different strategies for zero
anaphora and different systems for definiteness marking. This work reports our first findings within the task of the structural NP types’ distribution comparison in three languages under analysis.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Tento článek je pilotní srovnavací výzkum koreferenčních řetězců v češtině, angličtině a ruštině. Podrobili jsme analýze 16 srovnatelných textů ve třech jazycích. Naší motivací bylo zjistit lingvistickou strukturu koreferenčních řetězců v těchto jazycích a určit, které faktory ovlivňují tuto strukturu.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>SloNLP je workshop speciálně zaměřený na zpracování přirozeného jazyka a počítačovou lingvistiku. Jeho hlavním cílem je podpořit spolupráci mezi výzkumníky v oblasti NLP v Česku a na Slovensku.
Mezi témata workshopu patří automatické rozpoznávání mluvené řeči (ASR), automatická analýza a generování přirozeného jazyka (morfologie, syntax, sémantika...), dialogové systémy (DS), strojový překlad (MT), vyhledávání informací (IR), praktické aplikace NLP technologií, a další témata počítačové lingvistiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>SloNLP is a workshop focused on Natural Language Processing (NLP) and Computational Linguistics. Its primary aim is to promote cooperation among NLP researchers in Slovakia and Czech Republic.
The topics of the workshop include automatic speech recognition, automatic natural language analysis and generation (morphology, syntax, semantics, etc.), dialogue systems, machine translation, information retrieval, practical applications of NLP technologies, and other topics of computational linguistics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme vylepšení evaluace strojového překladu do češtiny pomocí cíleného parafrázování referenčních vět na rovině hloubkové syntaxe.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present a method of improving
quality of machine translation
(MT) evaluation of Czech sentences via
targeted paraphrasing of reference sentences
on a deep syntactic layer. For
this purpose, we employ NLP framework
Treex and extend it with modules
for targeted paraphrasing and word order
changes. Automatic scores computed using
these paraphrased reference sentences
show higher correlation with human judgment
than scores computed on the original
reference sentences.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá různými přístupy ke získávání informací napříč jazyky, zejména slovníkovými a korpusovými přístupy. Zaměříme se také na systémy, které používají statistický strojový překlad pro překládání dotazů nad kolekcí jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we will explore different approaches used in cross languages
information retrieval (CLIR) systems, mainly dictionary-based and corpus based
systems. Then we will focus on CLIR systems which use statistical machine
translation (SMT) systems to translate queries into collection language. Different
approaches which use SMT are studied in this paper including the using of SMT
as black box and looking inside the box of the SMT system to tune it to get better
performance in IR’s point of views rather than using SMT system to give better
results for human’s point of views.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato zpráva popisuje účast týmu Univerzity Karlovy v Praze na CLEF eHealth 2015 Task 2.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This report describes the participation of the team of Charles University in Prague at the CLEF eHealth 2015 Task 2.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení problému rozpoznávání řeči a dialogových systémů. Prezentace dialogového systému Alex.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Introducing the task of speech recognition, problems related with spoken dialogue systems. Presentation of Alex spoken dialogue system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dialogovový agent musí být uživateli velmi důkladně představen, aby se zabránilo četným nedorozumněním. Přesto však se uživatelé často ptají na funkcionalitu, které systém nerozumí a neumí jí poskytnout. V naší pokračující práci adresujeme tento problém tak, že agent umí popsat své možnosti a zároveň vysvětluje proč zvolil své akce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Having set up several dialogue systems for
multiple domains we realized that we need
to introduce each system thoroughly to
the users in order to avoid misunderstanding.
Even with well-described systems,
users still tend to request out-of-domain
information and are often confused by the
system response. In our ongoing work,
we try to address these issues by allowing
our conversational agent to speak about its
abilities. Our agent is able to simply describe
what it understands and why it decided
to perform one of its summary actions.
In this short paper, we present our
current system architecture.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje probíhající výzkum zaměřený na vývoj konverzačního agenta, který je schopen diskutovat o svých schopnostech. Náš implementovaný agent je schopen jednoduše popsat, čemu rozumí a proč se rozhodl pro kterou akci. Zaměřujeme se na sebeznalost agenta, protože to pomůže popsat uživatelům agentovu doménu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In our ongoing work, we try to build an conversational agent which is
able to chat about its abilities. Our implemented agent is able to simply describe
what it understands and why it decided to perform one of its summary actions.
We focus on self-awareness of our agent because it helps defining the agent’s
domain, and its architecture to its users. In this paper we introduce our current
implementation, outline our goals and describe our future intentions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace představila Archiv vizuální historie USC Shoah Foundation a související vzdělávací aktivity a projekty. Dva krátké sestřihy z úryvků vzpomínek tří pamětnic přiblížily z různých úhlů dějiny Podkarpatské Rusi jako někdejší součásti Československa.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presentation provided an overview of USC Shoah Foundation's Visual History Archive and related educational activities and projects. Two short videoclips from three witnesses' life story narratives captured different aspects of the history of Carpathian Ruthenia as a former part of Czechoslovakia.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení technik strojového překladu pro pracovníky státní správy a veřejných institucí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An introduction to machine translation for employees of governmental and other public institutions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kapitola v příručce o tvarosloví Oxford Handbook of Inflection se věnuje problematice bohatosti slovních forem v úloze strojového překladu. Po úvodu popisujícím několik klasických typů strojového překladu se podrobně zabývá jednotlivými fázemi či kroky strojového překladu a potížím, které v nich pestrost tvarosloví přináší.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The chapter on Machine translation in Oxford Handbook of Inflection starts by describing the few key approaches to machine translation and proceeds over individual stages of the process, highlighting the problems caused by the richness of word forms.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje výsledky společných úloh WMT15 -- překladu novinových textů, odhadu kvality překladu a automatické posteditace. Do standardní překladové úlohy v 10 překladových směrech se letos zapojilo 68 systémů strojového překladu z 24 institucí. Zároveň bylo vyhodnoceno 7 anonymizovaných systémů. Úloha odhadu kvality překladu měla 3 podúlohy, kterých se zúčastnilo 10 týmů a celkem 34 systémů. Pilotní úloha automatické posteditace měla 4 týmy, které odeslaly 7 systémů</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the
WMT15 shared tasks, which included a
standard news translation task, a metrics
task, a tuning task, a task for run-time
estimation of machine translation quality,
and an automatic post-editing task. This
year, 68 machine translation systems from
24 institutions were submitted to the ten
translation directions in the standard translation
task. An additional 7 anonymized
systems were included, and were then
evaluated both automatically and manually.
The quality estimation task had three
subtasks, with a total of 10 teams, submitting
34 entries. The pilot automatic postediting
task had a total of 4 teams, submitting
7 entries.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek představil problematiku strojového překladu při přenosu technologie z výzkumné sféry do rutinního komerčního využití a způsob hodnocení takového překladu v kontextu lokalizace dokumentace v oblasti ifnormačních technologií.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk presented solutions for successful transfer of machine translation technology from research to commercial use and its evaluation in the context of translation and localization of documentation in the area of information technology.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje náš systém pro Translation Task WMT15. Jedná se o hybridní systém pro anglicko-český překlad. Využíváme úspěšnou konfiguraci z předchozích dvou let.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes our WMT15 system submission for the translation task, a hybrid system for English-to-Czech translation. We repeat the successful setup from the previous two years.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nástroje a modely pro dva překladové systémy: anglicko-český překlad (především souvislých textů) a česko-anglický překlad (krátkých dotazů  i textů). V prvním případě jde o statistickou komponentu překladového systému Chiméra. Modely byly zmenšeny a uloženy ve formátu, který umožňuje velmi rychlý překlad, za cenu možné drobné ztráty kvality výstupu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Tools and models for two machine translation systems: English->Czech translation (mainly longer text) and Czech->English translation (short queries as well as text). In the first case, the models are the statistical component of the Chimera system. Models were pruned and stored in a format which allows for very fast translation, at the expense of possible slight reduction of output quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Shrnutí aktivit v oblasti strojového překladu v EU  včetně příspěvků několika projektů v této oblasti podporovaných Evropskou komisí, a směřování této oblasti směrem k jednotnému digitálnímu trhu v EU.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A summary of research activities in the area of machine translation in the EU (Technologies – Demands – Gaps – Roadmaps) has been presented, including contributions from multiple other EU-funded projects.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Abstraktní reprezentace významu, vyvinutá konsorciem amerických univerzit, slibuje vyšší úroveň reprezentace významu a jeho přiblížení i mezi vzdálenými jazyky. Přednáška představila analýzu různých problémů, které přitom vznikají.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Abstract Meaning Representation is a newly developed formalism for representing meaning, which abstracts from syntax and some other phenomena but is (still) language-dependent. It has been developed by a consortium of mostly U.S. universities, with team members including Martha Palmer, Kevin Knight, Philipp Koehn, Ulf Hermjakob, Kathy McKeown, Nianwen Xue and others. In the talk, the basic facts about the AMR will be presented, and then comparison will be made for Czech and English as carried out in detail on a small 100-sentence corpus; some examples from Chinese-English comparison will also be shown. In addition, AMR will be compared to the deep syntactic representation used in the set of Prague Dependency Treebanks (again, for Czech And English), and observations will be made about the level of abstraction used in these two formalisms. Plans for future studies and possible corpus annotation work in the nearest future will also be mentioned. The work reported has been done primarily by the CLAMR (Cross-Lingual AMR) team led by Martha Palmer of UCB at the Johns Hopkins Summer workshop in 2014.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Abstraktní reprezentace významu se snazží podchytit formálně význam vět v přirozeném jazyce. V příspěvku se porovnává s existujícími reprezentacemi a rovněž i napříč jazyky. Hlavní porovnání vylo provedeno s formalismem Pražského závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Abstract Meaning Representation is a newly developed formalism for representing meaning, which abstracts from syntax and some other phenomena but is (still) language-dependent. It has been developed by a consortium of mostly U.S. universities, with team members including Martha Palmer, Kevin Knight, Philipp Koehn, Ulf Hermjakob, Kathy McKeown, Nianwen Xue and others. In the talk, the basic facts about the AMR will be presented, and then comparison will be made for Czech and English as carried out in detail on a small 100-sentence corpus; some examples from Chinese-English comparison will also be shown. In addition, AMR will be compared to the deep syntactic representation used in the set of Prague Dependency Treebanks, and observations will be made about the level of abstraction used in these two formalisms. There is an ongoing work on possible conversion between the Prague deep syntactic representation and the AMR representation, and the main issues of such a conversion will also be described, as well as plans for future studies and possible corpus annotation work in the nearest future.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Byly prezentovány základy strojového překladu s použitím hloubkové jazykové analýzy podle tradičního systému "Vauquois trojúhelník", s využitím moderních statistických metod a metod strojového učení. Rovněž byly ukázány poslední výsledky, kdy hybridní systém Chiméra vytvořený na ÚFAL MFF UK v rámci různých projektů zvítězil v soutěži en-cs překladačá na WMT 2015 Shared Task.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Fundamentals of Machine Translation technology using Deep language analysis using the traditional "Vauquois triangle" have been presented, which now use advanced statistical and machine learning techniques. In addition, latest WMT 2015 Shared Task competition results for the en-cs pair have been shown in which the Chimera system created at UFAL MFF UK won.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Technologická agentura České republiky již šest let poskytuje podporu v oblasti aplikovaného výzkumu. Je odpovědná za tvorbu programů v oblasti aplikovaného výzkumu. V prezentaci je představeno hodnocení programu Center kompetence TAČR, která byla hodnocena v polovině jejich existence.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Technology Agency of the Czech Republic is less than six-years-old funding agency charged with promoting applied research and innovation. Its responsibility is to create programs, to be approved by the government, and then execute the whole lifecycle of each program -- from calls, ex-ante evaluation or proposals, handling the awards organizationally and financially throughout projects' execution, and performing ex-post evaluation of projects and the whole programs. While the ex-ante evaluation system has been described at AEA 2014, this year we plan to present results of a mid-term evaluation of a program for applied research in the area of social sciences and of the top program of the agency, the Centers of Competence program.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku byly prezentovány zušenosti Ústavu formáln a aplikované lingvistiky MFF UK s projekty v rámcových programech EU a zejména v programu Horizon 2020.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>EU Framework and Horizon 2020 projects carried out in the Institute of Formal and Applied Linguistics of the Faculty of Mathematics and Physics, Charles University in Prague, have been presented.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška představila projekty a další aktivity v oblasti jazykových technologií a speciálně strojového překladu pro tématickou oblast medicíny a lékařství.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk presented activities in the area of language technology and specifically machine translation in the medical domain in the context of EC-funded projects.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Projekt výzkumné infrastruktury LINDAT/CLARIN (“Vybudování a provoz českého uzlu panevropské infrastruktury pro výzkum”) je koncipován jako český „uzel“ mezinárodní sítě Clarin (Common Language Resources and Technology Infrastructure), která je zřízena ve formě ERIC (Evropské konsorcium pro výzkumnou  infrastrukturu) se sídlem v Utrechtu v Holandsku. Cílem Clarin ERIC je otevřený přístup k jazykovým datům a technologiím zejména pro humanitní a společenské obory. LINDAT/CLARIN je rovněž kompatibilní s technologickou sítí META-SHARE (http://www.meta-net.eu), která je zaměřená na jazykové technologie a aplikace, čímž tyto dvě oblasti propojuje. LINDAT/CLARIN shromažďuje,
zpracovává, anotuje (manuálně a automaticky) a uchovává jazyková data v českém jazykovém prostředí a poskytuje softwarové nástroje pro vyhledávání, analýzu a syntézu přirozeného jazyka. Použitá technologie a její kvalita a rozšiřitelnost je přímo aplikovatelná jak v humanitních a společenských vědách a výzkumu (jazykověda a interdisciplinární výzkum s jazykovou složkou, jako například formální a počítačová lingvistika, translatologie, lexikografie, sociolingvistika, psychologie, sociologie, historie, literární vědy, neurolingvistika, kognitivní vědy, umělá inteligence a strojové učení), tak ve výzkumu a
vývoji jazykových technologií založených na statistických metodách (zpracování přirozeného jazyka, rozpoznávání a syntéza řeči a kombinovaná analýza obrazu, textu a multimédií obecně, vč. text and data mining“, strojového překladu a extrakce informace).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The project LINDAT/CLARIN (Building and operation of a Czech node of the pan-European infrastructure for research) is a Czech node of the international network Clarin ERIC (Common Language Resources and Technology Infrastructure), headquartered in Utrecht, Netherlands. The goal of Clarin is to provide open access to research data, technology and services.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Strojový překlad přirozených jazyků je problém stejně starý jako počítačová lingvistika samotná, a v jejím rámci je to problém, který byl mnohokrát podceněn jak z hlediska metodologického, tak z hlediska doby potřebné k jeho vyřešení – a ani dnes vyřešen není. Pohled na překlad prošel vývojovou spirálou od primitivních pokusů o jednoduché statistické řešení přes nesmírně komplikované algoritmy popisující všechny detaily a výjimky v přirozeném jazyce až k dnešním přístupům, které kombinují statistický a lingvistický přístup.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Machine Translation is as old as the field of Computational Linguistics itself. It is also a problem that has been predicted to „be solved in the next five years“ many times, but in fact it is not yet solved today. Machine translation has been naively considered a simple problem solvable by simple statistical means, then studied in depth by complicated but unsuccessful detailed sets of rules trying to describe all details of natural language use, only to return to statistical approach on a completely different level, using a combination of linguistic analysis and powerful machine learning algorithms.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce byl prezentován LINDAT/CLARIN jako uzel mezinárodní infrastrukturní sítě Clarin ERIC. Zejména byl prezentován repozitář LINDAT/CLARIN a jeho možnosti ukládání a archivace jazykových a jiných dat, a s ním spojené služby.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>LINDAT/CLARIN, as a node of the pan-European research infrastructure Clarin ERIC, has been presented. Its repository has been featured together with data archivation techniques and related web services and applications.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce byly prezentovány známé a používané korpusy se zachycenou syntaktickou analýzou (Penn Treebank, Pražské závislostní korpusy) a jejich vztah k analýze váceslovných výrazů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presented were existing syntactically analyzed treebanks, such as the Penn Treebank and the Prague family of dependency Treebanks, and their relation to Multiword Expressions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Licence mezi UK v Praze a Google Inc., platná od 2. 12. 2014 na dobu neurčitou, PO 270122, fakturováno 2. 1. 2015. Obsahuje právo na užití dat PDT 2.5 a asociovaného software "MorphoDiTa" pro morfologické značkování na základě PDT 2.5, a to pro komerční účely. Jedná se o identické verze uložené v repozitáři LINDAT/CLARIN s nekomerční licencí typu Creative Commons.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Licence agreement between Charles University in Prague and Google Inc., effective Dec. 2, 2014, duration unless terminated, PO No. 270122, invoiced Jan. 2, 2015. The license covers the right to use the data of PDT 2.5 and the associated software "MorphoDiTa" for morphological tagging, for commercial purposes. The same data and software is otherwise available for free through the LINDAT/CLARIN repository for non-commercial use under the appropriate variant of the Creative COmmons license.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem tohoto příspěvku je podrobně prozkoumat, jak se v syntakticky anotovaných korpusech zachází s prvky věty, které jsou na povrchu vypuštěné, a pokusit se o kategorizaci výpustek v rámci víceúrovňového anotačního schématu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of the present contribution is to put under scrutiny the ways in which the so-called deletions of elements in the surface shape of the sentence are treated in syntactically annotated corpora and to attempt at a categorization of deletions within a multilevel annotation scheme.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Projekt Amalach vytvořil software pro prohledávání nahrávek archivu VHA v češtině a angličtině. Součástí projektu byl rovněž překlad tezauru použitého pro indexaci archivu. Přednáška uvedla celou problematiku vyhledávání a audioarchivu VHA.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Project Amalach has resulted in software for cross=language search in the VHA archive. It also resulted in the Czech version of the VHA thesaurus. The talk has presented the topic of audio search in the VHA archive.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nekrolog významného českého lingvisty prof. dr. Františka Daneše, DrSc. Zamyšlení nad rozpětím jeho zájmů a nad jeho přínosy české a světové lingvistice.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Obituary of prof. dr. František Daneš,DrSc. A survay of his wide scale of linguistic interests and of his important contributions to the Czech and word linguistics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Srovnávací struktury uvozené v češtině výrazy jako a než se reprezentují jako gramatikalizované elipsy, jejichž zdrojem jsou rekonstruované predikační struktury s opakováním slov z řídící predikace nebo zastoupené umělým uzlem s obecnou sémantickou platností. Dále se analyzuje srovnání uvozeneé sekundárními předložkami.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Czech adverbial constructions introduced by the expressions jako and než are understood as grmmaticalized ellipsis. Their undelying structures are proposed in the shape of reconstructed constructions containing either the repeated items from the governing predication or by the new artfitial nodes with general semantics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku se prezentuje způsob zachycení vybraných typů elips v češtině v rámci FGP. Jde zejména o vypouštění zájmenného subjektu v 1. a 2. osobě, o nepřítomnost subjektu v kontrolovaných infinitivních konstrukcích,o konstrukce uvozené "kromě" a "místo" a o srovnávací konstrukce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this contribution the treatment of selected types of deletions in Czech are presented. The pro-dropped constructions,absent subjects in controlled infinitive constructions, selected types of Czech "small clauses" and comparison constructions are analyzed and exemplified.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace pokrývá formát PML (Prague Markup Language) pro reprezentaci liongvistických dat, nástroje pracující s tímto formátem (Treex, TrEd, PML-TQ) a na příkladech anotovaných dat ukazuje využití PML a nástrojů pro reprezentaci a vyhledávání lingvistických jevů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presentation covers PML format and tools using it (Treex, PML-TQ, TrEd). Using real annotated data as examples representation of constructions and searching for them using the tools was demonstrated.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku se zabýváme problémem doménové adaptace statistického strojového překladu (SMT) s využitím doménově specifických dat získaných cíleným prohledáváním (crawling) v Internetu. Navrhujeme a empiricky vyhodnocujeme proces automatického získávání jednojazyčných a paralelních textů, stejně jako jejich využití v trénování, optimalizaci i testování v rámci frázového SMT. Navrhujeme strategii, jak tyto zdroje využít v závislosti na jejich dostupnosti a množství, která ja podpořena výsledky rozsáhlé evaluace. Ta byla provedena v doménách legislativy pro životní prostředí a práci, ve dvou jazykových párech (angličtina-francouzština a angličtina-řečtina) a v obou směrem: z angličtiny i do angličtiny. Obecně lze tvrdit, že systémy SMT trénované a optimalizované na datech z obecné domény na specifických doménách dosahují špatných výsledků. Ukazujeme, že takové systémy lze úspěšně adaptovat optimalizací parametrů modelu s využitím malého množství paralelních dat z cílové domény, a dále je lze zlepšit přidáním jednojazyčných i paralelních dat pro adaptaci jazykového, resp. překladového modelu. Průměrné pozorované zlepšení v BLEU o 15,3 absolutních bodů je významné.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we tackle the problem of domain adaptation of statistical machine
translation (SMT) by exploiting domain-specific data acquired by domain-focused crawling
of text from the World Wide Web. We design and empirically evaluate a procedure for auto-
matic acquisition of monolingual and parallel text and their exploitation for system training,
tuning, and testing in a phrase-based SMT framework. We present a strategy for using such
resources depending on their availability and quantity supported by results of a large-scale
evaluation carried out for the domains of environment and labour legislation, two language
pairs (English–French and English–Greek) and in both directions: into and from English.
In general, MT systems trained and tuned on a general domain perform poorly on specific
domains and we show that such systems can be adapted successfully by retuning model
parameters using small amounts of parallel in-domain data, and may be further improved
by using additional monolingual and parallel training data for adaptation of language and
translation models. The average observed improvement in BLEU achieved is substantial at
15.30 points absolute.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Webová služba pro podporu indexování naskenovaných obsahů knih. Pro Národní technickou knihovnu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A webservice supporting indexation of book table of contents. For the National Technical Library.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CzEngVallex je dvojjazyčný valenční slovník, který obsahuje provázané dvojice českých a anglických sloves. Zahrnuje 20835 odpovídajících slovesných dvojic (překladových ekvivalentů) valenčních rámců (významů slovesa) a zachycuje také propojení jejich argumentů. Tato databáze dvojic rámců a jejich argumentů je založena na reálných textech a může být využita například v aplikacích pro strojový překlad.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CzEngVallex is a bilingual valency lexicon of corresponding Czech and English verbs. It connects 20835 aligned valency frame pairs (verb senses) which are translations of each other, aligning their arguments as well. The CzEngVallex serves as a powerful, real-text-based database of frame-to-frame and subsequently argument-to-argument pairs and can be used for example for machine translation applications. It uses the data from the Prague Czech-English Dependency Treebank project.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Technická zprava shrnuje pravidla pro tvorbu valenčního slovníku (CzEngVallexu) českých a anglických sloves,  budovaného na základě paralelního závislostního korpusu (PCEDT) a dvou existujících valenčních slovníků: PDT-Vallexu a EngVallexu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This report presents a guideline for building a valency lexicon (CzEngVallex) of Czech and English verbs  based on the parallel dependency corpus (PCEDT) and two existing valency lexicons: PDT-Vallex a EngVallex.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje budování dvojjazyčného valenčního anglicko-českého slovníku CzEngVallex, budovaného na základě paralelního překladového Pražského česko-anglického závislostního korpusu. Díky propojení s korpusovými daty bude možné slovník využívat jak pro lingvistický výzkum, tak pro NLP aplikace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a resource and the associated annotation process used in a project of interlinking Czech and English verbal translational equivalents based on a parallel, richly annotated dependency treebank containing also valency and semantic roles, namely the Prague Czech-English Dependency Treebank. One of the main aims of this project is to create a high-quality and relatively large empirical base which could be used both for linguistic comparative research as well as for natural language processing applications, such as machine translation or cross-language sense disambiguation. This paper describes the resulting lexicon, CzEngVallex, and the process of building it, as well some interesting observations and statistics already obtained.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Sketch Engine byl využit zejména pro vytipování dvojic substantiv tvořících nominalizace verbonominálních predikátů s kategoriálním slovesem. V prezentaci byly rovněž představeny odlišné syntaktické vlastnosti konstrukcí s plnovýznamovými substantivy a konstrukcí s týmiž substantivy, užitými ve vyprázdněném významu jako nominalizace kategoriálních sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I use the Sketch Engine in order to match up collocations standing for particular nominalizations of SVCs. I also search for valency properties of nouns in question using CQL query type which, to some extent, allows searching for syntactic relations in linear corpora. Nominalizations of support verbs display different valency behaviour when compared to notional nouns, avoiding deverbal compounds in English, cf. selling the books / book(-/ )selling / sale of books / books sale vs. selling the idea / *sale of the idea / *idea sale, or loss of memory / memory loss vs. loss of confidence / *confidence loss. In Czech, semantic impoverishing of nominalizations of support verbs has an impact on ways of expressing their Agent, allowing double post-nominal genitives (e.g. sbírání zkušeností nejmladších závodníků ‘gaining experience-GEN.PL [by] youngest-GEN.PL racer-GEN.PL’) which are ungrammatical when the nouns have full semantic meaning. Some nominalized SVCs also allow so-called crossing of nominal valency relations which is distinctive esp. in Czech but also possible in English, cf. giving (opportunity) to the inhabitants vs. opportunity to provide in giving opportunity to the inhabitants to provide for themselves.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této přednášce popíšu svou práci na univerzálním zachycení morfologie a závislostní syntaxe v korpusech různých jazyků. Taková harmonizace je nejen výhodná pro lingvisty-uživatele korpusů, ale je také nezbytným předpokladem pro techniky mezijazykové adaptace parserů, např. delexikalizovaného parsingu. Představím Interset, nástroj podobný interlingvě pro překlad morfosyntaktických reprezentací mezi sadami značek; ukážu také, jak se rysy z Intersetu používají v novém formalismu nazvaném Universal Dependencies. Nakonec proberu důležitost jednotlivých morfologických rysů s ohledem na závislostní parsing.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this talk I will describe my work towards universal representation of morphology and dependency syntax in treebanks of various languages. Not only is such harmonization advantageous for linguists-users of corpora, it is also a prerequisite for cross-language parser adaptation techniques such as delexicalized parsing. I will present Interset, an interlingua-like tool to translate morphosyntactic representations between tagsets; I will also show how the features from Interset are used in a recent framework called Universal Dependencies. Some experiments with delexicalized parsing on harmonized data will be presented. Finally, I will discuss the extent to which various morphological features are important in the context of statistical dependency parsing.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zabýváme se aplikací UD na slovanské jazyky. Nejvíce prostoru věnujeme zájmenům, determinátorům, číslovkám a kvantifikátorům. Kromě toho diskutujeme i další jazykové jevy jako způsobová slovesa, elipsu, jmenné přísudky a zvratná zájmena. Většina našich příkladů pochází z češtiny, ale jazykové jevy, které na příkladech ukazujeme, jsou obvykle přenositelné na další slovanské jazyky. Tam, kde je to vhodné, uvádíme příklady i z jiných jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We address the application of UD to Slavic languages. We devote the most space to peculiarities of pronouns, determiners, numerals and quantifiers. Other language features that are discussed include modal verbs, ellipsis, nominal predicates, and reflexive pronouns. Most of our examples are from Czech but the language features demonstrated are usually portable to other Slavic languages. We include examples from the other languages where appropriate.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>HamleDT (HArmonized Multi-LanguagE Dependency Treebank) je sbírka existujících závislostních korpusů (nebo do závislostí převedených jiných syntaktických korpusů), transformovaných do jednotného anotačního stylu. Tato verze používá Universal Dependencies jako svůj ústřední anotační styl.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>HamleDT (HArmonized Multi-LanguagE Dependency Treebank) is a compilation of existing dependency treebanks (or dependency conversions of other treebanks), transformed so that they all conform to the same annotation style. This version uses Universal Dependencies as the common annotation style.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jako výzkumníci jsme přistoupili k vývoji systémů extrakce informací z textů legislativní a enviromentální domény. V prezentaci systémy představíme a podělíme se o zkušenosti z jejich vývoje. Zaměříme se na alternativy strojové učení vs. pravidla a surové texty vs. texty obohacené o informace. Aspekty správy systémů a způsobu hodnocení jejich kvality budou rovněž diskutovány.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>As researchers, we decided to develop information extraction systems focused on texts from the legislative and environmental domains. We will present them and we will share the experience on their development. We will focus on the alternatives machine learning vs. rules and raw texts vs. enriched texts. Aspects of their management and evaluation will be discussed as well.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme úvod do strojového učení prostřednictvím úlohy počítačového zpracování přirozeného jazyka, a sice přiřazení správného významu slov ve větách. Zaměřujeme se hlavně na praktické aspekty v systému R. Článek je urče studentům a mladým vědcům, kteří nemají znalosti  strojového učení a chtějí začít.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a gentle introduction to machine learning  and we explain it on its application in the field of natural language processing. Namely, we address the task of word sense disambiguation practically in the R software system. We focus on readers who approach some task and want to use machine learning, and who do not know how to start.</seg>
            </tuv>
        </tu>
        
        <tu>&amp;
            <tuv xml:lang="cs">
                <seg>Tento funkční vzorek slouží pro vícejazyčné (české a anglické) vyhledávání relevantních slov či krátkých frází v archivu přeživších Holocaustu, spravovaném USC (University of Southern California) Shoah Foundation Institute (http://dornsife.usc.edu/vhi/). Tento archiv obsahuje více než 110 tisíc hodin záznamů v 32 jazycích, přičemž přibližně polovina těchto rozhovorů je vedena v angličtině. Česká část archivu obnáší zhruba jeden tisíc hodin. Funkční vzorek se skládá ze serverového počítače, softwarových modulů MCLASS (http://www.kky.zcu.cz/cs/sw/MCLAAS), WFBAS (http://www.kky.zcu.cz/cs/sw/WFBAS), pracovní databáze sestavené softwary SEASR-CZE (http://www.kky.zcu.cz/cs/sw/SEASR-CZE) a SEASR-ENG (http://www.kky.zcu.cz/cs/sw/SEASR-ENG) a tenkého klienta s obvyklým webovým prohlížečem. Serverový počítač je počítač s konfigurací odpovídající náročnosti vykonávané úlohy s připojením k internetu. Počítač použitý pro funkční vzorek má 2 procesory Intel(R) Xeon(R) CPU E5-2620 v2 @ 2.10GHz. Pro účely vyhledávání v systému jsou česká a anglická řečová data nejprve zpracována příslušným modulem rozpoznávání řeči (SEASR-CZE, resp. SEASR-ENG).. Každý ze systémů v současnosti hledá výskyty slov či frází zhruba v 1000 hodin videozáznamů. V případě češtiny jde o veškerá dostupná data; v angličtině je k dispozici více než 50 tisíc hodin, ale rozpoznání a zaindexování celého tohoto objemu bude vyžadovat paralelizaci jednotlivých procesů. Pro křížové vyhledávání (dotaz v češtině, data/rozhovory v angličtině a češtině) v softwaru byl použit systém překladu dotazu. Implementace byla provedena jako zvláštní verze systému MTMonkey (http://ufal.mff.cuni.cz/mtmonkey)</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This functional prototype is used for multi-lingual (Czech and English) search for relevant words or short phrases in the archive of Holocaust survivors, managed by USC (University of Southern California) Shoah Foundation Institute (http://dornsife.usc.edu/vhi/), which contains more than 110,000 hours of records in 32 languages, with approximately half of these interviews is conducted in English. Czech part of the archive accounts for approximately one thousand hours. For the purposes of searching in the system MCLAAS are Czech and English speech data first processed with the appropriate speech recognition module (SEASR-CZE - see http://www.kky.zcu.cz/en/sw/SEASR-CZE or SEASR-ENG - see http://www.kky.zcu.cz/en/sw/SEASR-ENG) and then a so-called index is created, which is a machine representation of recognized utterances, which speeds up the search for a desired word or phrase. Those are all data available in the case of Czech; in English there are more than 50,000 hours, but the recognition and indexing of all this volume will require parallelization of individual processes. Cross-searching (query in English, data / interviews in English and Czech) in the system is facilitated by automatic query translation. Implementation was carried out as a special version of MTMonkey (http://ufal.mff.cuni.cz/mtmonkey).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje probíhající experiment soustřeďující se na kvantifikaci slovosledných vlastností tří indoevropských jazyků, češtiny, angličtiny a němčiny. Statistiky jsou získávány ze syntakticky anotovaných korpusů za pomoci dotazovacího jazyka PML-TQ.
tato studie slouží jako motivace pro formální modelování metod NLP.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes an ongoing experiment
consisting in the attempt to quantify word-order properties of three Indo-European languages (Czech, English and German). The statistics are collected from the syntactically annotated treebanks available for all three languages.
The treebanks are searched by means of a universal query tool PML-TQ. The search concentrates on the mutual order of a verb and its complements (subject, object(s)) and the statistics are calculated for all permutations of the three elements. The results for all three languages are compared and a measure expressing the degree of word order freedom is suggested in the final section of the paper.
This study constitutes a motivation for formal modeling of natural language processing methods.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje experiment soustřeďující se na zkoumání a kvantifikaci slovosledných vlastností na základě dat dostupných díky projektu HamleDT.
Tato studie usiluje o objektivní porovnání přirozených jazyků, a to kombinováním jak hledisek čistě lingvistických, tak hledisek kvantitativních, která jsou založena na velkém souboru dostupných dat. 
Tato studie slouží jako motivace pro formální modelování metod NLP; zároveň si klade za cíl představit výzkumný potenciál jazykových zdrojů se standardizovanou anotací.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes an experiment consisting in the attempt to quantify word-order properties of three Indo-European languages (Czech, English and Farsi). The investigation is driven by the
endeavor to find an objective way how to compare natural languages from the point of view of the degree of their word-order freedom. Unlike
similar studies which concentrate either on purely linguistic or purely statistical approach, our experiment tries to combine both - the observations are verified against large samples of sentences from available treebanks, and, at the same time, we exploit the ability of our tools to analyze selected important phenomena (as, e.g., the differences of the word order of a main and a subordinate clause) more deeply.
The quantitative results of our research are collected from the syntactically annotated treebanks available for all three languages. Thanks to the HamleDT project, it is possible to search all treebanks in a uniform way by means of a universal query tool PML-TQ. This is also a secondary goal of this paper - to demonstrate the research potential provided by language resources which are to a certain extent unified.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek shrnuje experimenty týkající se automatické analýzy koordinačních a apozičních konstrukcí v Pražském závislostním korpuse na základě metody redukční analýzy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This  paper  summarizes  results  of  automatic  analysis of  coordinating  constructions  and  appositions  in  the Prague Dependency Treebank using a method of analysis by reduction. Experiments are performed on a large subset of the treebank. This subset is obtained as a result
of a query providing a set of more than 4,300 suitable sentences and their tree structures containing coordinations and appositions. The automatic procedure is complemented by a manual analysis of reasons why certain sentences (trees) were not fully reduced. This analysis helps to gain a better insight into the phenomena of co-
ordination and apposition and their formal properties.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Spolu se vznikem stále dalších jazykových zdrojů – slovníků, lexikálních databází, korpusů, treebanků – roste i potřeba jejich účinného propojování, které by umožnilo snadné využití veškerých shromážděných vlastností a informací. V tomto ohledu je také aktuální téma univerzálních lexikografických formátů.

Tato práce zkoumá metody automatického propojování jazykových dat. Představíme zde systém na propojování slovníků, jakými jsou například VALLEX, PDT-Vallex, FrameNet, nebo SemLex, které poskytují syntaktickou informaci o svých heslech. Systém je automatický, umožňuje tudíž opakovanou aplikaci na novější verze vyvíjejících se jazykových zdrojů. Na základě syntaktické informace obsažené ve slovníku víceslovných výrazů SemLex navrhujeme metodu vyhledávající tyto výrazy v automaticky anotovaném textu.

Praktickým výstupem potvrzujícím úspěšnost použitých metod je mj. propojení slovníků VALLEX a PDT-Vallex vedoucí k doplnění desítek tisíc anotovaných vět z treebanků PDT a PCEDT do VALLEXu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Along with the increasing development of language resources – i.e., new lexicons, lexical databases, corpora, treebanks – the need for their efficient interlinking is growing. With such a linking, one can easily benefit from all their properties and information. Considering the convergence of resources, universal lexicographic formats are frequently discussed.

In the present thesis, we investigate and analyse methods of interlinking language resources automatically. We introduce a system for interlinking lexicons (such as VALLEX, PDT-Vallex, FrameNet or SemLex) that offer information on syntactic properties of their entries. The system is automated and can be used repeatedly with newer versions of lexicons under development. We also design a method for identification of multiword expressions in a parsed text based on syntactic information from the SemLex lexicon.

An output that verifies feasibility of the used methods is, among others, the mapping between the VALLEX and the PDT-Vallex lexicons, resulting in tens of thousands of annotated treebank sentences from the PDT and the PCEDT treebanks added into VALLEX.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Spojení slovníků na úrovni hesel či jejich částí přináší zpravidla rozšíření informací pro obě strany. Spojení českých valenčních slovníků VALLEX a PDT-Vallex přinese VALLEXu napojení na data PDT a tudíž desítky tisíc příkladových vět; PDT-Vallexu pak další syntakticko-sémantické informace (jako je reciprocita, kontrola, sémantická třída, ad.). Referát představí datový formát a postup použitý při propojování lexikálních jednotek a dosažené a zveřejněné výsledky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Linking Czech valency lexicons VALLEX and PDT-Vallex will connect VALLEX with tens of thousands of example sentences in corpus data of PDT. The presentation will introduce the data format and the approach used to automatick linking and the published results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Sestavili jsme slovník víceslovných entit BBN a následně jsme jej využili ke kontrolám anotací (anotace WSJ, BBN, PCEDT).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We compiled a lexicon of multiword BBN entities and used it for consistency checking of annotations (WSJ, BBN and PDT annotations).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Po krátkém úvodu ilustrujícím bohatost české morfologie se v přednášce zaměřuji na způsob zachycení morfologických kategorií v Pražském závislostním korpusu. V druhé části přednášky se věnuji přehledu historie taggerů češtiny, rozpoznávání pojmenovaných entit v češtině a naší aktuální práci na budování lexikálního zdroje derivační morfologie češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>After a brief introduction into the rich morphology of the Czech language, I focus on the multi-layered annotation scheme of the Prague Dependency Treebank. Morphological categories are described at a separate annotation layer. The meanings conveyed by this categories are involved in the deep-syntatic annotation layer as well. The semiautomatic annotation is illustrated by several examples in the talk. Development of Czech taggers, named entity recognition tools and our recent work on a resource of derivational morphology are outlined as a few topics based on Praguian morphology.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slovnědruhová kategorie a morfologické rysy tokenů jsou v Pražském závislostním korpusu zachyceny na zvláštní anotační rovině, významy vyjadřované morfologickými kategorie jsou ovšem také součástí hloubkově syntaktické roviny. V příspěvku podrobně představujeme zachycení morfologických kategorií v tomto korpusu a věnujeme se také aplikacím, pro které byla tato morfologicky anotovaná data použita.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Morphological annotation constitutes a separate layer in the multilayered annotation scenario of the Prague Dependency Treebank. At this layer, morphological categories expressed by a word form are captured in a positional part-of-speech tag. According to the Praguian approach based on the relation between form and function, functions (meanings) of morphological categories are represented as well, namely as grammateme attributes at the deep-syntactic (tectogrammatical) layer of the treebank. In the present paper, we first describe the role of morphology in the Prague Dependency Treebank, and then outline several recent topics based on Praguian morphology: named entity recognition in Czech, formemes attributes encoding morpho-syntactic information in the dependency-based machine translation system, and development of a lexical database of derivational relations based partially on information provided by the morphological analyser.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Na přednášce bude představena lexikální síť DeriNet, která zachycuje derivační vztahy v morfologii češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presentation will be focused on the lexical NetWork DeriNet that captures morphological derivations in Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je třetí vydání treebanků UD, verze 1.2.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the third release of UD Treebanks, Version 1.2.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku je navržen nový algoritmus pro závislostní syntaktickou analýzu, který umožňuje použít různé typy predikčních rysů a přitom zůstává výpočetně zvládnutelný. Základní myšlenka vychází z postupného upravování závislostního stromu opakovaným aplikovaním jednoduchých transformačních operací.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We propose a new dependency parsing algorithm, designed to allow
for any features while maintaining tractability. The main idea is to start with a
baseline parse tree rather than with no tree at all, and to transform that tree into
the correct one by repeated application of simple transformation operations. We
focus on inference, discussing anticipated issues and possible remedies. Suggestions
towards the training procedure and the feature set are also briefly presented.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Depfix -- open-souce systém pro automatickou post-editaci výstupů frázového strojového překladu. Depfix zapojuje řadu nástrojů pro automatické zpracování přirozeného jazyka, pomocí nichž získává rozbor vstupních vět, a používá sadu pravidel pro opravu závažných chyb či chyb obvyklých ve výstupech strojových překladačů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Depfix, an open-source system for automatic post-editing of phrase-based machine translation outputs. Depfix employs a range of natural language processing tools to obtain analyses of the input sentences, and uses a set of rules to correct common or serious errors in machine translation outputs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Toto je sada konfiguračních souborů parseru MSTperl a skriptů pro přenos delexikalizovaného parseru. Byly použity v práci popsané v článku arXiv:1506.04897 (http://arxiv.org/abs/1506.04897) a v několika souvisejících článcích. Parser MSTperl je dostupný na adrese http://hdl.handle.net/11234/1-1480</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This is a set of MSTperl parser configuration files and scripts for delexicalized parser transfer. They were used in the work reported in arXiv:1506.04897 (http://arxiv.org/abs/1506.04897), as well as several related papers. The MSTperl parser is available at http://hdl.handle.net/11234/1-1480</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>MSTperl je Perlovou reimplementací MST parseru  Ryana McDonalda, s několika pokročilými funkcemi navíc, jako je podpora pro paralelní rysy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>MSTperl is a Perl reimplementation of the MST parser of Ryan McDonald, with several additional advanced functions, such as support for parallel features.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Porovnáváme dva anotační styly, Pražské závislosti a univerzální
    Stanfordské závislosti, ve smyslu jejich vhodnosti pro parsing.
    Konkrétně se zaměřujeme na porovnání stylu zavěšení adpozic, použivaného v těchto dvou
    formalismech, na úloze vícezdrojového mezijazyčného přenosu delexikalizovaného parseru,
    používajíce MSTParser.
    Zjišťujeme, že v našem scénáři se stává zřetelnou výhoda Stanfordského stylu,
    neboť převod anotace adpozic v treebancích anotovaných v Pražském stylu do Stanfordského stylu vede k mírně lepšímu výsledku (+0.2%
    UAS).
    Dále ukazujeme, že nejlepších výsledků lze dosáhnout pomocí natrénování parserů
    na treebancích využívajících oba styly anotace adpozic, analýzy cílového
    treebanku pomocí všech těchto parserů a kombinace všech získaných stromů,
    po jejich převodu do stejného anotačního stylu (dalších +0.18% UAS).
    Rozdíly ve skóre jsou ještě vyšší, když se použije menší sada různorodých zdrojových
    treebanků (až 2.24% UAS oproti základní verzi).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We compare two annotation styles, Prague dependencies and Universal
Stanford Dependencies, in their adequacy for parsing.
We specifically focus on comparing the adposition attachment style, used in these two
formalisms, applied in multi-source cross-lingual delexicalized dependency
parser transfer performed by parse tree combination.
We show that in our setting, converting the adposition annotation to Stanford style in the
Prague style training treebanks leads to promising results.
We find that best results can be obtained by parsing the target sentences
with parsers trained on treebanks using both of the adposition annotation
styles in parallel,
and combining all the resulting parse trees together
after having converted them to the Stanford adposition style (+0.39% UAS over Prague
style baseline).
The score improvements are considerably more significant when using a smaller set of diverse source
treebanks (up to +2.24% UAS over the baseline).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme naši práci v oblasti částečně řízené syntaktické analýzy vět přirozeného jazyka, se zaměřením na mezijazyčný přenos delexicalizovaných závislostních parserů s více zdroji.
Nejprve vyhodnocujeme vliv anotačního stylu treebanku na úspěšnost parsingu,
se zaměřením na styl zavěšení adpozic. Poté představujeme KLcpos3, empirickou
míru podobnosti jazyků, navrženou a vyladěnou pro vážení zdrojových parserů 
při přenosu delexicalizovaných parserů s více zdroji. Nakonec představíme novou metodu kombinace zdrojů, založenou na interpolaci natrénovaných modelů parserů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our work on semi-supervised
parsing of natural language sentences, focusing
on multi-source crosslingual transfer
of delexicalized dependency parsers.
We first evaluate the influence of treebank
annotation styles on parsing performance,
focusing on adposition attachment
style. Then, we present KLcpos3, an empirical
language similarity measure, designed
and tuned for source parser weighting
in multi-source delexicalized parser
transfer. And finally, we introduce a novel
resource combination method, based on
interpolation of trained parser models.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme implementaci doménové adaptace pomocí  interpolace překladových modelů v TectoMT, překladovém systému s hloubkovým transferem. Vyhodnotili jsme tuto metodu na šesti jazykových párech s doménovým paralelním korpusem o 1000 větách, a získali jsme zlepšení až o 3 body BLEU. Váhy pro interpolaci jsou nastaveny uniformně, bez zapojení jakéhokoliv ladění.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an implementation of domain adaptation by translation model interpolation in the
TectoMT translation system with deep transfer. We evaluate the method on six language pairs
with a 1000-sentence in-domain parallel corpus, and obtain improvements of up to 3 BLEU
points. The interpolation weights are set uniformly, without employing any tuning.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme KLcpos3, míru podobnosti jazyků založenou na Kullbackově-Leiblerově
divergenci rozložení trigramů hrubých značek slovních druhů v otagovaných korpusech. Tato míra byla navržena pro vícejazyčný delexicalizovaný parsing, a to jak pro výběr zdrojového treebanku při přenosu parseru s jedním zdrojem, tak pro vážení zdrojových treebanků při přenosu parseru s více zdroji. V úloze výběru zdroje rozpozná KLcpos3
nejlepší zdrojový treebank v 8 z 18 případů. V úloze vážení zdroje přínáší zvýšení UAS o +4.5 procentního bodu oproti nevážené kombinaci stromů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present KLcpos3, a language similarity measure based on Kullback-Leibler
divergence of coarse part-of-speech tag trigram distributions in tagged
corpora. It has been designed for multilingual delexicalized parsing, both for
source treebank selection in single-source parser transfer, and for source
treebank weighting in multi-source transfer. In the selection task, KLcpos3
identifies the best source treebank in 8 out of 18 cases. In the weighting
task, it brings +4.5% UAS absolute, compared to unweighted parse tree
combination.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme interpolaci natrénovaných
modelů MSTParseru jako metodu kombinace zdrojů
pro vícezdrojový delexikalizovaný přenos parseru. Představujeme jak
neváženou metodu, tak variantu
ve které je každý zdrojový model vážen
podobností zdrojového a cílového jazyka. Vyhodnocení na sbírce treebanků
HamleDT ukazuje, že
vážená interpolace modelů má podobnou úspěšnost jako vážená kombinace stromů, přičemž je mnohem méně komputačně
náročná.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce interpolation of trained MSTParser models as a resource combination method for multi-source delexicalized parser transfer. We present both an unweighted method, as well as a variant in which each source model is weighted by the similarity of the source language to the target language. Evaluation on the HamleDT treebank collection shows that theweightedmodelinterpolationperforms comparably to weighted parse tree combination method, while being computationally much less demanding.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek je zaměřen na tři témate související s autorčiným prvním rokem doktorského studia. Zaprvé, zabývá se nástroji pro rozpoznávání koreference v češtině a angličtině. Zadruhé, popisuje metody extrakce číselných výrazů napojených na vybrané typy entit v prostředí GATE. Poslední část je věnovaná editoru Capek a získávání anotovaných větných diagramů</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper is devoted to three topics concerning my first year
postgraduate studies at UFAL. First, I will present the current state-of-the-art of
coreference resolution in the Czech and English language. Next, I will describe a
method of extraction of numerical expressions linked to certain entities with the
GATE tool and the last part will be dedicated to the Capek sentence diagramming
editor and a method of getting more annotated data from sentence diagrams.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce porovnává několik systémů pro rozpoznávání koreference v angličtině v překladu přes hloubkovou syntax z angličtiny do češtiny a holandštiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This work focuses on using anaphora for machine translation with deep-syntactic transfer. We compare multiple coreference resolvers for English in terms of how they affect the quality of pronoun translation in English-Czech and English-Dutch machine translation systems with deep transfer. We examine which pronouns in the target language depend on anaphoric information, and design rules that take advantage of this information. The resolvers’ performance measured by translation quality is contrasted with their
intrinsic evaluation results. In addition, a more detailed manual analysis of English-to-Czech translation was carried out.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V teto prednasce predstavujeme srovnani koreferencnich vyrazu v cestine a anglictine na zaklade paralelnich dat Prazskeho anglicko-ceskeho zavislostniho korpusu. V centru naseho vyzkumu jsou osobni, privlastnovaci, reflexivni a vztazna zajmena. Zvlastni pozornost venujeme rovnez vynechani zajmen v anaforicke pozici.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this talk, we present a comprehensive study on mappings between certain classes of coreferential expressions in English and Czech. We focused on central pronouns, relative pronouns and anaphoric zeros. For instance, the English sentence "It switched to a caffeine-free formula using its new Coke in 1985" has been in PCEDT translated to "V roce 1985 přešla na bezkofeinovou recepturu, kterou používá pro svojí novou kolu". This pair of sentences exhibits several types of changes in expressing coreference: English personal pronouns turns into a Czech zero, possessive pronoun into a possessive reflexive and finally, the -ing participle has been translated to a relative clause. In a similar manner, we have collected a statistics of mappings from a subsection of PCEDT, which we will support by multiple examples and contrast with the theoretical assumptions. For such a study, the quality of word alignment is crucial. Thus, we designed a rule-based refining algorithm for English personal and possessive pronouns and Czech relative pronouns, which served as an automatic alignment pre-annotation. Subsequently, this annotation has been manually corrected and completed, obtaining a basis for this empirical study.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>In this work, we present a comprehensive study on correspondences between certain classes of coreferential expressions in English and Czech. We focus on central pronouns, relative pronouns, and anaphoric zeros. We designed an alignment-refining algorithm for English personal and possessive pronouns and Czech relative pronouns that improves the quality of alignment links not only for the classes it aimed at but also in general. Moreover, the instances of anaphoric expressions we focus on were manually annotated with their alignment counterparts, which served as a basis for this empirical study. The collected statistics of correspondences are contrasted with theoretical assumptions regarding the use of anaphoric means in the languages under analysis, such as pro-drop properties, the use of finite and non-finite constructions, etc. Finally, we present the ways how the observed correspondences can be exploited in cross-lingual coreference resolution.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>V této práci představujeme podrobný srovnávací výzkum některých typů koreferenčních vztahů v češtině a angličtině na paralelním česko-anglickém korpusu. Zaměřujeme se na centrální, vztažná a anaforická zájmena. Vypracovali jsme algoritmus vylepšení automatického vyrovnání několika typů českých a anglických zájmen, který vylepšuje kvalitu vyrovnání nejen pro tyto typy ale a pro všechny ostatní. Korespondence daných typů zájmen byly anatovány ručně a dále lingvisticky analyzovány.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci představujeme podrobný srovnávací výzkum některých typů koreferenčních vztahů v češtině a angličtině na paralelním česko-anglickém korpusu. Zaměřujeme se na centrální, vztažná a anaforická zájmena. Vypracovali jsme algoritmus vylepšení automatického vyrovnání několika typů českých a anglických zájmen, který vylepšuje kvalitu vyrovnání nejen pro tyto typy ale a pro všechny ostatní. Korespondence daných typů zájmen byly anatovány ručně a dále lingvisticky analyzovány.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this work, we present a comprehensive study on correspondences between certain classes of coreferential expressions in English and Czech. We focus on central pronouns, relative pronouns, and anaphoric zeros. We designed an alignment-refining algorithm for English personal and possessive pronouns and Czech relative pronouns that improves the quality of alignment links not only for the classes it aimed at but also in general. Moreover, the instances of anaphoric expressions we focus on were manually annotated with their alignment counterparts, which served as a basis for this empirical study. The collected statistics of correspondences are contrasted with theoretical assumptions regarding the use of anaphoric means in the languages under analysis, such as pro-drop properties, the use of finite and non-finite constructions, etc. Finally, we present the ways how the observed correspondences can be exploited in cross-lingual coreference resolution.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nástroj MT-ComparEval byl navržen tak, aby umožnil vývojářům strojového překladu porovnávat a vyhodnocovat různé MT systémy a jejich verze.
MT-ComparEval obsahuje několik metrik pro vyhodnocení strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The tool described in this article has been designed to help MT developers by implementing
a web-based graphical user interface that allows to systematically compare and evaluate various
MT engines/experiments using comparative analysis via automatic measures and statistics.
The evaluation panel provides graphs, tests for statistical significance and n-gram statistics.
We also present a demo server http://wmt.ufal.cz with WMT14 and WMT15 translations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CloudASR je cloudová platforma pro automatické rozpoznávání řeči, která umožňuje dávkové i online zpracování nahrávek. Její hlavní přednosti jsou škálovatelnost, přizpůsobitelnost a snadný proces nasazaní.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CloudASR is a cloud platform for automatic speech recognition, which supports batch and online speech recognition mode. Its main features are scalability, customizability and easy deployment.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CloudASR je softwarová platforma a veřejná služba pro rozpoznávání řeči. Její tři silné stránky jsou: kvalita rozpoznávání na úrovni stavu poznání, jednoduché nasazení, a škálovatelnost. Dále, obsahuje anotační rozhraní pro přidávání transkripcí. API platformy podporuje jak dávkovou tak online rozpoznávání. Dávková verze je kompatibilní s Google Speech API. Platforma umožňuje přidání nových rozpoznávačů, které potom můžou fungovat paralelně vedle sebe.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CloudASR is a software platform and a public ASR web-service. Its three strong features are state-of-the-art online speech recognition performance, easy deployment, and scalability. Furthermore, it contains an annotation interface for the addition of transcriptions for the recordings. The platform API supports both batch and online speech recognition. The batch version is compatible with Google Speech API. New ASR engines can be added onto the platform and can work simultaneously.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>MT-ComparEval je nástroj, který umožňuje vývojářům strojového překladu porovnávat a vyhodnocovat různé MT systémy a jejich verze. MT-ComparEval obsahuje několik metrik pro vyhodnocení strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>MT-ComparEval is a tool for Machine Translation developers, which allows to compare and evaluate different MT systems (and their versions). MT-ComparEval includes several automatic MT evaluation metrics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme aktuální vývoj korektor, je
statistický systém kontroly pravopisu. Kromě lexikonu, Korektor používá jazyk modely najít chyby real-slovo, detekovatelná pouze v kontextu. Modely a chyba probanického, vyvozené z chyb korpusů, jsou také používány pro navrhovaly
GEST nejpravděpodobnější opravy. Korektor byl původně vyškolení na malé chyby korpusu a použité jazykové modely extrahuje z in-house corpus WebColl. Ukážeme dvě nedávná zlepšení:
• Postavili jsme nové jazykové modely z volne dostupný schopné (šoural) verze České národní korespondence hnis a ukazují, že tyto provádět trvale lepší na texty vyráběných jak rodilými mluvčími a non-nativní studenti češtiny.
• Trénovali jsme nové modely chyb na ručně s poznámkami žák korpus a ukázat, že lepší výkon než Standardní model chyba (detekce chyb) nejenom
pro texty studenty ", ale také pro naše standardní hodšpatne rozpoznaných zpráv data rodilého Čecha. Pro korekci chyb se
standardní model chyba překonaly non-nativní modulárně els ve 2 ze 3 testovaných datových sad.
Diskutujeme důvody pro tento ne zcela intuitivní zlepprostředí. Na základě těchto poznatků a na základě analýzy chyb základě
v obou domorodec a češtině frekventantů, navrhujeme směry pro další zlepšení korektor.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present recent developments of Korektor, a
statistical spell checking system. In addition to lexicon, Korektor uses language models to find real-word errors, detectable only in context. The models and error probabilities, learned from error corpora, are also used to suggest the most likely corrections. Korektor was originally
trained on a small error corpus and used language models extracted from an in-house corpus WebColl. We show two recent improvements:
• We built new language models from freely avail-
able (shuffled) versions of the Czech National Corpus and show that these perform consistently better on texts produced both by native speakers and non-native learners of Czech.
• We trained new error models on a manually annotated learner corpus and show that they perform better than the standard error model (in error detection) not only for the learners’ texts, but also for our standard eval-
uation data of native Czech. For error correction, the standard error model outperformed non-native models in 2 out of 3 test datasets.
We discuss reasons for this not-quite-intuitive improvement. Based on these findings and on an analysis of errors in both native and learners’ Czech, we propose directions
for further improvements of Korektor.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem práce je (i) srovnat přístupy k analýze a anotaci textových jevů v PDiT a GECCo a (ii) určit společné a rozdílné rysy mezi odpovídajícími vědeckymi paradigmaty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of this work is (i) to compare two frameworks for the analysis and annotation
of discourse-structuring devices (DSDs) and further discourse phenomena in GECCo X PDiT and (ii) identify commonalities and/or differences between the two frameworks</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek se zabývá použitím lingvistiké informace pro výběr dat pro trénování jazykových modelů. Navrhovaná metoda vychází ze známých a používaných postupů, které využívají povrchových tvarů slov, a obohacuje je o informace o lemmatech, pojmenovaných entitách a slovních druzích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper explores the use of linguistic information for the selection of data to train language models. We depart from the state-of-the-art method in perplexity-based data selection and extend it in order to use word-level linguistic units (i.e. lemmas, named entity categories and part-of-speech tags) instead of surface forms. We then present two methods that combine the different types of linguistic knowledge as well as the surface forms (1, naıve selection of the top ranked sentences selected by each method; 2, linear interpolation of the datasets selected by the different methods). The paper presents detailed results and analysis for four languages with different levels of morphologic complexity (English, Spanish, Czech and Chinese). The interpolation-based combination outperforms the purely statistical baseline in all the scenarios, resulting in language models with lower perplexity. In relative terms the improvements are similar regardless of the language, with perplexity reductions achieved in the range 7.72% to 13.02%. In absolute terms the reduction is higher for languages with high type-token ratio (Chinese, 202.16) or rich morphology (Czech, 81.53) and lower for the remaining languages, Spanish (55.2) and English (34.43 on the same dataset as Czech and 61.90 on the same dataset as Spanish).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje probíhající výzkum zaměřený na vyhledávání ilustrativních obrázků pro články "soft news", kde vhodnost ilustrativního obrázku není primárně určena jeho popisností. Popisujeme základní řešení a experimenty používající Denoising Autoencoders.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This work presents ongoing research on finding illustrative images to “soft
news” articles, magazine-style texts where the appropriateness of an illustrative image is
not judged primarily by its descriptiveness. We describe our baselines and experiments
using Denoising Autoencoders and present the web-pic multimodal dataset of Czech news
articles and their accompanying images. Finally, we briefly present the Safire library, a
Python framework for building and analyzing complex experimental pipelines suitable for
multimodal tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje případovou studii automatického překladu mezi příbuznými jazyky. Porovnává systémy pracující tradičními metodami s překladačem firmy Google. Porovnání je založeno na lidském hodnocení 200 náhodně vybraných vět z novinových článků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes an experiment comparing results of machine translation between two pairs of related Slavic languages. Two language pairs on three different translation platforms were observed in the experiment. One pair represents really very close languages (Czech and Slovak), the other pair are slightly less similar languages (Slovenian and Croatian).
The comparison is performed by means of three MT systems, one for each pair representing rule-based approach, the other one representing statistical (same system for both language pairs) approach to the task. Both sets of results are manually evaluated by native speakers of the target language. The results are discussed both from the linguistic and quantitative points of view.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Sledování dialogového stavu je důležitá komponenta moderních dialogových systémů. Ukazujeme inkrementální sledovač dialogového stavu založený na LSTM sítích. Ke sledování používá přímo výsledky rozpoznávání řeči. Ukazujeme klíčová nestandardní aspekty modelu, které pomáhají dostat úspěšnost sledovače k systémům v současném stavu poznání: zahrnutí skóre rozpoznávače, abstrakce málo viděných hodnot, použití transkripce pro trénování a průměrování modelu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A dialog state tracker is an important component in modern
spoken dialog systems. We present an incremental dialog
state tracker, based on LSTM networks. It directly uses automatic
speech recognition hypotheses to track the state. We
also present the key non-standard aspects of the model that
bring its performance close to the state-of-the-art and experimentally
analyze their contribution: including the ASR confi-
dence scores, abstracting scarcely represented values, including
transcriptions in the training data, and model averaging</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Sledování dialogového stavu je důležitou součástí moderních řečových dialogových systémů. Navrhujeme první inkrementální trénovatelný sledovač dialogového stavu, který používá přímo výstup z rozpoznávače řeči ke sledování stavu. Je založen na sítích s dlouhou krátkodobou pamětí a je plně trénovatelný z anotovaných dat. Dosahujeme slibných výsledků na sledování pod-úkolů Method a Requested v DSTC2.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A dialog state tracker is an important component in modern spoken dialog systems. We present the first trainable incremental dialog state tracker
that directly uses automatic speech recognition hypotheses to track the state. It is based on a long short-term memory recurrent neural network, and it is fully trainable from annotated data. The tracker achieves promissing performance on the Method and Requested tracking sub-tasks in DSTC2.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek pojednává o adaptaci modelu závislého typu Stanfordova typu (de Marneffe a Manning 2008) původně určeného pro angličtinu na požadavky typologicky odlišných jazyků z hlediska praktického rozboru. Argumentujeme pro rámec gramatiky funkční závislosti, který je založen na myšlence paralelnosti mezi syntaxí a sémantikou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper discusses the adaptation of the Stanford typed dependency model (de Marneffe and Manning 2008), initially designed for English, to the requirements of typologically different languages from the viewpoint of practical parsing. We argue for a framework of functional dependency grammar that is based on the idea of parallelism between syntax and semantics.
There is a twofold challenge: (1) specifying the annotation scheme in order to deal with the morphological and syntactic peculiarities of each language and (2) maintaining crosslinguistically consistent annotations to ensure homogenous analysis for similar linguistic phenomena. We applied a number of modifications to the original Stanford scheme in an attempt to capture the language-specific grammatical features present in heterogeneous CoNLL-encoded data sets for German, Dutch, French, Spanish, Brazilian Portuguese, Russian, Polish, Indonesian, and Traditional Chinese. From a multilingual perspective, we discuss features such as subject and object verb complements, comparative phrases, expletives,
reduplication, copula elision, clitics and adpositions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>RExtractor je systém pro extrakci informací. Vstupní dokumenty jsou zpracovány NLP nástroji. Extrakce následně probíhá pomocí dotazů nad závislostními stromy. Výsledkem je znalostní báze dokumentu, definována jako množina entit a vztahů mezi nima. Dotazy nad stromy jsou definovány manuálně. Architektura systému je navržena doménově a jazykově nezávisle. Systém demonstrujeme na českých a anglických právních dokumentech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The RExtractor system is an information extractor that processes input documents by natural language processing tools and consequently queries the parsed sentences to extract a knowledge base of entities and their relations. The extraction queries are designed manually using a tool that enables natural graphical representation of queries over dependency trees. A workflow of the system is designed to be language and domain independent. We demonstrate RExtractor on Czech and English legal documents.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme výsledky experimentů na úloze identifikace mateřského jazyka. Vytvořili jsme systém pro automatickou identifikaci mateřského jazyka autora anglicky psaného textu. Systém je založen na jazykovém modelování a využíva rysy spočítané na základe křížové entropie.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper reports on the task of Native Language Identification (NLI). We developed a machine learning system to identify the native language of authors of English texts written by non-native English speakers. Our system is based on the language modeling approach and employs cross-entropy scores as features for supervised learning, which leads to a significantly reduced feature space. Our method uses the SVM learner and achieves the accuracy of 82.4 % with only 55 features. We compare our results with the previous similar work by Tetreault et al. (2012) and analyze more details about the use of language modeling for NLI. We experiment with the TOEFL11 corpus (Blanchard et al., 2013) and provide an exact comparison with results achieved in the First Shared Task in NLI (Tetreault et al., 2013).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>RExtractor je systém pro extrakci informací. Vstupní dokumenty jsou zpracovány NLP nástroji. Extrakce následně probíhá pomocí dotazů nad závislostními stromy. Výsledkem je znalostní báze dokumentu, definována jako množina entit a vztahů mezi nima. Dotazy nad stromy jsou definovány manuálně. Architektura systému je navržena doménově a jazykově nezávisle. Systém demonstrujeme na českých a anglických právních dokumentech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The RExtractor system is an information extractor that processes input documents by natural  language  processing  tools  and  consequently  queries  the  parsed  sentences  to  extract a knowledge base of entities and their relations.   The  extraction  queries  are  designed manually  using  a  tool  that  enables  natural graphical  representation  of  queries  over  dependency trees.  A workflow of the system is designed to be language and domain independent.   We  demonstrate RExtractor  on  Czech and English legal documents.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Korpus Czech Legal Text Treebank (CLTT) je kolekcí 1 133 ručně syntakticky anotovaných vět. Věty pochází ze Zákona o účetnictví (563/1991 Sb.) a z Vyhlášky o účetnictví (500/2002 Sb.).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Czech Legal Text Treebank (CLTT) is a collection of 1133 manually annotated dependency trees. CLTT consists of two legal documents: The Accounting Act (563/1991 Coll., as amended) and Decree on Double-entry Accounting for undertakers (500/2002 Coll., as amended).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace představuje repozitář LINDAT/CLARIN a některé změny platformy DSpace, na které je repozitář založený.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presentation introduces the LINDAT/CLARIN repository and some of the modifications to DSpace - the platform the repository is based on.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nekrolog obsahující zhodnocení vědeckého přínosu jedné ze zakladatelek počítačové lingvistiky, americké badatelky prof. J. J. Robinson.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Obituary article evaluating the scientific contribution of J.J.Robinson, an American computational linguist, one of the founders of the field.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme CLARIN Concept Registry, nový registr sémantických konceptů, který je dostupný on-line a jeho smyslem je sloužit jako definice konceptů, na které se odkazují projekty CLARIN. Tento nový registr nahrazuje (pro účely CLARINu) ISOcat. Probíráme různé problémy, které komplikovaly používání ISOcatu, a navrhujeme řešení těchto problémů v CCR.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce the CLARIN Concept Registry, a new on-line registry of semantic concepts that can be used in (and referenced from) CLARIN projects. This new registry replaces (for CLARIN purposes) ISOcat. We discuss various issues that made ISOcat less useful, and propose solutions to those issues in CCR.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Studujeme metodu ručního hodnocení strojového překladu, v níž anotátoři uspořádávají podle kvality jen krátké úseky místo celých vět. Anotace je tak snazší a rychlejší.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We propose a manual evaluation method for machine translation (MT), in which annotators
rank only translations of short segments instead of whole sentences. This results in an easier
and more efficient annotation. We have conducted an annotation experiment and evaluated a
set of MT systems using this method. The obtained results are very close to the official WMT14
evaluation results. We also use the collected database of annotations to automatically evaluate
new, unseen systems and to tune parameters of a statistical machine translation system. The
evaluation of unseen systems, however, does not work and we analyze the reasons.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje výsledky soutěže v ladění systémů strojového překladu, WMT15 Tuning Shared Task. Účastníkům jsme poskytli úplný překladový systém a jejich úkolem bylo nastavit jeho interní váhy. Výsledné překlady byly porovnány v ručním hodnocení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the WMT15 Tuning Shared Task. We provided the
participants of this task with a complete machine translation system and asked them to tune its
internal parameters (feature weights). The tuned systems were used to translate the test set and
the outputs were manually ranked for translation quality. We received 4 submissions in the
English-Czech and 6 in the Czech-English translation direction. In addition, we ran
3 baseline setups, tuning the
parameters with standard optimizers for BLEU score.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje výsledky soutěže v automatickém hodnocení kvality strojového překladu (WMT15 Metrics Shared Task). Úkolem účastníků bylo vyhodnotit kvalitu překladových systémů, které se zúčastnily překladové úlohy WMT15. Získaných 46 metrik od 11 týmů jsme pak společně se 7 základními metrikami (BLEU, SentBLEU, NIST, WER, PER, TER a CDER) porovnali z hlediska korelace s lidským hodnocením pro celý testset i pro jednotlivé věty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the WMT15 Metrics Shared Task. We asked
participants of this task to score the outputs of the MT systems involved in
the WMT15 Shared Translation Task. We collected scores of 46 metrics from 11
research groups. In addition to that, we computed scores of 7 standard metrics
(BLEU, SentBLEU, NIST, WER, PER, TER and CDER) as baselines. The collected scores were
evaluated in terms of system level correlation (how well each metric's scores
correlate with WMT15 official manual ranking of systems) and in terms of segment
level correlation (how often a metric agrees with humans in comparing two
translations of a particular sentence).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje několik případů 
mezijazykových rozdílů závislosti na materiálu paralelního češko-anglického závislostního korpusu. Pozornost je soustředěna zejména na místa, kde dochází k neshodám v propojení slovesných argumentů. Článek se věnuje otázce, zda takové neshody pramení ze sémantických vlastností jednotlivých jazyků, nebo z charakteru použité lingvistické teorie. Autoři se zamýšlejí nad případným využitím získaných poznatků pro strojový překlad.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper analyses several points of interlingual dependency mismatch on the material of a parallel Czech-English dependency treebank. Particularly, the points of alignment mismatch between the valency frame arguments of the corresponding verbs are observed and described. The attention is drawn to the question whether such mismatches stem from the inherent semantic properties of the individual languages, or from the character of the used linguistic theory. Comments are made on the possible shifts in meaning. The authors use the findings to make predictions about possible machine translation implementation of the data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V monografii je představen výzkum vztahů působících společně ve výstavbě textu (syntaktická výstavba, aktuální členění, sémantické diskurzní vztahy v užším smyslu, koreference a asociativní anafora).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this monograph we present the results of our research on the interplay of intra-sentential relations such as deep syntactic relations and information structure of the sentence and the inter-sentential relations such as discourse relations and coreferential and other associative links.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Seznámení s anotací diskurzních vztahů a koreference a asociační anafory v Pražském závislostním korpusu</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Information about annotation of discourse relations, coreference and bridging anaphora in the Prague Dependency Treebank</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje metody a výsledky CLEF 2015 eHealth Evaluation Lab, Task 2, který se zaměřuje na efektivitu vyhledávání medicínských informací na webu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper details methods, results and analysis of the CLEF 2015 eHealth Evaluation Lab, Task 2. This task investigates the effectiveness of web search engines in providing access to medical information with the aim of fostering advances in the development of these technologies</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V práci se zabýváme popisem a analýzou diskurzních (tj. textových) konektorů v češtině v širším smyslu, tedy tím, jakými jazykovými prostředky je možné vyjadřovat v textu diskurzní vztahy. Výzkum přitom neomezujeme na předem stanovenou skupinu výrazů (danou například příslušností k určitým slovním druhům, jako jsou spojky či strukturující částice), ale snažíme se nalézt a obecně popsat všechny jazykové prostředky v češtině, které mají schopnost spojovat jednotlivé úseky textu v jeden koherentní celek. Zaměřujeme se především na méně probádané víceslovné konektivní struktury typu "to je důvod, proč"; "kvůli těmto skutečnostem"; "z těchto důvodů" atd., pro které užíváme označení sekundární konektory (za primární konektory považujeme především konektivní synsémantika typu "však", "nebo", "a", "ale", "proto" apod.).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The thesis focuses on description and analysis of discourse connectives in Czech in broader sense, i.e. by which language means it is possible to express sense relation within a text. The thesis is not limited to any parts of speech (like conjunctions or structuring particles) but it tries to find and describe all language means in Czech with the ability to connect two pieces or units of a text into one coherent complex. The thesis investigates discourse connectives in Czech with respect to the so called secondary connectives (i.e. mainly multiword phrases like "to je důvod, proč" – "that is the reason why"; "kvůli těmto skutečnostem" – "due to these facts" etc., in opposition to primary connectives like "však" – "however", "nebo" – "or", "a" – "and", "ale" – "but", "proto" – "therefore" etc.).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem posteru je přispět do diskuze o diskurzních konektorech, zejména o jejich definici a o kritériích, podle kterých je možné vymezit jejich hranice.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of the poster is to contribute to the general discussion on discourse connectives, 
especially on their definition and principles we may hold as boundaries surrounding this class of
expressions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje novou anotaci diskurzních vztahů v Pražském závislostním korpusu (PDT) – anotaci tzv. sekundárních konektorů (většinou víceslovných frází jako "podmínkou je", "to je důvod, proč", "na závěr", "to znamená" atd.). 
Nejprve se článek soustředí na na teoretické vymezení těchto výrazů (hlavně s ohledem na tzv. primární konektory jako "a", "ale", "nebo", "také" atd.) a pak se zaměřuje na popis a definici diskurzních konektorů jako takových (primárních i sekundárních).
Poté článek představuje možnosti anotací sekundárních konektorů ve velkých korpusech (jako je PDT). Příspěvek popisuje obecné anotační principy pro sekundární konektory použité v PDT pro češtinu a srovnává výsledky této anotace s výsledky anotací primárních konektorů v PDT. Hlavním cílem příspěvku je představit nový typ anotace diskurzu, která by mohla být využita i pro další jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper introduces a new annotation of
discourse relations in the Prague Dependency
Treebank (PDT), i.e. the annotation of the so
called secondary connectives (mainly
multiword phrases like "the condition is", "that is the reason why", "to conclude", "this means" etc.).
Firstly, the paper concentrates on theoretical
introduction of these expressions (mainly with
respect to primary connectives like "and", "but", "or", "too" etc.) and tries to contribute to the description and definition of discourse connectives in general (both primary and secondary). 
Secondly, the paper demonstrates possibilities of annotations of secondary connectives in large corpora (like PDT). The paper describes general annotation principles for secondary connectives used in PDT for Czech and compares the results of this annotation with annotation of primary connectives in PDT. In this respect, the main aim of the paper is to introduce a new type of discourse annotation that could be adopted also by other languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku překládáme pilotní studii zaměřenou na anotaci významu slov získaných z několika informačních zdrojů. Studie je prvním krokem z zamýšlené anotací "ukotvení".</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a pilot study in web-based annotation of words with senses coming
from several knowledge bases and sense inventories. The study is the first step in a
planned larger annotation of “grounding” and should allow us to select a subset of these
“dictionaries” that seem to cover any given text reasonably well and show an acceptable
level of inter-annotator agreement.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme pilotní experimenty s anotací významů slov ve webovém anotačním prostředí. Významy přitom pocházejí z několika různých zdrojů. Experimenty jsou prvním krokem v plánované větší anotaci a měly by nám pomoci s výběrem podmnožiny studovaných zdrojů tak, aby pokrývaly veškerý text v co největším rozsahu a současně vykazovaly přijatelnou úroveň shody mezi anotátory.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a pilot study of a web-based annotation of words with senses. The annotated senses come from several knowledge bases and sense inventories. The study is the first step in a planned larger annotation of grounding and should allow us to select a subset of the sense sources that cover any given text reasonably well and show an acceptable level of inter-annotator agreement.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V članku se popisuje system pro SemEval-
2015 Task 13: Multilingual All-Words Sense
Disambiguation and Entity Linking. My jsme se zameřili na monolingvalní disambiguaci a propojeni entit.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes our system for SemEval-
2015 Task 13: Multilingual All-Words Sense
Disambiguation and Entity Linking. We have
participated with our system in the sub-task
which aims at monolingual all-words disambiguation
and entity linking. Aside from system
description, we pay closer attention to the
evaluation of system outputs</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme podrobnou analýzu kombinace systému založeného na transferu TectoMT se statistickým systémem Moses pro překlad mezi angličtinou a češtinou. Popisujeme možnosti zkoumání této kombinace systémů jak pomocí ruční, tak i automatické evaluace. Přesto, že výstupy TectoMT často obsahují chyby, Moses dokáže z jeho výstupů vybrat vhodné části. V mnoha případech pak TectoMT poskytne nové, užitečné překladové varianty, které jsou pro statistickou komponentu jinak nedosažitelné, navzdory velikosti trénovacích dat. Naše analýzy potvrzují, že TectoMT napomáhá dodržení gramatické shody a požadavků valence, ale zároveň zlepšuje překlad rozmanité škály jazykových jevů. Zahrnutí výstupů systému založeného na transferu do frázového překladu má také zřejmě pozitivní vliv na prohledávaný prostor hypotéz. Zjišťujeme, že jednotlivé komponenty této kombinace jsou komplementární a výsledný systém překládá signifikantně lépe než kterákoliv jednotlivá komponenta.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a thorough analysis of a combination of a statistical and a transfer-based system for English->Czech translation, Moses and TectoMT. We describe several techniques for inspecting such a system combination which are based both on automatic and manual evaluation. While TectoMT often produces bad translations, Moses is still able to select the good parts of them. In many cases, TectoMT provides useful novel translations which are otherwise simply unavailable to the statistical component, despite the very large training data. Our analyses confirm the expected behaviour that TectoMT helps with preserving grammatical agreements and valency requirements, but that it also improves a very diverse set of other phenomena. Interestingly, including the outputs of the
transfer-based system in the phrase-based search seems to have a positive effect on the search space. Overall, we find that the components of this combination are complementary and the final system produces significantly better translations than either component by itself.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme diskriminativní model s bohatou sadou rysů pro strojový překlad, který využívá abstraktní sémantickou reprezentaci na zdrojové straně. Náš model využíváme jako nový rys ve frázovém překladači a dosahujeme mírných zlepšení BLEU skóre v experimentu n-best reranking.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a feature-rich discriminative model for machine translation which uses an abstract semantic representation on the source side. We include our model as an additional feature in a phrase-based decoder and we show modest gains in BLEU score in an n-best re-ranking experiment.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci se zaměřujeme na postojovou analýzu aspektů, což je relativně nová úloha z oblasti počítačového zpracování přirozených jazyků (NLP). Představujeme novou datovou sadu v češtině, která sestává z uživatelských hodnocení produktů z oblasti IT. Zároveň popisujeme průběh naší práce na automatické extrakci aspektů. Věříme, že tato oblast může účastníky workshopu zaujmout a že tento příspěvek podnítí diskuzi na toto téma s výzkumníky z příbuzných oborů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This work focuses on aspect-based sentiment analysis, a relatively recent task in natural language processing. We present a new dataset for Czech aspect-based sentiment analysis which consists of segments from user reviews of IT products. We also describe our work in progress on the task of aspect term extraction. We believe that this area can be of interest to other workshop participants and that this paper can inspire a fruitful discussion on the topic with researchers from related fields.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Parsito je rychlý závislostní parser napsaný v C++ vydaný jako open-source. Parsito je založené na transition-based parsingu, má vysokou úspěšnost a dosahuje rychlosti 30 tisíc slov za sekundu. Parsito lze natrénovat na libovolných vstupních datech, bez nutnosti navrhovat jazykově závislé rysy, protože používá klasifikátor založený na neuronových sítích. K dispozici jsou natrénované modely pro všechny treebanky z projektu Universal Dependencies (37 treebanků k prosinci 2015).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Parsito is a fast open-source dependency parser written in C++. Parsito is based on greedy transition-based parsing, it has very high accuracy and achieves a throughput of 30K words per second. Parsito can be trained on any input data without feature engineering, because it utilizes artificial neural network classifier. Trained models for all treebanks from Universal Dependencies project are available (37 treebanks as of Dec 2015).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>MorphoDiTa (morfologický slovník a tagger) je open-source nástroj pro morfologickou analýzu textů v přirozených jazycích. Provádí morfologickou analýzu, morfologické generování, tagování a tokenizaci a je distribuován jako samostatný nástroj nebo jako knihovna spolu s natrénovanými lingvistickými modely. V českém jazyce dosahuje MorphoDiTa state-of-the-art výsledků s rychlostí 10-200 tisíc slov za sekundu. MorphoDiTa je svobodný software pod LGPL licencí a jazykové modely jsou zdarma pro nekomerční použití a jsou distribuovány pod CC BY-NC-SA licencí, i když u některých modelů mohou původní data použitá k vytvoření modelu implikovat další licenční podmínky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>MorphoDiTa: Morphological Dictionary and Tagger is an open-source tool for morphological analysis of natural language texts. It performs morphological analysis, morphological generation, tagging and tokenization and is distributed as a standalone tool or a library, along with trained linguistic models. In the Czech language, MorphoDiTa achieves state-of-the-art results with a throughput around 10-200K words per second. MorphoDiTa is a free software under LGPL license and the linguistic models are free for non-commercial use and distributed under CC BY-NC-SA license, although for some models the original data used to create the model may impose additional licensing conditions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>NameTag je open-source nástroj pro rozpoznávání jmenných entity (Named Entity Recognition - NER). NameTag identifikuje vlastní jména v textu a zařazuje je do předem definovaných kategorií, jako jsou názvy osob, míst, organizací, atd. NameTag je distribuován jako samostatný nástroj nebo jako knihovna spolu s natrénovanými lingvistickými modely. V českém jazyce dosahuje NameTag state-of-the-art výkonu (Straková et al.,. 2013). NameTag je svobodný software pod LGPL licencí a jazykové modely jsou zdarma pro nekomerční použití a jsou distribuovány pod CC BY-NC-SA licencí, i když u některých modelů mohou původní data použítá k vytvoření modelu implikovat další licenční podmínky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>NameTag is an open-source tool for named entity recognition (NER). NameTag identifies proper names in text and classifies them into predefined categories, such as names of persons, locations, organizations, etc. NameTag is distributed as a standalone tool or a library, along with trained linguistic models. In the Czech language, NameTag achieves state-of-the-art performance (Straková et. al. 2013). NameTag is a free software under LGPL license and the linguistic models are free for non-commercial use and distributed under CC BY-NC-SA license, although for some models the original data used to create the model may impose additional licensing conditions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku popisujeme přechodový neprojektivní závislostní parser používající klasifikátor založený na neuronových sítích, který nevyžaduje tvorbu rysů. Dále představujeme nové přechodové orákulum, které zvyšuje úspěšnost parseru porovnatelně s dynamickým orákulem, ale je použitelné pro každý přechodový systém, jako například neprojektivní systém s operací swap. Parser je velmi rychlý, jeho modely kompaktní, přičemž dosahuje vysoké úspěšnosti bez potřeby dalších zdrojů jako například korpusů s čistým textem. Parser jsme otestovali na všech 19 korpusech z projektu Universal Dependencies. Implementaci parseru v jazyce C++ uvolňujeme jako open-source.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe a transition-based, non-projective dependency parser which uses a neural network classifier for prediction and requires no feature engineering.  We propose a new, search-based oracle, which improves parsing accuracy similarly to a dynamic oracle, but is applicable to any transition system, such as the fully non-projective swap system. The parser has excellent parsing speed, compact models, and achieves high accuracy without requiring any additional resources such as raw corpora. We tested it on all 19 treebanks of the Universal Dependencies project. The C++ implementation of the parser is being released as an open-source tool.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vytvorili sme citačnú službu na protokole OAI-PMH pre digitálnu knižnicu DSpace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Citing submissions is important but citing data submissions has not an established format yet.

We have created a citation service based on DSpace OAI-PMH endpoint implementation which returns citations of resources specified by PID and in desired format like simple html styled text. We display the citation box in DSpace item view but also in external applications.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CLARIN centrum sa môže nazývať CLARIN-B centrum iba ak spĺňa špecifické kritériá. Digitálny repozitár LINDAT/CLARIN postavený na DSpace spĺňa väčšinu týchto požiadaviek.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This talk presents the steps required for becoming a CLARIN-B centre using LINDAT/CLARIN digital library based on DSpace.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Identifikovali sme štyri základné vlastnosti, ktoré spravili zdieľanie dát v našom repozitári atraktívnejšie.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Our experience show that submitters of data to our repository request specific features in contrast to the established ones. We have identified four such features which we implemented into our repository based on DSpace. Three features are described in this paper in more detail. The last one is described in a separate paper.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zdieľanie dát sa stáva neoddeliteľnou súčasťou vedeckej práce. Pripraviť dáta na zdieľanie vyžaduje vybranie správnej licencie. Popisujeme framework, ktorý takéto zdieľanie umožňuje.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The necessity to share and preserve data and software is becoming more and more important. Without the data and the software, research cannot be reproduced and tested by the scientific community. Making data and software simply reusable and legally unequivocal requires choosing a license for data and software which is not a trivial task. We describe a legal/licensing framework which implements the complete support for licensing submissions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>S přibývajícím množstvím dat a služeb dostupných v infrastruktuře LINDAT/CLARIN roste potřeba správně citovat, sdílet, odkazovat a získávat statistiky využití. Představení frameworku, který zajišťuje jednotný vzhled a zmíněnou funkcionalitu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>With an increasing amount of data and services available in the LINDAT/CLARIN infrastructure comes the need to properly cite, share, cross-reference and gather usage statistics. We introduce a framework that ensures a consistent look and the mentioned functionality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CsEnVi Pairwise Parallel Corpora je soubor dvou paralelních korpusů, anglicko-vietnamského a česko-vietnamského, sestavených z dostupných filmových titulků a titulků přednášek TED. Korpusy byly očištěny naší poloautomatickou filtrací a jsou zarovnány po větách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CsEnVi Pairwise Parallel Corpora is a collection of two parallel corpora: an English-Vietnamese one and a Czech-Vietnamese one. The corpora contain translations of movie and TED subtitles that were already available. The corpora were cleaned using our semi-automatic filtering and are provided aligned at the sentence level.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nástroj pro spojování frázových tabulek prostřednictvím prostředního, pivotního, jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Over the past years, pivoting methods, i.e. machine translation via a third language, gained respectable attention. Various experiments with different approaches and datasets have been carried out but the lack of open-source tools makes it difficult to replicate the results of these experiments. This paper presents a new tool for pivoting for phrase-based statistical machine translation by so called phrase-table triangulation. Besides the tool description, this paper discusses the strong and weak points of various triangulation techniques implemented in the tool.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Soubor obsahuje standardní test set WMT z roku 2013, ručně přeložený z angličtiny do vietnamštiny. Soubor tak představuje obohacení dosavadní multiparalelní kolekce (anglicky, česky, německy, francouzsky, španělsky a rusky) o nový jazyk.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The file contains the standard WMT 2013 test set, manually translated from English to Vietnamese, thus extending the WMT 2013 multi-parallel set of languages (English, Czech, German, French, Spanish, Russian).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme model a algoritmus pro sémantickou analýzu ve formalismu lambda-kalkulus, který je založen na přístupu Constrined Conditional Model.Dále představujeme metodu pro učení omezujících podmínek přímo ze znalostních grafů. V našem probíhajícím průzkumu optimalizujeme model pomocí lineárního programování.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a model and an algorithm for semantic parsing in λ-calculus formalism which is
based on Constrained Conditional Model framework (Roth and Yih, 2005). We also introduce an
approach for learning the constraints of our model directly from knowledge graph which scales up for
new domains easily. In this ongoing project we optimize our model on a dataset using integer linear
programming. Our project includes two parts and our results in the first part show a meaningful
improvement over previous models.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Poruzumění mluvené řeči (Spoken language understanding, SLU) a konkrétněji sémantický parsing je zásadní komponentou každé řečové aplikace. Tento článek podává přehled současného výzkumu v oblasti SLU s důrazem na metody strojového učení používané k tomuto účelu. Sledujeme aktuální trendy v sémantickém parsingu a diskusi uzavíráme výhledem na nejslibnější směry výzkumu do budoucna.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Spoken Language Understanding (SLU) and more specifically, semantic parsing is an indispensable task in each speech-enabled application. In this survey, we review the current research on SLU and semantic parsing with emphasis on machine learning techniques used for these tasks. Observing the current trends in semantic parsing, we conclude our discussion by suggesting some of the most promising future research trends.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Doktorská práce se zabývá lingvistickou analýzou diskurzních vztahů jakožto jednoho z aspektů textové koherence. Diskurzními vztahy rozumíme významové vztahy mezi jednotlivými propozicemi v textu, tzv. diskurzními argumenty. Cílem práce je ucelený popis diskurzních vztahů v češtině a jeho vtělení do anotačního schématu Pražského závislostního korpusu. Práce je rozdělena do tří částí: První z nich je zaměřena na teoretický popis diskurzních vztahů a rozbor vhodnosti různých metodologických postupů při korpusovém zpracování. Druhá část podrobně popisuje navržené schéma pro anotaci diskurzních vztahů a proces vzniku takto značeného korpusu včetně evaluace konzistence značených dat. V poslední části práce se pak věnujeme některým problematickým okruhům při užití navrženého schématu a jejich řešení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This doctoral thesis is devoted to linguistic analysis of discourse relations as one of the aspects of discourse coherence. Discourse relations are semantic relations holding between propositions in a discourse (discourse arguments). The aim of the thesis is a complex description of discourse relations in Czech and its application in an annotation scheme in the Prague Dependency Treebank. The thesis is divided into three parts: The first one is focused on the theoretical description of discourse relations and on analysis of adequacy of various methodological concepts in corpus processing. The second part describes in detail the proposed scheme for the annotation of discourse relations and the process of the corpus build-up including the evaluation of consistency of the annotated data. Finally, in the last part of the thesis, we address some problematic issues arisen with the employment the proposed scheme and look for their possible solutions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek si klade za cíl nalézt v bohatě anotovaném korpusu příznaky přínosné pro automatickou anotaci/detekci atribuce – přiřazení obsahu textu zdrojům, které jej vyjadřují. Určení atribuce je významnou součástí reprezentace a modelování dirkurzních vztahů, ale je rovněž zásadní součástí např. analýzy sentimentu (identifikace zdrojů).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper aims at mining a richly annotated treebank for features relevant in automatic annotation/detection of attribution – ascription of texts contents to agents who expressed them. Resolving attribution is an important component
for representing and modeling discourse structure, but it is also an essential task in opinion mining and sentiment analysis (e.g. identification of sources).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje algoritmus na naučení se morfologických paradigmat a slovníku pro flektivní jazyky, který je nenáročný na zdroje.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a resource-light acquisition
of morphological paradigms and lexicon for fusional languages. It builds upon Paramor [10], an unsupervised system, by extending it: (1) to accept a small seed of manually provided word inflections with marked morpheme boundary; (2) to handle basic allomorphic changes acquiring
the rules from the seed and/or from previously
acquired paradigms. The algorithm has been tested on Czech and Slovene tagged corpora and has shown increased F-measure in comparison with the Paramor baseline.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je druhé vydání treebanků UD, verze 1.1.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the second release of UD Treebanks, Version 1.1.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem Rozpoznávání textu z fotografií (STR) je právně lokalizovat a přespat text zachycený na fotografii z reálného prostředí. Rostoucí úspěšnost rozpoznávání zároveň dělá z těchto textů zajímavý zdroj dat pro zpracování přirozeného jazyka a zároveň přináší nové problémy, které jsou specifické právě pro texty, které se na fotografiích vyskytují.
V tomto článku představujeme učení dekódování textových řetězců v systému STR pomocí metod strukturní predikce, které se využívají při dekódování v rozpoznávání řeči a strojovém překladu. Model při učení využívá jazykové a typografické rysy. Navržená metoda je evaluována  na standardní datové sadě a zvyšuje úspěšnost rozpoznávání znaků i rozpoznávání celých slov.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Scene Text Recognition (STR) is a task of localizing and transcribing textual information captured in real-word images. With its increasing accuracy, it becomes a new source of textual data for standard Natural Language Processing tasks and poses new problems because of the specific nature of Scene Text.
In this paper, we learn a string hypotheses decoding procedure in an STR pipeline using structured prediction methods that proved to be useful in automatic Speech Recognition and Machine Translation. The model allow to employ
a wide range of typographical and language features into the decoding process. The proposed method is evaluated on a standard dataset and improves both character and word recognition performance over the baseline.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této disertační práci zkoumáme strojový překlad mezi češtinou a ruštinou z hlediska lingvisty. 
Hlavním cílem práce je lingvistický rozbor chyb ve výstupu čtyř systémů strojového překladu, dvou experimentálních - TectoMT, Moses - a dvou komerčních - PC Translator a Google Translate. Analyzujeme každý typ chyb a řešíme, zda daná chyba souvisí s rozdílem mezi češtinou a ruštinou nebo zda je zapříčiněná architecturou jednotlivých systémů. Ve zvláštní kapitole se zaměřujeme na chyby v povrchové valenci sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this thesis we analyze machine translation between Czech and Russian languages from the perspective of a linguist. We explore the output of our two experimental systems and two commercial systems: PC Translator and Google Translate. We make a linguistically-motivated classification of errors for the language pair and describe each type of error in detail, analyzing whether it occurred due to some difference between Czech and Russian or is it caused by the system architecture. In particular, we focus on one specific error type - surface valency.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zkoumáme chyby ve VV v statistickém strojovém překladu pro několik jazyků. Poživáme několik metod pro integraci VV do překladového systému.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we analyse the usage of multiword expressions in Statistical Machine Translation. We exploit the  Moses SMT toolkit to train models for French-English and Czech-Russian language pairs. For each language pair, two models were built: a baseline model without additional MWE data and the model enhanced with information on MWE. For the French-English pair, we tried three methods of introducing the MWE data. For Czech-Russian pair, we used just one method - adding automatically extracted data as a parallel corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nový dialogový systém učící se z interakce s uživateli. Systém žádá uživatele o radu u neznámých otázek a zobecňuje je v souladu se svou znalostní bází. Dialogový systém je vyhodnocen prostředníctvím platformy CrowdFlower.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a novel knowledge based dialog system that learns from
interactions with users. It asks users for advice about unknown questions and
generalizes them according to its knowledge base. The dialog system was evaluated
through the CrowdFlower crowdsourcing platform and it showed the ability to adapt
without any prior information except the knowledge base.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje hybridní dialog state tracker, který kombinuje přístup pravidlový s přístupem strojového učení pro trackování stavu dialogu. Strojové učení je v našem trackeru realizováno s využitím Long Short Term Memory (LSTM) sítě. Podle našich současných znalostí hybridní tracker dosahuje state-of-the-art výsledků v soutěži Dialog State Tracking Challenge (DSTC) 2, v kategorii systémů využívající pouze originální SLU jako svůj vstup.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a hybrid dialog state tracker that combines a rule based and
a machine learning based approach to belief state tracking. Therefore, we call it
a hybrid tracker. The machine learning in our tracker is realized by a Long Short
Term Memory (LSTM) network. To our knowledge, our hybrid tracker sets a new
state-of-the-art result for the Dialog State Tracking Challenge (DSTC) 2 dataset
when the system uses only live SLU as its input.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CLARA (Common Language Resources and Their Applications) je projekt v rámci Marie Curie Initial Training Network, který probíhal v letech 2009-2014 a jehož cílem bylo poskytnout vědeckou přípravu mladým vědcům v oblasti jazykových zdrojů a zpracování jazykových dat (např. návrhy jazykových infrastruktur, lexikální sémantické modelování, modelování domény, multimediální a multimodální komunikace, technologie automatického zpracování jazyka a jejich aplikace). Projekt vyústil v nový teoretický vhled do problematiky a jeho výstupem jsou nové jazykové zdroje a nástroje, především však připravil novou generaci výzkumníků, kteří jsou schopni dále obor rozvíjet.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CLARA (Common Language Resources and Their Applications) is a Marie Curie Initial Training Network which ran from 2009 until 2014 with the aim of providing researcher training in crucial areas related to language resources and infrastructure. The scope of the project was broad and included infrastructure design, lexical semantic modeling, domain modeling, multimedia and multimodal communication, applications, and parsing technologies and grammar models. An international consortium of 9 partners and 12 associate partners employed researchers in 19 new positions and organized a training program consisting of 10 thematic courses and summer/winter schools. The project has resulted in new theoretical insights as well as new resources and tools. Most importantly, the project
has trained a new generation of researchers who can perform advanced research and development in language resources and technologies.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška prezentuje český žákovský korpus Czesl, jeho anotační schéma a způsob anotace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Czesl is a corpus of texts produced by non‐native speakers of Czech. To  adequately  annotate the  deviations  in  Czech word‐order  and  a fairly complex inflectional morphology, the corpus uses two tiers of annotation,  each  tier  correcting  different  types  of  errors. Links between the tiers allow capturing errors in word order and complex discontinuous  expressions.  Errors  are  not  only  corrected,  but  also classified. We combine (1) a grammar-based taxonomy of errors in spelling,  morphology,  morphosyntax,  lexicon  and  style,  and  (2)  a formal  error  classification based on surface alternations. The annotation scheme  was  tested  on  a  data set  of  approx.  175,000 words  with  fair  inter‐annotator  agreement  results.  In  addition  to presenting  the  current state  of  the  corpus,  I  will  discuss  various decisions (design, workflow, technical) made during its creation and revisit them with the advantage of hindsight.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem naší práce je prozkoumat možnost využití školních rozborů jako trénovací data pro automatickou syntaktickou analýzu. Implementujeme editor pro procvičování tvaroslovných a větných rozborů. Následně tyto rozbory shromažďujeme a transformujeme do tvaru potřebného pro trénování vybraného parseru. Pilotním jazykem je čeština a cílovým formátem jsou anotace ve stylu Pražského závislostního korpusu. Nyní se zaměřujeme na vyhodnocení větných rozborů a jejich kombinaci s cílem získat kvalitnější rozbory.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The purpose of our work is to explore the possibility of using sentence diagrams produced by schoolchildren as training data for automatic syntactic analysis. We have implemented a sentence diagram editor that schoolchildren can use to practice morphology and syntax. We collect their diagrams, combine them into a single diagram for each sentence and transform them into a form suitable for training a particular syntactic parser. In this study, the object language is Czech, where sentence diagrams are part of elementary school curriculum, and the target format is the annotation scheme of the Prague Dependency Treebank. We mainly focus on the evaluation of individual diagrams and on their combination into a merged better version.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška prezentuje český žákovský korpus Czesl, jeho anotační schéma a způsob anotace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Czesl is a corpus of texts produced by non‐native speakers of Czech. To  adequately  annotate the  deviations  in  Czech word‐order  and  a fairly complex inflectional morphology, the corpus uses two tiers of annotation,  each  tier  correcting  different  types  of  errors. Links between the tiers allow capturing errors in word order and complex discontinuous  expressions.  Errors  are  not  only  corrected,  but  also classified. We combine (1) a grammar-based taxonomy of errors in spelling,  morphology,  morphosyntax,  lexicon  and  style,  and  (2)  a formal  error  classification based on surface alternations. The annotation scheme  was  tested  on  a  data set  of  approx.  175,000 words  with  fair  inter‐annotator  agreement  results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Potřeba dat o akvizici češtiny cizinci vedla k vytvoření prvního žákovského korpusu češtiny. Po představení jeho základního designu a parametrů, se zaměřujeme na technické aspekty: přepis ručně psaných textů, proces anotace a možnosti využití výsledků, spolu s nástroji používanými pro tyto úkoly.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The need for data about the acquisition of Czech by non-native learners prompted the compilation of the first learner corpus of Czech. After introducing its basic design and parameters, including a multi-tier manual annotation scheme and error taxonomy, we focus on the more technical aspects: transcription of hand-written source texts, process of annotation, and options for exploiting the result, together with tools used for these tasks and decisions behind the choices. To support or even substitute manual annotation we assign some error tags automatically and use automatic annotation tools (tagger, spell checker).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Práce se zabývá popisem českého slovosledu kontextově nezapojených participantů. Sleduje, zda v povrchovém slovosledu existuje jejich základní (frekvenčně výrazně převažující) pořadí (srov. narodit se v Brně v roce 1950 vs. narodit se v roce 1950 v Brně). Zároveň se věnuje sledování faktorů, které slovosled ovlivňují (např. formě participantu, způsobu jeho lexikálního vyjádření nebo vlivu slovesné valence). V závěru krátce srovnává slovosledné tendence v češtině a v němčině. Pro ověření stanovených cílů užívá zejména data Pražského závislostního korpusu. Práce teoreticky vychází ze zásad funkčního generativního popisu. Výsledky výzkumu ukazují, že v českém povrchovém slovosledu lze alespoň v některých případech vysledovat určité obecnější tendence k jednomu ze dvou možných slovosledných pořadí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presented thesis is focused on the Czech word order of contextually non-bound verbal modifications. It monitors whether there is a basic order in the contextually non-bound part of the sentence (significantly predominant in frequency) in the surface word order (cf. narodit se v Brně v roce 1950 vs. narodit se v roce 1950 v Brně; literally to be born in Brno in 1950 vs. to be born in 1950 in Brno). At the same time, we try to find out the factors influencing the word order (such as the form of modifications, their lexical expression or the effect of verbal valency). Finally, we briefly compare the word order tendencies in Czech and German. For the verification of the objectives, mainly the data from the Prague Dependency Treebank are used. The work is based on the theoretical principles of Functional Generative Description. Research results demonstrate that, at least in some cases, it is possible to detect certain general tendencies to use preferably one of two possible surface word order sequences in Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Práce se zabývá slovosledem aktantů (aktoru a patientu) v ohniskové části českých vět. Slovosledná analýza výskytů aktoru a patientu přináší popis faktorů, které mohou ovlivnit uspořádání větných participantů ve slovosledu jako takových.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper deals with word order of inner participants (Actor and Patient) in the focus-part of Czech sentences. The analysis of the sequence of Actor and Patient reveals the criteria that may influence the arrangement of sentence participants as such.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Poster představil tabulku kontextové zapojenosti závislých vět v češtině a popisuje důvody, proč bývají některé typy závislých vět českými pisateli častěji prezentovány jako čtenáři známé.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper deals with the established scale of contextual boundness of subordinate clauses in Czech and describes the reasons why some types of categories are more often presented as known then others by the Czech writers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje 40. ročník Olympiády v českém jazyce – přináší komentovaný přehled úkolů a hodnotí úspěšnost řešitelů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article describes the 40th year of the Olympics in the Czech language – it provides an commented list of tasks and evaluate the success of investigators.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku je testována hypotéza, že valenčně obligatorní volná slovesná doplnění v povrchovém slovosledu následují za neobligatorními (fakultativními). Užité metody hypotézu nepotvrdily,  ale výsledky výzkumu ukázaly, že slovosled kontextově nezapojených volných slovesných doplnění v češtině ovlivňují jiné faktory – pozice věty v textu, forma a délka slovesného doplnění a jeho syntakticko-sémantická role (funktor).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper tests a hypothesis that obligatory adverbials (in terms of the valency)
follow the non-obligatory (i.e. optional) ones in the surface word order. Neither of the used methods has proved the given hypothesis but according to the results, there are several other
features that influence word order of contextually non-bound free modifiers of a verb in Czech, namely position of the sentence in the
text, form and length of the verb modifiers (the whole subtrees), and the semantic dependency relation (functor) of the modifiers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek je aktualizovanou a rozšířenou verzí článku z konference DepLing 2011. Reprezentuje první krok v porovnání vlastností syntakticko-sémantických vztahů přítomných ve struktuře věty a jejich ekvivalentů ve struktuře diskurzu. Studie byla provedena na ručně anotovaných datech Pražského závislostního korpusu (PDT). Podle analýzy podkladové syntaktické struktury věty (tektogramatiky) v PDT rozlišujeme řadu typů vztahů, které mohou být vyjádřeny jak v jedné větě (tj. ve stromu), tak i v delším textu, napříč hranicemi vět (mezi stromy). Tvrdíme, že tyto vztahy na jednu stranu zachovávají své sémantické vlastnosti jak uvnitř věty, tak v delším textu (např. příčinný vztah zůstává příčinným vztahem), na druhou stranu ale v souladu se sémantickými vlastnostmi vztahů je jejich distribuce uvnitř vět či mezi větami velice rozličná. V této studii toto pozorování ověřujeme na třech případech (na podmínkovém, specifikačním a opozičním vztahu) a dokládáme podobným chováním anglických dat projektu Penn Discourse Treebank.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present contribution is an updated and extended version of the paper from the conference DepLing 2011. It represents the first step in comparing the nature of syntactico-semantic relations present in the sentence structure to their equivalents in the discourse structure. The study is carried out on the basis of a Czech manually annotated material collected in the Prague Dependency Treebank (PDT). According to the analysis of the underlying syntactic structure of a sentence (tectogrammatics) in the PDT, we distinguish various types of relations that can be expressed both within a single sentence (i.e. in a tree) and in a larger text, beyond the sentence boundary (between trees). We suggest that, on the one hand, semantic nature of each type of these relations corresponds both within a sentence and in a larger text (i.e. a causal relation remains a causal relation) but, on the other hand, according to the semantic properties of the relations, their distribution in a sentence or between sentences is very diverse. In this study, this observation is analyzed in detail for three cases (relations of condition, specification and opposition) and further supported by similar behaviour of the English data from the Penn Discourse Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zpráva z 25. ročníku evropské letní školy logiky, jazyka a informace v Düsseldorfu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A report from the 25th European Summer School in Logic, Language and Information in Düsseldorf.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek je věnován automatické detekci vulgarismů, tj. význačných indikátorů negativního hodnocení, v rámci postojové analýzy. Zaměříme se na význam a kategorizaci vulgarismů a především popíšeme základní strategie při vytváření nových vulgarismů v internetové komunikaci, od přidávání nepísmenných znaků přes záměnu písmen po užívání speciálních zkratek a výrazů na pomezí argotu. Dále navrhneme základní řešení pro automatické zpracování těchto slov.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This talk is focused on automatioc swear words detection in sentiment analysis. We outline swear words categorization and describe basic strategies when creating new vulgarisms within online communication.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku představujeme Archiv vizuální historie MALACH jako multimodální zdroj dat pro postojovou analýzu v češtině, případně v dalších jazycích, které archiv obsahuje.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we introduce the Visual History Archive of the USC Shoah Foundation as a multimodal data resource for sentiment analysis in Czech, but potentially all thirty three languages it contains. We take the opportunity of having both physical access to these unique data and the well-established research group on sentiment analysis at Charles University in Prague. Our aim is to provide methodology for sentiment annotation of these multimodal data combining subjectivity detection within a treebank with spoken term detection.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento výzkum rozvíjí myšlenku ekvikomplexity typologicky odlišných jazyků za pomocí konstrukční analýzy struktur obsahujících  adkolokační verbální idiomy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This survey serves as an illustration of equalization tendency in typologically different languages, using constructional analysis of structures containing adcollocational verbal idioms.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předběžný výzkum struktury evaluativního významu v češtině. Popisujeme způsoby, jakými je evaluativní význam modelován na různých úrovních popisu jazyka, především však na rovině morfologické. Při formalizaci využíváme konstrukčně-gramatického formalismu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we present a preliminary research into the linguistic structure of evaluative meaning in Czech. We describe the ways the evaluative meaning is modelled in Czech using means from different layers of linguistic description, mainly morphology. Moreover, we use the construction grammar framework (see Fried and Östman 2004) to capture evaluative sentences and to depict the relationship between structure, meaning and use of evaluative expressions in language, joining the growing body of constructional research concerning the expressions of subjective judgement, broadly defined (e.g. Matsumoto 2008, Fried and Östman 2009, Terkourafi 2010).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce se zaměříme na problematiku postojové analýzy, tedy lingvistického zkoumání emocí, názorů a postojů vyjádřených v textu nebo v řeči. Shrneme dosavadní bádání v oboru a zaměříme se na jednotlivé aspekty vyjadřování emocí: na stránku lexikální (slovník hodnotících výrazů pro češtinu, evaluativní idiomy), gramatickou (význam jednotlivých slovních druhů, typické syntaktické vzorce hodnotících vět) a především sémantickou a pragmatickou. Dále představíme dostupné aplikace vycházející z postojové analýzy a nastíníme problémy a výzvy, které jsou v této disciplíně aktuální – např. automatická detekce cílů hodnocení či zapojení nadvětné syntaxe.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this talk we focus on sentiment analysis, i.e. linguistic analysis of emotions and opinions expressed in speech or in text.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek byl věnován problematice neshodného genitivního přívlastku, konkrétně možnostem jeho pronikání do antepozice. V hlavní části jsme se věnovali kategorizaci neshodných genitivních přívlastků vyskytujících se v antepozici na datech z PDT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk was dedicated to syntactic position of non-conguent attribute expressed by a noun in genitive, including categorization based on PDT data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje možnosti vylepšení automatické detekce hodnocených entit v rámci postojové analýzy za využití metod kvantitativní lingvistiky, přesněji tematické koncentrace textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This contribution describes improvements of opinion target identification in sentiment analysis, employing methods from quantitative linguistics, namely thematic concentration of the text.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem tohoto článku je představit Czech subjectivity lexicon, nový lexikální zdroj pro analýzu sentimentu v češtině. Popisujeme jednotlivé fáze manuálních úprav slovníku a demonstrujeme jeho využití při klasifikaci polarity, respektive pro trénování Maximum Entrophy klasifikátoru. Úspěšnost klasifikátoru obohaceného o slovník testujeme na několika datasetech. Na základě výsledků navrhujeme vylepšení stávajícího systému.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of this paper is to introduce the Czech subjectivity lexicon, a new lexical resource for sentiment analysis in Czech. We describe particular stages of the manual refinement of the lexicon and demonstrate its use in the state-of-the-art polarity classifiers, namely the Maximum Entrophy classifier. We test the success rate of the system enriched with the dictionary on different data sets, compare the results and suggest some further improvements of the lexicon-based classification system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přestože je postojová analýza široce zkoumaným odvětvím, většina výzkumu probíhá na prostém textu. Tento poster ukazuje využití závislostních dat z PDT pro nejrůznější úlohy v rámci opinion miningu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Although sentiment analysis has been gaining a booming interest recently, most experiments use only plain text corpora. This poster shows that an existing richly annotated Prague Dependency Treebank can be exploited as a source of valuable information important for various opinion mining subtasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku popisujeme generalizované vzorce výskytu postojových cílů (opinion targets) v evaluativních konstrukcích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we describe and generalize syntactic patterns of structures containing opinion targets, i.e. evaluated entities, taking into account their semantic properties.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje systém pro automatickou extrakci hodnocených cílů, se kterým jsme se zúčastnili soutěže SemEval 2014. Systém je pravidlový, bez zapojení strojového učení, přesto však dosahuje relativně dobrých výsledků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes our submission to SemEval
2014 Task 41 (aspect based sentiment analysis). The current work is based on the assumption that it could be advantageous to connect the subtasks into one workflow, not necessarily following their
given order. We took part in all four subtasks (aspect term extraction, aspect term polarity, aspect category detection, aspect category polarity), using polarity items detection via various subjectivity lexicons and employing a rule-based system applied on dependency data. To determine aspect categories, we simply look up their WordNet hypernyms. For such a basic method using no machine learning techniques, we consider the results rather satisfactory.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Používání funkčních slov v neřízeném závislostním parsingu. Používání funkčních slov v neřízeném závislostním parsingu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we show some properties of function words in dependency trees. Function
words are grammatical words, such as articles, prepositions, pronouns, conjunctions, or auxiliary verbs. These words are often short and very frequent in texts and therefore many of them can be easily recognized. We formulate a hypothesis that function words tend to have a fixed number of dependents and we prove this hypothesis on treebanks. Using this hypothesis, we are able to improve unsupervised dependency parsing and outperform previously published state-of-the-art results for many languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Při nasazování dialogového systému pro novou doménu je nutné se vypořádat s nedostatkem trénovacích dat pro doménově specifické statistické modely. V tomto článku popisujeme své zkušenosti s vytvářením dialogového systému pro informace o veřejné dopravě a počasí přímo za provozu s uživateli z řad veřejnosti. Postupovali jsme inkrementálně od minimálního systému, který byl nasazen na bezplatné telefonní číslo ke sběru řečových dat. Na získaných datech jsme byli schopni natrénovat statistické modely – doménové jazykové modely pro rozpoznávání řeči a model pro porozumění jazyku, který používá automaticky generovanou sémantickou anotaci. Náš postup ukazuje, že úspěšný systém lze postavit i s minimálním úsilím a bez předem dostupných trénovacích dat pro danou doménu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>When deploying a spoken dialogue system in a new domain, one faces a situation where little to no data is available to train domain-specific statistical models. We describe our experience with bootstrapping a dialogue system for public transit and weather information in real-word deployment under public use. We proceeded incrementally, starting from a minimal system put on a toll-free telephone number to collect speech data. We were able to incorporate statistical modules trained on collected data – in-domain speech recognition language models and spoken language understanding – while simultaneously extending the domain, making use of automatically generated semantic annotation. Our approach shows that a successful system can be built with minimal effort and no in-domain data at hand.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme metodu strojového učení („s učitelem“) pro detekci a výběr slovesných valenčních rámců, tj. specifický druh desambiguace významu sloves založený na informacích o subkategorizaci; odpovídá to detekci zmínek o událostech v textu. Používáme bohaté závislostní informace z Pražských závislostních korpusů češtiny a angličtiny a několika dříve vyvinutých nástrojů (taggery, parsery).

Výběr rámce je založen na slovnících ručně sestavených pro tyto korpusy – PDT-Vallexu pro češtinu a EngVallexu pro angličtinu. Výsledky ukazují, že detekce predikátů je snažší v češtině, ale při výběr správného rámce jsme dosáhli lepších výsledků v angličtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a supervised learning method for verbal valency frame detection and selection, i.e., a specific kind of word sense disambiguation for verbs based on subcategorization information, which amounts to detecting mentions of events in text. We use the rich dependency annotation present in the Prague Dependency Treebanks for Czech and English, taking advantage of several analysis tools (taggers, parsers) developed on these datasets previously. 

The frame selection is based on manually created lexicons accompanying these treebanks, namely on PDT-Vallex for Czech and EngVallex for English. The results show that verbal predicate detection is easier for Czech, but in the subsequent frame selection task, better results have been achieved for English.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento balíček obsahuje sady paralelních vět pro vývoj a testování strojového překladu souhrnů vědeckých článků z oboru medicíny mezi češtinou, angličtinou, francouzštinou a němčinou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This package contains data sets for development and testing of machine translation of sentences from summaries of medical articles between Czech, English, French, and German.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Lékařská překladová úloha na WMT 2014 představuje zajímavou výzvu pro strojový překlad (machine translation, MT). Ve standardní překladové úloze je koncovou aplikací překlad sám o sobě. Naproti tomu v této úloze je systém strojového překladu součástí většího systému pro mezijazykové vyhledávání informací (information retrieval, IR).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The WMT 2014 Medical Translation Task poses an interesting challenge for Machine Translation
(MT). In the standard translation task, the end application is the translation itself. In this task, the MT system is considered a part of a larger system for cross-lingual information retrieval (IR).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme sadu nahrávek telefonních hovorů v angličtině a češtině, vytvořenou pro trénování akustických modelů pro automatické rozpoznávání řeči v hlasových dialogových systémech. Data sestávají ze 45 hodin nahrávek v angličtině a více než 18 hodin v češtině. Všechna data a část transkripcí byla získána pomocí crowdsourcingu, zbytek byl přepsán profesionálně. Data zveřejňujeme společně se skripty pro preprocessing a sestavení akustických modelů v nástrojích HTK a Kaldi, včetně modelů natrénovaných na našich datech. Data jsou licencována pod CC-BY-SA 3.0, skripty pod licencí Apache 2.0. V článku popisujeme metodiku sběru dat, jejich velikost a vlastnosti, a trénovací skripty a jejich použití. Použitelnost dat a skriptů demostrujeme natrénováním a validací akustických modelů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a dataset of telephone conversations in English and Czech, developed to train acoustic models for automatic speech recognition (ASR) in spoken dialogue systems (SDSs). The data comprise 45 hours of speech in English and over 18 hours in Czech. All audio data and a large part of transcriptions was collected using crowdsourcing; the rest was transcribed by hired transcribers. We release the data together with scripts for data re-processing and building acoustic models using the HTK and Kaldi ASR toolkits. We publish the trained models described in this paper as well. The data are released under the CC-BY-SA 3.0 license, the scripts are licensed under Apache 2.0. In the paper, we report on the methodology of collecting the data, on the size and properties of the data, and on the scripts and their use. We verify the  usability of the datasets by training and valuating acoustic models using the presented data and scripts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vystadial 2013 ASR trénovací skripty obsahují skripty pro HTK a KALDI vyvinuté pro trénování akustických modelů pro automatické rozpoznávání řeči v dialogových systémech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Vystadial 2013 ASR training scripts  provides ASR training scripts for HTK or KALDI developed for training acoustic models for automatic speech recognition in spoken dialogue systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vystadial 2013 je databáze telefonních hovorů v češtině, vyvinuté pro trénování akustických modelů pro automatické rozpoznávání řeči v dialogových systémech. Data obsahují více než 15 hodin v českém jazyce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Vystadial 2013 is a dataset of telephone conversations in Czech, developed for training acoustic models for automatic speech recognition in spoken dialogue systems. The data comprise over over 15 hours in Czech, plus orthographic transcriptions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vystadial 2013 je databáze telefonních hovorů v anglickém jazyce, vyvinuté pro trénování akustických modelů pro automatické rozpoznávání řeči v dialogových systémech. Data obsahují více než 41 hodin v anglickém jazyce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Vystadial 2013 is a dataset of telephone conversations in English, developed for training acoustic models for automatic speech recognition in spoken dialogue systems. The data comprise over over 41 hours in English, plus orthographic transcriptions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>"Lehká" slovesa představující určitý typ komplexních predikátů představují značný problém jak pro teoretickou, tak pro komputační lingvistiku. Syntaktické struktury, které daná slovesa zakládají, jsou dány nejen slovesem, ale i predikativním elementem, se kterým se daná slovesa kombinují. V uvedeném příspěvku jsou analyzovány ze syntaktického i sémantického hlediska česká "lehká" slovesa kombinovaná s predikativními jmény. Je navržena jejich teoreticky adekvátní a zároveň úsporná lexikografická reprezentace. Dále jsou probírány zejména dva aspekty: možnost sestavení inventáře českých "lehkých" sloves a kritéria pro jejich rozpoznání.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Light verbs – representing a type of complex predicates – pose a serious challenge for both theoretical and applied linguistics as their syntactic structures are determined not solely by verbs alone but also by predicative elements with which light verbs combine. In this contribution, syntactic and semantic properties of Czech light verbs combined with predicative nouns are discussed and an adequate and economical lexicographic representation of these phenomena are proposed. Moreover, two tasks are addressed: compiling an inventory of Czech light verbs entering into combination with predicative nouns and adopting criteria for distinguishing light verbs from main verbs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Práce předkládá návrh formalizovaného lexikografického modelu alternací,tedy změn v syntaktické struktuře sloves, a jejich lingvisticky adekvátního, přitom ekonomického popisu ve slovníku. Jádrem práce je podrobný rozbor dvou vybraných typů lexikalizovaných alternací. Na základě této analýzy je navržen systém popisu alternací pro češtinu sestávající z lexikálních jednotek propojených atributy určujícími typy možných alternací (datová komponenta slovníku) a z obecných pravidel popisujících korespondenci situačních participantů a valenčních doplnění (pravidlová komponenta slovníku).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this work, a formal model of a lexicographic description of selected changes in valency structure of Czech verbs, alternations, is proposed. This model is based on a thorough analysis of two types of Czech alternations. The proposed lexicographic representation supposes separate lexical units of verbs linked by a special attribute in the data component of the lexicon and general rules determining the mapping between participants and valency complementations of verbs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek pojednává o českých reflexivních slovesech z lexikografického hlediska. Ukazujeme, že reflexivní morfémy se/si plní v češtině různé funkce: (i) jsou to buď prostředky slovotvorného procesu reflexivizace, nebo (ii) prostředky vlastní (syntaktické) reflexivity, reciprocity, příp. diatezí. Všechny uvedené jazykové procesy jsou spoojeny se změnami ve valenční struktuře sloves. V článku podáváme návrh na jejich zachycení ve valenčním slovníku VALLEX.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Thi paper deals with Czech reflexive verbs from the lexicographic point of view. We show that
the Czech reflexive morphemes se/si constitute different linguistic meanings: either they are formal means of the word formation process of the so called reflexivization, or they are associated with the syntactic phenomena of reflexivity, reciprocity, and diatheses.
All of these processes are associated with changes in the valency structure of verbs. We formulate a proposal for their lexicographic representation for the valency lexicon of Czech verbs, VALLEX.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Roland Wagner v článku zaměřeném na česká reflexivní slovesa publikovaném v PBML prezentuje kritické výhrady k Funkčnímu generativnímu popisu (FGP). Pro autory FGP představuje jeho článek výzvu k doplnění mezer v analýze daného jevu v rámci FGP a k vyjasnění některých teoretických předpokladů týkajících se valence sloves a jejich reflexivních protějšků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Roland Wagner’s contribution published in the last volume of the PBML journal – focusing
(among other ideas) on the role of Czech reflexives – presents several critical remarks concerning the Functional Generative Description. These remarks represent a good challenge for the
authors developing this model to fill empirical gaps and to make clear some theoretical presuppositions concerning valency frames of verbs and their respective reflexive counterparts that are primarily addressed by RW’s critical survey.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Závislostní sémantickou analýzu se širokým pokrytím definujeme jako úlohu nalézt skladební dvojice mezi všemi autosémantickými slovy ve větě, tj. strukturu sémantických závislostí, která reprezentuje jádro významu věty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We define broad-coverage semantic dependency parsing as the task of recovering sentence-internal predicate-argument relationships for all content words i.e. a semantic dependency structure that constitutes the core structure of sentence meaning.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci představujeme dva nedávno vydané open-source nástroje: NameTag je volně šiřitelný software pro rozpoznávání pojmenovaných entit, který dosahuje nejlepších známých výsledků na češtině; MorphoDiTa provádí morfologickou analýzu (s lematizací), morfologické generování, značkování a tokenizaci s nejlepšími známými výsledky pro češtinu a rychlostí zpracování kolem 10-200 tisíc slov za sekundu. Nástroje mohou být natrénovány pro libovolný jazyk, pro který jsou k dispozici anotovaná data, jsou však zvlášť navrženy tak, aby byly efektivní pro flexivní jazyky. Oba nástroje jsou volně šiřitelné pod licencí LGPL a jsou distribuovány spolu z předtrénovanými lingvistickými modely, které jsou zdarma pro nekomerční využití podle licence CC BY-NC-SA. Vydání zahrnují samostatné nástroje, knihovny v C++ s vazbami pro Javu, Python a Perl, a konečně webové služby.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present two recently released open-source taggers: NameTag is a free software for named entity recognition (NER) which achieves state-of-the-art performance on Czech; MorphoDiTa (Morphological Dictionary and Tagger) performs morphological analysis (with lemmatization), morphological generation, tagging and tokenization with state-of-the-art results for Czech and a throughput around 10-200K words per second. The taggers can be trained for any language for which annotated data exist, but they are specifically designed to be efficient for inflective languages. Both tools are free software
under LGPL license and are distributed along with trained linguistic models which are free for non-commercial use under the CC BY-NC-SA license. The releases include standalone tools, C++ libraries with Java, Python and Perl bindings and web services.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Votter Corpus je nový anotovaný soubor sociálních dotazů a odpovědí. Votter Corpus je nový ve svém použití formátu mobilních aplikací a románu v jeho pokrytí specifických demografických. S více než 26 000 hlasy a téměř 1 miliony hlasů pokrývá Votter Corpus každodenní otázky a odpovědi, a to především pro uživatele, kteří jsou ve věku od 13 do 24 let.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Votter Corpus is a new annotated corpus of social polling questions and answers. The Votter Corpus is novel in its use of the mobile application format and novel in its coverage of specific demographics. With over 26,000 polls and close to 1 millions votes, the Votter Corpus covers everyday question and answer language, primarily for users who are female and between the ages of 13-24. The corpus is annotated by topic and by popularity of particular answers. The corpus contains many unique characteristics such as emoticons, common mobile misspellings, and images associated with many of the questions. The corpus is a collection of questions and answers from The Votter App on the Android operating system. Data is created solely on this mobile platform which differs from most social media corpora. The Votter Corpus is being made available online in XML format for research and non-commercial use. The Votter android app can be downloaded for free in most android app stores.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Reprezentace AMR abstrahují od morfologicko- syntaktických jevů, protože mnohé z nich mohou být důvodem mezijazykových rozdílů. V článku se snažíme zjistit, zda by AMR mohly napomoci počítačovému překladu. Využíváme k tomu porovnání překladu a anotace anglických vět do češtiny a čínštiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Abstract Meaning Representations (AMRs) are rooted, directional and labeled graphs that abstract away from morpho-syntactic idiosyncrasies such as word category (verbs and nouns), word order, and function words (determiners, some prepositions). Because these syntactic idiosyncrasies account for many of the cross-lingual differences, it would be interesting to see if this representation can serve, e.g., as a useful, minimally divergent transfer layer in machine translation. To answer this question, we have translated 100 English sentences that have existing AMRs into Chinese and Czech to create AMRs for them. A cross-linguistic comparison of English to Chinese and Czech AMRs reveals both cases where the AMRs for the language pairs align well structurally and cases of linguistic divergence.
We found that the level of compatibility of AMR between English and Chinese is higher than between English and Czech. We believe this kind of comparison is beneficial to further refining the annotation standards for each of the three languages and will lead to more compatible annotation guidelines between the languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Univerzální závislosti (Universal Dependencies) je projekt, jehož cílem je vytvořit mezijazykově konzistentní závislostní anotaci pro mnoho jazyků, s cílem zjednodušit vývoj jazykově nezávislých parserů, mezijazykové učení a výzkum parsingu z pohledu jazykové typologie. Anotační schéma je založeno na (univerzálních) Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a Intersetu, interlinguy pro sady morfologických značek (Zeman, 2008).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). The general philosophy is to provide a universal inventory of categories and guidelines to facilitate consistent annotation of similar constructions across languages, while allowing language-specific extensions when necessary.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku popisujeme našu účasť v úlohe Search v  Search and Hyperlinking Task vrácmi Benchmarku MediaEval 2014. Na základe našich experimentov porovnávame dva typy segmentácie: segmentácia na úseky rovnakej dĺžky a segmentácia, ktorá  využíva rozhodovacie stromy. Taktiež ukážeme užitočnosť využitia metadát a preskúmame odstraňovanie prekrývajucich sa relevantných segmentov.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we describe our participation in the Search part of the Search and Hyperlinking Task in MediaEval Benchmark 2014. In our experiments, we compare two types of segmentation: fixed-length segmentation and segmentation employing Decision Trees on various features. We also show usefulness of exploiting metadata and explore removal of overlapping retrieved segments.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článok sa zaoberá vyhľadávaním informácií v audio-vizuálnych nahrávkach. Takéto nahrávky sú často dlhé a pre užívateľa môže byť preto dôležité vyhľadanie presného relevantného úseku. Pri vyhľadávaní relevantných segmentov sú nahrávky najprv rozdelené na menšie časti, na ktoré sú aplikované štandardné metódy vyhľadávania informácií. V článku porovnávame niekoľko metód, ktoré slúžia na segmentáciu nahrávok, zvlášť sa zameriavame na prístupy založené na strojovom učení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper deals with Information Retrieval from audio-visual recordings. Such recordings are often quite long and users may want to find the exact starting points of relevant passages they search for. In Passage Retrieval, the recordings are automatically segmented into smaller parts, on which the standard retrieval techniques are  applied. In this paper, we discuss various techniques for segmentation of audio-visual recordings and focus on machine learning approaches which decide on segment boundaries based on various features combined in a decision-tree model. Our experiments are carried out on the data used for the Search and Hyperlinking Task and Similar Segments in Social Speech Task of the MediaEval Benchmark 2013.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku prezentujeme naše experimenty vrámci úlohy Hyperlinking v Search and Hyperlinking Task na Benchmarku MediaEval 2014. Náš systém úspešne kombinuje príznaky rôznych modalít (textové, vizuálne, prozodické). V článku tiež ukážeme vhodnosť použitia nami navrhnutých segmentačných metód, ktoré využívajú rozhodovacie stromy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this report, we present our experiments performed for the Hyperlinking part of the Search and Hyperlinking Task in MediaEval Benchmark 2014. Our system successfully combines features from multiple modalities (textual, visual, and prosodic) and confirms the positive effect of our former method for segmentation based on Decision Trees.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Podmnožina organizátorů Úlohy 8 na SemEvalu 2014 se pokusila použít k řešení úlohy parsery, které byly již dříve vyvinuty pro jednotlivé datové formáty využité v této úloze. Kombinace výstupů těchto parserů byla zařazena jako samostatné řešení otevřené části úlohy (open track). Použité systémy byly typicky vyvíjeny souběžně s anotací dat, konkrétně (a) pro formát DM jde o parser nad ručně sestavenou English Resource Grammar; (b) pro formát PAS jde o systém Enju s pravděpodobnostní HPSG, získanou lingvistickou projekcí PTB; a (c) pro formát PCEDT jde o scénář anglické tektogramatické analýzy v prostředí Treex, zahrnující statistický závislostní analyzátor a řadu cílených zpracovacích bloků, které převádějí stromy z analytické na tektogramatickou rovinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>As a submission of its own to the open track of Task 8 at SemEval 2014, a subset of the organizers sought to connect the task to pre-existing, ‘in-house’ parsing systems that can output the same types of semantic dependency graphs as used in the task. For each of the three target formats, there is an existing parsing system, which typically was developed in parallel to the creation of the target dependency graphs, viz. (a) for the DM format, the parser using the hand-engineered English Resource Grammar; (b) for the PAS format, the Enju parsing system, with its probabilistic HPSG acquired through linguistic projection of the PTB; and (c) for the PCEDT format, the scenario for English tectogrammatical analysis within the Treex framework, comprising a pipeline of a syntactical data-driven dependency parser and a number of hand-engineered processing blocks that convert trees from the analytical to the tectogrammatical layer.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku popisujeme vydání rozsáhlého jednojazyčného korpusu urdštiny s automatickým značkováním slovních druhů. Navazujeme na práci Jawaid a Bojar (2012), kde byly pro značkování použity tři taggery a finální výsledek určilo jejich hlasování. Používáme stejnou komplexní sestavu na velký jednojazyčný korpus a výsledek zpřístupňujeme veřejnosti. Kromě toho na tomto velkém korpusu trénujeme jeden samostatný tagger, což, doufáme, podstatě zjednoduší zpracování urdštiny. Tento samostatný tagger na nezávislých testovacích datech dosahuje přenosti 88,74 %.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we describe a release of a sizeable monolingual Urdu corpus automatically tagged with part-of-speech tags.
We extend the work of Jawaid and Bojar (2012) who use three different taggers and then apply a voting scheme to
disambiguate among the different choices suggested by each tagger. We run this complex ensemble on a large monolingual
corpus and release the tagged corpus. Additionally, we use this data to train a single standalone tagger which will
hopefully significantly simplify Urdu processing. The standalone tagger obtains the accuracy of 88.74% on test data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje shrnutí existujících jazykových zdrojů pro překlad z angličtiny do urdštiny a demonstruje výsledky dosažené frázovým a hierarchickým statistickým překladem na třech oficiálních testovacích sadách vět. Ve všech případech hierarchický překlad dopadá lépe.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of this paper is to categorize and present the existence of resources for English-
to-Urdu machine translation (MT) and to establish an empirical baseline for this task.
By doing so, we hope to set up a common ground for MT research with Urdu to allow
for a congruent progress in this field. We build baseline phrase-based MT (PBMT) and
hierarchical MT systems and report the results on 3 official independent test sets. On all
test sets, hierarchial MT significantly outperformed PBMT. The highest single-reference
BLEU score is achieved by the hierarchical system and reaches 21.58% but this figure
depends on the randomly selected test set. Our manual evaluation of 175 sentences
suggests that in 45% of sentences, the hierarchical MT is ranked better than the PBMT
output compared to 21% of sentences where PBMT wins, the rest being equal.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek navazuje na starší práce o dvoukrokovém překladu a rozšiřuje je ve dvou směrech. Jednak v prvním kroku nahrazujeme frázový překlad hierarchickým, a jednak na rozhraní mezi prvním a druhým krokem používáme svaz slov jako záměrně víceznačnou reprezetanci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The idea of two-step machine translation was introduced to divide the complexity of the search space into two independent steps: (1)
lexical translation and reordering, and (2) conjugation and declination in the target language. In this paper, we extend the two-step
machine translation structure by replacing state-of-the-art phrase-based machine translation with the hierarchical machine translation in
the 1st step. We further extend the fixed string-based input format of the 2nd step with word lattices (Dyer et al., 2008); this provides
the 2nd step with the opportunity to choose among a sample of possible reorderings instead of relying on the single best one as produced
by the 1st step.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme metodu poloautomatické extrakce slovenských víceslovných výrazů ze závislostního korpusu. Proces používá automatickou konverzi ze závislostních syntaktických stromů do hloubkové syntaxe a automatické značkování slovesných doplnění na základě valenčního slovníku. Jak valenční slovník tak konverze syntaktického korpusu vznikla úpravou podobného nástroje pro češtinu; autmomaticky přeložený valenční slovník byl ručně zkontrolován a opraven. Přínos této práce je dvojí – valenční slovník slovenských víceslovných výrazů s přímými odkazy na odpovídající výrazy v českém slovníku PDT-Vallex a metoda pro extrakci víceslovných výrazů ze Slovenského závislostního korpusu. Práce na projektu stále probíhá, cílem je 1) vytvořit slovenský valenční slovník paralelní k českému a 2) použít extrahované slovesné rámce v kolokačním slovníku slovenských sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe a method for semi-automatic extraction of Slovak multiword expressions (MWEs) from a dependency treebank. The process uses an automatic conversion from dependency syntactic trees to deep syntax and automatic tagging of verbal argument nodes based on a valency dictionary. Both the valency dictionary and the treebank conversion were adapted from the corresponding Czech versions; the automatically translated valency dictionary has been manually proofread and corrected. There are two main achievements – a valency dictionary of Slovak MWEs with direct links to corresponding expressions in the Czech dictionary, PDT-Vallex, and a method of extraction of MWEs from
the Slovak Dependency Treebank. The extraction reached very high precision but lower recall in a manual evaluation. This is a work in progress, the overall goal of which is twofold: to create a Slovak language valency dictionary paralleling the Czech one, with bilingual links; and to use the extracted verbal frames in a collocation dictionary of Slovak verbs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Poster představující souhrnné informace o Centru vizuální historie Malach, jeho aktivitách a dostupných archivních zdrojích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Poster summarizing the basic information on Malach Center for Visual History, its activities and available archival resources.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Různé pokusy o konceptualizaci často vágně užívaného pojmu kolektivní paměť dospívají k závěru, že kolektivní paměť je hluboce provázaná s jazykovými a narativními fenomény. V tomto příspěvku je mým cílem předložit přehled a diskusi souvislosti mezi jazykem a kolektivní pamětí v kontextu sociální teorie. V díle zakládajících teoretických postav M. Halbwachse a J. Assmanna je význam jazyka ve vztahu k otázkám kolektivní paměti chápán jako ústřední. Stejně tak v posledních dvou dekádách se ze specifické role narativu a konverzace stává významný předmět teoretického výzkumu kolektivní paměti. Na druhou stranu, v empirických šetřeních jako by byl vztah jazyka a kolektivní paměti spíše podceňován. Ukazuje se, že různé společenskovědní a humanitní disciplíny se s podobnými tématy vyrovnávají značně odlišně a rozdíly lze nalézt také v míře explicitní pozornosti věnované tématu kolektivní paměti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Various attempts to conceptualize the often vaguely used term collective memory come to the conclusion that collective memory is deeply related to linguistic and narrative phenomena. In the present paper, I aim to provide an overview and discussion of the link between language and collective memory in the context of social theory. In the case of the founding theoretical figures, M. Halbwachs and J. Assmann, the importance of language in relation to the issues of collective memory is profound. In the past two decades, the specific role of narrative and conversation had become an important subject in researching collective memory. In empirical research, on the other hand, the relationship of language and collective memory seems to be rather underrepresented. Various fields and disciplines deal with similar topics quite differently, and they also differ in the degree of explicit scrutiny of the collective memory phenomena.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dialektika paměti a identity je ve společenských vědách často spojována s tématem narativity. Identita je artikulována (či dokonce utvářena) jazykovými prostředky jako smysluplný životní příběh na základě zapamatovaných prožitých zkušeností a biografické práce. Orální historie (konkrétně Archiv vizuální historie USC Shoah Foundation) poskytuje bohatý zdroj materiálu pro zkoumání osobní, sociální i kolektivní identifikace narátorů, ale zároveň klade výzkum identit před určitá úskalí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The topics of memory, identity and narrative are deeply intertwined. Since John Locke (1632–1704), it is quite widely acknowledged that the very sense of one’s personal identity is based on (autobiographical) memory. During the 20th century, study of collective identity and collective memory emerged in the social sciences, along with questions about possible analogy between the mechanisms of individual identity/memory and collective identity/memory. The dialectics of memories and identities have also been connected by the notion of narrative. Therefore the identity is articulated (or even constructed) by linguistic means as a meaningful life story on the basis of past experiences and biographic work. In case of oral historical interviews, the triad of identity—memory—narrative takes on specific features. I will focus on the USC Shoah Foundation’s Visual History Archive as a data resource and try to provide answers to several questions, including: How are different social identities manifested in the audiovisual oral histories? Is there a narrative expression of plurality of identities, conflicts and tensions? Are these topics somehow present in the testimonies of the Holocaust survivors? How is the sociological point of view contributing to this kind of research? As we will see, oral history is indeed a rich data resource for researching the social aspects of collective and personal identities. However, because language is privileged in oral history, we might be missing another important means of identity expression (in non-verbal ways of individual or collective action). To a certain extent, this may be overcome due to the increasing number of videotaped oral history interviews.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek poskytuje základní informace o Centru vizuální historie Malach a dostupných archivních zdrojích v kontextu orální historie jako specifické metody zachycení minulosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article provides readers with basic information on the Malach Center for Visual History and available archival resources in the context of oral history as a specific method of capturing the past.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Orální historie – ať už jako kvalitativní výzkumná metoda nebo disciplína sui generis – se v posledních desetiletích bezpochyby stává předmětem rostoucího zájmu akademiků, badatelů, ale také různých organizací a veřejnosti jako takové. Přibývá orálně historických projektů, které jsou realizovány v odlišných kontextech a regionech, zaměřují se na rozmanitá témata a historická období moderních dějin, vzájemně se liší rozpočtem, výzkumnými otázkami i technologickým zázemím. V tomto příspěvku přistupuji k problematice orální historie ze sociologického hlediska a pokusím se nastínit možné sociologické přístupy ke zkoumání tohoto sociálního fenoménu, které by bylo možno souhrnně označit souslovím sociologie orální historie. Jelikož toto odvětví sociologie zatím není formálně utvořeno (natož institucionálně zakotveno), má můj příspěvek spíše charakter programový a ukazuje různé teoretické a epistemologické cesty, jimiž by zkoumání na tomto poli mohlo v budoucnu směřovat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Oral history - whether as a qualitative method or 
discipline *sui generis* - is indeed receiving growing attention in the past decades, from scholars as well as NGO's and also the wide public. Many different oral historical projects are being conducted, in varying contexts and regions, focusing on a variety of historical events, issues and research questions, with different budgets and technological backgrounds. From a sociological point of view, it is possible to interpret the universal practice of oral history as a social phenomenon of its own kind. Sociology can approach oral history from two different points of view, which are also shaping the basic structure of my talk: oral history (1) *as a subject itself* - the sociological interpretation of oral history as a social practice; (2) *as a secondary data source* - the sociological interpretation of existing oral historical data. First, I will present some approaches to oral history as a subject of  sociological research, consisting of two distinctive but complementary dimensions: (1.1) *micro-social*, and (1.2) *macro-social*. The traditional (and tricky) antinomy of individual and society is also manifesting itself in oral history, but certainly with some specific features. I will elaborate on the possible facilitation of oral history in sociological research, and also on the limitations of it. The central argument is that sociological insight can generate new ideas and knowledge about the process of oral history, but also about the role and meaning of oral history in (post)modern society. Building upon this, the contemporary praxis of oral history (and its methodology and epistemology) could be enhanced in different ways, including the conscious reflection of contextual, situational, socio-historical and linguistic aspects of an oral historical interview.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jedním z důsledků "digitálního obratu" je narůstající objem jazykových narativních dat dostupných on-line. Ve svém příspěvku jsem se zaměřil na případovou studii z oblasti "sociologie orální historie", konkrétně na různé způsoby, jak může sociologie využívat orálně historická data. Naznačil jsem také klíčovou roli ICT v usnadňování tohoto využití na příkladu Archivu vizuální historie USC Shoah Foundation.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>One of the consequences of the “digital turn” is a growing amount of linguistic narrative data available on-line. Large portion of this data is generated directly by the individual social actors. But there is also a second realm, which is a result – or sometimes a by-product – of qualitative research and oral history projects. In my talk, based on the case study of “sociology of oral history”, I intend to provide an insight into the ways of possible usage of oral history data in sociology, but also outline the crucial role of ICT in facilitating these efforts. Quite often, the results of process-oriented oral history projects (i.e. oral history projects without a specific research question) are published on-line in various digital environments. “Digital oral history” usually includes digital archives of interview collections with metadata, search tools, user work-space, Web 2.0 features etc. Due to this practice, large quantity of data is potentially available for reuse and secondary analysis by researchers, who did not participate in the initial data collection process. The analysis of this data could be supported by a wide variety of ICT tools. However, within the broad field of social sciences, different disciplines have very different requirements for exploitable data. I will use the data from USC Shoah Foundation’s Visual History Archive (http://vhaonline.usc.edu) in the context of my own research experience, in an attempt to illustrate this complexity.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valenční slovníky typicky obsahují informace o nepříznakovém užití sloves (aktivní formu); nicméně slovesa typicky vstupují do různých povrchových struktur. Zde se soustřeďujeme na popis změn ve valenční struktuře, a to změn spojených s diatezemi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Valency lexicons typically describe only unmarked usages of verbs (the active form); however verbs prototypically enter different surface
structures. In this paper, we focus on the so-called diatheses, i.e., the relations between different surface syntactic manifestations of verbs
that are brought about by changes in the morphological category of voice, e.g., the passive diathesis. The change in voice of a verb is
prototypically associated with shifts of some of its valency complementations in the surface structure. These shifts are implied by changes
in morphemic forms of the involved valency complementations and are regular enough to be captured by syntactic rules. However, as
diatheses are lexically conditioned, their applicability to an individual lexical unit of a verb is not predictable from its valency frame
alone. In this work, we propose a representation of this linguistic phenomenon in a valency lexicon of Czech verbs, VALLEX, with the
aim to enhance this lexicon with the information on individual types of Czech diatheses. In order to reduce the amount of necessary
manual annotation, a semi-automatic method is developed. This method draws evidence from a large morphologically annotated corpus,
relying on grammatical constraints on the applicability of individual types of diatheses.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje návrh a první implementaci systému pro lidský a strojový překlad tweetů. V prvních experimentech se omezujeme na překlad vybraných zdrojů z Ukrajiny (zdrojovým jazykem je primárně ukrajinština a ruština, občas angličtina). Zdrojové tweety jsou distribuovány registrovaným 'překladatelům', překlady je možné na webu ručně hodnotit a dostatečně dobré překlady odesíláme jako tweety do světa. Výhledově všechny tyto kroky bude provádět i stroj, zatím máme zapojen jen strojový překlad.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes the design and implementation of a system for human and machine translation of tweets. In these early experiments, we limit the system to follow some selected sources from Ukraine (the source language is primarily Ukrainian and Russian, sometimes English).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek prezentuje sadu morfologických nástrojů pro současnou arabštinu založenou na korpusu a konečných automatech a poskytovanou pod GPLv3 licencí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We develop an open-source large-scale ﬁnite-state morphological processing toolkit (AraComLex) for Modern StandardArabic (MSA) distributed under the GPLv3 license (http://aracomlex.sourceforge.net). The morphological transducer is based on a lexical database speciﬁcally constructed for this purpose. In contrast to previous resources, the database is tuned to MSA, eliminating lexical entries no longer attested in contemporary use. The database is built using a corpus of 1,089,111,204 word tokens, a pre-annotation tool, machine learning techniques and knowledge-based pattern matching to automatically acquire lexical knowledge. Our morphological transducer is evaluated and compared to LDC’s SAMA(StandardArabic Morphological Analyser). We also develop a ﬁnite-state morphological guesser as part of a methodology for extracting unknown word forms, lemmatizing them, and giving them a priority weight for inclusion in the lexicon.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Technická zprava shrnuje pravidla anotace koreference v Pražském česko-anglickém závislostním korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The technical report presents the guidelines for manual annotation of nominal coreference in the Prague Czech-English Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku se popisují principy evaluační soutěže na automatické rozřešení koreferenčních a anaforických vztahů pro ruštinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper reports on the recent forum RU-EVAL—a new initiative for evaluation of Russian NLP resources, methods and toolkits. The first two events were devoted to morphological and syntactic parsing correspondingly. The third event was devoted to anaphora and coreference resolution. Seven participating IT companies and academic institutions submitted their results for the anaphora resolution task and three of them presented the results of the coreference resolution task as well. The event was organized in order to estimate the state of the art for this NLP task in Russian and to compare various methods and principles implemented for Russian. We discuss the evaluation procedure. The anaphora and coreference tasks are specified in the present work. The phenomena taken into consideration are described. We also give a brief outlook of similar evaluation events whose experience we lay upon. In our work we formulate the training and Gold Standard corpora construction guidelines and present the measures used in evaluation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek uvádí sadu slovesných předpon, které spolu s reflexním morfémem mění význam slovesa, a to vždy stejným způsobem. Předpony tvoří posloupnost podle stupně intenzity, kterou pozměňují akci popisovanou daným slovesem. Představujeme proces intenzifikace slovesa, popisujeme sémantiku cirkumfixů, morfologické, syntaktické a sémantické omezení na jejich použití v ruštině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The current paper addresses verbal circumfixal derivation patterns in modern Russian. The discussion is focused on a series of circumfixes which trigger the intensified usage of the basic verb (~'keep doingP too much'). Derivatives built up by adding a prefix and a reflexive -OR to an imper-fective verb are examined. Although each prefix adds specific shades of meaning to the verb, such patterns are, however, claimed to share common features at different levels of linguistic analysis, such as morphology, syntax, and semantics. Furthermore, such patterns are highly productive in modern language; once certain constraints are fulfilled, an intensified derivative can be formed from any imperfective verb. This fact, along with the patterns in question sharing certain common features, allow us to argue that they can be considered inflectional, rather than derivational.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje Parmesan, náš příspěvěk na  Workshop on Statistical Machine Translation 2014.
Ukazuje, že parafrázovací tabulky Meteoru pro češtinu obsahují tolik šumu, že jejich použití ve skutečnosti může poškodit výkon metriky. Nicméně po důkladní filtraci mohou být velmi užitečné v cíleném parafrázovní referenčních vět předcházejícím evaluaci.
Parmesan nejprve provede cílené parafrázování referenčních vět a poté spočítá Meteor score s pouze přímou shodou na těchto nových referencích. Na datech z WMT12 a WMT13 ukazuje signifikantně vyšší shodu s lidským hodnocením než Meteor.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes Parmesan, our submission to the 2014 Workshop on Statistical
Machine Translation (WMT) metrics task for evaluation English-to-Czech translation. 
We show that the Czech Meteor Paraphrase tables are so noisy that they actually can harm 
the performance of the metric. However, they can be very useful after extensive filtering 
in targeted paraphrasing of Czech reference sentences prior to the evaluation.
Parmesan first performs targeted paraphrasing of reference sentences, then it computes 
the Meteor score using only the exact match on~these new reference sentences. It shows 
significantly higher correlation with human judgment than Meteor on the WMT12 and WMT13 data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce prezentuje možnost zpřesnění hodnocení strojového překladu českých vět. Náš algoritmus dostává na vstupu referenční větu a upravuje jí tak, aby byla bližší výstupu strojového překladu a současně si zachovala původní význam a gramatickou správnost.
Tu zajišťuje systém Depfix, který byl původně navržen pro post-editaci strojových překladů z angličtiny do češtiny, zde upravený pro opravu chyb specifických pro parafrázovaní.
Kvůli šumu ve zdrojích parafrázích experimentujeme se zarovnáváním slov. Ale jako nejlepší metoda se ukázala prostá hladová metoda s pouze jednoslovnými parafrázemi, díky jejich důkladnému filtrování. 
BLEU spočítané na nových referencích vykazuje významně vyšší korelaci s lidským hodnocením než skóre počítané na původních referencích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present a method of improving the accuracy of machine translation 
evaluation of Czech sentences. Given a reference sentence, our algorithm transforms it 
by targeted paraphrasing into a new synthetic reference sentence that is closer in 
wording to the machine translation output, but at the same time preserves the meaning of
the original reference sentence. 
Grammatical correctness of~the new reference sentence is provided by applying Depfix on 
newly created paraphrases. Depfix is a system for post-editing English-to-Czech machine 
translation outputs. We adjusted it to fix the errors in paraphrased sentences.
Due to a noisy source of our paraphrases, we experiment with adding word alignment. However, 
the alignment reduces the number of paraphrases found and the best results were achieved 
by~a~simple greedy method with only one-word paraphrases thanks to their intensive filtering.
BLEU scores computed using these new reference sentences show significantly higher correlation 
with human judgment than scores computed on the original reference sentences.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme metodu cíleného parafrazování pro zpřesnění hodnocení strojového překladu. V jejím rámci, využíváme strojový překld samotný a upravujeme jej tak, aby překládal v rámci jediného jazyka.
Popisujeme tento přístup na dvou typech strojového překladu - statistickém a pravidlovém. V rámci statistického překladu experimentujeme s Mojžíšem, volně dostupným nástrojem pro statický strojový překlad. Překladové modely tvoříme uměle ze dvou dostupých zdrojů českých parafrází - českého WordNetu a parafrázovacích tabulek Meteor. Rozšiřujeme Mojžíše o nový rys, který vynucuje cílené parafrázování.
Bohužel, naše výsledky jsou spíše negativní. S ohledem na chyby, které se objevily v parafrázovaných větách, navrhujeme nové řešení - parafrázování pomocí pravidlového systému Treex.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a method for improving machine translation (MT) evaluation by targeted 
paraphrasing of reference sentences. For this purpose, we employ MT systems themselves 
and adapt them for translating within a single language. 
We describe this attempt on two types of MT systems -- phrase-based and rule-based. 
Initially, we experiment with the freely available SMT system Moses. We create translation 
models from two available sources of Czech paraphrases -- Czech WordNet and the Meteor 
Paraphrase tables. We extended Moses by a new feature that makes the translation targeted. 
However, the results of this method are inconclusive. In the view of errors appearing 
in the new paraphrased sentences, we propose another solution -- targeted paraphrasing 
using parts of a rule-based translation system included in the NLP framework Treex.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato zpráva popisuje účast týmu Univerzity Karlovy v Praze na ShARe/CLEF eHealth Evaluation Lab in 2014.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This report describes the participation of the team of Charles University in Prague at the ShARe/CLEF eHealth Evaluation Lab in 2014.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jak funguje rozpoznávání řeči?
Jaké je studium na Matfyzu?
Co je Alex? (dialogový systém)
Povídání o machine learning a artificial intelligence</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>How does automatic speech recognition work?
How are the studies at MFF UK?
What is the Alex spoken dialogue system?
Notes on machine learning and artificial intelligence</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tématem této práce je implementace výkonného rozpoznávače v open-source systému trénování ASR Kaldi (http://kaldi.sourceforge.net/) pro dialogové systémy. Kaldi již obsahuje ASR dekodéry, které však nejsou vhodné pro dialogové systémy. Hlavními důvody jsou jejich malá optimalizace na rychlost a jejich velké zpoždění v generování výsledku po ukončení promluvy. Cílem této práce je proto vyvinutí real-time rozpoznávače pro dialogové systémy optimalizovaného na rychlost a minimalizujícího zpoždění. Zrychlení může být realizováno například pomocí multi-vláknového dekó- dování nebo s využitím grafických karet pro obecné výpočty. Součástí práce je také příprava akustického modelu a testování ve vyvíjeném dialogovém systému ”Vystadial”.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The topic of this thesis is to implement efficient decoder for speech recognition training system ASR Kaldi (http://kaldi.sourceforge.net/). Kaldi is already deployed with decoders, but they are not convenient for dialogue systems. The main goal of this thesis to develop a real-time decoder for a dialogue system, which minimize latency and optimize speed. Methods used for speeding up the decoder are not limited to multi-threading decoding or usage of GPU cards for general computations. Part of this work is devoted to training an acoustic model and also testing it in the "Vystadial" dialogue system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme rozšíření Kaldi automatického rozpoznávání řeči toolkit pro podporu on-line rozpoznávání řeči. Výsledný rozpoznáč podporuje state-of-the-art  akustické modely. (MFCC, MLLT + LDA, BMMI) Vzhledem k tomu, že rozpoznávání produkuje slovní posteriorní lattice, je užitečné zejména v systémech statistických dialogu.
V evaluaci se zaměříme na on-line rozpoznávání řeči v dialogovém systému Alex pro doménu "veřejné dopravy České republiky". Dialogový systém je k dispozici na veřejném bezplatném 800 899 998 řádek.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an extension of the Kaldi automatic speech recognition toolkit to support on-line speech recognition. The resulting recogniser supports acoustic models trained using state-of-the-art acoustic modelling techniques. (MFCC, MLLT+LDA, BMMI) As the recogniser produces word posterior lattices, it is particularly useful in statistical dialogue systems, which try to exploit uncertainty in the recogniser's output.
The evaluation is focused on on-line speech recognition in a dialogue system Alex for the "public Czech transportation" domain. The dialogue system is available at a public toll-free 800 899 998 line.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku je presentováno rozšíření Kaldi toolkitu pro rozpoznávání řeči, které podporuje on-line rozpoznávání.
Vysledný rozpoznávač řeči podporuje moderní modelovací metody.
Jelikož rozpoznávač vytváří slovní posteriorní svazy, je obzvlášt vhodný pro statistické dialogové systémy, které umí pracovat s nejistotou rozpoznávače řeči.
Experimenty ukazují, že online rozpoznávač má výrazně lepší výsledky vzhledem k latenci pri srovnání s rozpoznávačem v cloudu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents an extension of the Kaldi automatic speech recognition toolkit to support on-line recognition. The resulting recogniser supports acoustic models trained using state-of-the-art acoustic modelling techniques. As the recogniser produces word posterior lattices, it is particularly useful in statistical dialogue systems, which try to exploit uncertainty in the recognizer's output. Our experiments show that the on- line recogniser performs significantly better in terms of latency when compared to a cloud-based recogniser.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje integrace on-line Kaldi rozpoznávače řeči do frameworku dialogovýho systémů Alex (ADSF). Jelikož Kaldi OnlineLatgenRecogniser je implementován v C++, bylo nejprve nutno převést API rozpoznávače do jazyka Python. Skripty pro trénování akustických a jazykových modelů byly integrovány do ADSF a příslušné modely byly natrénovány. Na závěr optimální parametry rozpoznávače byly zvoleny z dat a ověřeny na "Public Transport Information (PTI)" doméně.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the integration of an on-line Kaldi speech
recogniser into the Alex Dialogue Systems Framework (ADSF). As the Kaldi OnlineLatgenRecogniser is written in C++, we first developed a Python wrapper for the recogniser so that the ADSF, written in Python, could interface with it. Training scripts for acoustic and language modelling were developed and integrated into ADSF, and acoustic and language models were build. Finally, optimal recogniser parameters were determined and evaluated. The dialogue system Alex with the new speech recogniser is evaluated on Public Transport Information (PTI) domain.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přehled problémů, které ve strojovém překladu způsobuje bohatá morfologie na cílové straně a naše techniky, jimiž se s nimi vyrovnáváme.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An overview of problems caused in MT by rich target-side morphology, and our techniques of addressing them.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představil jsem projekt MosesCore (CSA FP7) a zkušenosti s jeho řešením kolegům z katedry KSI.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I have introduced the project MosesCore (CSA FP7) and my experience with its managing to the colleagues of the KSI department.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popularizační video představuje náš překladač Chiméra včetně všech jeho tří komponent: hloubkový a frázový systém a následná korekce gramatických chyb.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A Czech popularization videotalk introduces our MT system Chimera including its three components: deep transfer-based MT system, Moses phrase-based system and the final grammatical correction by Depfix.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme dosažené výsledky ve statistickém strojovém překladu z angličtiny, němčiny, španělštiny a francouzštiny do češtiny. Probíráme specifické vlastnosti jednotlivých zdrojových jazyků a popisujeme techniky, které těchto vlastností využívají a zaměřují se na odstranění chyb specifických pro daný jazyk. Kromě vlastního překladu také prezentujeme náš příspěvek k chybové analýze.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present various achievements in statistical machine translation from English, German, Spanish and French into Czech. We discuss specific properties of the individual source languages and
describe techniques that exploit these properties and address language-specific errors. Besides the translation proper, we also present our contribution to error analysis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje výsledky společných úloh WMT14 -- překladu novinových textů, překladu textů z oblasti medicíny, odhadu kvality překladu a metrik strojového překladu. Do standardní překladové úlohy v 10 překladových směrech se letos zapojilo 143 systémů strojového překladu z 23 institucí. Zároveň bylo vyhodnoceno 6 anonymizovaných systémů. Úloha odhadu kvality překladu měla 4 podúlohy, kterých se zúčastnilo 10 týmů a celkem 57 systémů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the
WMT14 shared tasks, which included a
standard news translation task, a separate
medical translation task, a task for
run-time estimation of machine translation
quality, and a metrics task. This year, 143
machine translation systems from 23 institutions
were submitted to the ten translation
directions in the standard translation
task. An additional 6 anonymized systems
were included, and were then evaluated
both automatically and manually. The
quality estimation task had four subtasks,
with a total of 10 teams, submitting 57 entries.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme HindEnCorp, paralelní hindsko-anglický korpus, a HindMonoCorp, jednojazyčný hindský korpus ve verzi 0.5. Oba korpusy byly získány z webových zdrojů a předzpracovány primárně pro trénování systémů statistického strojového překladu. HindEnCorp sestává z 274k paralelních vět (3,9 miliónů hindských a 3,8 miliónů anglických tokenů). HindMonoCorp obsahuje 787 miliónů tokenů ve 44 miliónech vět. Oba korpusy jsou zdarma přístupné pro nekomerční výzkum a jejich předběžné vydání bylo využito řadou účastníků společné překladové úlohy WMT 2014.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present HindEnCorp, a parallel corpus of Hindi and English, and HindMonoCorp, a monolingual corpus of Hindi in their release version 0.5. Both corpora were collected from web sources and preprocessed primarily for the training of statistical machine translation systems. HindEnCorp consists of 274k parallel sentences (3.9 million Hindi and 3.8 million English tokens). HindMonoCorp amounts to 787 million tokens in 44 million sentences. Both the corpora are freely available for non-commercial research and their preliminary release has been used by numerous participants of the WMT 2014 shared translation task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představili jsme přehled činností naší katedry s důrazem na nástroje NLP užitečné pro textovou analytiku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We presented an overview of the work of our department, highlighting NLP tools useful in text analytics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V úvodní stati sborníku se předkládají argumenty o závažnosti zpracování valence v teoretickém i aplikačním rámci. Hlavní pozornost je věnována valenci substantiv a ověřování platnosti hypotéz uplatněných při zpracování valence sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the introductory chapter of the volume the importance of introduction of valency both in theoretical and applied description is stressed. The hypothesis submitted for the valency description of verbs are applied and verified for the valency of nouns.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dvojice termínů koordinace - determinace vs. hypotaxe a parataxe jsou srovnávány a popisují se přesahy, kdy je koordinace vyjádřená hypotakticky a naopak koordinace vyjádžená paratakticky. Některé konstrukce zpravidla sem řazené se z této oblasti vylučují.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The notions of coordination and subordination are compared with their counterparts hypotaxis and parataxis are discussed from the point of view of linguistic meaning.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Na příkladu atributivních vedlejších vět a jejich nominalizací pomocí slovesných adjektiv v češtině byl analyzován jejich vzájemný vztah. Demonstrovaly se případy gramatických omezení na jeden nebo druhý způsob vyjádření a poukazovalo se na jejich vzájemnou (ne)synonymii.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this contribution the attributive dependent clauses and their corresponding nominalisations in Czech were demonstrated from the point of view of their respective synonymy and from the point of view of the restrictions on the generation of one from  these analyzed forms.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve zvané plenární přednášce v rámci konference českých mladých lingvistů byla zdůrazněna důležitost syntaxe a formulovány požadavky na konsistenci jejího popisu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Within the conference of young linguists from Czech Republic in the invited talk the importance of the syntax was stressed and the reqirements   on the conistency of its description were exemplified by examples.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška určená doktorandům jazykovědných oborů FF MU v Brně sdružených v Jazykovědném spolku jako úvod do teorie Funkčního generativního popisu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the lecture for the post-graduate students members of Linguistic society the approach of Functional Generative Description and ite application was presented.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce pro doktorandy jazykovědných oborů FF MU byla představena valenční teorie uplatňovaná ve FGP - od slovesné valence přes valenci jména a jejich použití při anotování.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the lecture for post-graduate students of FF MU the principles of valency theory developed within FGD was explained and exemplified by the examples of valency of verbs and nouns and their application for the annotation procedure.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pozvaná úvodní přednáška o historii a metodologických principech zpracování valence s důrazem na kooperci při popisu gramatiky a slovníku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the invited introduction talk the history and methodological basis for valency studies was presented with the stress on cooperation between grammatical and lexical modules in the language description.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kniha je shrnutím studií získaných na základě dat uložených v PDT. Prezentuje se v ní hloubková syntax českých vět konsistentním způsobem založeným na funkčním generativním popisu. Vysvětlují se v ní principy závislostního přístupu k syntaxi, popisují se a exemlifikují jednotlivé typy syntakticko-sémantických vztahů a jejich souhra s kategoriemi morfologickými, analyzuje se úloha slovosledu v češtině. Kniha je doprovázena praktickými ukázkami závislostních stromových struktur.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this book the description of Czech  dependency syntax based on the data stored in Prague Dependency Treebank is given. The deep syntax of Czech sentences is explained in the consistent way using the principles of Functional Generative Description. The repertoir of semantic relation is given and exemplified, the role of word-order is studied and the interplay of syntax and morphology is demonstrated here. The figures representing the dependency deep structure of selected examplesare included in the book as well.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Text je příspěvkem k dlouhé lingvistické diskuzi ohledně hranic mezi gramatikou a lexikonem. V teoretickém rámci Funkčního generativního popisu analyzujeme valenci sloves, modalitu závislých klauzí, diateze českého slovesa a souborový význam českých substantiv s cílem ukázat potřebu zachycovat tyto jevy v gramatickém i lexikálním komponentu jazykového popisu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper contributes to the long-term linguistic discussion on the boundaries between grammar and lexicon by analyzing four related issues from Czech. The analysis is based on the theoretical framework of Functional Generative Description (FGD), which has been elaborated in Prague since 1960s. First, the approach of FGD to the valency of verbs is summarized. The second topic, concerning dependent content clauses, is closely related to the valency issue. We propose to encode the information on the conjunction of the dependent content clause as a grammatical feature of the verb governing the respective clause. Thirdly, passive, resultative and some other constructions are suggested to be understood as grammatical diatheses of Czech verbs and thus to be a part of the grammatical module of FGD. The fourth topic concerns the study of Czech nouns denoting pair body parts, clothes and accessories related to these body parts and similar nouns. Plural forms of these nouns prototypically refer to a pair or typical group of entities, not just to many of them. Since under specific contextual conditions the pair/group meaning can be expressed by most Czech concrete nouns, it is to be described as a grammaticalized feature.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Lexikální síť DeriNet zachycuje derivační vztahy mezi zhruba 266 tisíci českými lexémy. Síť je omezena na derivační vztahy, což je reflektováno v její struktuře: každý lexém smí být spojen právě s jedním základovým slovem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The lexical network DeriNet captures core word-formation relations on the set of around 266 thousand Czech lexemes. The network is currently limited to derivational relations because derivation is the most frequent and most productive word-formation process in Czech. This limitation is reflected in the architecture of the network: each lexeme is allowed to be linked up with just a single base word; composition as well as combined processes (composition with derivation) are thus not included.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kniha se zabývá stylistickými a gramatickými prohřešky proti kultivovanému vyjadřování v češtině, zejména ve stylu odborném. Na množství příkladu z oblasti lexika, morfologie, syntaxe a výstavby textu ukazuje na nedopatření proti srozumitelnosti a korektnosti vyjádření, která jsou v současné češtině běžná.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this book the grammatical and stylistic faults in contemporary Czech, esp. in the scientific style, are demonstrated by the broad scale of examples and they are analyzed from the point of view of lexicon, morphology, syntax and text structure.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nekrolog a přehled lingvistických vědeckých příspěvků velkého amerického lingvisty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Obituary and a survey of the scientific contribution of a great Americal linguist to linguistics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek zpracovává otázku interoperability anotačních schémat na základě testování tohoto problému ve víceúrovňových anotačních schématech u typologicky různých jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The so-called interoperability is proposed to be viewed from three angles, each of which has its advantages and weak points. The application of the original interpretation of interoperability as a collaboration of components seems to be rather inspiring and has been already tested on several multilayered annotation schemes. One of the main obstacles for an adaptation of a single scenario for different languages is not only the different (typological) features of these languages but also the fact that each scenario (if well developed) has behind it a certain linguistic theory and people working with these theories have been ”born”. Obviously, obstacles concern the fact that there was a parallel development of some of the schemes, the older and more “elaborated” ones (more advanced, used for more languages etc.) are (obviously) not open to big changes. Communication or collaboration between different schemes requires, on the one hand, an explicit specification of each particular scheme, and, on the other, offers a reliable material for linguistic research and also for NLP applications. In the latter respect, the initiatives such as CoNNL shared tasks on multilingual dependency parsing or similar activities are important endeavors.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popis současných právních problémů s jazykovými daty a jazykovými technologiemi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Curent law situation with development, storage and deployment of language data and technologies</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Situace a možné perspektivy využívání persistentních identifikátorů (PID) v LINDAT/CLARIN.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Current situation and perspective of usage of persistent IDs at LINDAT/CLARIN.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace o ustavení a budování důvěry ve vztahu poskytovatel dat - infrastruktura - uživatel.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presentation on building trust between the data producer, infrastructure and users.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace vývoje a současného stavu repozitáře projektu EUDAT k bezpečnému ukládání a sdílení vědeckých dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presentation of development and current state of the repository for storing and sharing scientific data run by EUDAT project.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přehled služeb poskytovaných centrem LINDAT/CLARIN a jejich demonstrace online.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Overview of the services provided by the LINDAT/CLARIN centre and their online demonstration.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento balíček obsahuje datové sady pro vývoj a testování modelů strojového překladu pro krátké vyhledávací dotazy v oblasti medicíny, a to pro čestinu, angličtinu, francouzštinu a němčinu. Dotazy obsažené v datech pochází jak od zdravotnických profesionálů, tak od laické veřejnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This package contains data sets for development and testing of machine translation of medical search short queries between Czech, English, French, and German. The queries come from general public and medical experts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce se zabývá strojovým překladem vyhledávacích dotazů pro vícejazyčné vyhledávání informací v oblasti medicíny. Hlavní pozornost je věnována jednak adaptaci překladových technik pro zvýšení překladové kvality, ale také přímo pro zlepšení výsledků vyhledávání.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this work, we investigate machine translation (MT) of search queries in the context of cross-lingual information retrieval (IR) in the domain of medicine. The main focus is on MT adaptation techniques to increase translation quality, however we also explore MT adaptation to improve cross-lingual IR directly. The experiments described herein have been performed and thoroughly evaluated for MT quality on the datasets created within the Khresmoi project and for IR performance on the CLEF eHealth 2013 datasets on three language pairs: Czech–English, German–English, and French–English. The search query translation results achieved in our experiments are outstanding – our systems outperformed not only our strong baselines, but also the Google Translate and Microsoft Bing Translator in direct comparison carried out on all the language pairs. In terms of the retrieval performance on this particular test collection, a significant improvement over the baseline has been achieved only for French–English. Throughout the article, we provide discussion and details on the contribution of the state-of-the-art features and adaptation techniques under exploration and provide future research directions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje vývojové a testovací datové sady pro strojový překlad dotazů k vyhledávání informací v oboru medicíny napříč jazyky. Data sestávají z 1508 skutečných uživatelských dotazů zadaných v angličtině a přeložených do češitny, němčiny a francouzštiny. Popisujeme proces překladu a korektur, kterého se účastnili odborníci na medicínu, a představujeme základní experiment, při kterém jsou naše datové sady použity pro ladění a evaluaci systému pro strojový překlad.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents development and test sets for machine translation of search queries in cross-lingual information retrieval in the medical domain. The data consists of the total of 1,508 real user queries in English translated to Czech, German, and French. We describe the translation and review process involving medical professionals and present a baseline experiment where our data sets are used for tuning and evaluation of a machine translation system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek srovnává české a anglické anotace na pozadí formalismu Abstract Meaning representation.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper compares Czech and English annotation using Abstract Meaning Represantation formalism.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>PDT-Vallex je český valenční lexikon propojený s reálnými texty v několika korpusech s českými daty: Pražský závislostní korpus a jeho nástupci Pražský česko-anglický závislostní korpus a Pražský závislostní korpus mluvené češtiny. Obsahuje přes  11000 valenčních rámců pro více než 7000 sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The valency lexicon PDT-Vallex has been built in close connection with the annotation of the Prague Dependency Treebank project (PDT) and its successors (mainly the Prague Czech-English Dependency Treebank project, PCEDT). It contains over 11000 valency frames for more than 7000 verbs which occurred in the PDT or PCEDT. It is available in electronically processable format (XML) together with the aforementioned treebanks (to be viewed and edited by TrEd, the PDT/PCEDT main annotation tool), and also in more human readable form including corpus examples (see the WEBSITE link below). The main feature of the lexicon is its linking to the annotated corpora - each occurrence of each verb is linked to the appropriate valency frame with additional (generalized) information about its usage and surface morphosyntactic form alternatives.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Korpusová studie se zabývá víceúrovňovou specifikací slovesných víceslovných výrazů a jejich vlastnostmi v češtině a v angličtině. Zároveň na základě využití paralelního korpusu (PCEDT)  slovesné idiomy v obou jmenovaných jazycích porovnává.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This is a corpus-based study,concentrating on multilayer specification of verbal MWEs, their properties in Czech and English,and a comparison between the two languages using the parallel Czech-English Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nominalizované struktury se dvěma aktanty ve formě bezpředložkového genitivu byly doposud v české odborné literatuře reprezentovány pouze jediným typem zcela přijatelných a gramatických konstrukcí, např. zbavení ženy starostí. V této studii vymezujeme tři další typy nominalizovaných struktur se dvěma aktanty v bezpředložkovém genitivu a vyhledáváme jejich frekvence v Pražském závislostním korpusu a v Českém národním korpusu. Nově vymezené struktury jsou méně přijatelné a jejich frekvence jsou velmi nízké, přesto se domníváme, že je gramatika češtiny umožňuje. Zvláštní pozornost je věnována nominalizacím verbonominálních predikátů s kategoriálním slovesem; tyto konstrukce mohou být interpretovány jako jedna lexikální jednotka, což umožňuje jejich užití v rámci nominalizovaných struktur se dvěma aktanty v bezpředložkovém genitivu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Double post-nominal genitives in Czech have been illustrated by the only type of a nominalized
structure, e.g., zbavení ženy starostí ‘relieving woman-GEN worry-GEN.PL, i.e. relieving
the woman of worries’. In this paper, we specify three other types of double post-nominal
genitive constructions and search for their frequency in the Prague Dependency Treebank and
in the Czech National Corpus. Although the constructions are rare and less acceptable we try
to show that Czech grammar system allows them. Special attention is paid to nominalizations
of support verb constructions; they can be interpreted as one lexical unit which enables them
to be used within double post-nominal genitive constructions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Česká substantiva mluvení mají tři sémantické participanty, Mluvčí, Informace a Příjemce. Tyto tři participanty mohou být vyjádřeny různými formami, v různých kombinacích. V subkorpusech Českého národního korpusu zjišťujeme četnosti vybraných kombinací aktantů u pěti skupin substantiv mluvení. Srovnáváme je s četnostmi obdobných kombinací aktantů u vzorku substantiv výměny a podáváme jejich kvantitativní analýzu. Ukazujeme, že dvě zkoumané sémantické třídy se podstatně liší v četnostech kombinací zahrnujících Konatele (Mluvčího u substantiv mluvení, Posesora 1 u substantiv výměny). Zatímco u substantiv výměny je Konatel téměř vždy elidován, u substantiv mluvení je výrazně četnější, u některých typů substantiv dokonce srovnatelně častý jako participant Informace. Můžeme tedy potvrdit naši hypotézu, že nezanedbatelný výskyt Konatele (při současném vyjádření Adresátu) je charakteristickou vlastností celé skupiny substantiv mluvení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Czech nouns of communication can mostly be modified by three participants, Speaker, Information and Addressee. These participants can be expressed by various forms but only some of them can be combined with each other. We search for frequencies of selected combinations of participants modifying several types of nouns of communication in subcorpora of the Czech National Corpus. We compare them with frequencies of similar combinations of participants modifying a sample of nouns of exchange and provide a quantitative analysis of them. The two semantic classes considerably differ in frequencies of combinations including Agent (Speaker of nouns of communication, Posesor 1 of nouns of exchange). While Agent is deleted in almost all occurrences of nouns of exchange, it is comparably frequent as Information in occurrences of some types of nouns of communication. We confirm our hypothesis that Agent plays an important role in valency behaviour of nouns of communication.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato kapitola přináší detailní analýzu valenčních vlastností českých deverbativních substantiv. Zaměřuje se na formy valenčních doplnění; ty mohou být typické, odpovídající formám valenčních doplnění základového slovesa, nebo specifické, bez jakéhokoliv formálního vztahu k formám doplnění u základového slovesa. Přinášíme přehled specifických posunů v povrchových formách (realizacích) participantů. Specifické formy participantů mají dopad na syntaktické chování daného substantiva a na jeho význam. Ukazujeme, že se nejedná vždy o jasný významový posun, ale že někdy jde pouze o jemný významový odstín. Navrhujeme pro tato substantiva se specifickými formami participantů vytvořit nový valenční rámec; tato substantiva představují zvláštní kategorii substantiv na hranici mezi syntaktickou a lexikální derivací.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This chapter provides an in-depth analysis of valency properties of Czech deverbal nouns. It focuses on forms of complementations they take. These can be typical, related to the source verbs, or special, without any relationship with forms of participants of the verbs related. We present an overview of special shifts in valency. Special forms of participants have an impact for syntactic behaviour of the noun and for its meaning. We argue that it is not always a shift of the meaning but sometimes only a slight nuance. Such nouns with special forms of participants require creating a new valency frame; they represent a separate category on the boundary between syntactic and lexical derivation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce přináší detailní analýzu valenčních vlastností českých deverbativních substantiv. Zaměřuje se na formy valenčních doplnění; ty mohou být typické, odpovídající formám valenčních doplnění základového slovesa, nebo specifické, bez jakéhokoliv formálního vztahu k formám doplnění u základového slovesa. Nejprve shrnujeme nejnovější poznatky v oblasti typických posunů v povrchových realizacích (formách) participantů. Poté přinášíme přehled posunů specifických. Specifické formy participantů mají dopad na syntaktické chování daného substantiva a na jeho význam. Ukazujeme, že se nejedná vždy o jasný významový posun, ale že někdy jde pouze o jemný významový odstín. Navrhujeme pro tato substantiva se specifickými formami participantů vytvořit nový valenční rámec ve valenčním slovníku PDT-Vallex. Tato substantiva představují zvláštní kategorii substantiv na hranici mezi syntaktickou a lexikální derivací.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This work provides an in-depth analysis of valency properties of Czech deverbal nouns. It focuses on the forms of complementation they take. These can be typical, related to the source verbs, or specific, without any relationship to those of the source verbs. First we summarize new findings concerning typical shifts in valency. Then we present an overview of the specific shifts in valency. Specific forms of participants have an impact on the syntactic behaviour of the noun and its meaning. We argue that it is not always a plain shift in meaning but sometimes only a slight meaning nuance. Such nouns with specific forms of participants require creating a new valency frame in the valency lexicon PDT-Vallex; they represent a separate category on the boundary between syntactic and lexical derivation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek má tři cíle. Za prvé seznamuje čtenáře se současnou verzí anotačního nástroje pro diskurzní vztahy v Pražském závislostním korpusu 3.0. Za druhé představuje samotné diskurzní vztahy v tomto korpusu, včetně novinek ve srovnání s předchozím vydáním. A za třetí ukazuje, jak v tomto korpusu vyhledávat, se zaměřením na diskurzní vztahy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of the paper is threefold. First, it introduces the current version of the annotation tool for discourse relations in the Prague Dependency Treebank 3.0. Second, it presents the discourse relations in the treebank themselves, including new additions in comparison with the
previous release. And third, it shows how to search in the treebank, with focus on the discourse
relations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cíl příspěvku je poměrně skromný: sebrat jednoduché statistiky z různých rovin Pražského závislostního korpusu a na nich ukázat jejich užitečnost pro jazykový výzkum, a to jako doklad pro existující jazykové teorie nebo jako inspirace pro nové směry výzkumu či nová vysvětlení existujících otázek. Za tímto účelem jsme shromáždili data z již publikovaných článků o PDT (s odkazy na příslušných místech), přidali některé novější výsledky a vyvodili obecnější důsledky podstatné pro autory zabývající se českou gramatikou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The goal of the present contribution is rather modest: to collect simple statistics carried out on different layers of the annotation scenario of the Prague Dependency Treebank in order to illustrate their usefulness for linguistic research, either by supporting existing hypotheses or suggesting new research questions or new explanations of the existing ones. For this purpose, we have collected the data from the already published papers on PDT (quoted at the relevant places), adding some more recent results and drawing some more general consequences relevant for Czech grammar writers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Lingua::Interset je univerzální sada morfosyntaktických rysů, do které je možné zobrazit všechny sady značek všech korpusů a jazyků. Verze 2.026 pokrývá 64 různých sad značek pro 37 jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Lingua::Interset is a universal morphosyntactic feature set to which all tagsets of all corpora/languages can be mapped. Version 2.026 covers 64 different tagsets of 37 languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>HamleDT 2.0 je sada 30 existujících treebanků (syntakticky anotovaných korpusů), harmonizovaných do jednotného anotačního stylu, tzv. Pražských závislostí, a dále transformovaných do Stanfordských závislostí, anotačního stylu, který se v poslední době stal populární. Používáme nejnovější verzi tzv. základních Universal Stanford Dependencies, bez přidaných jazykově závislých podtypů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>HamleDT 2.0 is a collection of 30 existing treebanks harmonized into a common annotation style, the Prague Dependencies, and further transformed into Stanford Dependencies, a treebank annotation style that became popular recently. We use the newest basic Universal Stanford Dependencies, without added language-specific subtypes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme HamleDT – Harmonizovaný vícejazyčný závislostní korpus (HArmonized Multi-LanguagE Dependency Treebank). HamleDT je sbírka existujících závislostních korpusů (nebo jiných korpusů převedených do závislostní syntaxe), transformovaných tak, aby všechny odpovídaly jednotnému anotačnímu stylu. V tomto článku představujeme podrobný rozbor řady jevů, které jsou v různých jazycích srovnatelné, jejich zachycení v korpusech se však často liší. Tvrdíme, že je možné navrhnout takové transformační procedury, které většinu zmíněných jevů automaticky rozpoznají a převedou do jednotného stylu. Tato normalizace je důležitá jak pro komparativní lingvistiku, tak pro strojové učení syntaktické analýzy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present HamleDT – a HArmonized Multi-LanguagE Dependency Treebank. HamleDT is a compilation of existing dependency treebanks (or dependency conversions of other treebanks), transformed so that they all conform to the same annotation style. In the present article, we provide
a thorough investigation and discussion of a number of phenomena that are comparable across languages, though their annotation in treebanks often differs. We claim that transformation procedures can be designed to automatically identify most such phenomena and convert them to a unified annotation style. This unification is beneficial both to comparative corpus linguistics
and to machine learning of syntactic parsing.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jedním ze způsobů, jak učit gramatiku, tj. tvaroslovný a větný rozbor, je kreslit diagramy, které zachycují vztahy mezi slovy ve větě. Podobně, ale mnohem komplexněji jsou tyto vztahy zachyceny v korpusech, které představují základní stavebni kameny v počítačovém zpracování přirozeného jazyka. Ovšem budování korpusů je velmi náročná aktivita, proto hledáme alternativní levnější a rychlejší způsoby organizace anotování, jako je např. crowdsourcing. Cílem této práce je prozkoumat možnosti získávání větných rozborů od žáků a jejich učitelů. V naší pilotní studii pracujeme s češtinou a větnými rozbory vyučovanými na českých školách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>One way of teaching grammar, namely morphology and syntax, is to visualize sentences as diagrams capturing relationships between words. Similarly, such relationships are captured in a more complex way in treebanks serving as key building stones in modern natural language processing. However, building treebanks is very time consuming, thus we have been seeking for an alternative cheaper and faster way, like crowdsourcing. The purpose of our work is to explore possibility to get sentence diagrams produced by students and teachers.
In our pilot study, the object language is Czech, where sentence diagrams are part of elementary school curriculum.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Technická zpráva shrnuje nová anotační pravidla pro anotovaní českých textů na tektogramatické rovině Pražských závislostních korpusů, která doplňují velký anotační manuál (ÚFAL/CKL TR-2005-28). Pravidla vznikla v souvislosti s anotováním korpusů PCEDT 2.0 a PDTSC 2.0.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Technical report provides new annotation rules for annotation of czech text on tectogrammatical level of the Prague Dependency Treebanks. The rules supplement main annotation manual (ÚFAL/CKL TR-2005-28) and were formed with respect to annotation of PCEDT 2.0 and PDTSC 2.0.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek přináší odpověď na otázku, co je a co není elipsa, a stanovuje kritéria určování elips ve větě. Analýzu jednotlivých typů elips podává z pohledu významové (sémanticko-syntaktické) reprezentace vět. Nezabývá se podmínkami a příčinami vzniku elips (kdy a proč je možné něco elidovat), zaměřuje se výhradně na identifikaci eliptických míst (zda je ve větě něco elidováno a co) a na jejich významovou reprezentaci, konkrétně na jejich zachycení na tektogramatické rovině pražských závislostních korpusů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article answers the question what is and what is not ellipsis and specifies criteria for identification of elliptical sentences. It reports on an analysis of types of ellipsis from the point of view of semantic representation of sentences. It does not deal with conditions and causes of the constitution of elliptical positions in sentences (when and why is it possible to omit something in a sentence) but it focuses exclusively on the identification of elliptical positions (if there is something omitted and what) and on their semantic representation in a treebank, specifically on their representation on the deep syntactic level of the Prague Dependency Treebanks. The theoretical frame of the approach to ellipsis presented in this article is dependency grammar.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška se zaměří na základní syntaktické vztahy, především závislost a slovosled. Přiblížíme si jejich vzájemné vazby a ukážeme příklady vzájemného ovlivňování (včetně tzv. neprojektivních konstrukcí). Seznámíme se s Pražským závislostním korpusem, který zprostředkovává "znalost" češtiny i některých jiných jazyků počítačům, a tak umožňuje jejich automatické zpracování.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk focuses on basic syntactic relations, namely dependency and word order. We will discuss their mutual relations and show their inferences (including so called non-projectivity). We will get familiar with Prague Dependency Treebank, which provide "knowledge" of Czech (and other languages) to computers and makes their automatic processing possible.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přirozený jazyk a matematika: jak popsat češtinu tak, aby jí „porozuměly“ počítače? Přednáška se zaměří na základní syntaktické vztahy, především závislost a slovosled. Přiblížíme si jejich vzájemné vazby a ukážeme příklady vzájemného ovlivňování (včetně tzv. neprojektivních konstrukcí). Seznámíme se s Pražským závislostním korpusem, který zprostředkovává "znalost" češtiny i některých jiných jazyků počítačům, a tak umožňuje jejich automatické zpracování.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Natural Language and Mathematics: How to describe Czech for Computers? The talk focuses on basic syntactic relations, namely dependency and word order. We will discuss their mutual relations and show their inferences (including so called non-projectivity). We will get familiar with Prague Dependency Treebank, which provide "knowledge" of Czech (and other languages) to computers and makes their automatic processing possible.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek se věnuje identifikaci zajímavých konstrukcí v syntakticky anotovaném korpusu (Pražském závislostním korpusu, PDT) metodou auto- matické redukční analýzy. Rozšiřujeme zkoumané konstrukce zejména o koordinační a apoziční vztahy, které mají zřetelně ne-závislostní charakter a vedou tedy k zobecnění používané redukční metody. Přinášíme klasifikaci zkoumaných konstrukcí a soustřeďujeme se na popis a analýzu jednotlivých jazykových jevů, které při zpracování působí problémy. Tato studie je motivací pro formální modelování metod zpracování přirozeného jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article focuses on the identification of interesting phenomena in the syntactically annotated corpus (Prague Dependency Treebank) using a method of Analysis by Reduction. Investigated phenomena are enriched with coordination and apposition. The study supports and motivated formal modeling of natural languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek nabízí lingvistická pozorování jako motivaci pro studium formální redukční analýzy. Využívá přitom třídu restartovacích automatů s metainstrukcemi, které pracují s "oblázky" a se dvěma operacemi - delete a shift.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper provides linguistic observations as a motivation for a formal study of analysis by
reduction (AR). It concentrates on a study of the whole mechanism through a class of restarting
automata with meta-instructions using pebbles, delete, and shift operations (DS-automata). The
complexity of DS-automata is naturally measured by the number of deletions, and the number
of word order shifts used in a single meta-instruction. We study reduction languages, backward correctness preserving AR, correctness preserving AR, and show unbounded hierarchies (scales) for various classes of reduction languages by the same finite witness languages. The scales make it possible to estimate relevant complexity issues of analysis by reduction for natural languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspšvek se zabývá minimalismem redukční analýzy pomocí restartovacích automatů. Definují se v něm 4 třídy jazyků - základní jazyky nad slovními formami oznašenými gramatickými kategoriemi, vlastními jazyky nad slovními formami, hategoriální jazyky a jazyk redukcí. Složitost jazyků je měřena počtem "oblázků", počtem vyouštění a počtem přsunů během analýzy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper provides linguistic observations as a motivation for a formal study of an analysis by reduction. It concentrates on a study
of the whole mechanism through a class of restarting automata with meta-instructions using pebbles, with delete and shift operations (DSautomata).
Four types of (in)finite sets defined by these automata are considered as linguistically relevant: basic languages on word forms marked
with grammatical categories, proper languages on unmarked word forms, categorial languages on grammatical categories, and sets of reductions
(reduction languages). The equivalence of proper languages is considered for a weak equivalence of DS-automata, and the equivalence of reduction languages for a strong equivalence of DS-automata.
The complexity of a language is naturally measured by the number of pebbles, the number of deletions, and the number of word order shifts
used in a single reduction step. We have obtained unbounded hierarchies (scales) for all four types of classes of finite languages considered here,
as well as for Chomsky’s classes of infinite languages. The scales make it possible to estimate relevant complexity issues of analysis by reduction for natural languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>The European project Khresmoi aims to develop a multilingual and multimodal search and access system  for bilomedical information and documents.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Evropský projekt Khresmoi vyvíjí multilinguální a multimodální vyhledávací systém v oboru biomedicíny.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vyhledávání tzv. stupňovaných sloves, tj. sloves s intenzifikační předponou, v korpusech řady SYN. Vyhodnocení spolehlivosti intenzifikační interpretace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Processing of intensified verbs (verbs with an intensifying prefix) using corpora SYN. Evaluation of the results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek uvádí sadu slovesných předpon, které spolu s reflexním morfémem mění význam slovesa, a to vždy stejným způsobem.
Předpony tvoří posloupnost podle stupně intenzity, kterou pozměňují akci popisovanou daným slovesem.
Představujeme proces intenzifikace slovesa ve třech slovanských jazycích, a to v češtině, slovenštině a ruštině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper discusses a~set of verbal prefixes which, when added to a~verb together with a~reflexive morpheme,
change the verb's meaning always in the same manner. 
The prefixes form a~sequence according to the degree of intensity with which they modify the verbal action. 
We present the process of verb intensification in three Slavic languages, namely Czech, Slovak and Russian.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Software slouží pro vícejazyčné (české a anglické) vyhledávání relevantních slov či krátkých frází v archivu přeživších Holocaustu, spravovaném USC (University of Southern California) Shoah Foundation Institute (http://dornsife.usc.edu/vhi/). Tento archiv obsahuje více než 110 tisíc hodin záznamů v 32 jazycích, přičemž přibližně polovina těchto rozhovorů je vedena v angličtině. Pro účely vyhledávání v systému MCLAAS jsou česká a anglická řečová data nejprve zpracována příslušným modulem rozpoznávání řeči (SEASR-CZE - viz http://www.kky.zcu.cz/cs/sw/SEASR-CZE, resp. SEASR-ENG - viz http://www.kky.zcu.cz/cs/sw/SEASR-ENG) a poté je vytvořen tzv. index, což je strojová reprezentace rozpoznaných promluv, která umožňuje co nejrychlejší vyhledání požadovaného slova či fráze. Oba vyhledávací systémy pracují momentálně pouze s indexem založeným na slovní reprezentaci – fonémové vyhledávání bude implementováno později. Každý ze systémů v současnosti hledá výskyty slov či frází zhruba v 1000 hodin videozáznamů. V případě češtiny jde o veškerá dostupná data; v angličtině je k dispozici více než 50 tisíc hodin, ale rozpoznání a zaindexování celého tohoto objemu bude vyžadovat paralelizaci jednotlivých procesů. Pro křížové vyhledávání (dotaz v češtině, data/rozhovory v angličtině a češtině) byl použit systém překladu dotazu. Implementace byla provedena jako zvláštní verze systému MTMonkey (http://ufal.mff.cuni.cz/mtmonkey).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This software is used for multi-lingual (Czech and English) search for relevant words or short phrases in the archive of Holocaust survivors, managed by USC (University of Southern California) Shoah Foundation Institute (http://dornsife.usc.edu/vhi/), which contains more than 110,000 hours of records in 32 languages, with approximately half of these interviews is conducted in English. Czech part of the archive accounts for approximately one thousand hours. For the purposes of searching in the system MCLAAS are Czech and English speech data first processed with the appropriate speech recognition module (SEASR-CZE - see http://www.kky.zcu.cz/en/sw/SEASR-CZE or SEASR-ENG - see http://www.kky.zcu.cz/en/sw/SEASR-ENG) and then a so-called index is created, which is a machine representation of recognized utterances, which speeds up the search for a desired word or phrase. Both retrieval systems currently operate only with an index based on word representation - phonetic search will be implemented later. Each system is currently looking for occurrences of words or phrases in about 1000 hours of video. Those are all data available in the case of Czech; in English there are more than 50,000 hours, but the recognition and indexing of all this volume will require parallelization of individual processes. Cross-searching (query in English, data / interviews in English and Czech) in the system is facilitated by automatic query translation. Implementation was carried out as a special version of MTMonkey (http://ufal.mff.cuni.cz/mtmonkey).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek rozebírá výsledky překladu z češtiny do slovenštiny pomocí dvou systémů - Google Translate a Česílko. Součástí článku je také rozbor chyb obou systémů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes an experiment compar-
ing results of machine translation between two
closely related languages, Czech and Slovak.
The comparison is performed by means of two
MT systems, one representing rule-based ap-
proach, the other one representing statistical
approach to the task. Both sets of results are
manually evaluated by native speakers of the
target language. The results are discussed both
from the linguistic and quantitative points of
view.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje plně automatické propojování dvou valenčních slovníku českých sloves: VALLEXu a PDT-Vallexu. I přes společný teoretický základ, z něhož oba vycházejí, a přes stejné jazykové jevy, na něž se zaměřují, není plně automatické propojování snadné.

Ukážeme, že konverze slovníku do společného formátu představuje tu snadnější část úlohy, kdežto automatická identifikace dvojic odpovídajících si valenčních rámců přináší obtíže.

Celkovou dosaženou přesnost 81% lze považovat za uspokojivou. Je však třeba si uvědomit, že s rostoucím počtem lexikálních jednotek slovesa klesá přesnost automatického mapování. Ukážeme, že značně pomůže, pokud (i) poskytneme doplňující informace o lexikálních jednotkách a (ii) odhalíme a sladíme pravidelné nesrovnalosti v anotaci v obou slovnících.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the fully automatic linking of two valency lexicons of Czech verbs: VALLEX and PDT-VALLEX. Despite the same theoretical background adopted by these lexicons and the same linguistic phenomena they focus on, the fully automatic mapping of these resouces is not straightforward.

We demonstrate that converting these lexicons into a common format represents a relatively easy part of the task whereas the automatic identification of pairs of corresponding valency frames (representing lexical units of verbs) poses difficulties. The overall achieved precision of 81% can be considered satisfactory. However, the higher number of lexical units a verb has, the lower the precision of their automatic mapping usually is. Moreover, we show that especially (i) supplementing further information on lexical units and (ii) revealing and reconciling regular discrepancies in their annotations can greatly assist in the automatic merging.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V posteru ukazujeme anotaci vnitřní struktury vybraných typů víceslovných pojmenovaných entit. Porovnáváme tuto strukturu se syntaktickou strukturou uvnitř i vně těchto VV.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>What we demonstrate in this poster is annotation of structure of selected types of multiword named entities and how this structure relates to the syntactic structures around and inside these MWEs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Práce se zabývá syntaktickou identifikací výskytů víceslovných výrazů (VV) ze slovníku v textovém korpusu.
Porovnáváme tři přístupy odlišné v hloubce lingvistické analýzy -- od povrchového pořadí slov po hloubkovou syntax.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We deal with syntactic identification of occurrences of multiword expression (MWE) from an existing dictionary in a text corpus. The MWEs we identify can be of arbitrary length and can be interrupted in the surface sentence. We analyse and compare three approaches based on linguistic analysis at a varying level, ranging from surface word order to deep syntax. We use the dictionary of multiword expressions SemLex, that includes deep syntactic dependency trees of its MWEs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>The development of a database of Czech derived words is described, focusing on linguistic aspects of this task. In Czech, which is a Slavic language with both rich inflectional and derivational morphology, derivation is the most frequent and most productive word-formation process. The database of Czech derived words was designed as a lexical network, which consists of lexemes and derivational relations (links) between them, oriented from the base to the derived word. The derivational relations were processed automatically under a thorough manual supervision in order to ensure efficiency and consistency on the one hand and theoretical adequacy on the other.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>V příspěvku je popisováno budování databáze českých derivátů, pozornost je zaměřena na lingvistické aspekty tohoto procesu. V češtině je derivace nejfrekventovanějším a nejvíce produktivním slovotvorným procesem. Budovaná databáze je navržena jako lexikální síť, která sestává z lexémů (uzlů) a derivačních vztahů (hran) mezi nimi. Derivační vztahy byly generovány automaticky, ovšem za důkladné ruční kontroly.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá českými deadjektivními deriváty s příponou -ost. V české lingvistice je tento sufix popisován jako monofunkční sufix odvozující substantiva s významem kvality (př. sladkost; kvalitativní význam). Korpusová data ovšem ukazují, že jsou tato substantiva v současné češtině běžně používána také pro odvozování jmen s významem nositele vlastnosti (sladkost ve významu cukrovinka; nekvalitativní význam). Navrhujeme proto jak názvy vlastností tak názvy jejich nositelů hodnotit jako přímé deadjektivní deriváty s příponu -ost. Návrh je podložen analýzou rozsáhlého materiálu z korpusu SYN2010. Tato analýza potvrdila vztah mezi frekvencí substantiva s příponou -ost a jeho užíváním v nekvalitativním významu a také popisovanou skutečnost, že existence plurálových forem může být interpretována jako signál, že dané substantivum (s výjimkou vysoce frekventovaných jmen) má nekvalitativní význam.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present study deals with Czech deadjectival derivates with the suffix -ost. In the Czech linguistics, the suffix is described as a monofunctional suffix that derives names of qualities (e.g. sladkost `sweetness’; qualitative meaning). However, as the corpus data demonstrate that the suffix is commonly used for deriving names of bearers of quality (sladkost `sweet thing’; non-qualitative meaning) in contemporary Czech as well, we propose to consider both the names of qualities and the names of their bearers as direct deadjectival derivates with the suffix -ost. The proposal is supported by an analysis of large data from the SYN2010 corpus. The material documents that, first of all, there is a relation between the frequency of a noun with the suffix -ost and its usage in the non-qualitative meaning and, secondly, that the existence of plural forms can be interpreted as a signal that the particular noun (except for the most frequent nouns) has a non-qualitative meaning.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pojetí produktivity a její zjišťování patří k základním otázkám slovotvorného výzkumu; s etablováním korpusů jako materiálových zdrojů lingvistické práce se výzkum produktivity stal ústředním tématem (především evropského) slovotvorného bádání. V zahraničních pracích je od 90. let 20. století diskutována možnost vyčíslit produktivitu jednotlivých formantů na základě frekvenčních údajů z korpusových dat, zásadní důležitost je přitom přikládána derivátům, které jsou v korpusových datech doloženy jediným výskytem. V příspěvku ilustrujeme problematičnost zjišťování slovotvorné produktivity z korpusových dat analýzou čtyř přípon, které v češtině vystupují jako součást názvů vlastností (zpravidla vedle jiných užití). Aplikací etablovaných kvantitativních přístupů a pokusem o důsledný rozbor systémových vlastností této sady přípon totiž dospíváme k různým výsledkům, tzn. jako nejproduktivnější jsou stanoveny různé přípony.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the “corpus age” of linguistics, the research in productivity in word-formation has focused on development of measures that enable to calculate productivity from frequency data gained from large corpora. In the contribution, we determine productivity of four suffixes that are used in names of qualities in Czech. The results obtained by respected productivity measures are compared with a tentative approach (inspired esp. by Dokulil, 1962) to determine productivity of the suffixes on the basis of their systemic features. The disparity of results based on quantitative data vs. systemic features are interpreted in favor of combining both aspects.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Lexikální síť AdjDeriNet zachycuje derivační vztahy mezi téměř 18 tisíci adjektivy a 26 tisíci lexémy od nich odvozenými (patřícími k různým slovním druhům); data tak obsahují celkem 44 tisíc lexémů. Nejfrekventovanějším deadjektivními deriváty jsou adverbia s příponou -e/ě, substantiva s příponou -ost a adverbia na -sky/-cky. Substantiva s příponou -as (př. kliďas), slovesa na -at (zelenat) nebo adjekiva s příponou -ičký (maličký) patří k nejméně frekventovaným derivátům v těchto datech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The lexical resource contains nearly 18 thousand adjective lexemes, which are base words for about 26 thousand lexemes of several parts of
speech; the database thus consists of 44 thousand derivationally interlinked lexemes in total. The most frequent derivatives are adverbs with the suffix -e/ě, nouns with the suffix -ost and adverbs ending in -sky/-cky. Nouns ending in -as (ex. kliďas ‘phlegmatic person’), verbs with -at (zelenat ‘to turn green’), or adjectives with the suffix -ičký (maličký ‘very small’) belong to the least frequent deadjectival derivatives in the database.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku popisujeme vývoj lexikální sítě DeriNet, která zachycuje slovotvorné vztahy mezi zhruba 266 tisíci českými lexémy. Síť je v současné době omezena na procesy odvozování, které je v české slovotvorbě nejčastější a také nejproduktivnější. Toto omezení je reflektováno v architektuře sítě: každý lexém smí být spojen pouze s jedním základovým slovem; skládání a kombinované slovotvorné procesy (kompozice s derivací) nejsou do sítě zahrnuty. Po krátkém shrnutí teoretického popisu derivace v češtině a prací věnujících se české derivaci z komputačního hlediska popisujeme lingvistická rozhodnutí, ze kterých návrh sítě vychází, a následně formální strukturu sítě a poloautomatickou anotaci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the present paper, we describe the development of the lexical network DeriNet, which captures core word-formation relations on the set of around 266 thousand Czech lexemes. The network is currently limited to derivational relations because derivation is the most frequent and most productive word-formation process in Czech. This limitation is reflected in the architecture of the network: each lexeme is allowed to be linked up with just a single base word; composition as well as combined processes (composition with derivation) are thus not included. After a brief summarization of theoretical descriptions of Czech derivation and the state of the art of NLP approaches to Czech derivation, we discuss the linguistic background of the network and introduce the formal structure of the network and the semi-automatic annotation procedure. The network was initialized with a set of lexemes whose existence was supported by corpus evidence. Derivational links were created using three sources of information: links delivered by a tool for morphological analysis, links based on an automatically discovered set of derivation rules, and on a grammar-based set of rules. Finally, we propose some research topics which could profit from the existence of such lexical network.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje metodu faktorového diskriminativního porozumění řeči, vhodnou pro parsování rozpoznané řeči v reálném čase. Je založena na sadě klasifikátorů logistické regrese, které se používají pro převod vstupních textů do dialogových aktů. Navrhovanou metodu evaluujeme na řečovém korpusu z domény informací o veřejné dopravě. Ta představuje soubor záznamů telefonních hovorů uživatelů dialogového systému, kteří hledají spojení městské nebo meziměstské veřejné dopravy, popř. se ptají na počasí v určitém městě. Výsledky ukazují, že za podmínek nepříznivých pro statistické rozpoznávání řeči náš statistický parser funguje lépe než základní dobře vyladěný pravidlový parser.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes a factored discriminative spoken language understanding method suitable for real-time parsing of recognised speech. It is based on a set of logistic regression classifiers, which are used to map input utterances into dialogue acts. The proposed method is evaluated on a corpus of spoken utterances from the Public Transport Information (PTI) domain. In PTI, users can interact with a dialogue system on the phone to find intra- and inter-city public transport connections and ask for weather forecast in a desired city. The results show that in adverse speech recognition conditions, the statistical parser yields significantly better results compared to the baseline well-tuned handcrafted parser.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje framework dialogových systémů Alex (ADSF).
ADSF v současné době zahrnuje komponenty pro telefonii, detekci hlasu, rozpoznávání řeči, porozumění řeči a pravěpodobností sledování stavu dialogu. ADSF je používáno v reálné aplikace pro poskytování informací o veřejné dopravě, "PTI" doméně. V "PTI" doméně uživatelé komunikují s dialogovým systémem pomocí telefonu, aby získali informace o dopravních spojeních jak v MHD tak mezi městy.
Na základě uživatelských hodnocení je naprostá většina uživatelů se systémem spokojena.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the Alex Dialogue Systems Framework (ADSF).
The ADSF currently includes mature components for public telephone network connectivity, voice activity detection, automatic speech recognition, statistical spoken language understanding, and probabilistic belief tracking. The ADSF is used in a real-world deployment within the Public Transport Information (PTI) domain. In PTI, users can interact with a dialogue system on the phone to find intra- and inter-city public transport connections and ask for weather forecast in a desired city. Based on user responses, vast majority of the system users are satisfied with the system performance.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přesné sledování stavu dialogu je zásadní pro návrh efektivního mluveného dialogového systému. Až do nedávné doby bylo kvantitativní srovnání různých metod sledování stavu obtížné. Soutěž sledování stavu dialogu v roce 2013 (DSTC) zavedla společou datovou a metriky, které umožňují vyhodnotit výkon různých systémů na jedné standardizované úloze. V tomto příspěvku předtsavujeme náš systém založený na modelu HIS (Hidden Information State) s upravenou komponentou uživatelského modelu. Dále uvádíme výsledky našeho trackeru na datovém souboru test3 z DSTC. Náš tracker je konkurenceschopný s trackery z DSTC, a to i bez dalšícho trénovaní. Dosahuje nejlepších výsledků v metrikách L2 a provádí přesnost mezi druhým a třetím místem. Po dotrénování systémů pomocí poskytnutých dat naše metoda překonala ostatní systémy v přesnosti a ještě zlepšila v metrice L2. Dále předkládáme předběžné výsledky o dalších dvou sadách dat test1 a test2, které byly použity v DSTC. Výsledky metrice L2 znamenají, že náš systém vytváří hypotézy s dobře kalibrovanými pravděpodobnostmi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Accurate dialog state tracking is crucial for the design of an efficient spoken dialog system. Until recently, quantitative comparison of different state tracking methods was difficult. However the 2013 Dialog State Tracking Challenge (DSTC) introduced a common dataset and metrics that allow to evaluate the performance of trackers on a standardized task. In this paper we present our belief tracker based on the Hidden Information State (HIS) model with an adjusted user model component. Further, we report the results of our tracker on test3 dataset from DSTC. Our tracker is competitive with trackers submitted to DSTC, even without training it achieves the best results in L2 metrics and it performs between second and third place in accuracy. After adjusting the tracker using the provided data it outperformed the other submissions also in accuracy and yet improved in L2. Additionally we present preliminary results on another two datasets, test1 and test2, used in the DSTC. Strong performance in L2 metric means that our tracker produces well calibrated hypotheses probabilities</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje dvě diskriminativní znalostní metody sledování stavu dialogu a jejich výsledky na datových sadách DSTC 2 a 3. První systém byl zařazen do soutěže DSTC3 a umístil se na druhém místě. Druhý tracker vyvinutý po termínu soutěze dává ještě lepší výsledky. Metoda nabízí srovnatelné výsledky jako nejlepší existující systémy a zorveň je dobře interpertovatelná. Na základě výsledků DSTC2 a DSTC3 analyzujeme vhodnost různých technik pro každý z dílčích problémů sledování stavu dialogu. Výsledky systémů ukazují důležitost sémantické analýzy (SLU) pro poslední dvě DSTC.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Abstract:
This paper presents two discriminative knowledge-based dialog state trackers and their results on the Dialog State Tracking Challenge (DSTC) 2 and 3 datasets. The first tracker was submitted to the DSTC3 competition and scored second in the joint accuracy. The second tracker developed after the DSTC3 submission deadline gives even better results on the DSTC2 and DSTC3 datasets. It performs on par with the state of the art machine learning-based trackers while offering better interpretability. We summarize recent directions in the dialog state tracking (DST) and also discuss possible decomposition of the DST problem. Based on the results of DSTC2 and DSTC3 we analyze suitability of different techniques for each of the DST subproblems. Results of the trackers highlight the importance of Spoken Language Understanding (SLU) for the last two DSTCs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představil jsem chatbota Pohádkové dítě návštěvníkům Dne otevřených dveří, t.j. zejména možných studentů MFF a potenciálně ÚFALu. Návštěvníci měli možnost si sami popovídat s chatbotem, kterou mnoho z nich využilo a užilo si.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I presented the Fairytale Child Chatbot to visitors of the Day of Open Doors, i.e. especially prospective students of MFF and potentially ÚFAL. The visitors had the opportunity to chat with the chatbot themselves, which many of them used and enjoyed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Manuál podává instrukce pro instalaci a běh Depﬁxu, našeho open-source nástroje pro automatickou post-editaci strojového překladu. V dokumentu pokrýváme kroky potřebné k užití Depﬁxu pro zpracování Vašich vlastních dat. Manuál také obsahuje popis Depﬁxové pajplajny, včetně instrukcí, které vám umožní změnit fungování Depﬁxu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The manual gives instructions on installing and running Depﬁx, our open-source tool for automatic post-editing of machine translation. We cover the steps required to use Depﬁx to process your own data. The manual also contains a description of the Depﬁx pipeline, including instructions that will enable you to modify the operation of Depﬁx.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme Depfix -- open-souce systém pro automatickou post-editaci výstupů frázového strojového překladu. Depfix zapojuje řadu nástrojů pro automatické zpracování přirozeného jazyka, pomocí nichž získává rozbor vstupních vět, a používá sadu pravidel pro opravu závažných chyb či chyb obvyklých ve výstupech strojových překladačů. Depfix je momentálně implementován pouze pro překladový směr z angličtiny do češtiny, ale v plánu je jeho rozšíření na další jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present Depfix, an open-source system for automatic post-editing of phrase-based machine translation outputs. Depfix employs a range of natural language processing tools to obtain analyses of the input sentences, and uses a set of rules to correct common or serious errors in machine translation outputs. Depfix is currently implemented only for English-to-Czech translation direction, but extending it to other languages is planned.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Depfix je open-souce systém pro automatickou post-editaci výstupů frázového strojového překladu. Depfix zapojuje řadu nástrojů pro automatické zpracování přirozeného jazyka, pomocí nichž získává rozbor vstupních vět, a používá sadu pravidel pro opravu závažných chyb či chyb obvyklých ve výstupech strojových překladačů. Depfix je momentálně implementován pouze pro překladový směr z angličtiny do češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Depfix is an open-source system for automatic post-editing of phrase-based machine translation outputs. Depfix employs a range of natural language processing tools to obtain analyses of the input sentences, and uses a set of rules to correct common or serious errors in machine translation outputs. Depfix is currently implemented only for English-to-Czech translation direction.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Depfix is a system for automatic post-editing of phrase-based English-to-Czech machine translation outputs, based on linguistic knowledge. We analyzed the types of errors that a typical machine translation system makes, and created a set of rules and a statistical component that correct some of the errors. We use a range of natural language processing tools to provide us with analyses of the input sentences. Moreover, we reimplemented the dependency parser and adapted it in several ways to parsing of statistical machine translation outputs. We performed both automatic and manual evaluations which confirmed that our system improves the quality of the translations.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Depfix je systém pro samočinnou post-edititaci výstupů frázových strojových překladů z angličtiny do češtiny, založený na jazykovědných znalostech. Nejprve jsme rozebrali druhy chyb, kterých se dopouští typický strojový překladač. Poté jsme vytvořili sadu pravidel a statistickou komponentu, které opravují takové chyby, které jsou běžné nebo závažné a může přicházet v úvahu jejich oprava pomocí našeho přístupu. Používáme řadu nástrojů pro zpracování přirozeného jazyka, které nám poskytují rozbor vstupních vět. Navíc jsme reimplementovali závislostní analyzátor a několika způsoby jej upravili pro provádění rozboru výstupů statistických strojových překladačů. Provedli jsme automatická i ruční vyhodnocení, která potvrdila, že kvalita překladů se zpracováním v našem systému zlepšuje.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popularizační přednáška pro účastníky České lingvistické olympiády. Jak funguje frázový statistický strojový překlad, jaké má problémy, a jak se dají opravovat Depfixem, systémem pro automatickou post-editaci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A popularization talk about Statistical machine translation and Automatic post-editing for the participants of Czech Linguistics Olympics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pohádkové dítě je jednoduchý chatbot, simulující zvídavé dítě. Požádá uživatele, aby mu vyprávěl pohádku, ale často ho přerušuje, aby se zeptal na detaily a vysvětlení. Pamatuje si ale, co mu uživatel řekl, a snaží se to pokud možno dát najevo.  Chatbot umí komunikovat česky a anglicky. Analyzuje tvarosloví každé uživatelovy věty pomocí NLP nástrojů, pokusí se nalézt chodnou otázku, a tu pak položí. Aby tvořené české věty zněly co nejpřirozeněji, využívá se pro skloňování tvaroslovný generátor.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Fairytale Child is a simple chatbot trying to simulate a curious child. It asks the user to tell a fairy tale, often interrupting to ask for details and clarifications. However, it remembers what it was told and tries to show it if possible.  The chatbot can communicate in Czech and in English. It analyzes the morphology of each sentence produced by the user with natural language processing tools, tries to identify potential questions to ask, and then asks one. A morphological generator is employed to generate correctly inflected sentences in Czech, so that the resulting sentences sound as natural as possible.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pohádkové dítě (Fairytale Child) je jednoduchý chatbot, snažící se napodobovat zvídavé dítě. Žádá uživatele, aby mu vyprávěl pohádku, a často jej přerušuje, aby se zeptal na podrobnosti či si ujasnil některé věci. Pamatuje si nicméně, co mu bylo řečeno, a snaží se to dát pokud možno najevo. 
Chatbot umí komunikovat česky a anglicky. Analyzuje morfologii každé věty, kterou uživatel zadal, za pomoci nástrojů pro zpracování přirozeného jazyka, snaží se rozpoznat potenciální otázky, na které by se mohl zeptat, a poté jednu z nich uživateli položí. Ke stvoření správně vyskloňované české věty se používá morfologický generátor.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Fairytale Child (Pohádkové dítě) is a simple chatbot trying to simulate a
curious child. It asks the user to tell a fairy tale, often interrupting to
ask for details and clarifications. However, it remembers what it was told and
tries to show it if possible.
The chatbot can communicate in Czech and in English. It analyzes the
morphology of each sentence produced by the user with natural language
processing tools, tries to identify potential questions to ask, and then asks
one. A morphological generator is employed to generate correctly inflected
sentences in Czech, so that the resulting sentences sound as natural as possible.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jak funguje Google Translate, proč nepoužívá slovník ale fráze z internetu, jaké má problémy.
Jak to jde dělat lépe, což řeší moje diplomová práce, a proč se programátorovi někdy hodí znát českou gramatiku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>How Google Translate works, why it doesn't use a dictionary, but uses phrases from the internet instead, what problems it has.
How it can be done in a better way, which was the topic if my diploma thesis, and why it is sometimes useful for a programmer to know Czech grammar.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>MSTperl je Perlovou reimplementací MST parseru  Ryana McDonalda, s několika pokročilými funkcemi navíc, jako je podpora pro paralelní rysy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>MSTperl is a Perl reimplementation of the MST parser of Ryan McDonald, with several additional advanced functions, such as support for parallel features.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení překladového systému Chiméra, zejména systémů Moses a TectoMT. Jednotlivé kroky, vstupy a výstupy, předvedeno na konkrétním příkladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presentation of the Chimera translation system, especially Moses and TectoMT systems. Individual steps, inputs and outputs, shown on a concrete example.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>A simple way of browsing CoNLL format files in your terminal. Fast and text-based.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Snadný způsob prohlížení souborů ve formátu CoNLL ve vašem terminálu. Rychlý a textový.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Existuje sbírka 30 závislostních korpusů (HamleDT) a existují i další závislostní korpusy; avšak na světě je kolem 7000 jazyků, a budovat závislostní korpus pro každý z nich se zdá být nepraktickým. Proto navrhuji prozkoumat možnosti částečně řízeného závislostního větného rozboru (pravděpodobně pomocí promítnutí z více zdrojů). Taktéž navrhuji obecně použití mlhavějších vstupních rysů (zejména tvaroslovných značek).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>There is a collection of 30 treebanks (HamleDT) and more treebanks exist, but there are about 7000 languages in the world, and it seems impractical to build treebanks for all of these. Therefore, I propose to explore the possibilities of semi-supervised parsing (probably via multisource projection). Also, I suggest generally adding more fuzziness to input features (especially morphological tags).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme HamleDT 2.0 (HArmonized Multi-LanguagE Dependency Treebank). HamleDT 2.0 je sbírka 30 existujících závislostních korpusů, harmonizovaných do společného anotačního stylu – Pražských závislostí – a dále transformovaných do Stanfordských závislostí – anotačního stylu, který se v nedávné době stal oblíbeným.

Používáme nejnovější základní Universal Stanford Dependencies, bez dodaných jazykově specifických subtypů. Popisujeme oba anotační styly, včetně úprav, které bylo nutné provést, a poskytujeme detaily o procesu konverze. Diskutujeme též rozdíly mezi těmito dvěma styly, vyhodnocujíce jejich výhody a nevýhody, a zmiňujeme vliv těchto rozdílů na konverzi.

Stanfordizaci obecně považujeme za úspěšnou, přestože uznáváme několik nedostatků – zejména v rozlišení přímých a nepřímých předmětů – kterým je nutné v budoucnosti věnovat pozornost.

Část HamleDT 2.0 volně zveřejňujeme; nemáme svolení k redistribuci celé datové sady, ale poskytujeme nástroje pro konverzi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present HamleDT 2.0 (HArmonized Multi-LanguagE Dependency Treebank). HamleDT 2.0 is a collection of 30 existing treebanks harmonized into a common annotation style, the Prague Dependencies, and further transformed into Stanford Dependencies, a treebank annotation style that became popular recently.

We use the newest basic Universal Stanford Dependencies, without added language-specific subtypes. We describe both of the annotation styles, including adjustments that were necessary to make, and provide details about the conversion process. We also discuss the differences between the two styles, evaluating their advantages and disadvantages, and note the effects of the differences on the conversion.

We regard the stanfordization as generally successful, although we admit several shortcomings, especially in the distinction between direct and indirect objects, that have to be addressed in future.

We release part of HamleDT 2.0 freely; we are not allowed to redistribute the whole dataset, but we do provide the conversion pipeline.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce je prvním pokusem o mezijazyčné rozpoznávaní koreference metodami strojového učení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This work is, to our knowledge, a first attempt at a machine learning approach to cross-lingual
coreference resolution, i.e. coreference resolution (CR) performed on a bitext. Focusing on CR of English pronouns, we leverage language differences and enrich the feature set of a standard monolingual CR system for English with features extracted from the Czech side of the bitext. Our work also includes a supervised pronoun aligner that outperforms a GIZA++ baseline in terms of both intrinsic evaluation and evaluation on CR. The final cross-lingual CR system has successfully outperformed both a monolingual CR and a cross-lingual projection system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Od publikace v roce 2011, ziskala CEFR vedoucí úlohu ve výuce a certifikaci cizích jazyků. Bohužel jednotlivé úrovně CEFR nejsou dostatečně ilustrovány. Cílem projektu Merlin je vyřešit tento problém pro češtinu, němčinu a italštinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Since its publication in 2001, the Common European Framework of Reference for Languages (CEFR) has gained a leading role as an instrument of reference for language teaching and certification and for the development of curricula. Nonetheless, there is a growing concern about CEFR reference levels being insufficiently illustrated in terms of authentic learner data, leaving practitioners without comprehensive empirical characterizations of the relevant distinctions. This is particularly the case for languages other than English (cf. e.g. Hulstijn 2007, North 2000). The MERLIN project addresses this demand to illustrate and validate the CEFR levels for Czech, German and Italian by developing a didactically motivated online platform that enables CEFR users to explore authentic written learner productions. The core of the multilingual online platform is a trilingual learner corpus composed of roughly 200 learner texts per CEFR level, produced in standardized language certifications validly related to the CEFR, covering the levels A1-C1. The aim of this paper is to both present the MERLIN project with the motivation behind and its corpus and to discuss its current state.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje výsledky evaluace vyhledávacího nástroje vyvinutého v rámci projektu Khresmoi. Evaluace byla zaměřena na laickou veřejnost v ČR a ve Francii.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article presents the results of one of the stages of the user-centered evaluation conducted in a framework of the EU project Khresmoi. In a controlled environment, users were asked to perform health-related tasks using a search engine specifically developed for trustworthy online health information. Twenty seven participants from largely the Czech Republic and France took part in the evaluation. All reported overall a positive experience, while some features caused some criticism. Learning points are summed up regarding running such types of evaluations with the general public and specifically with patients.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Klíč k rychlému přizpůsobení jazykových technologií pro libovolný jazyk 
závisí na dostupnosti základních nástrojů a datových zdrojů, jako jsou jednojazyčné nebo paralelní korpusy, anotované korpusy, značkovače slovních druhů, syntaktické analyzátory, a podobně. Jazyky, pro něž tyto základní zdroje 
neexistují, označujeme jako zdrojově chudé jazyky.

V této práci se zabýváme otázkou závislostního syntaktického rozboru zdrojově 
chudých jazyků za pomoci zdrojů pro jiné jazyky. Pro nalezení závislostní struktury používáme tři postupy: (i) promítnutí závislostí ze zdrojově bohatého jazyka do zdrojově chudého jazyka za pomoci slovního zarovnání v paralelním 
korpusu (ii) analýze pod-zdroji jazyků pomocí parserů, jejichž modely jsou vyškoleni na 
stromových korpusů z jiných jazyků, a nedívejte se na skutečných slovních forem, ale pouze na 
POS kategorie. Zde se zabýváme problémem neslučitelnosti různých anotačních stylů používaných zdrojovými analyzátory a 
cílovými závislostně anotovanými korpusy používanými pro evaluaci, který řešíme 
pomocí harmonizace anotací do jednotného standardu; a konečně (iii) zavádíme 
nový postup, ve kterém pro promítnutí závislostí do zdrojově chudého jazyka používáme paralelní korpusy vytvořené pomocí strojového překladu namísto lidského překladu.

Výše uvedené postupy jsme použili na pět indických jazyků: hindštinu, urdštinu, 
telugštinu, bengálštinu a tamilštinu (seřazeno sestupně podle dostupnosti závislostně anotovaných dat). Abychom prokázali použitelnost uvedených postupů v praxi, vyvinuli jsme závislostně anotovaný korpus pro tamilštinu, pro niž dosud žádný takový zdroj neexistoval, a takto získaná data využíváme pro evaluaci a také jako zdroj pro závislostní rozbor jiných indických jazyků. Nakonec jsme seznam se strategie, které může být použit k získání závislost struktury pro cílových jazyků pod jiný scénáře s omezenými zdroji.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Key to fast adaptation of language technologies for any language hinges on the availability of fundamental tools and resources such as monolingual/parallel corpora, annotated corpora, part-of-speech (POS) taggers, parsers and so on. The languages which lack those fundamental resources are often referred as under-resourced
languages.

In this thesis, we address the problem of cross-lingual dependency parsing of under-resourced languages. We apply three methodologies to induce dependency structures: (i) projecting dependencies from a resource-rich language to under-resourced languages via parallel corpus word alignment links (ii) parsing under-
resourced languages using parsers whose models are trained on treebanks of other
languages, and do not look at actual word forms, but only on POS categories. Here
we address the problem of incompatibilities in annotation styles between source side parsers and target side evaluation treebanks by harmonizing annotations to a common standard; and finally (iii) we add a new under-resourced scenario in which we use machine translated parallel corpora instead of human translated corpora for
projecting dependencies to under-resourced languages.

We apply the aforementioned methodologies to five Indian languages (ILs): Hindi, Urdu, Telugu, Bengali and Tamil (in the order of high to low availability of treebank data). To make the evaluation possible for Tamil, we develop a depen
dency treebank resource for Tamil from scratch and we use the created data in
evaluation and as a source in parsing other ILs. Finally, we list out strategies that
can be used to obtain dependency structures for target languages under different
resource-poor scenarios.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento dokument se vrací k přístupu projekce založené na závislost gramatiky indukční úkol. 
Tradiční cross-kulturní závislost indukční úkoly jeden tak či onak, závisí na existenci 
z bitexts nebo cílového jazyka nástrojů, jako je část-of-speech (POS) značkovače, abychom získali přiměřenou analýze přesnosti. V tomto článku jsme se přenést závislost analyzátory pomocí pouze orientační zdroje, tj, stroje přeloženy bitexts namísto ručně vytvořených bitexts. Děláme to tak, 
získání zdrojového stranu textu ze strojového překladu (MT) systém a pak použít Přenos přístupy k vyvolání analyzátor pro cílové jazyky. Dále snižují potřebu Informace o dostupnosti značených cílového jazyka zdrojů pomocí voze cílové tagger. 
Ukázali jsme, že náš přístup stále překonává bez dozoru analyzátory o větší rozpětí 
(8,2% absolutní), a výsledky v podobném provedení, kdy ve srovnání s delexicalized převodem 
analyzátory.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper revisits the projection-based approach to dependency grammar induction task.
Traditional cross-lingual dependency induction tasks one way or the other, depend on the existence
of bitexts or target language tools such as part-of-speech (POS) taggers to obtain reasonable
parsing accuracy. In this paper, we transfer dependency parsers using only approximate
resources, i.e., machine translated bitexts instead of manually created bitexts. We do this by
obtaining the the source side of the text from a machine translation (MT) system and then apply
transfer approaches to induce parser for the target languages. We further reduce the need
for the availability of labeled target language resources by using unsupervised target tagger.
We show that our approach consistently outperforms unsupervised parsers by a bigger margin
(8.2% absolute), and results in similar performance when compared with delexicalized transfer
parsers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jedním z nejdůležitějších aspektů cross-kulturní přenos závislostí 
je to, jak různé anotace styly, které často podceňují přesnost syntaktické jsou zpracovány. 
Novým trendem je, že styl anotace různých jazykových stromových korpusů může být
harmonizovány do jednoho stylu, a tak se lze vyhnout těžkopádné pravidla manuální transformace. 
V tomto článku budeme používat harmonizované stromových korpusů (POS tagsets a závislost struktury původní 
stromových korpusů mapované do společného stylu) pro vyvolání závislosti na nastavení cross-kulturní. 
Nabízíme převod závislostí pomocí delexicalized analyzátory, které používají harmonizované verze původních stromových korpusů. 
Tento přístup aplikovat na pět indických jazyků (Hindština, Urdu, Telugu, bengálský a Tamil) a ukazují, že 
Nejlepšího výkonu lze získat delexicalized analýze, kdy dojde k přemístění z indického jazyka (IL) na IL stromových korpusů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>One of the most important aspect of cross-lingual dependency transfer 
is how different annotation styles which often underestimate the parsing accuracy are handled. 
The emerging trend is that the annotation style of different language treebanks can be 
harmonized into one style and the cumbersome manual transformation rules thus can be avoided.
In this paper, we use harmonized treebanks (POS tagsets and dependency structures of original 
treebanks mapped to a common style)  for inducing dependencies in a cross-lingual setting. 
We transfer dependencies using delexicalized parsers that use harmonized version of the original treebanks. 
We apply this approach to five Indian languages (Hindi, Urdu, Telugu, Bengali and Tamil) and show that
best performance can be obtained in delexicalized parsing when the transfer takes place from Indian language (IL) to IL treebanks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek prezentuje výsledky úlohy 3 z ShARe/CLEF eHealth Evaluation Lab 2014 zaměřené na 
uživatelskou evaluace vyhledávání informací v oblasti zdraví</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of task 3 of the ShARe/CLEF eHealth Evaluation Lab 2014 focused on user-centred health information retrieval.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Test Your Bot (teyb) je framework pro testování dialogových systémů přes internet proti sdíleným simulátorům uživatele.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Test Your Bot (teyb) is a framework for testing dialog systems over the internet against a shared collection of user simulators.

Features:

Written in Django &amp; Python &amp; AngularJS
Web interface
Task creation
Team registration
Statistics
REST API for communication</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace aplikace JTagger, která detekuje reference v českých soudních rozhodnutích. Tento úkol chápeme jako typický úkol pro identifikaci jmenných entit, kde entity jsou reference (odkazy) na jiné dokumenty. Aplikujeme přístupy strojového učení s učitelem, konkrétně skryté Markovovy modely. Protože metody strojového učení s učitelem vyžadují manuálně anotovaná trénovací data, anotovali jsme 300 soudních rozhodnutí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We address the task of reference detection and classification in Czech court decisions. This task is a typical named-entity recognition task where the entities are references (links) to other documents. We apply a supervised machine learning approach, namely Hidden Markov Models. A supervised methodology requires manual annotation of training data so we annotated 300 court decisions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Aplikace detekuje reference v českých soudních rozhodnutích. Tento úkol chápeme jako typický úkol pro identifikaci jmenných entit, kde entity jsou reference (odkazy) na jiné dokumenty. Aplikujeme přístupy strojového učení s učitelem, konkrétně skryté Markovovy modely. Protože metody strojového učení s učitelem vyžadují manuálně anotovaná trénovací data, anotovali jsme 300 soudních rozhodnutí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We address the task of reference detection and classification in Czech court decisions. This task is a typical named-entity recognition task where the entities are references (links) to other documents. We apply a supervised machine learning approach, namely Hidden Markov Models. A supervised methodology requires manual annotation of training data so we annotated 300 court decisions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládáme detailní studii automatické lexikální disambiguace na pilotním vzorku třiceti anglických sloves za použití lexikonu vzorů slovesných užití (patterns), který vychází z Corpus Pattern Analysis (CPA). Tato inovátorská lexikografická metoda namísto na abstraktních definicích jednotlivých významů staví na souhře morfosyntaktické, lexikální a sémantické/pragmatické podobnosti slovesných užití. Natrénovali jsme několik statistických klasifikátorů na rozpoznávání těchto vzorů. Klasifikátory využívají jak morfosyntaktických, tak sémantických rysů. V naší studii se soustředíme na procedury pro extrakci rysů, jejich výběr a jejich evaluaci. Ukazujeme, že rysy na míru uzpůsobené jednotlivým slovesům, jež jsou implicitně obsaženy v definici každého vzoru v lexikonu, mají potenciál významně zvýšit přesnost statistických klasifikátorů s učitelem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We give a report on a detailed study of automatic lexical disambiguation of 30 sample English verbs. We have trained and evaluate several statistical classifiers that use both morphosyntactic and semantic features to assign semantic patterns according to a pattern lexicon. Our system of semantic classification draws on the Corpus Pattern Analysis (CPA) — a novel lexicographic method that seeks to cluster verb uses according to the morpho-syntactic, lexical and semantic/pragmatic similarity of their contexts rather than their grouping according to abstract semantic definitions. In this paper we mainly concentrate on the procedures for feature extraction and feature selection. We show that features tailored to particular verbs using contextual clues given by the CPA method and explicitly described in the pattern lexicon have potential to significantly improve accuracy of supervised statistical classifiers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme systém pro extrakci znalostní báze z nestrukturovaných textů. Bázy definujeme jako množinu entit a vztahů mezi nimi a reprezentujeme ji v ontologickém frameworku. Extrakční procedura zpracovává vstupní texty lingvistickými procedurami a extrahuje entity a vztahy mezi nimi z jejich syntaktické reprezentace. Následně jsou extrahované informace reprezentovány dle principů Linked Data. Systém je navržen nezávisle na doméně a jazyce textů tak, aby poskytl uživatelům inteligentnější vyhledávání než fulltextové. Prezentujeme první výsledky na českých legislativních dokumentech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a system that extracts a knowledge base from raw unstructured texts that is designed as a set of entities and their relations and represented in an ontological framework. The extraction pipeline processes input texts by linguistically-aware tools and extracts entities and relations from their syntactic representation. Consequently, the extracted data is represented according to the Linked Data principles. The system is designed both domain and language independent and provides users with data for more intelligent search than full-text search. We present our first case study on processing Czech legal texts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci detekujeme a klasifikujeme reference v českých soudních rozhodnutích, především reference na jiné soudní rozhodnutí a zákony. Reference chápeme jako entity v úloze rozpoznávání jmenných entit. Dále se zabýváme detekcí institucí, které publikují odkazované dokumenty. Náš přístup aplikuje metody strojového učení, konkrétně používáme Markovovy modely a Perceptron. Dosahujeme úspěšnosti nad 90% F-measure pro každý typ entity. Výsledky výrazně převyšují dosud publikované systémy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We address the task of detection and classification of references in Czech court decisions, mainly we focus on references to other court decisions and acts. We handle these references like entities in the task of Named Entity Recognition. In addition, we are interested in detection of institutions that issued documents under consideration. Attributes like the applicability of law have been studied as well. We approach the task using machine learning methods, namely HMM and Perceptron algorithm. We report F-measure over 90% for each entity. The results significantly outperform the systems published previously.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Podle statistik společnosti International Data Corporation, 90% všech digitálních dat je nestrukturovaných. Toto množství navyše roste dvakrát tak rychle jako množství strukturovaných dat. V mnoha doménách tvoří obrovské kolekce nestrukturovaných dokumentů hlavní zdroj informací. Jejich efektivní prohlížení a  dotazování přestavuje klíčový aspekt v mnoha oblastech lidské činnosti. Projekt INTLIB (Inteligentní knihovna) očekává na vstupu velké množství dokumentů souvisejících s určitou doménou. V první fáze jsou z dokumentů extrahována základní informace pomocí nástrojů pro zpracování přirozeného jazyka. V druhé fáze se zaoberáme efektivní a uživatelsky přívětivou vizualizací a dotazováním nad extrahovanými znalostmi. Prezentujeme naše výsledky na legislativní a enviromentální doméne.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>According to the statistics provided by the International Data Corporation, 90% of all available digital data is unstructured and its amount currently grows twice as fast as structured data. In many domains, large collections of unstructured documents form main sources of information. Their efficient browsing and querying present key aspects in many areas of human activities. The project INTLIB, an INTelligent LIBrary, assumes a collection of documents related to a particular problem domain on the input. In the first phase we extract a knowledge base from the collection using natural language processing tools. In the second phase we deal with efficient and user friendly visualization and querying the extracted knowledge. We will present our results on both legislative and environmental domain.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci popisujeme projekt INTLIB - inteligentní knihovna, kterého císlem je vytvořit více sofistikovaný a uživatelsky přívětivější systém pro dotazování nad textovými dokumenty než full-text. Na vstupu předpokládáme kolekci dokumentů související s určitou doménou (např. legislatíva, medicína, enviromentální doména). V první fáze extrahujeme z dokumenů znalostní bázy - kolekci objektů a vztahů mezi nima. Ve druhé fáze se zaoberáme vizualizací a prohlížením extrahovaných znalostí. Celý systém je navhován jako obecní framework který může být modifikován a rozšířen na specifikované domény. V pilotní fázy projektu pracujeme s legislativní doménou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we describe the project INTLIB – an INTelligent LIBrary whose aim is to provide a more sophisticated and user-friendly tool for querying textual documents than full-text search. On the input we assume a collection of documents related to a particular problem domain (e.g., legislation, medicine, environment, etc.). In the first phase we extract from the documents a knowledge base, i.e. a set of objects and their relationships, which is based on a particular
ontology (semantics). In the second phase we deal with sophisticated and user friendly visualization and browsing (querying) of the extracted knowledge. The whole system is proposed as a general framework which can be modified and extended for particular data domains. To depict its features we use the legislation domain.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Korpus Merlin je žákovský korpus s psanými texty v čestině, němčině a italštině. Jeho cílem je ilustrovat úrovně CEFR autentickými daty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The MERLIN corpus is a written learner corpus for Czech, German, and Italian that has been designed to illustrate the Common European Framework of Reference for Languages (CEFR) with authentic learner data. The corpus contains 2,290 learner texts produced in standardized language certiﬁcations covering CEFR levels A1–C1. The MERLIN annotation scheme includes a wide range of language characteristics that enable research into the empirical foundations of the CEFR scales and provide language teachers, test developers, and Second Language Acquisition researchers with concrete examples of learner performance and progress across multiple proﬁciency levels. For computational linguistics, it provide a range of authentic learner data for three target languages, supporting a broadening of the scope of research in areas such as automatic proﬁciency classiﬁcation or native language identiﬁcation. The annotated corpus and related information will be freely available as a corpus resource and through a freely accessible, didactically-oriented online platform.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje výsledky soutěže v automatickém hodnocení kvality strojového překladu (WMT14 Metrics Shared Task). Úkolem účastníků bylo vyhodnotit kvalitu překladových systémů, které se zúčastnily překladové úlohy WMT14. Získaných 23 metrik od 12 týmů jsme pak společně se 6 základními metrikami (BLEU, NIST, WER, PER, TER a CDER) porovnali z hlediska korelace s lidským hodnocením pro celý testset i pro jednotlivé věty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the
WMT14 Metrics Shared Task. We asked
participants of this task to score the
outputs of the MT systems involved in
WMT14 Shared Translation Task. We col-
lected scores of 23 metrics from 12 re-
search groups. In addition to that we com-
puted scores of 6 standard metrics (BLEU,
NIST, WER, PER, TER and CDER) as
baselines. The collected scores were eval-
uated in terms of system level correlation
(how well each metric’s scores correlate
with WMT14 official manual ranking of
systems) and in terms of segment level
correlation (how often a metric agrees with
humans in comparing two translations of a
particular sentence).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku jsme se zabývali jedním typem rozdílu v zachycení valenčních struktur v českém a anglickém valenčním slovníku a paralelním česko-anglickém syntakticky anotovaném korpusu. Syntakticky jde o konstrukce, které se projevují např. v tzv. alternaci subjektu a instrumentu (Instrument-Subject Alternation) (1), alternaci subjektu a abstraktní příčiny (Abstract Cause-Subject Alternation) (2) nebo alternaci subjektu a locata (Locatum Subject Alternation) (3) (Levin, 1993). Vzhledem k hloubkové valenci jde o dvojí možnou valenční strukturaci slovesa, přičemž vnější (non-core) argument přechází do pozice vnitřního (core) argumentu, a zároveň dochází k dekauzativizaci významu, tj. odsunutí původního aktora do pozadí situační perspektivy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the talk, we have dealt with one type of valency complementation mismatch in a paralel Czech-English dependency treebank, the so-called Instrument-Subject Alternation. In this type of transformation, the non-core argument takes the position of the core-argument, with the decausativisation of meaning.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tutoriál a vedení workshopu Emoce v jazyce, zaměřeného na zkoumání různých způsobů vyjadřování emocionality v psaném i v mluveném projevu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Hands-on tutorial and supervision of "Emotions in language" workshop focused on different approaches to evaluation expression in both spoken and written language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku se zabýváme syntaktickou a sémantickou analýzou slovesných hesel v českém slovníku subjektivních výrazů Czech Sublex 1.0. Zabýváme se jejich sémantickými a valenčními vlastnostmi v souvislosti s jejich subjektivním a hodnotícím charakterem. Ukazujeme, že hodnotící slovesa sdílejí určité abstraktní vzorce v nichž je zdroj a cíl evaluativního stavu mapován na valenční pozice. Podobnost sloves vzhledem k těmto vzorcům odpovídá podobnosti jejich sémantických vlastností.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we present a syntactic and semantic analysis of verbal entries in the Czech subjectivity lexicon Czech SubLex 1.0 concerning their semantic and valency properties with respect to the roots and degree of subjectivity and evaluativeness. We demonstrate that evaluative verbs share certain abstract syntactic patterns with valency positions encoding the position of the source and target of evaluative stance. These patterns then roughly correspond to semantic properties of the verbs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku ukazujeme, jak syntakticky anotovaný korpus pomáhá při zkoumání aplikovatelnosti a revizích vyvinuté lingvistické teorie. Na materiálu Pražského česko-anglického závislostního koprusu zkoumáme případy, kdy se liší provázání valenčních doplnění v odpovídajících větách a hledáme důvody nekorespondence aktantových označení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we would like to exemplify how a syntactically annotated bilingual treebank can help us in exploring and revising a
developed linguistic theory. On the material of the Prague Czech-English Dependency Treebank we observe sentences in which an
Addressee argument in one language is linked translationally to a Patient argument in the other one, and make generalizations about
the theoretical grounds of the argument non-correspondences and its relations to the valency theory beyond the annotation practice.
Exploring verbs of three semantic classes (Judgement verbs, Teaching verbs and Attempt Suasion verbs) we claim that the Functional
Generative Description argument labelling is highly dependent on the morphosyntactic realization of the individual participants, which
then results in valency frame differences. Nevertheless, most of the differences can be overcome without substantial changes to the
linguistic theory itself.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje pokus o vyřešení problému rozpoznávání klauzí a jejich vzájemných vztahů v českých souvětích na základě omezených informací, jmenovitě informací toliko získaných morfologickou analýzou. Metodu popsaná v tomto článku lze v budoucnu použít pro rozdělení procesu automatické syntaktické analýzy do dvou fází, a to 1) určení klauzí a jejich vzájemných vztahů a 2) syntaktická analýza jednotlivých klauzí. Tento přístup by měl zlepšit úspěšnost automatické syntaktické analýzy dlouhých souvětí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes an attempt to solve the problem of recognizing clauses and their mutual relationship in complex Czech sentences on the basis of limited information, namely the information obtained by morphological analysis only. The method described in this paper may be used in the future for splitting the parsing process into two phases, namely 1) Recognizing clauses and their mutual relationships; and 2) Parsing the individual clauses. This approach should be able to improve the result of parsing long complex sentences.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Poster je zaměřen na popis možností vyjadřování textových vztahů v češtině. Textové vztahy bývají vyjadřovány zejména diskurzními konektory, ale také jejich alternativními lexikálními vyjádřeními, jako např. "důvodem je".</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper focuses on various possibilities of expressing textual relations in Czech. This possibility is held mainly by 
discourse connectives but also by their 
alternatives lexicalizations like "the reason is".</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem prezentace bylo představit průzkum širších možností vyjadřování textových vztahů, resp.  představit, které jazykové prostředky (kromě "klasických" konektorů jako "a" nebo "avšak") mají schopnost signalizovat určitý vztah mezi dvěma jednotkami v textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of the presentation was to examine broader possibilities of expressing textual 
relations, in other words which language means (apart from “classic” connectives like "and" or 
"however") have an ability to signal certain relation between two units within a text.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace představila výsledky výzkumu diskurzních konektorů v širším smyslu založeném na korpusové analýze (s využitím Pražského závislostního korpusu a korpusu DIALOG).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presentation described results of the research on discourse connectives in broader sense based on  corpus probe (Prague Dependency Treebank; corpus DIALOG).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá definicí diskurzních konektorů v obecné rovině. Věnuje se konektorům v širším smyslu, tj. všem jazykovým prostředkům, které mají schopnost  vyjadřovat diskurzní vztah v textu (např. spojkám jako "ale", "a", "nebo", ale i vyjádřením typu "podmínkou pro to je", "kvůli této situaci" ap.).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper tries to contribute to the general definition of discourse connectives. It examines connectives in broader sense, i.e. all language expressions that have an ability to express discourse relations within a text (e.g. both conjunctions like "but", "and", "or" and expressions like "the condition for this is", "due to this situation" etc.).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje možnosti využití vícevrstvé anotace Pražského závislostního korpusu. Zaměřuje se přitom na překrývání koreferenčních vztahů a diskurzních vztahů vyjádřených víceslovnými konektory. Článek se snaží prozkoumat, jaké aspekty tyto dvě jazykové oblasti spojují a jakým způsobem můžeme toto vzájemné překrývání využít při automatickém vyhledávání konektivních spojení typu "navzdory tomu", "díky této skutečnosti", která mají v textu platnost víceslovných konektorů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper introduces a possibility of new research offered by a multi-dimensional annotation of the Prague Dependency Treebank. It focuses on exploitation of the annotation of coreference for the annotation of discourse relations expressed by multiword expressions. It tries to find which as-pect interlinks these linguistic areas and how we can use this interplay in automatic searching for Czech expressions like "despite this" ("navzdory tomu"), "because of this fact" ("díky této skutečnosti") functioning as multiword discourse markers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Současné přístupy ke statistickému strojovému překladu spoléhají na jednoduché rysy, které předpokládají nezávislost jednotlivých frází nebo pravidel SCFG. Přesto je obecně známo, že diskriminativní modely dokážou využít bohaté rysy extrahované z kontextu zdrojové věty mimo aktuální frázi nebo pravidlo. Tento kontext je během překladu dostupný. Představujeme framework pro open-source nástroj Moses, který umožňuje snadno trénovat a aplikovat diskriminativní modely zdrojového kontextu s využitím velkého počtu trénovacích příkladů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Current state-of-the-art statistical machine translation (SMT) relies
on simple feature functions which make independence assumptions at the 
level of phrases or CFG rules. However, it is well-known that
discriminative models can benefit from rich features extracted from
the source sentence context outside of the applied phrase or CFG rule,
which is available at decoding time. We present a framework for the 
open-source decoder Moses that allows discriminative models over
source context to easily be trained on a large number of examples and 
then be included as feature functions in decoding.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme naše systémy pro překlad angličtina→čeština a
angličtina→hindština, se kterými jsme se zúčastnili letošní překladové soutěže WMT. Pro směr
angličtina→čeština vycházíme z loňského systému CHIMERA a vyhodnocujeme několik nastavení.
Angličtina→hindština je letos novým jazykovým párem. Provedli jsme pokusy se zpětným
samoučením pro získání většího množství (umělých)
paralelních dat a s modelováním tvarosloví cílové strany.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our English→Czech and
English→Hindi submissions for this
year’s WMT translation task. For
English→Czech, we build upon last year’s
CHIMERA and evaluate several setups.
English→Hindi is a new language pair for
this year. We experimented with reverse
self-training to acquire more (synthetic)
parallel data and with modeling target-side
morphology.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Aktuálny stav infraštruktúry umožňujúcej Autorizáciu a Autentizáciu ukazuje odlišné vnímanie požiadavkov autormi a používateľmi. Predkladáme naše skúsenosti s vytvorením služby pre užívateľov z celého sveta.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Looking at the current state-of-the-art of the services and frameworks for Authentication and Authorization Infrastructure (AAI) reveals different visions and requirements by the creators and the potential users. We summarise our expectations and requirements together with our experience. The text is divided into blocks containing general guidelines and examples.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentácie popisuje skúsenosti a rozhodnutia s vytvorením lingvistického repozitára.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presentation describes our experience with building a sustainable linguistic repository built on DSpace</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Použitie EUDAT B2SAFE replikácie z repozitára postavenom na DSpace vrátane implementácie</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Description of EUDAT B2SAFE safe replication from a DSpace digital repository.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška představuje projekt anotace nadvětných vztahů v Pražském závislostním korpusu 3.0.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk introduces the project of large-scale manual annotation of language phenomena "beyond the sentence boundary" in Czech. It was conducted as a part of the Prague Dependency Treebank version 3.0 project and released in 2013. It provides annotations for 50k sentences of Czech journalistic text, emphasizing (1) discourse connectives and their arguments and senses, (2), textual coreference, and (3), bridging (or associative) anaphora. Together with the linguistic information already annotated earlier on the same data (morphology, surface and deep syntax, information structure), this resource is particularly useful for linguistic and NLP tasks that combine features from various levels of language description.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato studie je teoreticko-metodologickou
úvahou o tom, jak a do jaké míry je v současné době možné korpusově zpracovávat a značit jevy „nadvětné“, jevy textové lingvistiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present contribution is a theoretical and methodological study of the possibilities of
discourse processing by corpus methods. Despite the description complexity of phenomena
“beyond the sentence boundary”, we argue that, keeping in mind all the specific issues this
task brings along, even more ways of a systematic analysis are possible. Taking into account
various attempts of the last decade in creating discourse-annotated corpora, a reliable way to
proceed in any such analysis shows to be to distinguish among different layers of discourse
analysis (in particular between “semantic” and “pragmatic” aspects) and to stick with the language
form in opposition to classifying phenomena with no surface realization.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá klasifikací žánrů v Pražském závislostním korpusu a distribucemi diskurzních vztahů v jednotlivých typech žánrů v PDT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the project of classification of Prague Discourse Treebank documents (Czech journalistic texts) for their genres. Our main interest lies in opening the possibility to observe how text coherence is realized in different types (in the genre sense) of language data and, in the future, in exploring the ways of using genres as a feature for multi-sentence-level language technologies. In the paper, we first describe the motivation and the concept of the genre annotation, and briefly introduce the Prague Discourse Treebank. Then, we elaborate on the process of manual annotation of genres in the treebank, from the annotators' manual work to post-annotation checks and to the inter-annotator agreement measurements. The annotated genres are subsequently analyzed together with discourse relations (already annotated in the treebank) ― we present distributions of the annotated genres and results of studying distinctions of distributions of discourse relations across the individual genres.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje metriku pro automatickou evaluaci strojového překladu, odeslanou jako řešení soutěžního úkolu WMT14 Metrics Task. Jedná se o jednoduchou modifikaci známého BLEU skóre, která používá párování slov v testovací a referenční větě v cílovém jazyce. Počítá se vždy nejmenší párování zhledem k relativní editační vzdáleností prefixů a sufixů slov. Spárovaná slova v testovací věty jsou nahrazena slovy z referenčního překladu. Při výpočtu n-gramové přestnoti jsou pak n-gramy obsahující taková slova penalizovány proporcionálně ke vzdálenosti, podlé které se slova párovala.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes a machine translation metric submitted to the WMT14 Metrics Task. It is a simple modification of the standard BLEU metric using a monolingual alignment of reference and test sentences. The alignment is computed as a minimum weighted maximum bipartite matching of the translated and the reference sentence words with respect to the relative edit distance of the word prefixes and suffixes. The aligned words are included in the n-gram precision computation with a penalty proportional to the matching distance. The proposed tBLEU metric is designed to be more tolerant to errors in inflection, which usually does not effect the understandability of a sentence, and therefore be more suitable for measuring quality of translation into morphologically richer languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku popisujeme automatické generování česko-ruských valenčních rámců (formémů) na základě Vallexu, paralelního korpusu a slovníku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes an experiment combining several existing data resources (parallel corpora, valency lexicon, morphological taggers, bilingual dictionary etc.) and exploiting
them in a task of building a valency lexicon for a related language (Russian) derived from a high quality manually created valency lexicon for Czech (Vallex) containing several thousands of verbs with very rich syntactic and semantic information. The experiment is restricted only to nominal constituents in both simple and prepositional cases. The results discussed
in the second half of the paper seem to justify the method used and to encourage further experiments in this direction. The paper also discusses most frequent sources of errors.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje prototyp japonsko-českého strojového překladače založeného
na hloubkovém větném rozboru. Tento typ strojového překladu není v~současné
době ve srovnání s~jinými metodami tolik rozšířen, věříme však, že některé jeho
aspekty jsou schopny přispět k~celkově lepší kvalitě výstupu. Nutnou součástí
našeho úkolu je i získání a zpracování potřebných paralelních dat. Jelikož
japonsko-česká paralelní data nejsou prakticky vůbec dostupná, snažili jsme se
vyzkoušet různé postupy, které by nám pomohly tento nedostatek nahradit.
Náš systém je založen na stejném principu jako anglicko-český překladač TectoMT.
V~naší práci jsme se snažili zachytit alespoň základní jazykové jevy
charakteristické pro japonštinu. Náš hloubkový systém též porovnáváme se zavedeným frázovým
modelem překladu. Navzdory počátečním očekáváním pracuje frázový překlad lépe
i při relativním nedostatku paralelních dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article describes a prototype of Japanese-Czech machine translation system based on deep syntactic analysis. A required preparation step includes the collection and processing of parallel data. We circumvent the lack of directly parallel data by machine-translating the English side of our collection of Japanese-English data into Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Potřeba dat o akvizici češtiny cizinci vedla k vytvoření prvního žákovského korpusu češtiny. Po představení jeho základního designu a parametrů, se zaměřujeme na technické aspekty: přepis ručně psaných textů, proces anotace a možnosti využití výsledků, spolu s nástroji používanými pro tyto úkoly.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The need for data about the acquisition of Czech by non-native learners prompted the compilation of the first learner corpus of Czech. After introducing its basic design and parameters, including a multi-tier manual annotation scheme and error taxonomy, we focus on the more technical aspects: transcription of hand-written source texts, process of annotation, and options for exploiting the result, together with tools used for these tasks and decisions behind the choices. To support or even substitute manual annotation we assign some error tags automatically and use automatic annotation tools (tagger, spell checker).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kurz seznámí studenty s metodami zpracování morfologie přirozených jazyků. Začíná lingvistickým přehledem morfologie. Dále se zabývá korpusy, tagsety a anotací. Jádrem kurzu jsou supervised, unsupervised, a částečně supervised metody morfologické analýzy, morfologické segmentace, tvorba slovníků atd. Kurz zahrnuje jak standardní metody, tak diskuzi nedávných důležitých článků například Yarowsky &amp; Wicentowski 2000, Creutz &amp; Lagus 2007, Monson 2009 a Tepper &amp; Xia 2010.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The course will introduce the students to the methods of processing morphology of natural languages. It starts with a linguistic overview of morphology, discussing features of a wide variety of languages including fusional languages such as German and Czech and agglutinative such as Turkish and Esperanto. After that, a discussion of corpora, tagsets and annotation will follow. The core of the course covers supervised, unsupervised and semi-supervised methods of morphological analysis, morpheme segmentation, lexicon creation, etc. The course covers standard and well established methods (two level morphology, Porter stemmer), but also includes discussion of recent important papers, for example, Yarowsky &amp; Wicentowski 2000, Creutz and Lagus 2007, Monson 2009, and Tepper &amp; Xia 2010.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace představila připravované zásady anotace aktuálního členění v anglické části Pražského česko-anglického závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presentation demonstrated the upcoming principles of annotation of information structure in the English part of Prague Czech-English Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popis subjektivního slovosledu v češtině, okrajově v němčině a v angličtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Description of subjective word order in Czech, partly in German and in English.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška představila současné zásady a proces anotací aktuálního členění v Pražském česko-anglickém závislostním korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The lecture demonstrated current principles and process of annotation of topic-focus articulation in the Prague Czech-English Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška představila možnosti zkoumání slovosledu a aktuálního členění větného s pomocí anotovaného závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The lecture presented possibilities of exploring of word order and topic-focus articulation with help of  the annotated treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška popisovala výzkum kontextové zapojenosti na základě dat Pražského závislostního korpusu. Sledována byla například kontextová zapojenost závislých vět vs. větných členů vyjádřených nevětně, míra kontextové zapojenosti jednotlivých funktorů nebo lemmat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The lecture described research on contextual boundness based on the data from the Prague Dependency Treebank. The contextual boundness of depencent clauses vs. sentence elements expressed as non-clauses, the degree of contextual boundness of individual functors or lemmas were monitored.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace představila zjištěné tendence v povrchovém slovosledu českých aktantů a volných slovesných doplnění v datech Pražského závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presentation demonstrated the tendencies in the surface word order of Czech actants and free verbal modifications in data from Prague Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje lingvistickou soutěž Olympiáda v českém jazyce - uvádí přehled soutěžních úkolů, jejich rozbor a komentář.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes a competition in linguistics the Olympiad in the Czech language - it provides an overview of the tasks, their analysis and comment.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme naši práci o generování Karminy, staromalajské poetické formy indonéštiny. Karmina je báseň o dvou řádcích, kde první řádek obsahuje "háček" (sampiran) a druhá obsahuje poselství. Jedním z jedinečných rysů Karminy je chybějící diskursní vztah mezi háčkem a poselstvím. Proto generujeme háčky a poselství v oddělených procesech s využitím předdefinovaných schémat a ručně vybudované báze znalostí. Karminy byly vytvořeny náhodným párováním poselství s háčky při splnění podmínek na rýmy a strukturální podobnost.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our work in generating Karmina, an old Malay poetic form for Indonesian language. Karmina is a poem with two lines that consists of a hook (sampiran) on the first line and a message on the second line. One of the unique aspects of Karmina is in the absence of discourse relation between its hook and message. We approached the problem by generating the hooks and the messages in separate processes using predefined schemas and a manually built knowledge base. The Karminas were produced by randomly pairing the messages with the hooks, subject to the constraints imposed on the rhymes and on the structure similarity. Syllabifications were performed on the cue words of the hooks and messages to ensure the generated pairs have matching rhymes. We were able to generate a number of positive examples while still leaving room for improvement, particularly in the generation of the messages, which currently are still limited, and in filtering the negative results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem workshopu bylo vytvořit prostor pro diskuzi o textových jevech a o využitelnosti jejich anotace v lingvistických aplikacích. Textovými jevy jsou např. aktuální členění, koreference, asociační anafora nebo mezivýpovědní významové vztahy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of this workshop was to trigger a discussion about textual phenomena and its usage in linguistic applications. Textual phenomena are topic focus articaluation, coreference, anaphora or discourse relations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek je zaměřen na popis hypotaktických
konstrukcí (konstrukce s podřadicí spojkou s "elaborativními" významy v češtině a angličtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper is focused on description of hypotactic
constructions (constructions with subordinating
conjunctions) with “elaborative” meanings. In dependency based linguistic literature,
they are referred to as hypotactic coordinations
or also (a subset of) false dependent clauses. The analysis makes use of syntax and discourse annotated corpora of Czech and English and thus offers an empirically grounded contrastive study of the phenomena.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem této studie je zjistit zda žákovský text ve flektivním jazyce jako je čeština obsahují příznaky, pomocí kterých lze identifikovat rodný jazyk autora.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The goal of this study is to investigate whether learners’ written data in highly inflectional Czech can suggest a consistent set of clues for automatic identification of the learners’ L1 background. For our experiments, we use texts written by learners of Czech, which have been  automatically and manually annotated for errors. We define two classes of learners: speakers of Indo-European languages and speakers of non-Indo-European languages. We use an SVM classifier to perform the binary classification. We show that non-content based features perform well on highly inflectional data. In particular, features reflecting errors in orthography are the most useful, yielding about 89% precision and the same recall. A detailed discussion of the best performing features is provided.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje SubLex 1.0, nový lexikální zdroj pro klasifikaci polarity v češtině. Slovník SubLex obsahuje 4947 hodnotících výrazů s příslušným slovním druhem a pozitivní nebo negativní polaritou. V článku popisujeme metodologii vytváření slovníku a kritéria pro jeho ruční čištění a případné další rozšiřování. Dále testujeme spolehlivost slovníku, a sice jeho implementací v rámci již existujících klasifikátorů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper introduces Czech subjectivity lexicon – the new lexical resource for sentiment analysis in Czech. The lexicon is a dictionary of 4947 evaluative items annotated with part of speech and tagged with positive or negative polarity. We describe the method for building the basic vocabulary and the criteria for its manual refinement. Also, we suggest possible enrichment of the fundamental lexicon. We evaluate the current version of the dictionary by implementing it to the classifiers for automatic polarity detection and compare the results of both plain and supplemented system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozvoj Webu 2.0 přinesl množství textů generovaných samotnými uživateli Internetu. Jejich příspěvky nezřídka obsahují subjektivní názory, emoce, hodnocení… K čemu a jak můžeme tato data použít? Je možné emoce v textu spolehlivě automaticky třídit? Příspěvek z oblasti postojové analýzy představí metody a úspěchy automatické extrakce emocí z textu s důrazem na využití detailní lingvistické analýzy – a možná vyřeší i otázku, jak vybrat ten správný vejcovar.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The rise of Web 2.0 brought large quantity of texts generated by the Internet users themselves. Their contributions usually contain subjective opinions, emotions, reviews... How can we use this data? Is it possible to classify online emotions? The presentation from the area of sentiment analysis introduces methods and success rate of automatic emotion extraction with emphasis on linguistic analysis - and may also answer the question on how to buy the good egg boiler.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V anonymizovaném prostředí internetových diskusí a sociálních sítí pravidelně dochází k verbálním výpadům, které nezřídka naplňují skutkovou podstatu trestného činu (např. hanobení rasy a národa nebo pomluvy). Pachatele kybernetických útoků lze jen obtížně dohledat a jejich činy proto často zůstávají nepotrestány. O řešení tohoto problému se v posledních letech pokouší evropská forenzní lingvistika s přispěním postojové analýzy, která se v nedávné době začala rozvíjet také v tuzemsku (viz např. Veselovská, 2012).

Postojová analýza se zabývá možnostmi extrakce subjektivní informace z textu a zkoumáním takovéto informace z jazykovědného hlediska (viz Liu, 2009). Jedním z hlavních cílů postojové analýzy je detekce hodnotících výrazů, tedy slov a frází, které inherentně obsahují pozitivní nebo negativní hodnocení (viz také Wiebe et al., 2004). Povaha a frekvence těchto výrazů (zejména pak těch s negativní polaritou), pro češtinu shromážděných ve slovníku SubLex1.0 (Veselovská a Bojar, 2013), může mít zásadní vliv na odhalení nesnášenlivého textu a při existenci i krátkých referenčních pasáží také jeho autora. Úspěšnost postupu roste zejména v kombinaci s analýzou funkčních slov, která určují gramatické vztahy v souvětích, a celkového projevu pisatele, jak dokazuje například výzkum Brennana et al. (2011).

V příspěvku budou představeny metody současné stylometrie (automatické identifikace autora na základě jím vyprodukovaného textu) a možnosti uplatnění postojové analýzy ve forenzní lingvistice, včetně včasného odhalování sebevražedných tendencí nejen na sociálních sítích (viz také Pestian et al., 2012) nebo ověřování pravdivosti výpovědí žadatelů o evropský azyl (Eades, 2009).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper I will introduce methods employed in current stylometry (the automatic authorship identification) and possible use of sentiment analysis in forensic linguistics, including e.g. automatic detection of suicidial tendencies on social networks (see also Pestian et al., 2012).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem workshopu je vytvořit prostor pro diskuzi o textových jevech a o využitelnosti jejich anotace v lingvistických aplikacích. Textovými jevy jsou např. aktuální členění, koreference, asociační anafora nebo mezivýpovědní významové vztahy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of this workshop is to trigger a discussion about textual phenomena and its usage in linguistic applications. Textual phenomena are topic focus articaluation, coreference, anaphora or discourse relations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentovaný výzkum měl za cíl vylepšit automatickou detekci a kontrolu správnosti zarovnání pleonastického it a jeho českých protějšků v Prague Czech-English Dependency Treebank (Hajič et al. 2011) na základě pravidel vypozorovaných porovnáváním české a anglické části paralelního korpusu InterCorp.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of presented contribution was to improve automatic detection and control of alignment of pleonastic it and its Czech counterparts in Prague Czech-English Dependency Treebank (Hajič et al., 2011) with the help of cs-en data from parallel corpus InterCorp.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Český slovník hodnotících výrazů, tedy seznam hodnotících slov pro sentiment analysis v češtině. Seznam obsahuje 4626 hodnotících položek (1672 pozitivních a 2954 negativních) označkovaných morfologicky a s příslušnou orientací hodnocení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Czech subjectivity lexicon, i.e. a list of subjectivity clues for sentiment analysis in Czech. The list contains 4626 evaluative items (1672 positive and 2954 negative) together with their part of speech tags, polarity orientation and source information.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slovníkový klasifikátor je z dlouhodobého hlediska jedním z nejvyužívanějších klasifikátorů v oblasti postojové analýzy. Přestože dosahuje relativně dobrých výsledků také pro češtinu, vykazuje tento klasifikátor jistou míru chybovosti. Článek nabízí detailní analýzu chyb v klasifikaci způsobených jak systémově, tak vinou recenzentů, a na základě této analýzy navrhuje další úpravy systému.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Lexicon-based classifier is in the long term one of the main and most effective methods of polarity classification used in sentiment analy-sis, i.e. computational study of opinions, sen-timents and emotions expressed in text (see Liu, 2010). Although it achieves relatively good results also for Czech, the classifier still shows some error rate. This paper provides a detailed analysis of such errors caused both by the system and by human reviewers. The identified errors are representatives of the chal-lenges faced by the entire area of opinion min-ing. Therefore, the analysis is essential for fur-ther research in the field and serves as a basis for meaningful improvements of the system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek je založen na první korpusové analýze vyjadřovacích prostředků českého evaluativního jazyka a je z velké části věnován modelování evaluativního významu v rámci konstrukčně-gramatického formalismu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This contribution presents the first corpus-based analysis of Czech evaluative language and the first steps towards a model of evaluative meaning in Czech.
For the current study, we have used three main sources of data:
•	the newly created Czech subjectivity lexicon SubLex 1.0 (see [2])
           containing 4950 unique evaluative lemmas tagged with positive 
	or negative polarity
•	the Czech National Corpus (see [1]).
•	the manually annotated evaluative data from three different domains 
           (news, movie reviews and kitchen appliances reviews)
	First, we will confront selected SubLex items with the evidence in Czech National Corpus and offer a complex linguistic analysis of the obtained evaluative sentences, observing especially lexical and morphological means of expressing evaluation in the text. However, since we are aware of the fact that evaluative items are subject to the interaction with context both at the sentence level, and in a larger text span (including e.g. the influence of negation or changes in aspect), we will briefly analyze the syntactic and semantic structure of the evaluative sentences. In addition, we will introduce and examine the most common Czech evaluative idioms. To put our general findings under a more specific scrutiny, we will then explore the use of evaluation in different domain-specific corpora of evaluative sentences.
	Finally, as a result of the above mentioned analysis, we will introduce the overview of generalized structures bearing evaluative meaning in Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Even though the quality of unsupervised
dependency parsers grows, they often fail
in recognition of very basic dependencies.
In this paper, we exploit a prior knowledge
of STOP-probabilities (whether a given
word has any children in a given direction), which is obtained from a large raw
corpus using the reducibility principle. By
incorporating this knowledge into Dependency Model with Valence, we managed to
considerably outperform the state-of-the-art results in terms of average attachment
score over 20 treebanks from CoNLL 2006
and 2007 shared tasks</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Even though the quality of unsupervised
dependency parsers grows, they often fail
in recognition of very basic dependencies.
In this paper, we exploit a prior knowledge
of STOP-probabilities (whether a given
word has any children in a given direction), which is obtained from a large raw
corpus using the reducibility principle. By
incorporating this knowledge into Dependency Model with Valence, we managed to
considerably outperform the state-of-the-art results in terms of average attachment
score over 20 treebanks from CoNLL 2006
and 2007 shared tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato technická zpráva se zabývá alternativními reprezentacemi koordinačních struktury a vlivem jednotlivých řešení na úspěšnost závislostního parsingu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this report we explore alternative representations of coordination structures within dependency trees and study the impact of particular solutions on performance of two selected state-of-the-art dependency parsers across a typologically diverse range of languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek uvádí do problému generovaní přirozeného jazyka a podává krátké shrnutí nedávného vývoje tohoto oboru, přičemž se zaměřuje na využití statistických metod. Největší důraz je kladen na generování jazyka v hlasových dialogových systémech, ale ostatní oblasti využití jsou také zmíněny. Článek končí přehledem problémů, je nutné vyřešit při vývoji vícejazyčného generátoru jazyka pro hlasový dialogový systém.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an introduction to the problem of Natural Language Generation (NLG) an give a brief survey of recent advances in this field, focusing on the usage of statistical methods. Most stress is put on NLG within Spoken Dialogue Systems, but other usage areas are included as well. The paper concludes with an overview of problems posed by the development of a multilingual NLG system for a Spoken Dialogue System.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme novou metodu statistického generování morfologie, tj. predikce konkrétní slovní formy z lemmatu, slovního druhu a morfologických kategorií, která cílí na robustnost vůči neznámým vstupům. Náš systém používá trénovatelný klasifikátor pro predici „editačních scénářů“, které posléze použije k transformaci lemmat na cílové slovní formy. Pro dosažení robustnosti jsou jako atributy pro klasifikaci použity také sufixy lemmat. Náš systém byl vyhodnocen na šesti jazycích s různým stupněm morfologické bohatosti. Výsledky ukazují, že systém je schopen se naučit většinu morfologických jevů a generalizuje na neznámé vstupy, takže dosahuje signifikatně lepších výsledků než baseline založený na slovníku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a novel method of statistical morphological generation, i.e. the prediction  of  inflected  word  forms  given lemma, part-of-peech and morphological features, aimed at robustness to unseen inputs. Our system uses a trainable classifier to predict “edit scripts” that are then used to transform lemmas into inflected word forms. Suffixes of lemmas are included as features to achieve robustness.  We evaluate our system on 6 languages with a varying degree of morphological richness. The results show that the system is able to learn most morphological phenomena and generalize to unseen  inputs, producing significantly better results than a dictionary-based baseline.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek prezentuje experimenty strojového překladu
z angličtiny do češtiny s použitím
velkého množství ručně anotovaných textových konektorů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents machine translation experiments
from English to Czech with a
large amount of manually annotated discourse
connectives. The gold-standard
discourse relation annotation leads to better
translation performance in ranges of
4–60% for some ambiguous English connectives
and helps to find correct syntactical
constructs in Czech for less ambiguous
connectives. Automatic scoring confirms
the stability of the newly built discourseaware
translation systems. Error analysis
and human translation evaluation point to
the cases where the annotation was most
and where less helpful.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje koncept korespondenčního semináře jakožto prostředku, jak doplnit a podpořit jednorázové soutěže, zejména olympiády. Vyhodnocujeme jednotlivé výhody tohoto způsobu výuky lingvistiky a porovnáváme jeho charakter s charakterem lingvistické olympiády. Věříme, že korespondenční seminář je výborný způsob, jak přivést talentované středoškolské studenty k lingvistice.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the concept of a correspondence seminar as a way to complement and support one-time contests, especially olympiads. We evaluate specific payoffs of this way of teaching linguistics, and compare its nature to that of a linguistics olympiad. We believe that the correspondence seminar is a great way to introduce talented high school students to linguistics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této studii se zabýváme vztahy mezi užitími téhož slovesného lexému, které jsou charakterizovány změnou v přiřazení situačních participantů a povrchových syntaktických pozic; tyto vztahy označujeme jako alternace. Tato práce se soustředí na lexikalizované alternace, které jsou vyjádřeny prostředky lexikálně-sémantickými, tj. změnou lexikální jednotky slovesa. V češtině jsou lexikalizované alternace konverzní (tzv. lexikálně-sémantické konverze, např. naložit seno na vůz -- naložit vůz senem), nebo nekonerzní (tzv. různé strukturní vyjádření téhož participantu, např. vyjít na kopec -- vyjít kopec, a strukturní rozpad situačního participantu, např. rozpad tématu a dikta). V práci představujeme zachycení těchto vztahů ve valenčním lexikonu VALLEX.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we study specific relations between uses of the same verb lexeme that stem from the changes in the linking of situational participants, valency complementations and surface syntactic positions – we refer to them as to alternations. The focus is on those types that are expressed by lexical-semantic means and result in changes in valency frames of verbs. 
We present and analyze three basic types of lexicalized alternations in Czech. They may be characterized as either conversive, or non-conversive. The conversive lexicalized alternations represent the central ones in the language system and here are referred to as (i) the lexical-semantic conversions (e.g., to load hay on the truck -- to load the truck with hay). On the other hand, the non-conversive alternations are rather peripheral; we can introduce esp. (ii) the different structural expression of a single situational participant (e.g., to climb up the hill - to climb the hill) and (iii) the structural splitting of a situational participant (e.g., theme-dictum splitting for Czech verbs of communication and mental action).
Further, we propose a formal framework that allows us to represent different types of lexicalized alternations in the valency lexicon of Czech verbs, VALLEX.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje formální lexikografickou reprezentaci konstrukcí s tzv. lehkým slovesem. Informaci o daných konstrukcí rozdělujeme mezi valenční rámec lehkého slovesa a rámec predikativního jména. Povrchově syntaktickou realizaci (včetně morfematického vyjádření) valenčních doplnění lze snadno odvodit pomocí formálních syntaktických pravidel.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Light verb constructions (LVCs) pose a serious
challenge for both theoretical and applied
linguistics as their syntactic structures
are not solely determined by verbs
alone but also by predicative nouns. In this
contribution, we introduce an initial step to
a new formal lexicographic representation
of LVCs for the valency lexicon of Czech
verbs, VALLEX.
The main idea underlying our representation
is to decompose the information on an
LVC between (i) the verbal valency frame
and (ii) the nominal valency frame. Both
deep and surface syntactic structures of
LVCs can be easily derived from the information
given in the verbal and nominal
frames by application of formal rules
as they are introduced in this contribution.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje experiment identifikující analytické predikáty v českém textu. Prvním cílem experimentu bylo vytvoření seznamu českých sloves, které umožňují vazbu s predikativními substantivy; druhým cílem bylo ověřit stanovená kritéria pro určení, zda je daný výskyt slovesa analytickým, či plnovýznamovým predikátem.

Tři anotátoři paralelně anotovali stejné věty s vybranými slovesy podle stanovených kritérií. Mezianotátorská shoda měřená váženou kappou dosáhla hodnoty 0,686.

Výsledkem experimentu je 893 verbonominálních kombinací českých analytických predikátů s predikativními substantivy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we describe a corpus based experiment focused on the possibility of identifying Czech light verbs. The experiment had two main aims: (i) to establish the inventory of Czech light verbs entering into combinations with predicative nouns, and (ii) to verify the adopted criteria for distinguishing light usages from full usages of the given verbs. As for the inventory of light verbs, we propose and verify the hypothesis that the possibility of light usages of verbs is related to their semantic class membership rather than to their high frequency.

In the second part of the experiment, we exploited the compiled inventory of Czech verbs inclined to occur as light verbs. The criteria adopted for distinguishing light usages of a verb from its full ones were applied to selected corpus sentences with these verbs. Three annotators were asked to determine whether a verb occurrence in an extracted corpus sentence corresponds to the full or to the light usage of the given verb. The feasibility of this task has been proven by the achieved κw statistics 0.686 and by the interannotator agreement 85.3%.

As a result of this experiment, we obtained 893 verbonominal combinations of Czech light verbs and predicative nouns. These combinations will be further utilized for the lexicographic representation of these phenomena.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nový rozpoznávač pojmenovaných entit pro český jazyk, který dosahuje 82.82 F-measure na korpusu Czech Named Entity Corpus 1.0 a statisticky významně překonává dříve publikované české rozpoznávače pojmenovaných entit. Na anglické úloze CoNLL-2003 shared task dosahuje 89.16 F-measure. Tento výsledek je srovnatelný s anglickými současnými výsledky. Rozpoznávač je založen na maximum entropy markovském modelu a optimální sekvence pojmenovaných entit je dosaženo globálním dekódováním Viterbiho algoritmem pomocí pravděpodobností odhadnutých maximum entropy klasifikátorem. Klasifikátor využívá morfologickou analýzu, dvojúrovňovou predikci, clusterizaci slov a gazetteers.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a new named entity recognizer for the Czech language. It reaches 82.82 F-measure on the Czech Named Entity Corpus 1.0 and significantly outperforms previously published Czech named entity recognizers. On the English CoNLL-2003 shared task, we achieved 89.16 F-measure, reaching comparable results to the English state of the art. The recognizer is based on Maximum Entropy Markov Model and a Viterbi algorithm decodes an optimal sequence labeling using probabilities estimated by a maximum entropy classifier. The classification features utilize morphological analysis, two-stage prediction, word clustering and gazetteers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nová verze ručně anotovaného korpusu zaměřeného na pojmenované entity.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A new version of the Czech corpus with manually annotated named entities, classified using a rich hierarchy of types.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Závislostní syntaktické analyzátor jsou většinou vyhodnocovány pomocí podílu správně zavěšených hran, to ale nevypovídá přesně o závažnosti a důsledcích jednotlivých chyb pro reálné aplikace. V tomto článku popisujeme úspěšnost sedmi analyzátorů a tří jejich kombinací z hlediska jejich vlivu na kvalitu strojového překladu využívajícího hloubkovou syntax.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Dependency parsers are almost ubiqui-
tously evaluated on their accuracy scores,
these scores say nothing of the complex-
ity and usefulness of the resulting struc-
tures. The structures may have more com-
plexity due to their coordination structure
or attachment rules. As dependency parses
are basic structures in which other systems
are built upon, it would seem more reason-
able to judge these parsers down the NLP
pipeline.
We show results from 7 individual parsers,
including dependency and constituent
parsers, and 3 ensemble parsing tech-
niques with their overall effect on a Ma-
chine Translation system, Treex, for En-
glish to Czech translation. We show that
parsers’ UAS scores are more correlated
to the NIST evaluation metric than to the
BLEU Metric, however we see increases
in both metrics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Závislostní anotace anglických vět z datové sady WMT 2012 (vzniklé pro Workshop in Machine Translation) je popsána v článku Improvements to Syntax-based Machine Translation using Ensemble Dependency Parsers presented at The Second Workshop on Hybrid Approaches to Translation (HyTra) at ACL 2013. Anotace byly provedeny podle pravidel analytické roviny Pražského závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>WMT annotations are described in the paper Improvements to Syntax-based Machine Translation using Ensemble Dependency Parsers presented at The Second Workshop on Hybrid Approaches to Translation (HyTra) at ACL 2013. The annotations are done on the analytically layer of the Czech to English Data set from the Workshop on Machine Translation 2012 data.

 

The main purpose of this data set is to give researchers gold level dependency annotations for a data set that contains a gold level translations. This should allow researchers to analyze dependency annotations effect on machine translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme naše pokusy s frázovým strojovým překladem pro společnou úlohu WMT 2013. Natrénovali jsme jeden systém pro 18 překladových směrů mezi angličtinou nebo češtinou na jedné straně a angličtinou, češtinou, němčinou, španělštinou, francouzštinou nebo ruštinou na straně druhé. Popisujeme sadu výsledků s různými velikostmi trénovacích dat. Pro páry obsahující ruštinu navíc popisujeme sadu nezávislých pokusů s lehce odlišnými překladovými modely.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe our experiments with phrase-based machine translation for the WMT 2013 Shared Task. We trained one system for 18 translation directions between English or Czech on one side and English, Czech, German, Spanish, French or Russian on the other side. We describe a set of results with different training data sizes and subsets. For the pairs containing Russian, we describe a set of independent experiments with slightly different
translation models.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje experimenty s přiřazováníém sémantických kategorií českým podstatným jménům pomocí metod strojového učení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we experiment with supervised machine learning techniques for the task of assigning semantic categories to nouns in Czech. The experiments work with 16 semantic categories  based on available manually annotated data.  The paper compares two possible approaches - one based on the contextual information,  the other based upon morphological properties  - we are trying to automatically extract final segments  of lemmas which might carry semantic information. The central problem of this research is finding the features for machine learning  that produce better results for relatively small training data size.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vyhľadávanie relevantných webových stránok a využívanie odkazov na relevantný obsah je rozšírený a efektívny prístup k vyhľadávaniu informácií. Existujúce práce, ktoré sa zaoberajú vyhľadávaním informácií v multimédiách sú však zamerané buď na vyhľadávanie jednotlivých relevantných položiek alebo na prepájanie obsahu bez toho aby sa zaoberali vyhľadávaním. V našej práci popisujeme prepojenie multimodálneho vyhľadávania a následné prepojenie multimediálnych dát. Experimenty sú založené na Search and Hyperlinking tasku organizovanom v rámci MediaEval 2012 Benchmarku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Searching for relevant webpages and following hyperlinks to related content is a widely accepted and effective approach to information seeking on the textual web. Existing work on multimedia information retrieval has focused on search for individual relevant items or on content linking without specific attention to search results. We describe our research exploring integrated multimodal search and hyperlinking for multimedia data. Our investigation is based on the MediaEval 2012 Search and Hyperlinking task. This includes a known-item search task using the Blip10000 internet video collection, where automatically created hyperlinks link each relevant item to related items within the collection. The search test queries and link assessment for this task was generated using the Amazon Mechanical Turk crowdsourcing platform. Our investigation examines a range of alternative methods which seek to address the challenges of search and hyperlinking using multimodal approaches. The results of our experiments are used to propose a research agenda for developing effective techniques for search and hyperlinking of multimedia content.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dôležitosť vyhľadávania v audio-vizuálnych nahrávkach kvôli prudko rastúcemu počtu audio-vizuálnych nahrávok dostupných on-line v súčasnosti stúpa. V porovnaní s tradičným IR metódami vyžaduje táto úloha špeciálne techniky ako je napríklad passage retrieval, ktorý môže urýchliť celý proces vyhľadávania tak, že je vyhľadaný presný relevantný úsek nahrávky namiesto celého dokumentu. Pri využití passage retrievalu sú nahrávky rozdelené na kratšie segmenty. V tejto práci porovnávame dva prístupy využité v passage retrieval: segmentáciu na prekrývajúce sa úseky rovnakej dĺžky a segmentáciu na rôzne dlhé úseky na základe sémantiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The importance of Information Retrieval (IR) in audio-visual recordings has been increasing with steeply growing numbers of audio-visual documents available on-line. Compared to traditional IR methods, this task requires specific techniques, such as Passage Retrieval which can accelerate the search process by retrieving the exact relevant passage of a recording instead of the full document. In Passage Retrieval, full recordings are divided into shorter segments which serve as individual documents for the further IR setup. This technique also allows normalizing document length and applying positional information. It was shown that it can even improve retrieval results.

In this work, we examine two general strategies for Passage Retrieval: blind segmentation into overlapping regular-length passages and segmentation into variable-length passages based on semantics of their content.

Time-based segmentation was already shown to improve retrieval of textual documents and  audio-visual recordings. Our experiments performed on the test collection used in the Search subtask of the Search and Hyperlinking Task in MediaEval Benchmarking 2012 confirm those findings and show that parameters (segment length and shift) tuning for a specific test collection can further improve the results. Our best results on this collection were achieved by using 45-second long segments with 15-second shifts.

Semantic-based segmentation can be divided into three types: similarity-based (producing segments with high intra-similarity and low inter-similarity), lexical-chain-based (producing segments with frequent lexically connected words), and feature-based (combining various features which signalize a segment break in a machine-learning setting). In this work, we mainly focus on feature-based segmentation which allows exploiting various features from all modalities of the data (including segment length) in a single trainable model and produces segments which can eventually overlap.

Our preliminary results show that even simple semantic-based segmentation outperforms regular segmentation. Our model is a decision tree incorporating the following features: shot segments, output of TextTiling algorithm, cue words (well, thanks, so, I, now), sentence breaks, and the length of the silence after the previous word. In terms of the MASP measure, the relative improvement over regular segmentation is more than 19%.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku popisujeme prístup, ktorý sme použili v
Search Subtasku v úlohe Search and Hyperlinking vrácmi Benchmarku MediaEval 2013. Popisujeme rôzne prístupy k segmentácii nahrávok na menšie úseky, ktoré sú potom využité pri štandardnom vyhľadávaní. Na segmentáciu používame rozdelenie na krátke úseky rovnakej dĺžky a tiež metódy strojového učenia, založené na našom riešení použitom v úlohe Similar Segments in Social Speech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe our approach to the Search Subtask of the Search and Hyperlinking Task at MediaEval 2013. We experiment with various methods for segmentation of the recordings into shorter segments which are then used in a standard
retrieval setup to search for relevant passages. We use regular segmentation into equilong segments and experiment with machine-learning based segmentation expanding our approach to Similar Segments in Social Spech Task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku popisujeme naše experimenty vrámci úlohy Similar Segments in Social Speech v Benchmarku MediaEval 2013. Zameriavame sa najmä na segmentáciu nahrávok na kratšie úseky, na ktoré aplikujeme štandardné metódy vyhľadávania. Ďalej popisujeme naše pokusy so segmentáciou založenou na technikách strojového učenia, pričom využívame textové (n-gramy slov, n-gramy tagov, veľké písmená, lexikálnu konzistentnosť, ...) a prozodické vlastnosti (dĺžka ticha).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe our experiments for the Similar Segments in Social Speech Task at MediaEval 2013 Benchmark. We mainly focus on segmentation of the recordings into shorter passages on which we apply standard retrieval techniques. We experiment with machine-learning-based segmentation employing textual (word n-grams, tag n-grams, letter cases, lexical cohesion, etc.) and prosodic features (silence) and compare the results with those obtained by regular segmentation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V práci popisujeme dva anglicko-české prekladové systémy, ktoré boli odoslané do WMT 2013 shared tasku: TectoMT a PhraseFix. TectoMT je systém založený na syntaktickom preklade, PhraseFix používa štatistickú posteditáciu (SPE), ktorá je aplikovaná na výstup systému TectoMT. V krátkom prehľade porovnáme SPE a ďalšie techniky kombinácie prekladových systémov - použijeme dáta, ktoré vznikli prekladom pomocou systému TectoMT, aby sme natrénovali štatistický prekladový systém (SMT). V práci ďalej potvrdíme hypotézu, že PhraseFix (SPE) zlepšuje výsledky TectoMT, zároveň však ukážeme, ze pridanie trénovacích dát do SMT je napriek tomu stále efektívnejšie.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present two English-to-Czech systems that took part in the WMT 2013 shared task: TectoMT and PhraseFix. The former is a deep-syntactic transfer-based system, the latter is a more-or-less standard statistical post-editing (SPE) applied on top of TectoMT. In a brief survey, we put SPE in context with other system combination techniques and evaluate SPE vs. another simple system combination technique: using synthetic parallel data from TectoMT to train a statistical MT system (SMT). We confirm that PhraseFix (SPE) improves the output of TectoMT, and we use this to analyze errors in TectoMT. However, we also show that extending data for SMT is more effective.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozhovor pro anglické vysílání Radia Praha o strojovém překladu a nové knížce, která jej přibližuje českému čtenáři.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An interview on machine translation and a new book that introduces the field to the Czech reader.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Archiv Hlasy uprchlíků (Refugee Voices) je sbírkou svědectví o holocaustu obsahující 150 filmových interview s židovskými přeživšími a uprchlíky před nacismem, kteří začali nový život ve Velké Británii. Archiv byl vytvořen Asociací židovských uprchlíků (Association of Jewish Refugees – AJR) v letech 2003–2007. Projekt vedli Dr. Anthony Grenville a Dr. Bea Lewkowicz. Kompletní archiv a přidružené databáze jsou od února 2013 dostupné v CVH Malach. Sbírka sestává z více než 450 hodin filmového materiálu a je neocenitelným zdrojem pro akademiky, výzkumníky, vzdělávací pracovníky a další zájemce o studium migrace nebo holocaustu. Archiv byl od samého počátku vytvářen s ohledem na potřeby a požadavky akademiků a dalších profesionálů. Všechny rozhovory jsou katalogizovány a v plném rozsahu přepsány. Aby bylo umožněno snadné odkazování na konkrétní pasáže interview, jsou přepisy i nahrávky opatřeny časovým kódem, díky čemuž je možné s minimálním úsilím najít specifické úseky rozhovorů. Součástí archivu je rozsáhlá databáze obsahující seznam rozhovorů s podrobnostmi o narátorech a jejich životních příbězích. Interview byla katalogizována prostřednictvím 44 samostatných kategorií.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Refugee Voices is a groundbreaking Holocaust testimony collection of 150 filmed interviews with Jewish survivors and refugees from Nazism who rebuilt their lives in Great Britain. It was commissioned by the Association of Jewish Refugees (AJR), with Dr Anthony Grenville and Dr Bea Lewkowicz directing the project. The archive is available at the Malach Centre since February 2013. The collection consists of more than 450 hours of film and forms an invaluable resource for academics, researchers, educationalists and others with a professional interest in the field of refugee, migration and Holocaust studies. The collection has been designed precisely with the requirements of scholars and other professionals in mind. All interviews have been fully transcribed and catalogued enabling a researcher to be able to see an interview and then to read a transcript of the words spoken in it, or vice versa. For ease of reference, both the films and the transcripts are time-coded, making it possible to locate specific passages with a minimum of effort. Accompanying the collection is a comprehensive database, containing an index of the interviews and details of the interviewees and their life stories. The interviews have been catalogued with 44 separate categories.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Archiv vizuální historie USC Shoah Foundation (VHA) obsahuje více než 52 000 interview s přeživšími a svědky holocaustu. Vzhledem k povaze dat je archiv využíván nejen v historickém bádání, ale také dalších oborech jako je ekonomie, právo, medicína, literatura, antropologie či sociologie. V příspěvku reflektuji svou uživatelskou zkušenost s archivním rozhraním v rámci probíhajícího sociologického výzkumu, jenž tvoří součást mé disertační práce. Ukazuji, že stávající uživatelské rozhraní VHA lze sice pro podobné účely efektivně využít, ale zároveň před badatele klade různé problémy a překážky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>USC Shoah Foundation's Visual History Archive (VHA) contains almost 52,000 interviews with Holocaust survivors and witnesses. Although the interviews are focused mostly on the World War II era, they were conducted as the narrators' complete life stories. VHA can therefore be effectively used in different scientific fields apart from historiography, e.g. economics, law, medicine, literature, anthropology or sociology. I will reflect on my user experience with the VHA during the ongoing sociological research, which is a crucial part of my dissertation thesis. On one hand, it is clear that the VHA user interface already provides very effective facilities for the identification and analysis of the relevant interviews and segments. On the other hand, however, there are still some serious technical limitations for this kind of research practice.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zpráva ze slavnostního setkání ke 3. výročí založení CVH Malach, které se konalo 28. ledna 2013.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Report from the third annual meeting of the Malach Centre for Visual History.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valenční lexikony typicky zachycují pouze bezpříznakové užití slovesa (aktivum), zatímco přirozený jazyk umožňuje řadu pravidelných změn v počtu, typu a/nebo povrchové formě valenčních doplnění (např. při pasivizaci).
Vzhledem k jejich pravidelnosti je výhodné tyto změny zachytit v pravidlové komponentě slovníku.
Jednotlivé změny typicky nacházíme u některých, ale ne u všech slovníkových jednotek (rámců), a proto je nutné u každého rámce uvést, která pravidla na něj lze aplikovat.
Ve článku popisujeme třífázový přístup k určení, které diateze lze aplikovat na daný slovesný význam.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The valency behavior (argument structure) of lexical items  is so varied that it cannot be described by general rules and must be captured in lexicons separately for each lexical item.
For verbs, lexicons typically describe only unmarked usage (the active form), while natural languages allow for certain regular changes in the number, type and/or realization of complementations (e.g. passivization).
Thanks to their regularity, such changes may be described in a separate rule component of the lexicon;
however, they are typically seen in many but not all verbs and their applicability to a given lexical unit (verb meaning) is not predictable from its valency alone.
In this paper, we describe our initial experiments with using a large morphologically annotated corpus of Czech for determining which diatheses are applicable to a given lexical unit.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Korpus Merlin je žákovský korpus s psanými texty v čestině, němčině a italštině. Jeho cílem je ilustrovat úrovně CEFR autentickými daty. Článek popisuje metodologii tvorby korpusu (sběr dat, transkripci, anotaci, ověření kvality, atd.), možnosti jeho použití při validaci CEFR.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Since its publication in 2001, the Common European Framework of Reference for Languages (CEFR) has gained a leading role as an instrument of reference for language teaching and certification. Nonetheless, there is a growing concern about CEFR levels being insufficiently illustrated in terms of authentic learner data. Such concern grows even stronger when considering languages other than English (cf., e.g., Hulstijn 2007, North 2000). In this paper, we present the MERLIN project that addresses this need by illustrating and validating the CEFR levels for Czech, German, and Italian. To achieve its goal, we are developing a didactically motivated online platform to enable CEFR users to explore authentic written learner productions that have been related in a methodologically sophisticated and rigorous way to the CEFR levels. By making a significant number of learner productions freely accessible and easily searchable in a form that is richly annotated with linguistic characteristics and learner error types, the platform will assist teachers, learners, test developers, textbook authors, teacher trainers, and educational policy makers in developing a more comprehensive conceptualization of CEFR levels based on authentic learner data. 

In the first, methodology-oriented part of this paper, we explain how the learner textual data were collected, re-rated, transcribed, double-checked and prepared for additional manual and automatic processing. We then illustrate the indicators we built to analyze L2 productions. Indicators were derived through (a) linguistic analyses of the performance samples, (b) the operationalization of the CEFR scale descriptors, (c) the study of relevant literature on SLA and language testing, (d) textbook analyses and (e) a questionnaire study. This study allowed us to devise a harmonized annotation schema taking into account both common and language-specific features (e.g., gender/article in German, reflexive possessive pronouns in Czech, pronoun particles in Italian).

In the second, application-oriented part, we explain how, by offering a large corpus of freely accessible empirical material, the project helps provide a fine-grained characterization of the CEFR levels and how it serves language teaching and learning. MERLIN thereby aims at responding to the suggestions of the Council of Europe itself, which solicits the development of supplementary tools for illustrating the CEFR levels (http://purl.org/net/CEFR-Goullier.doc). Furthermore, we explain how the platform enables the targeted users to retrieve authentic information about the relationship of the CEFR levels to a wide spectrum of well-defined, user-need-oriented L2 challenges. MERLIN users, such as teacher or learners, can thus compare their students’ or their own performances and get a clearer picture of their strengths and weaknesses. 

In the third, research-oriented part, we situate MERLIN with regards to two current topics in Second Language Acquisition: validation of CEFR scales and natural language processing for learner language</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Projekt Khresmoi vyvýjí multilinguální a multimodální vyhledávací a přístupový systém pro medicínské a zdravotní informace a dokumenty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Khresmoi project is developing a multilingual multimodal search and access system for medical and health information and documents. This scientific demonstration presents the current state of the Khresmoi integrated system, which includes components for text and image annotation, semantic search, search by image similarity and machine translation. The flexibility in adapting the system to varying requirements for different types of medical information search is demonstrated through two instantiations of the system, one aimed at medical professionals in general and the second aimed at radiologists. The key innovations of the Khresmoi system are the integration of multiple software components in a flexible scalable medical search system, the use of annotation cycles including manual correction to improve semantic search, and the possibility to do large scale visual similarity search on 2D and 3D (CT, MR) medical images.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V dnešní době stoupá zájem a potřeba inovativních řešení pro medicínské vyhledávání. V tomto článku představujeme systém Khresmoi pro medicínské vyhledávání a získávání informací, podporovaný Evropskou unií, na kterém se podílí dvanáct partnerů a který je momentálně ve třetím roce vývoje ze čtyř. Systém Khresmoi používá architekturu založenou na komponentách a provozovanou v cloudu, tak aby umožňovala vývoj několika inovativních aplikací, uspokojujících medicínské informační potřeby cílových uživatelů. Vyhledávací systémy Khresmoi založené na této architektuře byly navrženy tak, aby podporovaly vícejazyčné a víceúčelové informační potřeby tří cílových skupin: všeobecné veřejnosti, praktických lékařů a radiologů. V tomto článku se zaměřujeme na představení systémů pro praktické lékaře a radiology s využitím sémantického vyhledávání, vícejazyčného textového vyhledávání a vyhledávání založeného na obrázcích (zahrnujícího 2D a 3D radiologické obrázky).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>There is increasing interest in and need for innovative solutions to medical search. In this paper we present the EU-funded Khresmoi medical search and access system, currently in year 3 of 4 of development across 12 partners. The Khresmoi system uses a component-based architecture housed in the cloud to allow for the development of several innovative applications to support target users' medical information needs. The Khresmoi search systems based on this architecture have been designed to support the multilingual and multimodal information needs of three target groups: the general public, general practitioners and consultant radiologists. In this paper we focus on the presentation of the systems to support the latter two groups using semantic, multilingual text and image-based (including 2D and 3D radiology images) search.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku se analyzují problémy anotace generických jmenných frází ve velkém jazykovém  korpusu. Podává se přehled existujících teorií referencí generických NP, tyto teorie se pak srovnávají s přístupy k generickým NP v aplikačních projektech a se situací v anotací koreference v různých korpusech. Po představení anotace generických NP v Pražském závoslostním korpusu se rozebírají typické problematické situace a obecné potíže a nabízí se některá řešení vylepšující kvalitu anotace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper discusses the problem of annotation
of  coreference  relations  with  generic 
expressions in a large scale corpus. We present 
and  analyze  some  existing  theories  of 
genericity,  compare  them  to  the  approaches  to generics  that  are  used  in  the  state-of-the-art coreference  annotation  guidelines  and  discuss how  coreference  of  generic  expressions  is processed  in  the  manual  annotation  of  the Prague  Dependency  Treebank.  After  analyzing some  typical  problematic  issues  we  propose some  partial  solutions  that  can  be  used  to enhance  the  quality  and  consistency  of  the annotation</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme výsledky anotace koreference a asociační anafory v Pražském závislostním korpusu 2.0. Anotace byla provedena na závislostních stromech tektogramatické roviny. Popisujeme měření mezianotátorksé shody a klasifikujeme a analyzujeme nejčastější typy anotátorské neshody. Na dvou vybraných delších textech jsme požádali anotátory o značení jistoty jejich rozhodnutí; tuto jistotu porovnáváme s výsledky měření mezianotátorské shody.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the results of the coreference and bridging annotation in the Prague Dependency Treebank 2.0. The annotation is carried out on dependency trees (on the tectogrammatical layer). We describe the inter-annotator agreement measurement, classify and analyse the most common types of annotators’ disagreement. On two selected long texts,  we asked the annotators to mark the degree of certainty they have in some most problematic points; we compare the results to the inter-annotator agreement measurement.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zkoumáme výhody závislostních stromů a tektogramatických struktur použitých v Pražském závislostním korpusu pro anotaci jazykových jevů překračujících hranici věty, jmenovitě koreference a asociační anafory. Uvádíme výhody závislostních stromů jako podrobné zpracování elips, syntaktické řešení koordinace a apozice umožňující značkování koreference v případech, které jsou přímo na textu složitější.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We explore the benefits of dependency trees and tectogrammatical structure used in the Prague Dependency Treebank for annotating language phenomena that cross the sentence boundary, namely coreference and bridging relations. We present the benefits of dependency trees such as the detailed processing of ellipses, syntactic decisions for coordination and apposition structures that make possible coding coreference relations in cases that are not so easy when annotating on the raw texts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme ukončený projekt anotace koreference a asociační anafory na Pražském závislostním korpusu. Korpus obsahuje ruční anotace textové  koreference včetně koreference jmenných frází a tzv. bridging anafory, to vše na 50k větách korpusu. Také podáváme stručný popis automatických experimentů na těchto datech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents an overview of a finished project focused on annotation of grammatical, pronominal and extended nominal coreference and 
bridging relations in the Prague Dependency Treebank (PDT 2.0). We give an overview of existing similar projects and their interests and compare them with our project. We describe the annotation scheme and the typology of coreferential and bridging relations and give the statistics of these types in the annotated corpus. Further we give the final results of the interannotator agreement with some explanations. We also briefly present the anaphora resolution experiments trained on the coreferentially annotated corpus and the future plans.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Standardní metrika pro hodnocení strojového překladu BLEU nebere v úvahu synonymii mezi referenční větou a překladem. Tuto nevýhodu zkoušíme zmírnit cíleným parafrázováním referenční věty. Nově vzniklá věta by si tak zachovává význam a správnost původní referenční věty a současně je bližší strojovému překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we discuss the possibility of making the automatic evaluation of machine translation (MT) more accurate by targeted paraphrasing. The most common automatic metric for MT evaluation BLEU disregards synonymous phrases and word order variants. One way how to alleviate this drawback is to transform the reference translation into a one that is closer to the MT output and keeps its original meaning. We start with a basic algorithm for lexical substitution and discuss extending the process with word order changes and grammar checking.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Toto je vydání PyKaldi pro rok 2013 - verze 1.0.

Tento kód obsahuje modifikace gmm-lat-gen-faster dekodéru, které umožňují jeho použití v úloze rozpoznávání mluvené řeči v reálném čase. Změny jsou založeny na povrchové modifikaci původního kódu pomocí dědění z původního dekodéru a přidání online zpracování vstupu včetně LDA+MLLT transformací.

Navíc, tento kód obsahuje Python knihovnu takže samotný dekodér může být použit z libovolného programu napsaného v Python programovacím jazyku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This is the PyKaldi - release 2013.

This code include modification of the gmm-lat-gen-faster decoder so that it can be used for online decoding. The modification based on a light weight modification to the original code by subclassing the main decoder class and adding online processing of input, including LDA+MLLT transforamtions.

In addtition, this code includes a Python wrapper so that the decoder can be used from any python application.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek se zabývá neřízenou metodu pro morfologické značkování, založené na projekci morfologické informace v paralelních datech a konstrukci prediktivního modelu pro optimální výběr zdrojového jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Bilingual corpora offer a promising bridge between resource-rich and resource-poor languages, enabling the development of natural language processing systems for the latter. English is often selected as the resource-rich language, but another choice might give better performance. In this paper, we consider the task of unsupervised cross-lingual POS tagging, and construct a model that predicts the best source language for a given target language. In experiments on 9 languages, this model improves on using a single fixed source language. We then show that further improvements can be made by combining information from multiple source languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje neřízenou metodu pro morfologické značkování založenou na projekci značek v bilinguálním paralelním korpusu. Ve srovnání s existujícími metodami je tato metoda jednodušší a rychlejší, vybírá spolehlivější trénovací příklady využívá tzv. selftraining.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an unsupervised approach to part-of-speech tagging based on projections of tags in a word-aligned bilingual parallel corpus. In contrast to the existing state-of-the-art approach of Das and Petrov, we have developed a substantially simpler method by automatically identifying “good” training sentences from the parallel corpus and applying self-training. In experimental results on eight languages, our method achieves state-of-the-art results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popularizační přednáška o počítačové lingvistice a strojovém překladu na tzv. Science Café v Kladně.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A popularizing talk on computational linguistics and machine translation given at Science Café in Kladno.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Seznámení zájemců o studium informatiky s naším překladovým systémem, který v soutěži při překladu do češtiny dopadl lépe než Google Translate.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A brief description of our machine translation system that performed better than Google Translate in an English-to-Czech translation task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Stručné seznámení účastníků Semináře TMC pořádaného CESNETem s výzkumem na naší katedře s důrazem na potenciální styčné body pro budoucí spolupráce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A brief introduction to UFAL research presented at the seminar TMC organized by CESNET, with the focus on possible shared interests and future collaboration.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dodávka korpusů CzEng a PCEDT a překladových modelů pro systém Moses pro překlad mezi angličtinou a češtinou podle zadání společnosti Lingea. Součástí dodávky bylo zprovoznění překladového systémů na počítačích objednatele.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A customized delivery of the corpora CzEng and PCEDT and English<->Czech translation models for the MT system Moses on request of the company Lingea. As part of the delivery, we also provided a running setup on Lingea's machines.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje naše příspěvky na WMT, systémy CU-BOJAR a CU-DEPFIX, později nazvaný "chiméra", protože kombinuje tři rozdílné přístupy: TectoMT, systém s transferem na úrovni hloubkové syntaxe, faktorový frázový překlad pomocí nástroje Moses, a konečně automatický pravidlový korektor častých gramatických a významových chyb. Nepoužíváme žádnou standardní metodu systémové kombinace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes our WMT submissions CU-BOJAR and CU-DEPFIX, the latter
dubbed "chimera" because it combines on three diverse approaches: TectoMT, a system
with transfer at the deep syntactic level of representation, factored
phrase-based translation using Moses, and finally automatic rule-based correction
of frequent grammatical and meaning errors.
We do not use any off-the-shelf system-combination method.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme výsledky společných úloh z workshopu WMT: strojový překlad a odhad kvality překladu. Úloha metrik strojového překladu je prezentována v samostatném článku.
Letos se překladové soutěže účastnilo 143 systémů z celkem 23 institucí. Dalších 6 anonymních systémů jsme do hodnocení přidali sami. Ruční i automatické hodnocení výstupů bylo provedeno v dosud největším rozsahu. Úloha odhadu kvality překladu měla čtyři části a celkem se jí účastnilo 55 příspěvků od 14 týmů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the results of the WMT13 shared tasks, which included a translation task, a task for run-time estimation of machine translation quality, and an unofficial metrics task. This year, 143 machine translation systems were submitted to the ten translation tasks from 23 institutions. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually, in our largest manual evaluation to date. The quality estimation task had four subtasks, with a total of 14 teams, submitting 55 entries.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jedna z hlavních obtíží při automatickém vyhodnocování strojového překladu je závislost na několika (typicky na jediném) lidských referenčních překladech, se kterými se výstup překladového systému srovnává. Navrhujeme metodu, jak zachytit milióny možných překladů, a implementujeme nástroj, s jehož pomocí mohou překladatelé takové překlady popsat v kompaktní reprezentaci. Vyhodnocujeme takto vzniklou novou referenční sadu s použitím editační vzdálenosti a korelace s lidským hodnocením kvality překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>One of the key obstacles in automatic evaluation of machine translation systems is the reliance on a few (typically just one) human-made reference translations to which the system output is compared. We propose a method of capturing millions of possible translations and implement a tool for translators to specify them using a compact representation. We evaluate this new type of reference set by edit distance and correlation to human judgements of translation quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme emana, nástroj pro správu velkého množství výpočetních experimentů. V průběhu let našeho výzkumu strojového překladu (MT) jsme sesbírali několik nápadů pro efektivní experimentování. Věříme, že tyto nápady jsou obecně použitelné v (počítačovém) výzkumu v libovolném oboru. Zahrnuli jsme je do emana, aby byly dostupné v prostředí příkazové řádky Unixu. Cílem článku je zvýraznit hlavní myšlenky v pozadí těchto nápadů. Doufáme, že text bude sloužit jako sbírka tipů pro správu experimentů pro každého, nezávisle na konkrétním oboru nebo počítačové platformě. Konkrétní příklady, které ukazujeme v současné syntaxi nástroje eman, jsou méně důležité, ale umožňují užití konkrétních termínů. Článek tedy zároveň vyplňuje mezeru v dokumentaci emana tím, že podává abstraktnější pohled na tento nástroj.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present eman, a tool for managing large numbers of computational experiments. Over
the years of our research in machine translation (MT), we have collected a couple of ideas for
efficient experimenting. We believe these ideas are generally applicable in (computational)
research of any field. We incorporated them into eman in order to make them available in a
command-line Unix environment.
The aim of this article is to highlight the core of the many ideas. We hope the text can serve
as a collection of experiment management tips and tricks for anyone, regardless their field of
study or computer platform they use. The specific examples we provide in eman’s current syntax
are less important but they allow us to use concrete terms. The article thus also fills the gap in eman documentation by providing some high-level overview.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Spojená morfologická a syntaktická analýza pro jazyky s bohatou flexí je založena na datech pro několik slovanských jazyků a finštinu, a porovnána s angličtinou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Joint morphological and syntactic analysis has been proposed as a way of improving parsing accuracy for richly inflected languages. Starting from a transition-based model for joint part-of-speech tagging and dependency parsing, we explore different ways of integrating morphological  features into the model. We also investigate the use of rule-based morphological analyzers to provide hard or soft lexical constraints and the use of word clusters to tackle the sparsity of lexical features. Evaluation on five morphologically rich languages (Czech, Finnish, German, Hungarian, and Russian) shows consistent improvements in both morphological and syntactic accuracy for joint prediction over a pipeline model, with further improvements thanks to lexical constraints and word clusters. The final results
improve the state of the art in dependency parsing for all languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládají se protipříklady k běžnému tvrzení, že všeobecný konatel (PRO-arbitrary) nemůže být ve větě vyjádřen. Jde o okrajové případy recipročních konatelů vyjadřovaných v češtině předložkovým pádem mezi + Instrumentál. Zvažují se i jiné možnosti interpretace této konstrukce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Some counterexamples to the view that the general Actor (PRO-arbitrary) has no slot in the surface structure of the sentence are submitted. The reciprocal actors expressed in Czech by the prepositional case mezi + Instrumental case (among/between) are anlyzed from the point of view of their possible actor function.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>K prezentaci hloubkových syntaktických struktur vyžadujících doplnění povrchové reprezentace do vyhovujícícho formátu byly vybrány: srovnávací struktury, dvojznačné struktury uvozené výrazem kromě a tzv. konstrukce substituční (uvozeneé výrazem místo).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The necessity to fill the surface structures of selected ellipsis by reconstruction of predicates was demonstrated by the comparison konstructions, by the ambiguous constructions introduced by the expression kromě and by the so-called substituting constructions introduces by the expression místo.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve stati se ilustruje kooperace gramatického komponentu se slovníkovými údaji týkajícími se valence slovesa, jeho gramatických diatezí, požadavků na koreferenci. Ve slovníku je třeba zachytit omezení na tvorbu analyzovaných konstrukcí. Tyto požadavky se exemplifikují několika pravidly pro uplatnění slovníkových údajů při vytváření rezultativu a infinitivní konstrukce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The cooperation of the grammatical component and lexical information included in the lexical entry is exemplified by the valency of verbs, grammatical diathesis and constructions with coreference.  The restrictions must be reflected in the lexical entries. Several rules for the combining of lexical and syntactical information are demonstrated.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V rámci tematického bloku Vztah gramatiky a slovníku byla přednesena přednáška o lexikálních omezeních některých gramatických (morfologických) kategorií (pasiva, rezultativu, recipientu). Byl zvažován způsob jejich zachycení v explicitním popisu češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Within the  workshop The relations between grammar and lexicon the lexical restrictions on the application of some grammatical (morphological) categories to the lexicon were demonstrated by the examples of Czech passive, rezultativ and recipient constructions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nekrolog je věnován přední české lingvistce, která pracovala jako zástupkyně ředitele Ústavu pro jazyk český, měla bohatou publikační činnost, působila zejména v oblasti výzkumu mluveného jazyka a textové lingvistiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article was devoted to the memory of Světla čmejrková, the famous Czech linguist. Her contribution to the domain of science as well as in the domain of management of science are evaluated.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá vybranými morfologickými a syntaktickými rysy českých sloves, které jsou v teoretickém rámci Funkčního generativního popisu je uváděny ve slovníkovém hesle daného slovesa. Probírány jsou slovesné diateze (kromě pasiva také dva typy rezultativní diateze a recipinentní diatezi), reciprokalizace, kontrola a modalita závislých klauzí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper deals with selected morphological and syntactic features of Czech verbs. Working within the framework of Functional Generative Description (FGD), we demonstrate which features of lexical entries are required by the syntactic component of the description. In addition to the passive voice, traditionally described as diathesis, we briefly describe other kinds of proposed diatheses (resultative-1, resultative-2, and recipient). The constraints for their application will be present as features in the corresponding lexical entry; they will be a part of verbal paradigm in formal morphology. Regular operations within hierarchy of valency participants and their surface-syntactic positions are introduced into the grammatical component. Reciprocalization is characterized as a kind of shifting of valency complementations into the surface-syntactic position We also specify the requirements of the verbs governing the infinitive and content clauses and point out to the interplay between the governing verb and modality of the content clause.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek zaujímá kritické stanovisko k názoru, že vícerovinný popisu přirozeného jazyka již vyčerpal své možnosti a mohl by sehrát limitující úlohu, a ukazuje, že vztah mezi rovinami popisu lze vymezit na základě přísných kritérií a že při vymezení rovin je třeba brát v úvahu rozdíl mezi jazykovým významem a mimojazykovým obsahem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The contribution presents a criticism of the opinion that a multilevel language model has already exhausted the possibilites it offered and may play a limited role. The author argues that the specification of the relation between levels can be based on strict criteria taking into account the difference between linguistic meaning and extralinguistic content.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zamyšlení nad dílem významného českého lingvisty a slavisty prof. L. Matějky, který byl významným propagátorem české lingvistiky i kultury vůbec v mezinárodnim měřítku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A summary of the life achievements of a well-known Czech linguist and slavisist prof. L. Matejka, who was an important propagator of Czech linguistics and Czech culture in general in international community.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>O velkém českém lingvistovi a jeho přispění lingvistice, zejména o jeho aktuálním členění.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>About a great Czech linguist Vilém Mathesius and his scientific contribution, in particular, about  his functional sentence perspective.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Většina systémů počítačového zpracování přirozeného jazyka pracuje s kontextem. Tj. se znalostí slov a vztahů mezi nimim.
My pracujeme s konceptem aktivovanosti, formulovaným v rámci funkčně-generativního popisu jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We work with the notion of the degrees of salience (activation) of the items in the stock of shared knowledge together with the representation of the dynamic development of the discourse by means of changes of these degrees.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kniha Bitext Alignment od Jörga Tiedemanna je kompletním a detailním přehlededm početných a rozličných technik pro zarovnávání paralelních textů na různých úrovních: dokumentech, větách, slovech i syntaktických strukurách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The book Bitext Alignment by Jörg Tiedemann is a complete and detailed survey of numerous and diverse techniques for alignment of bitexts on various levels of granularity. It covers the full range of alignment tasks: document alignment, sentence alignment, word alignment, and tree structure alignment. The main text is divided into seven chapters (covering 124 pages) accompanied by three appendices (27 pages).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Při práci na propojení českého a anglického valenčního slovníku bylo nutno navhnout, jak zacházet s víceslovními jednotkami, jejichž základem je sloveso; bylo stanoveno jejich formální zachycení. Ve studii založené na paralelních korpusech PCEDT a PDT popisujeme, jak si tyto jednotky odpovídají v textech, jak byly anotovány v obou jazycích a porovnáváme jejich charakteristiky jak v originále, tak v překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>While working on valency lexicons for Czech
and English, it was necessary to define treatment
of multiword entities (MWEs) with the
verb as the central lexical unit. We
present a corpus-based study, concentrating on
multilayer specification of verbal MWEs, their
properties in Czech and English, and a comparison
between the two languages using the
parallel Czech-English Dependency Treebank
(PCEDT). This comparison revealed interesting
differences in the use of verbal MWEs in
translation (discovering that such MWEs are
actually rarely translated as MWEs, at least
between Czech and English) as well as some
inconsistencies in their annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato studie se zabývá valenčními vlastnostmi českých deverbativních substantiv odvozených ze sloves s předmětovým genitivem (GenAdverb). Věnuje se otázkám vztahu adverbálních vyjádření valenčních doplnění a jejich adnominálních protějšků; zvláštní pozornost je věnována možnosti vyjádření adnominálního aktantu odpovídajícího adverbálnímu předmětovému genitivu pomocí posesivního adjektiva nebo zájmena (Adjpos/Pronpos). Užívání formy Adjpos/Pronpos (← GenAdverb) bylo ověřováno na datech Českého národního korpusu: tato forma byla nalezena u jedenácti produktivně tvořených substantiv (např. zanechání studia // jeho zanechání) a u dvou neproduktivně tvořených substantiv (např. dotyk volantu // jeho dotyk). Předpokládáme, že alternace GenAdnom // Adjpos / Pronpos (← GenAdverb) je v češtině užívána analogicky k běžnější alternaci GenAdnom // Adjpos / Pronpos (← Ak).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the present paper, valency properties of Czech nouns derived from verbs with an object expressed by prepositionless genitive (GenAdverb) are studied. Special attention is paid to the possibility of the participant with an object function to be expressed by a possessive adjective or a possessive pronoun (Adjposs / Pronposs), when it modifies the derived noun. Usage of Adjposs / Pronposs (← GenAdverb) was verified in data of the Czech National Corpus; the form was found with eleven nouns derived from verbs by productive means (e.g. zanechání studia ‘quitting study-Gen.Sg’, i.e. ‘quitting one’s studies’ // jeho zanechání ‘it-Poss quitting’, i.e. ‘its quitting’) and with two nouns derived from verbs by non-productive means (e.g. dotyk volantu ‘touch wheel-Gen.Sg’, i.e. touch of wheel // jeho dotyk ‘it-Poss touch’, i.e. ‘its touch’). We suppose that the alternation GenAdnom // Adjposs / Pronposs (← GenAdverb) is used in Czech as an analogy to the more frequent alternation GenAdnom // Adjposs / Pronposs (← Ak).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato studie, založená na korpusovém mateiálu, popisuje česká deverbativní substantiva, která umožňují rozvití pomocí Aktora v bezpředložkovém instrumentálu, tj. A1(Ins). Jako východisko pro možné srovnání uvádíme frekvenční údaje týkající se vybraných substantiv odvozených z tranzitivních sloves. Poté se zaměřujeme na substantiva odvozená z intranzitivních sloves a ukazujeme, že rozvití pomocí A1(Ins) je možné nejen u substantiv odvozených ze sloves, která mohou být převedena do pasiva, ale také u substantiv odvozených ze sloves, která pasivizaci neumožňují. Vzhledem k tomu, že možnost vyjádření A1(Ins) u substantiv bývá s možností pasivizace základových sloves spojována, je užívání A1(Ins) u substantiv odvozených ze sloves, ktará pasivizaci neumožňují, zajímavým jevem. Týká se to především substantiv odvozených z reflexivních sloves, a to jak intranzitivních, tak tranzitivních. Studie také přispívá k dosavadnímu popisu tím, že zkoumá nejen česká produktivně tvořená deverbativní substantiva (např. domlouvání), ale rovněž substantiva neproduktivně tvořená (např. domluva), která jsou často opomíjena. Poukazuje rovněž na skutečnost, že korpusový materiál podává důkazy o užívání teoreticky negramatických konstrukcí, ve kterých je vyjádřen pouze A1(Ins), zatímco druhé doplnění A2 je povrchově vypuštěno, např. vyhrožování zaměstnavatelem, domluva strážníky; domníváme se, že nalezené korpusové doklady vedou k přehodnocení tvrzení o negramatičnosti těchto konstrukcí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper aims to provide corpus-based description of Czech deverbal nouns that allow for modification by Agent expressed by prepositionless instrumental, A1(Ins). As the starting point we give frequency data of selected nouns derived from transitive verbs. Then we focus on nouns derived from intransitive verbs and show that modification by A1(Ins) is possible not only with nouns derived from verbs that can be passivized, but also with nouns the source verbs of which cannot be changed to passive. The latter issue represents the most contributive finding of the paper; it concerns especially nouns derived from reflexive verbs, both transitive and intransitive. We also improve the up-to-now description by taking into consideration not only Czech nouns derived from verbs by productive means (e.g. domlouvání ‘talking’) but also the non-productively derived ones (e.g. domluva ‘caution’), mostly left aside. Finally, the corpus material also gives an evidence for usage of theoretically ungrammatical constructions in which the second complementation (A2) is omitted on the surface and only A1(Ins) is expressed, e.g. vyhrožování zaměstnavatelem ‘threatening by the employer’, domluva strážníky ‘caution by police officers’; the corpus-based examples lead to the revision of the statement about ungrammaticality of such constructions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popis valenčních vlastností substantiv odvozených od sloves s předmětovým genitivem nám umožňuje upřesnit teoretické poznatky týkající se čtyř následujících skupin jevů z oblasti teorie valence substantiv:
(i) korespondence adverbálního a adnominálního genitivu (dotknout se puku - dotknutí se puku / dotyk puku);
(ii) nominalizované struktury se dvěma aktanty ve formě bezpředložkového genitivu (kromě tradičně uváděných konstrukcí typu zbavení ženy.ADDR starostí.PAT popisujeme další typy, tříděné podle typu aktantů v základové větné struktuře, např. dotek včelky.ACT pestíku.PAT květu, a podle příslušnosti k víceslovným predikátům, např. zanechání činnosti.PAT řady.ACT klíčových hráčů);
(iii) posesivum odpovídající adverbálnímu objektu (např. zanechání studia.PAT // jeho.PAT zanechání, dotyk volantu.PAT // jeho.PAT dotyk);
(iv) konatel vyjádřený bezpředložkovým instrumentálem u substantiv odvozených z intranzitivních sloves (např. ujímání se zvířátek.PAT hodnými lidmi.ACT).
Jazykový materiál byl získán z dat morfologicky značkovaných subkorpusů ČNK.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Czech nouns derived from verbs with an objective genitive considerably contribute to the theoretical description of valency of Czech deverbal nouns. We focus on four topics, namely (i) Correspondence GENAdverb – GENAdnom?;
(ii) Double post-nominal genitives; 
(iii) Possessives corresponding to an adverbal objective genitive;
(iv) Agents expressed by a prepositionless intrumental modifying nouns derived from intransitive verbs.
We used morphologically annotated subcorpora of the Czech National Corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popis valenčních vlastností substantiv odvozených od sloves s předmětovým genitivem nám umožňuje upřesnit teoretické poznatky týkající se tří následujících skupin jevů z oblasti teorie valence substantiv:
(i) posesivum odpovídající adverbálnímu objektu (např. zanechání studia.PAT // jeho.PAT zanechání, dotyk volantu.PAT // jeho.PAT dotyk);
(ii) nominalizované struktury se dvěma aktanty ve formě bezpředložkového genitivu (kromě tradičně uváděných konstrukcí typu zbavení ženy.ADDR starostí.PAT popisujeme další typy, tříděné podle typu aktantů v základové větné struktuře, např. dotek včelky.ACT pestíku.PAT květu, a podle příslušnosti k víceslovným predikátům, např. zanechání činnosti.PAT řady.ACT klíčových hráčů);
(iii) konatel vyjádřený bezpředložkovým instrumentálem u substantiv odvozených z intranzitivních sloves (např. ujímání se zvířátek.PAT hodnými lidmi.ACT).
Jazykový materiál byl získán z dat morfologicky značkovaných subkorpusů ČNK.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Czech nouns derived from verbs with an objective genitive considerably contribute to the theoretical description of valency of Czech deverbal nouns. We focus on three topics, namely (i) Possessives corresponding to an adverbal objective genitive;
(ii) Double post-nominal genitives; 
(iii) Agents expressed by a prepositionless intrumental modifying nouns derived from intransitive verbs.
We used morphologically annotated subcorpora of the Czech National Corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem příspěvku je představit anotaci aktuálního členění v české části Pražského česko-anglického závislostního korpusu. Podáváme zprávu o tomto prvním kroku v procesu vytváření paralelní anotace aktuálního členění v tomto korpusu a podrobně popisujeme automatickou předanotaci české části. Výsledky předanotace jsou vyhodnoceny na základě srovnání automatické a ruční anotace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The objective of the present contribution is to give a survey of the annotation of information structure in the Czech part of the Prague Czech-English Dependency Treebank. We report on this first step in the process of building a parallel annotation of information structure in this corpus, and elaborate on the automatic pre-annotation procedure for the Czech part. The results of the pre-annotation are evaluated, based on the comparison of the automatic and manual annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Keynote přednáška bloku Paralelní slavistické korpusy. Obsahem přednášky bylo představení anotačních principů Pražského závislostního korpusu a rovněž představení dalších korpusů, které tyto anotační principy využívají, včetně korpusů paralelních.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A keynote speech in the section Parallel slavic corpora. The speech presented annotation principles of the Prague Dependency Treebank, as well as other corpora based on these principles, incl. parallel corpora.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pražský závislostní korpus arabštiny, verze 1.5, je kolekce textů v moderní arabštině, obohacená o ruční morfologickou a závislostní anotaci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Prague Arabic Dependency Treebank, version 1.5 (PADT 1.5) is a collection of  texts in Modern Standard Arabic, entiched with morphological and analytical (surface-syntax dependency) annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Omezená webová demoverze aplikace statistického strojového překladu z angličtiny, němčiny, španělštiny a francouzštiny do češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Restricted web-based demo version of application of statistical machine translation from English, German, Spanish and French to Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato stránka vznikla jako doprovodný web k semináři o lingvistických nástrojích pro studenty bohemistiky na Univerzitě Palackého v Olomouci. Zabývá se především daty a nástroji poskytovanými ÚFALem, ale nejenom jimi. Občas upozorňuje i na související software vzniklý buď zcela na jiném pracovišti, nebo jako výsledek společného projektu ÚFALu s dalšími ústavy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Accompanying website to the workshop on linguistic tools for students of Czech at Palacký University in Olomouc. Most of the time it describes data and tools provided by ÚFAL, but not solely. Sometimes it draws attention to related software created at a different institute or as a result of a common project of ÚFAL and other institutes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vztah mezi větou v přirozeném jazyce, tak jak je napsaná, a jejím významem je velmi složitý jev. Existuje mnoho variant téže věty, které zachovávají význam, zatímco jiné, povrchově velmi malé změny mohou význam zkreslit nebo zcela otočit. Abychom mohli věty algoritmicky zpracovávat a generovat, musíme najít způsob, jak zachytit jejich sémantickou identitu a podobnost.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The relationship between a sentence in a natural language as written down and
its meaning is a very complex phenomenon. Many variations of the sentence
preserve the meaning while other superficially very small changes can distort or
completely reverse it. In order to process and produce sentences
algorithmically, we need to somehow capture the semantic identity and similarity
of sentences.

The issue has been extensively studied from a number of directions, starting
with thesauri and other lexical databases that capture synonymy of indivitual
words (most notably the WordNet \cite{wordnet}), automatic paraphrasing
of longer phrases or even sentences
(e.g.
\cite{bannard-callison-burch-2005},
\cite{kauchak-barzilay},
\cite{denkowski-lavie:2010:WMT}) or
textual entailment \cite{textual-entailment}.
We are still far away from a satisfactory solution.

The field of machine translation
makes the issue tangible in a couple of ways, most importantly within the
task of automatic MT evaluation.
Current automatic MT evaluation techniques rely on the availability of some
reference translation, i.e. the translation as produced by a human translator.
Obtaining such reference translations is relatively costly, so most datasets
provide only one reference translation, see e.g.
\cite{callisonburch-EtAl:2012:WMT}.

While there are many possible
translations of a given input sentence, only a handful of them are available as
reference translations. The sets of hypotheses considered or finally selected by
the MT system can be completely disjoint from the set of reference translations.
\cite{bojar-kos:2010:WMT} observed that only about 5--10\% of reference translations were \emph{reachable} for
Czech-to-English translation , regardless whether the system
would actually search towards them.
Relying mostly on unreachable reference translations is detrimental for MT
system development. Specifically, automatic MT evaluation methods perform worse
and consequently automatic model optimization suffers.

Occasionally there are a few more references available.
For example, in the NIST OpenMT evaluation\footnote{\url{http://www.itl.nist.gov/iad/mig/tests/mt/2006/doc/mt06eval_official_results.html}}
or in the Prague Czech-English Dependency Treebank \cite{pcedt}
there are four.
Difficult as they may be to obtain, even four references fail to provide
an adequate representation of the enormous space of correct translations.

We want to trigger discussion about how many and what sort of references
we actually need for MT, how can we obtain and use them.

Obviously the set of possible translations is exponentional in the length of the sentence.
Dreyer and Marcu \cite{dreyer-marcu} generated translation networks for the HyTER
evaluation metric; the networks provided them with a compact representation
of the large space of possible translations.
Along the same lines, we propose unification-based approach to describe the space
\cite{tsd-hyter}.
Using a Prolog-based annotation tool
and applying a 2-hour upper working limit per sentence,
our translators were able to generate tens of thousands (or more) Czech references
for each English sentence; the record figure was 19 billion translations
of one long sentence.
The way of actually using such data is an open question.
If we just take the \texttt{mteval} script and run it with 19 billion references instead of four,
we will probably not live long enough to see the results.
Randomly selected and manually checked samples from these generated translations
indicated that the translations are indeed plausible.
We used a Levenshtein-based string similarity measure to confirm diversity of the generated set.

We also measured the string similarity between system outputs and various reference translations:
those from the generated set
and also two manually post-edited system outputs.
Unsurprisingly, each of the two post-edited translations were much closer to the
original system outputs than the single reference translation that was
originally available for that test set.
However, our annotators managed to produce references which are almost
exactly as close as the post-edited translations,
even though they did not have access to them or the system outputs.

\cite{wmt2013} showed that with several references (and with the BLUE and NIST metrics)
smaller test set with multiple post-edited references
has the same level of correlation with human judgments
as a much larger test set with just one reference, created independently of MT.
In future we might further investigate that in terms of
\textit{which} system outputs to post-edit (average? good? all?)

During the past 20 years we have accummulated impressive amount of bitext and it keeps growing.
Our current conclusion is that not every bitext is equally valuable.
Diversity of translation alternatives in either language is at least as important.
So where will we be after the next 20 years?
Quite likely, the amount of bitext available will multiply accordingly.
However, we also hope that for every sentence ``pair'', the number of available translation variants
will also multiply.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Úvodní kurz je věnován teoretickému a praktickému pohledu na základní principy a algoritmy strojového učení v počítačovém zpracování přirozeného jazyka. Posluchačům jsou představeny základy statistického systému R.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The course provides a concise introduction to principles and algorithms of machine learning in natural language processing both theoretically
and practically. We will focus on fundamental ideas in machine learning and the basic theory behind them. The principles of machine learning
will be presented in a gentle way so that students do not have to be afraid of scary mathematical formulas. We will give a brief introduction to the R system for statistical computing, which we use as a tool for practical demonstration. Students
will gain practical know-how needed to apply the machine learning techniques to new problems.
The presented methods of machine learning will be practically demonstrated on selected tasks from the field of natural language processing. Understanding these tasks will not require any extra linguistic knowledge. We will show how to master NLP tasks using the R
system and how to experiment with real data sets.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Naším cílem je predikovat rodný jazyk (L1) autorů anglických esejí za pomoci korpusu TOEFL11, ve kterém jsou známy jazykové úrovně autorů a témata esejí. Úlohu řešíme jako klasifikační úlohu pomocí řízených metod strojového učení. Zaměřujeme se na ladění atributů, mezi které jazykovou úroveň a témata nezahrnujeme. Atributy navrhujeme napříč jazyky L1. Experimentujeme s několika technikami pro filtrování a kombinaci atributů s ohledem na kritéria z informační teorie. Celkem jsme natrénovali čtyři modely SVM a pomocí většinového hlasování je zkombinovali do modelu dosahujícího úspěšnosti 72.5%.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Our goal is to predict the ﬁrst language (L1)
of English essays’s authors with the help of
the TOEFL11 corpus where L1, prompts (topics) and proﬁciency levels are provided. Thus
we approach this task as a classiﬁcation task
employing machine learning methods. Out
of key concepts of machine learning, we focus on feature engineering. We design features across all the L1 languages not making
use of knowledge of prompt and proﬁciency
level. During system development, we experimented with various techniques for feature ﬁltering and combination optimized with respect
to the notion of mutual information and information gain. We trained four different SVM
models and combined them through majority
voting achieving accuracy 72.5%.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tvaroslovné a větné rozbory jsou povinnou součástí hodin češtiny, pravděpodobně tou nejméně oblíbenou. Pro zařízení iPhone a iPad připravujeme editor Čapek (publikace na AppStore duben 2013), díky kterému by mohlo být provádění rozborů "hustý". Z akademického pohledu jsou školské rozbory základnou pro rozšíření banky dat, kterou využíváme v aplikacích počítačového zpracování přirozeného jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Manual tagging and parsing are obligatory parts of Czech lessons, probably the most unpopular. We implement Capek editor for iPhone and iPad (to be published at AppStore in April 2013) to make tagging and parsing cool. Annotations by school children are automatically transformed into the Prague Dependency Treebank annotation framework.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Technická zpráva shrnuje nová anotační pravidla pro anotovaní českých textů na tektogramatické rovině Pražských závislostních korpusů, která doplňují velký anotační manuál (ÚFAL/CKL TR-2005-28). Pravidla vznikla v souvislosti s anotováním korpusů PCEDT 2.0 a PDTSC 2.0.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Technical report provides new annotation rules for annotation of czech text on tectogrammatical level of the Prague Dependency Treebanks. The rules supplement main annotation manual (ÚFAL/CKL TR-2005-28) and were formed with respect to annotation of PCEDT 2.0 and PDTSC 2.0.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Technická zprava popisuje inovace, které byly provedeny ve verzi PDT 3.0 oproti verzi PDT 2.0. Vysvětluje jejich lingvistickou podstatu, zdůvodnění, pokyny pro anotaci a exemplifikaci. Její jedna část je věnována nově zavedeným nebo upraveným jevům tektogramatické roviny. V její další části jsou představeny jevy textové koreference, mezivětných vztahů, žánrové klasifikace dat a způsob jejich anotování v PDT 3.0.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Technical report describes improvement of the older scenario of PDT 2.0 that was applied in PDT 3.0. The description of changes and additions for the annotation procedure, the motivation for them, the procedure of their annotation and exemplification are given in the first part of this report. The extensions of the textual coreference, inter-sentential relations and genre classification of the texts included in PDT 3.0 are described in the other part of the report.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Technická zprava popisuje inovace, které byly provedeny ve verzi PDT 3.0 oproti verzi PDT 2.0.  Vysvětluje jejich lingvistickou podstatu, zdůvodnění, pokyny pro anotaci a exemplifikaci. Její jedna část je věnována nově zavedeným nebo upraveným jevům tektogramatické roviny. V její další části jsou představeny jevy textové koreference, mezivětných vztahů, žánrové klasifikace dat a způsob jejich anotování v PDT 3.0.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Technical report describes improvement of the older scenario of PDT 2.0 that was applied in PDT 3.0. The description of changes and additions for the annotation procedure, the motivation for them, the procedure of their annotation and exemplification are given in the first part of this report. The extensions of the textual coreference, inter-sentential relations and genre classification of the texts included in PDT 3.0 are described in the other part of the report.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku představujeme PDTSC a jeho anotaci, která vedla k rozšíření slovníku PDT-Vallex, valenčnímu slovníku českých sloves, původně vytvořenému pro psané texty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this article, we introduce the Prague Dependency Treebank of Spoken Czech.
The syntactic and semantic annotation of this corpus has led to the expansion of PDT-Vallex,
a valency lexicon of Czech verbs, which has previously been linked only to the annotation of written
texts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška se soustřeďuje na základní syntaktické vztahy, jmenovitě na závislostní relace a na slovosled, a dále na jejich vzájemnou souhru. Prezentujeme formální rámec, který nám umožňuje ekonomicky a přitom jazykově adekvátně popsat tyto vztahy a jejich automatizované zpracování. Nakonec krátce zmiňujeme využití tohoto formálního ránce v aplikacích NLP.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk focuses on basic syntactic relations, namely on dependency and word order, and on their mutual interplay. We present a formal framework that gives us a possibility of an economic and linguistically adequate description of these relations as well as their automatic processing. Finally, we shortly mention the application of the framework in language resources and NLP tools.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška se zaměří na základní syntaktické vztahy, především závislost a slovosled, na jejich vzájemné vztahy a příklady vzájemného ovlivňování (včetně tzv. neprojektivních konstrukcí). Dále představíme formální rámec, který umožňuje popis těchto vztahů a jejich automatické zpracování. V závěru bude krátce zmíněno využití tohoto rámce v jazykových zdrojích a (na nich budovaných) nástrojích NLP.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk focuses on basic syntactic relations, dependency relationship and word order in particular, on their mutual relations, and examples of mutual interplay (including so called non-projective constructions). Then we present a formal frame allowing us to describe these relations ant their automated processing. Finally, we shortly mention various applications of the proposed framework in NLP tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Fenomén volného slovosledu hraje důležitou roli v syntaktické analýze mnoha přirozených jazyků.
Tento příspěvek zavádí pojem slovosledného posunu ('shiftu'), který odrážející stupeň volnosti slovosledu. Slovosledný posun je operace založená na slovosledných změnách zachovávajících syntaktickou správnost, jednotlivé slovní tvary, jejich morfologické charakteristiky a jejich syntaktické vztahy v průběhu tzv. redukční analýzy, tedy postupného zjednodušování věty.
Příspěvek přináší lingvistickou motivaci pro tuto operaci a zaměřuje se na formální popis celkového
mechanismu pomocí speciální třídy automatů, tzv.
restartovacich automatů s operací shift  obohacených strukturovaným výstupem strukturovaný výstup.
Cílem této studie je objasnit vlastnosti výpočtů
potřebných k provedení (rozšířené) redukční analýzy pro přirozené jazyky s volným slovosledem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The phenomenon of word order freedom plays
an important role in syntactic analysis of many natural languages. This paper introduces a notion of a word order shift, an operation reflecting the degree of word order freedom of natural languages. The word order shift is an operation
based upon word order changes preserving syntactic
correctness, individual word forms, their morphological characteristics, and their surface dependency relations in a course of a stepwise simplification of a sentence, a so called analysis by reduction.
The paper provides linguistic motivation for this operation and concentrates on a formal description of the whole mechanism through a special class of automata, so called restarting automata with the shift operation enhanced with
a structured output, and their computations.
The goal of this study is to clarify the properties of computations needed to perform (enhanced) analysis by reduction for free word order languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Budu mluvit o tématu, kterému se říká "automatické zpracování přirozeného jazyka" a myslí se tím způsoby, jak přimět počítač, aby rozuměl nějaké lidské řeči. Povíme si něco o jazykových korpusech, různých úrovních a způsobech lingvistického značkování, elektronických slovnících, metodách analýzy a syntézy přirozeného jazyka, automatickém překladu. Vše si ukážeme na skutečných příkladech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I will talk about a topic called "natural language processing" which deals with ways of getting the computer to understand human languages. I will tell you something about language corpora, different levels and methods of linguistic tagging, electronic dictionaries, methods of analysis and synthesis of natural languages, automatic translation. Everything will be shown on real examples.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Wikipedie slouží nejen jako rozsáhlá encyklopedie zasahující do mnoha odvětví, ale v poslední době stále častěji i jako zdroj jazykových dat pro nejrůznější aplikace. Jednotlivé jazykové mutace umožňují získat i paralelní data ve více jazycích. Zařazení článků wikipedie do kategorií potom může sloužit k filtrování jazykových dat. 
V našem projektu se zabýváme automatickým překladem textů v oboru biologie a lékařství, proto jsme potřebovali větší množství paralelních dat. Jedním ze zdrojů byla právě wikipedie. Pro výběr dat splňujících daná kritéria – tedy dané obory v daných jazycích – jsme využili projektu Dbpedia, který ze stránek wikipedie extrahuje strukturované informace a ve formátu RDF je zpřístupňuje uživatelům.
V příspěvku popíšeme postup extrakce dat a problémy, které jsme museli řešit, neboť u otevřeného projektu jako wikipedie, do něhož může přispívat kdokoli, nelze spoléhat na konzistenci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Wikipedia is not only a large encyclopedia, but
lately also a source of linguistic data for various applications.
Individual language versions allow to get the parallel data in multiple languages. Inclusion of Wikipedia articles into categories can be used to filter the language data according to a domain.
In our project, we needed a large number of parallel data for training systems of machine translation in the field of biomedicine. One of the sources was Wikipedia. To select the data from the given domain we used the results of
the DBpedia project, which extracts structured information from the Wikipedia articles and makes them available to users in RDF format.
In this paper we describe the process of data extraction and the problems that we had to deal with, because the open source project like Wikipedia, to which anyone can contribute, is not very reliable concerning consistency.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V češtině a v ruštině existuje množina prefixů, které mění význam nedokonavých sloves vždy stejným způsobem. Změna často (v českém jazyce vždy) vyžaduje přidání reflexivního morfému. Tato vlastnost nedokonavých sloves může být použita pro automatické rozpoznávání slov, aniž by bylo nutné ukládat takto vzniklé tvary do morfologických slovníků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In Czech and in Russian there is a set of prefixes changing the meaning of imperfective verbs always in the same manner. The change often (in Czech always) demands adding a reflexive morpheme. This feature can be used for automatic recognition of words, without the need to store them in morphological dictionaries.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vytváříme systém pravidel, s jejichž pomocí značkujeme slovesa a jejich kolokáty v závislostním korpusu. Místo klasických syntaktických kategorií používáme šablonu, do níž zařazujeme nalezené kolokáty v přesně daném pořadí. Porovnáním jednotlivých pozic pro nejčastější syntaktické alternace daného slovesa získáme jeho přesnější syntaktický popis.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We report on a rule-based procedure of extracting
and labeling English verb collocates from a dependency-parsed corpus. Instead of relying on the syntactic labels provided by the parser, we use a simple topological sequence that we fill with the extracted collocates in a prescribed
order. A more accurate syntactic labeling
will be obtained from the topological fields
by comparison of corresponding collocate
positions across the most common syntactic
alternations. So far, we have extracted and labeled verb forms and predicate complements according to their morphosyntactic structure. In the next future, we will provide the syntactic labeling of the complements.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek zkoumá jev volného slovosledu. Soustředí se na vztak mezi (formální) závislostí a slovosledem. Zkoumání je provedeno aplikací poloautomatické metody redukční analýzy na česká syntaktický anotovaná data.
Volnost slovosledu je vyjádřena pomocí počtu nutných záměn v průběhu redukční analýzy. Článek ukazuje, že tato míra je ortogonální k mírám vyjadřujícím volnost slovosledu pomocí počtu neprojektivních konstrukcí či klitik ve větě.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper aims at the investigation of the phenomenon of free word order. It concentrates on the relationship between (formal) dependencies and word order. The investigation is performed by means of a semiautomatic application of a method of analysis by reduction to Czech syntactically annotated data.
The freedom of word order is expressed by means of a number of necessary shifts in the process of analysis by reduction. The paper shows that this measure provides a new view of the problem, it is orthogonal to measures reflecting the word order freedom based on a number of non-projective constructions or clitics in a sentence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá metodou identifikace zajímavých konstrukcí v syntakticky anotovaném korpusu češtiny (Pražském závislostním korpusu) pomocí aplikace automatické procedury redukční analýzy na stromy tohoto korpusu. Procedura odhaluje některé lingvistické jevy, které jdou za hranici "závislosti".
Článek obsahuje diskuzi a analýzu jednotlivých jevů a rovněž kvantifikuje výsledky automatické procedury na podmnožině korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes a method of identifying a set of interesting constructions in a syntactically annotated corpus of Czech (the Prague Dependency Treebank) by application of an automatic procedure
of analysis by reduction to the trees in the treebank. The procedure reveals certain linguistic phenomena that go beyond `dependency
nature'.
The paper contains discussion and analysis of individual phenomena, as well as the quantification of results of the automatic procedure on a subset of the treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Závislostní korpus anotovaný na rovině morfologické (2 miliony slov), syntaktické (1,5 milionu slov) a sémantické (0,8 milionu slov). Oproti verzi 2.5 přibyly opravy morfologie, revize gramatémů, anotace diskurzních vztahů, asociační anafory, rozšířené textové koreference a žánrů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Dependency corpus annotated at morphological, syntactic, and deep syntactic levels. Previous version 2.5 has been enriched by annotation of discourse relations, bridging anaphora, extended textual coreference, and genres. Morphological level has been improved and some grammatemes have been revised.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku se zabýváme identifikací výskytů víceslovných výrazů z existujícího slovníku v textovém korpusu. Víceslovné výrazy mohou být libovolné délky a přerušeny v povrchovém pořádku slov. Analyzujeme a porovnáváme tři různé přístupy využívající lingvistické analýza na různých rovinách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We deal with syntactic identification of occurrences of multiword expression (MWE) from an existing dictionary in a text corpus. The MWEs we identify can be of arbitrary length and can be interrupted in the surface sentence. We analyse and compare three approaches based on linguistic analysis at a varying level, ranging from surface word order to deep syntax. The evaluation is conducted using two corpora: the Prague Dependency Treebank and Czech National Corpus. We use the dictionary of multiword expressions SemLex,
that was compiled by annotating the Prague Dependency Treebank and includes deep syntactic dependency trees of all MWEs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku analyzujeme syntaktické funkce deadjektivních adverbií s příponou -o na základě materiálu z Českého národního korpusu. Pouze část těchto adverbií se specializuje na predikativní funkci, jiná plní kromě této funkce také funkci příslovečného určení a některá se v predikativní funkci neuplatňují vůbec. Kromě syntaktické funkce těchto adverbií zkoumáme také jejich význam a vztah k základovému adjektivu a navrhujeme, jak je reprezentovat v hloubkově syntaktické anotaci Pražského závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Both an adverb with the suffix -e and an adverb with the suffix -o are derived from many adjectives in Czech. Adverbs with the latter suffix are often a component of the predicate. The predicative syntactic function has become one of the main arguments to isolate the adverbs with the suffix -o as a separate part of speech (Komárek, 1954); nevertheless, this proposal has not been widely accepted. In the present paper, a detailed analysis of the adverbs with the suffix -o is carried out on the basis of material from the Czech National Corpus. The analysis documents that solely a part of the adverbs with the suffix -o specializes in the predicative function, other adverbs with this suffix fulfill an adverbial function as well, or do not fulfill the predicative function at all. Besides the syntactic function and the lexical meaning of these adverbs, their relation to the base adjective and the impact of context is taken into consideration when discussing the representation of these adverbs within the deep-syntactic annotation of the Prague Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slova odvozená od adjektiv představují zajímavý, reprezentativní vzorek slovotvorných derivačních procesů v češtině: patří ke slovním druhům substantiv, adjektiv, sloves a adverbií, vzájemně se liší v řadě aspektů, mimo jiné z hlediska sepjetí jejich lexikálního významu s významem slovotvorným. 
V příspěvku probereme možnost třídění deadjektivních derivátů právě z hlediska jejich vztahu k základovému adjektivu, pracujeme přitom s Kuryłowiczovou (1936) koncepcí derivace syntaktické a lexikální, která je v Dokulilových slovotvorných pracích reflektována. Z klasifikace deadjektivních derivátů jako syntaktických nebo lexikálních vyvodíme přímé důsledky pro zachycení těchto slov v hloubkověsyntaktické anotaci Pražského závislostního korpusu. Syntaktické deriváty je možné reprezentovat základovým adjektivem, protože jeho odlišná syntaktická funkce je v tomto syntakticky anotovaném korpusu zachycena speciálním atributem. Oproti tomu lexikální deriváty se budou od svého základového slova lišit lexikální hodnotou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Deadjectival derivates are a representative sample of Czech derivation. On the basis of the semantic relation to their base adjectives, they can be classified into two groups: into syntactic derivates, which have the same meaning as their base adjectives but fulfill different syntactic functions (e.g. deadjectival adverbs), and lexical derivates, which share the syntactic function with the base adjectives but differ from them in meaning (e.g. diminutive adjectives). The present paper focuses on deadjectival nouns with the suffix -ost, which express a quality. Based on the analysis of data from the corpus SYN2010, we try to document that the meaning of quality is closely interconnected with the meaning of a bearer of the quality, and propose to revise Dokulil’s classification of these nouns as syntactic derivates. The classification of a derivate as a syntactic or lexical one is directly reflected in the deep-syntactic annotation of the Prague Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku shrnujeme základní rysy Dokulilovy teorie slovotvorné produktivity a zasazujeme ji do kontextu zkoumání produktivity v posledních desetiletích. Na materiálu z velkých korpusů češtiny zkoumáme produktivitu čtyř českých sufixů (-ost, -ství/ctví, -ismus, -ita) a ukazujeme, že jednotlivé míry používané ke kvantifikaci produktivity vedou k rozdílným výsledkům.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the present paper, central ideas of the work on productivity by the Czech linguist Miloš Dokulil are presented and placed in the context of recent approaches to productivity. Based on material from large corpora of Czech, productivity of four Czexh suffixes (-ost, -ství/ctví, -ismus, -ita) is analyzed; it is demonstrated that productivity measures used for quantification of productivity give different results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce se zaměříme na Dokulilovu teorii produktivity, která je integrální součástí jeho široce respektované slovotvorné teorie, připomeneme především jeho distinkci produktivity systémové a empirické. Při analýze přípon -ost, -ství/-ctví, -ismus a -ita, které jsou v češtině používány (mimo jiné) k odvozování názvů vlastností, se kromě jejich systémových rysů pokusíme stanovit míru produktivity také na základě frekvenčních údajů z velkých korpusových dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Though the term of productivity in word-formation seems to be intuitively clear and the idea (under various terms) has been present in linguistics for centuries, there are many approaches to productivity in contemporary linguistics, which differ in several aspects. 
In the talk, I briefly sketch selected issues of the study of productivity focusing on the last two or three decades, during which productivity has become one of the central topics of the research into word-formation. In this context, I refer to the main ideas of the work on productivity by Miloš Dokulil, whose word-formation theory has become a widely respected and, in fact, the only common ground of word-formation descriptions in Czech linguistics. I focus on his opposition of the so-called systemic and empirical productivity and present some suggestions on how Dokulil’s ideas could be applied to the current, corpus-based research in productivity, by analyzing the productivity of four suffixes which are used to derive names of qualities in Czech (-ost, -ství/-ctví, -ismus and -ita).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Workshop představil Pražský závislostní korpus a základní nástroje, které lze používat k jeho prohledávání. Na rozdíl od dnes již poměrně známého Českého národního korpusu obsahuje PDT i anotace syntaktických struktur, a to hned na dvou rovinách. Seznámili jsme se s teorií funkčního generativního popisu, na jejímž základě byl korpus vybudován, a popsali jsme si aspekty, kterými se od teoretických východisek odchyluje.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The workshop presented the Prague Dependency Treebank and basic tools that can be used to search it. Unlike more widely known Czech National Corupus, the PDT contains two layers of syntactic annotation. The theory of the Functional Generative Description upon which the treebank is built was presented, and the aspects were described where the theory and the treebank diverge.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tutoriál přibližující práci s nástrojem PML-TQ určeným pro prohledávání syntakticky anotovaných korpusů. Představeny byly dvě verze klientského rozhraní, příklady používaly data v češtině, angličtině a němčině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A tutorial describing PML-TQ, a tool for searching syntactically annotated corpora. Two forms of the client interface were presented, with examples over Czech, English, and German data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Toto je vydání Alex Dialogue Systems Framework pro rok 2013. 

Tento framework je vyvýjen skupinou zabývající se výzkumem v oblasti hlasových dialogových systémů na ÚFAL - http://ufal.mff.cuni.cz/ - Ústav formální a aplikované lingvistiky, MFF, UK. 

Hlavní cíle vývoje jsou: poskytnout základní komponenty pro vývoj SDS, poskytnout příklady hlasových dialogových systémů, poskytnout nástroje pro zpracopvání záznamů dialogů, například audio přepisy, sémantické anotace, nebo vyhodnocení dialogových systémů.

Implemntované vlastnosti: VOIP používající PJSIP 2.1, ASR používající OpenJulius, GoogleASR nebo KALDI, VAD používající Gaussian Mixure Models nebo Feed-Forward Neural Networks, SLU používající množinu klasifikátorů založených na logistické regresi, DM používající diskriminativní pravěpodobnostní aktualizaci stavu a ručně psané strategie řízení, NLG používající šablon s možností využití šablon založených na tektogramatických stromech pro správnou syntézu povrchové realizace, TTS používající flite, VoiceRSS nebo SpeechTech, vyhodnocení dialogových systémů používajáící Amazon Mechanical Turk, tvorba akustických modelů pomocí HTK a KALDI.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This is the Alex Dialogue Systems Framework - 2013 release. 

This framework is being developed by the dialogue systems group at UFAL - http://ufal.mff.cuni.cz/ - the Institute of Formal and Applied Linguistics, Faculty of Mathematics and Physics, Charles University in Prague, Czech Republic. The purpose of this work is to facilitate research into and development of spoken dialogue systems.

The main goals are: to provide baseline components need for a building spoken dialogue systems (SDSs), to provide example implementations of SDSs for several domains, to provide tools for processing dialogue system interactions logs, e.g. for audio transcription, semantic annotation, or SDSs evaluation.

Implemented features: VOIP using PJSIP 2.1 with some modifications, ASR using OpenJulius, GoogleASR or KALDI, VAD using Gaussian Mixture Models or Feed-Forward Neural Networks, SLU using a set of logistic regression classifiers for detecting dialogue acts, DM using probabilistic discriminative dialogue state tracking and handcrafted policies, NLG using template based generation possibly with efficient inflection into the correct surface form for morphologically rich languages, TTS using flite, VoiceRSS and SpeechTech, evaluation of dialogue systems using Amazon Mechanical Turk crowdsourcing platform, building acoustic models using the HTK and KALDI toolkits.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme Depfix, systém pro samočinnou post-edititaci výstupů frázových strojových překladů z angličtiny do češtiny, založený na jazykovědných znalostech. Nejprve jsme rozebrali druhy chyb, kterých se dopouští typický strojový překladač. Poté jsme vytvořili sadu pravidel a statistickou komponentu, které opravují takové chyby, které jsou běžné nebo závažné a může přicházet v úvahu jejich oprava pomocí našeho přístupu. Používáme řadu nástrojů pro zpracování přirozeného jazyka, které nám poskytují rozbor vstupních vět. Navíc jsme reimplementovali závislostní analyzátor a několika způsoby jej upravili pro provádění rozboru výstupů statistických strojových překladačů. Provedli jsme automatická i ruční vyhodnocení, která potvrdila, že kvalita překladů se zpracováním v našem systému zlepšuje.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present Depfix, a system for automatic post-editing of phrase-based English-to-Czech machine translation outputs, based on linguistic knowledge. First, we analyzed the types of errors that a typical machine translation system makes. Then, we created a set of rules and a statistical component that correct errors that are common or serious and can have a potential to be corrected by our approach. We use a range of natural language processing tools to provide us with analyses of the input sentences. Moreover, we reimplemented the dependency parser and adapted it in several ways to parsing of statistical machine translation outputs. We performed both automatic and manual evaluations which confirmed that our system improves the quality of the translations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Depfix je systém pro samočinnou post-edititaci výstupů frázových strojových překladů z angličtiny do češtiny, založený na jazykovědných znalostech. Rozebrali jsme druhy chyb, kterých se dopouští typický strojový překladač, a vytvořili jsme sadu pravidel a statistickou komponentu, které opravují některé z těchto chyby. Používáme řadu nástrojů pro zpracování přirozeného jazyka, které nám poskytují rozbor vstupních vět. Navíc jsme reimplementovali závislostní parser a několika způsoby jej upravili pro provádění rozboru výstupů statistických strojových překladačů. Provedli jsme automatická i ruční vyhodnocení, která potvrdila, že kvalita překladů se zpracováním v našem systému zlepšuje.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Depfix is a system for automatic post-editing of phrase-based English-to-Czech machine translation outputs, based on linguistic knowledge. We analyzed the types of errors that a typical machine translation system makes, and created a set of rules and a statistical component that correct some of the errors. We use a range of natural language processing tools to provide us with analyses of the input sentences. Moreover, we reimplemented the dependency parser and adapted it in several ways to parsing of statistical machine translation outputs. We performed both automatic and manual evaluations which confirmed that our system improves the quality of the translations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Některé veci jsou pro PB SMT obtížné -- negace, shoda, trpný rod...
Automatická post-editace může často pomoci -- i hrstka jednoduchých pravidel může fungovat velmi dobře.
NLP nástroje jsou typicky užitečné -- tagger, parser, morphologický generátor; adaptujte je pro zpracování výstupů SMT, je-li to možné.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Some things are hard for PB SMT -- negation, agreement, passives...
Automatic post-editing can often help -- a handful of simple rules might do the job very well.
NLP tools are typically useful -- tagger, parser, morphological generator; adapt them to SMT outputs if possible.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Deepfix je statistický post-editovací systém pro zlepšování
kvality výstupů statistického strojového překladu.
Pokouší se opravovat chyby ve slovesné a substantivní valenci za použití hloubkové syntactické
analýzy a jednoduchého pravděpodobnostního valenčního modelu.
Na jazykovém páru angličtina-čeština ukazujeme že, pokud je podporována hlubokou lingvistickou znalostí, statistická post-editace
statistického strojového překladu
vede ke zlepšení 
kvality překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Deepfix is a statistical post-editing system for improving
the quality of statistical machine
translation outputs.
It attempts to correct errors in verb-noun valency using deep syntactic
analysis and a simple probabilistic model of valency.
On the English-to-Czech translation pair, we show that statistical post-editing of
statistical machine translation
leads to 
an improvement
of the translation quality when helped by deep linguistic knowledge.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se věnuje překladu anglického zájmena "it" do češtiny. Navrhli jsme diskriminativní model a zapojili ho do překladu přes tektogramatickou rovinu systémem TectoMT. Výsledkem je zlepšení kvality překladu v 8.5% vět obsahujícich "it".</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a novel approach to the translation of the English personal pronoun it to Czech. We conduct a linguistic analysis on how the distinct categories of it are usually mapped to their Czech counterparts. Armed with these observations, we design a discriminative translation model of it, which is then integrated into the TectoMT deep syntax MT framework. Features in the model take advantage of rich syntactic annotation TectoMT is based on, external
tools for anaphoricity resolution, lexical co-occurrence frequencies measured on a large parallel corpus and gold coreference annotation. Even though the new model for it exhibits no improvement in terms of BLEU, manual evaluation shows that it outperforms the original solution in
8.5% sentences containing it.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zaměřuje na zlepšování překladu anglického zájmena "it" a anglických reflexivních zájmen v systému strojového překladu z angličtiny do češtiny přes tektogramatickkou rovinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We focus on improving the translation of the English pronoun it and English reflexive pronouns in an English-Czech syntax-based machine translation framework. Our evaluation both from intrinsic and extrinsic perspective shows that adding specialized syntactic and coreference-related features leads to an improvement in trans-
lation quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Od publikace v roce 2011, ziskala CEFR vedoucí úlohu ve výuce a certifikaci cizích jazyků. Bohužel jednotlivé úrovně CEFR nejsou dostatečně ilustrovány. Cílem projektu Merlin je vyřešit tento problém pro češtinu, němčinu a italštinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Since its publication in 2001, the Common European Framework of Reference for Languages (CEFR) has gained a leading role as an instrument of reference for language teaching and certification and for the development of curricula (cf. CEFR 2001). At the same time, there is a growing concern about CEFR reference levels being insufficiently illustrated, leaving practitioners without comprehensive empirical characterizations of the relevant distinctions. This is particularly the case for languages other than English (cf. e.g. Hulstijn 2007, North 2000).
The MERLIN project addresses this demand for the first time for Czech, German and Italian by developing a didactically motivated online platform that enables CEFR users to explore authentic written learner productions that have been related to the CEFR levels in a methodologically sophisticated and rigorous way. The core of the multilingual online platform is a trilingual learner corpus relying on a cross-linguistic design and composed of roughly 2300 learner texts produced in standardized language certifications (as used by telc, DE,  UJOP-Charles University in Prague, CZ as well as UNIcert, DE) validly related to the CEFR, covering the levels A1-C1.
The aim of this paper is both to present the MERLIN project and its corpus, and to discuss its current state. In addition to providing preliminary statistics, we detail three key aspects: (1) the data collection and transcription, (2) the creation of the annotation schemata and (3) the technical design of the workflow devised to compile and query the corpus. For each of these aspects, we highlight its requirements and the means employed for the underlying tasks. We also discuss the challenges induced by the cross-linguistic nature of the project: particularities of the languages from three different families (Slavic, Germanic and Romance) have to be covered, both on a linguistic and a technical perspective.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek prezentuje komparativní studii 5 různých typů slovních vektorových modelů a 4 různých měr pro měření kompozicionality slovních výrazů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a comparative study of 5 different types of Word Space Models (WSMs) combined with 4 different compositionality measures applied to the task of automatically determining semantic compositionality of word expressions. Many combinations of WSMs and measures have never been applied to the task before. The study follows Biemann and Giesbrecht (2011) who attempted to find a list of expressions for which the compositionality assumption – the meaning of an expression is determined by the meaning of its constituents and their combination does not hold. Our results are very promising and can be appreciated by those interested in WSMs, compositionali</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek shrnuje dosavadní modely pro určování kompozicionality slovních výrazů pomocí vektorových modelů a prezentuje náš vlastní přístup založený na latentní sémantické analýze. Experimenty jsou provedeny na testovacích datech DISCO 2011.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This research focuses on determining semantic compositionality of word expressions using word space models (WSMs). We discuss previous works employing WSMs and present differences in the proposed approaches which include types of WSMs, corpora, preprocessing techniques, methods for determining compositionality, and evaluation testbeds. We also present results of our own approach for determining the semantic compositionality based on comparing distributional vectors of expressions and their components. The vectors were obtained by Latent Semantic Analysis (LSA) applied to the ukWaC corpus. Our results outperform those of all the participants in the Distributional Semantics and Compositionality (DISCO) 2011 shared task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Shromáždili jsme anglicky Tamil dvojjazyčných data z některé z veřejně dostupných internetových stránek pro NLP výzkum zahrnující Tamil.Standardní sada zpracování byla použita na nezpracovaná data, než webových údaje byly k dispozici ve větě vyrovnaném anglicky Tamil paralelní korpus vhodný pro různé úlohy NLP. Paralelní korpusy krycí texty z bible, kino a zpravodajských domén.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We have collected English-Tamil bilingual data from some of the publicly available websites for NLP research involving Tamil. The standard set of processing has been applied on the the raw web data before the data became available in sentence aligned English-Tamil parallel corpus suitable for various NLP tasks. The parallel corpora cover texts from bible, cinema and news domains.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>TamilTB 1.0 zlepšuje v průběhu prvního vydání Tamil závislostní korpus (TamilTB v0.1). Aktuální verze přináší řadu vylepšení oproti pravopis a morfologické tagset údajů. Nyní, primární datový reprezentace korpusu používá kódování UTF-8.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>TamilTB 1.0 improves over the very first release of the Tamil dependency treebank (TamilTB v0.1). The current version introduces many improvements over the orthography and morphological tagset of the data. Now, the primary data representation of the treebank uses UTF-8 encoding.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku představujeme naši práci na rozšíření projektu IBM Content Analytics o analýzu sentimentu za využití softwaru UIMA. Popisujeme jednotlivé komponenty a jejich propojení v rámci komplexnějších dataminigových aplikací.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we present UIMA – the Unstructured
Information Management Architecture, an architecture and software framework for creating, discovering, composing and deploying a broad range of multi-modal analysis capabilities and integrating them with search technologies. We describe the elementary components of the framework and how they are deployed into more complex data mining applications. The contribution is based on our experience in work on the sentiment analysis task for IBM Content Analytics project.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku Srovnání Baysovských diskriminativiních a generativních modelů pro sledování dialogového stavu. Bylo zjistěno že oba prezentované modely mají přibližně stejnou přesnost, ale diskriminativní model je zhruba 20 krát výpočetně efektivnější.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we describe two dialogue state tracking models competing in the 2012 Dialogue State Tracking Challenge (DSTC). First, we detail a novel discriminative dialogue state tracker which directly estimates slot-level beliefs using deterministic state transition probability distribution. Second, we present a generative model employing a simple dependency structure to achieve fast inference. The models are evaluated on the DSTC data, and both significantly outperform the baseline DSTC tracker.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace prvních kroků směrem ke lingvistickému zpracování textů pro detekci sémantických vztahů v nich. Naše práce je klíčovou součastí projektu INTLIB, kterého cílem je vytvořit efektivní a uživatelsky přívětivý nástroj pro dotazování nad textovými dokumenty, odlišný od full-textu. Tento nástroj je navrhován jako obecní framework, který lze snadno modifikovat a rozšiřovat pro další domény. Pilotní doménou projektu jsou legislativní texty. Prezentujeme aplikaci JTagger, která detekuje reference v soudních rozhodnutích a aplikaci RExtractor, která extrahuje vztahy mezi entitami v českých zákonech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our initial steps towards a linguistic processing of texts to detect semantic relations there. Our work is an essential part of the INTLIB project whose aim is to provide a more efficient and user-friendly tool for querying textual documents other than full-text search. This tool is proposed as a general framework which can be modified and extended for particular data domains. Currently, we focus on Czech legal texts. We present the application JTagger which detects references in court decisions and the application RExtractor which extracts relations between entities from Czech laws.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme první kroky směrem k lingvistickému zpracování textů pro detekci sémantických vazeb. Naše práce je důležitou částí projektu INTLIB, který má za cíl vytvořit efektivnější a uživatelsky více přívětivý nástroj na dotazování v textových dokumentech než full-textové vyhledávání. Tento nástroj je navrhnut jako obecný framework, který bude možno modifikovat a rozšiřovat pro konkrétní datové domény. Aktuálně se zaměřujeme na české legislativní dokumenty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our initial steps towards a linguistic processing of texts to detect semantic relations in them. Our work is an essential part of the INTLIB project whose aim is to provide a more efficient and user-friendly tool for querying textual documents other than full-text search. This tool is proposed as a general framework which can be modified and extended for particular data domains. Currently, we focus on Czech legal texts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Aplikace detekuje reference v českých soudních rozhodnutích. Tento úkol chápeme jako typický úkol pro identifikaci jmenných entit, kde entity jsou reference (odkazy) na jiné dokumenty. Aplikujeme přístupy strojového učení s učitelem, konkrétně skryté Markovovy modely. Protože metody strojového učení s učitelem vyžadují manuálně anotovaná trénovací data, anotovali jsme 300 soudních rozhodnutí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We address the task of reference detection and classification in Czech court decisions. This task is a typical named-entity recognition task where the entities are references (links) to other documents. We apply a supervised machine learning approach, namely Hidden Markov Models. A supervised methodology requires manual annotation of training data so we annotated 300 court decisions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem projektu INTLIB, an INTelligent LIBrary, je poskytnout efektivní a uživatelsky přívětivý nástroj pro inteligentnější dotazování než nabízí full text [1]. Projekt je financován Technolofgickou agenturou ČR, grant č. TA02010182.

Na vstupu uvažujeme kolekci dokumentů určité domény (např. legislativa, medicína, životní prostředí, architektura). V první fázi jsou z této kolekce extrahovány znalosti pomocí nástrojů počítačového zpracování přirozeného jazyka. Ve druhé fázi je navrženo inteligentní vyhledávání spolu s vizualizací.

Projektový tým tvoří badatelé z MFF UK ([2], [3]) a podnikatelé ze společnosti Sysnet Ltd. [4]. Představíme výsledky, které jsme doposud dosáhli na legislativní doméně.

[1] http://ufal.mff.cuni.cz/intlib
[2] http://www.ksi.mff.cuni.cz/xrg/
[3] http://ufal.mff.cuni.cz
[4] http://www.sysnet.cz/</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The project INTLIB, an INTelligent LIBrary, aims to provide a more efficient and user-friendly tool for querying documents other than full-text [1]. The work is funded by the Technology Agency of the Czech Republic (grant no. TA02010182).

On the input we assume a collection of documents related to a particular problem domain (e.g., legislation, medicine, environment, architecture, etc.). In the first phase we extract a knowledge base from the documents using natural language processing tools. In the second phase we deal with efficient and user friendly visualization and browsing (querying) of the extracted knowledge. 

The project team consists of researchers coming from MFF UK ([2], [3]) and businessmen from Sysnet Ltd. [4]. We will present the results we achieved so far on legislative domain. In addition, we, researchers, will describe our experience gained while cooperating with the commercial partner.

[1] http://ufal.mff.cuni.cz/intlib
[2] http://www.ksi.mff.cuni.cz/xrg/
[3] http://ufal.mff.cuni.cz
[4] http://www.sysnet.cz/</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme přístup k publikování legislativních dokumentů ve formě Propojených otevřených dat(LOD). LOD je sada principů, jak publikovat data na webu ve strojově čitelné podobě, aby bylo možné propojit různé zdroje dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present an approach of publishing legislative content as Linked Open Data (LOD). LOD is a set of principles of publishing data on the Web in a machine-readable way so that links between different data sets, possibly published by different publishers, can be created. Therefore, LOD enable not only to publish data but also enrich with other existing data published according to the principles. We present what is the motivation for publishing legislation as LOD and what benefits can be gained. We then introduce a legislative ontology which builds on existing commonly used ontologies. We also show how we converted existing sources of legislation in Czech Republic to LOD.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato zpráva popisuje Addicter, nástroj pro automatickou detekci a vyhodnocení chyb, který dává uživateli k dispozici také grafické rozhraní, užitečné pro prohlížení dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This report describes Addicter, tool for automatic error detection and evaluation, providing its user also with graphical interface useful for browsing through the dataset.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek přináší výsledky společné úlohy metrik strojového překladu na workshopu WMT13. Účastníci úlohy nechali svými systémy vyhodnotit výstupy strojového překladu. Sebrali jsme hodnocení pomocí celkem 16 metrik od 8 týmů a doplnili hodnocení pomocí 5 standardních metrik (BLEU, WER, PER ad.). Hodnocení pak byla porovnána z hlediska korelace s lidským hodnocením a to jak na úrovni jednotlivých vět, tak na úrovni celého testovacího textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the WMT13 Metrics Shared Task. We asked participants of this task to score the outputs of the MT systems involved in WMT13 Shared Translation Task. We collected scores of 16 metrics from 8 research groups. In addition to that we computed scores of 5 standard metrics such as BLEU, WER, PER as baselines. Collected scores were evaluated in terms of system level correlation (how well each metric’s scores correlate with WMT13 official human scores) and in terms of segment level correlation (how often a metric agrees with humans in comparing two translations of a particular sentence).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popis soutěžního systému Chiméra populární formou pro HNFuture, vědeckou přílohu elektronického deníku iHNed.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A popularizing description of Chimera, our MT system, for HNFuture, the scientific supplement of the online journal iHNed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku je popsán proces vytváření korpusu anotovaných vět pro účely postojové analýzy v češtině. Dále je popsán vznik slovníku subjektivních výrazů Sublex 1.0, struktura použitých dat, proces anotace evaluativních stavů a komputační experimenty hodnotící kvalitu anotace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes the process of building a Czech language corpus of annotated sentences for sentiment analysis tasks, the SubLex 1.0 subjectivity lexicon, the data, the process of annotation and evaluating experiments.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku představujeme kontrastivní studii jednoho případu provázání nestejných valenčních doplnění v rámci paralelních syntaktických struktur dvou jazyků. Na materiálu Pražského česko-anglického závislostího korpusu (PCEDT) sledujeme věty, v nichž je adresátové valenční doplnění z jednoho jazyka překladově provázáno s patientovým doplněním v jazyce druhém. Konkrétně se zabýváme zejména slovesy ze sémantické třídy "Judgement" (slovesa soudu) v širokém pojetí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we present a contrastive study of one interesting non-correspondence between deep syntactic valency structures of two different languages. On the material of the Prague Czech-English Dependency Treebank we observe sentences in which an Addressee argument in one language is linked translationally to a Patient argument in the other one, particularly we aim our attention at the class of judgement verbs (in a broad sense).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popis textových anotací a jejich vybraných problémů (anotace na stromech a na lineárním textu; nesoulad mezi syntaktickou a diskurzní strukturou; nevyjádřená myšlenka jako diskurzní argument)</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Description of text annotations and some selected problems of the annotations (annotation on dependency trees and on a plain text; the discrepancy between syntactic structure and discourse structure; an unexpressed thought as a discourse argument)</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek představuje jednotlivé body automatické předanotace aktuálního členění v české části korpusu PCEDT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presentation introduces the individual points of the automatic pre-annotation of topic-focus articulation in the Czech part of the corpus PCEDT.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá rozlišením explikativních a kauzálních vztahů v češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper deals with distinguishing of explicative and causal relationships in Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Současné metody pro statistický strojový překlad obvykle využívají pouze omezený kontext ve zdrojové větě. Mnoho jazykových jevů tak zůstavá mimo jejich dosah, např. gramatická shoda v morfologicky bohatých jazycích nebo lexikální výběr často vyžadují informace z celé zdrojové věty. V této práci představujeme přehled metod modelování širšího kontextu ve strojovém překladu a popisujeme naše první experimenty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Current methods for statistical machine translation typically utilize only a
limited context in the input sentence. Many language phenomena thus remain out                                   
of their reach, for example long-distance agreement in morphologically rich
languages or lexical selection often require information from the whole source
sentence. In this work, we present an overview of approaches for including wider
context in SMT and describe our first experiments.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>MTMonkey je webová služba, která obsluhuje a distribuuje žádosti na strojový překlad předávané přes HTTP ve formátu JSON. Rozděluje je na několik strojů, na kterých běží systém pro strojový překlad, a zařizuje preprocessing a postprocessing textu.

Sestává z aplikačního serveru a jednotlivých workerů, které provádějí zpracování textu a komunikaci s překladovým systémem. Komunikace mezi aplikačním serverem a workery je založena na protokolu XML-RPC.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>MTMonkey is a web service which handles and distributes JSON-encoded HTTP requests for machine translation (MT) among multiple machines running an MT system, including text pre- and post processing.

It consists of an application server and remote workers which handle text processing and communicate translation requests to MT systems. The communication between the application server and the workers is based on the XML-RPC protocol.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme webovou službu, která zpracovává a distribuuje požadavky na strojový překlad (posílané přes HTTP ve formátu JSON). V současné době se používá pro poskytování strojového překladu mezi několika jazyky ve vícejazyčném vyhledávání informací v rámci projektu Khresmoi. Software sestává z aplikačního serveru a vzdálených workerů, kteří zajišťují zpracování textu a komunikují požadavky na překlad systémům strojového překladu. Komunikace mezi aplikačním serverem a workery je založena na protoklolu XML-RPC. Představujeme celkový návrh softwaru a výsledky testů, který dokumentují rychlost a škálovatelnost našeho řešení. Software podléhá licenci Apache 2.0 a je dostupný ke stažení v repozitáři Lindat-Clarin a na serveru GitHub.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a web service which handles and distributes JSON-encoded HTTP 
requests for machine translation (MT) among multiple machines running
an MT system, including text pre- and post processing.
It is currently used to provide MT between several languages 
for cross-lingual information retrieval in the Khresmoi project.
The software consists of an application server and remote workers which handle
text processing and communicate translation requests to MT 
systems. The communication between the application server and the workers is
based on the XML-RPC protocol. We present
the overall design of the software and test results which document
speed and scalability of our solution.  
Our software is licensed under the Apache 2.0 licence and is available for 
download from the Lindat-Clarin repository and Github.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Stručně představujeme frázový strojový překlad a nástroj Moses. Popisujeme Emana, nástroj pro správu experimentů, a ukazujeme, jak ho lze využít k trénování několika jednoduchých systémů pro strojový překlad.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We give a brief introduction to phrase-based machine translation and the Moses toolkit. We present Eman, an experiment manager, and show how to use it to train several simple MT systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Faktorové modely byly úspěšně nasazeny pro řadu jazykových párů, kde zlepšily různé aspekty kvality překladu. V této práci analyzujeme toto paradigma a pokoušíme se automatizovat hledání kvalitních překladových systémů. Zkoumáme prostor možných faktorových systémů a shledáváme, že plně automatické hledání není proveditelné. Ukazujeme, že i když jsou k dispozici výsledky automatické evaluace, zacílení prohledávání je obtížné kvůli malým rozdílům mezi jednotlivými systémy. Tyto rozdíly jsou dále rozmazány náhodností ladění vah. Popisujeme heuristiku pro odhad složitosti faktorových modelů. Závěrem popisujeme možnosti "polo-automatického" prohledávání prostoru v několika směrech a vyhodnocujeme získané systémy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Factored models have been successfully used in many language pairs to improve translation quality in various aspects. In this work, we analyze this paradigm in an attempt at automating the search for well-performing machine translation systems. We examine the space of possible factored systems, concluding that a fully automatic search for good configurations is not feasible. We demonstrate that even if results of automatic evaluation are available, guiding the search is difficult due to small differences between systems, which are  further blurred by randomness in tuning. We  describe a heuristic for estimating the complexity of factored models. Finally, we discuss the possibilities of a "semi-automatic" exploration of the space in several directions and evaluate the obtained systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje CzeSL, žákovský korpus češtiny jako druhého jazyka. Nejdříve je popsán v kontextu  Akces, souboru akvizičních korpusů češtiny. Dále je diskutováno zamýšlené využití CzeSLu. Jádro příspěvku se zabývá transkripcí a anotací.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes CzeSL, a learner corpus of Czech as a Second Language, together with its design properties. We start with a brief introduction of the project within the context of AKCES, a programme addressing Acquisition Corpora of Czech; in connection with the programme we are also concerned with the groups of respondents, including differences due to their L1; further we comment on the choice of the sociocultural metadata recorded with each text and related both to the learner and the text production task. Next we describe the intended uses of CzeSL. The core of the paper deals with transcription and annotation. We explain issues involved in the transcription of handwritten texts and present the concept of a multi-level annotation scheme including a taxonomy of captured errors. We conclude by mentioning results from an evaluation of the error annotation and presenting plans for future research.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popis a analýza anotace žákovského korpusu češtiny. Anotační schéma sestává ze tří propojených rovin, každá opravující jiný typ chyb. Anotační schéma bylo testováno na cca 175.000 slovech s rozumnou výslednou IAA. Na závěr diskutujeme možnost automatické anotace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes a corpus of texts produced by non-native speakers of Czech. We discuss its annotation scheme, consisting of three interlinked tiers, designed to handle a wide range of error types present in the input. Each tier corrects different types of errors; links between the tiers allow capturing errors in word order and complex discontinuous expressions. Errors are not only corrected, but also classified. The annotation scheme is tested on a data set including approx. 175,000 words with fair inter-annotator agreement results. We also explore the possibility of applying automated linguistic annotation tools (taggers, spell checkers and grammar checkers) to the learner text to support or even substitute manual annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Radikálně nový lexikální, na korpusech založený přístup k lingvistické teorii a praktické analýze jazyka. Kniha ukazuje, že přirozený jazyk se v zásadě skládá z množství pravidelných vzorů klauzí ("norem"), ve kterých paradigmatické množiny lexikálních položek odpovídají klauzálním rolím neboli "argumentům", s ohledem na běžnou valenční strukturu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A radically new lexical, corpus-driven approach to linguistic theory and the practical analysis of language. The book argues that a natural langauge consists, basically, of a number of regular clause patterns ("norms"), in which paradigm sets of lexical items map into clause roles or "arguments", given normal valency structure. The picture is complicated because every pattern allows certain alternations, and every aspect of normal usage can be "exploited" creatively by language users. The book offers a typology of linguistic exploitations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozhovor pro stanici Rádio Česko popisující různé přístupy ke strojovému překladu a potíže s přirozeným jazykem obecně.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An interview for Radio Czech describing various approaches to machine translation and the difficulties with handling natural languages in general.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme Pražský diskurzní korpus 1.0, soubor českých textů anotovaných na rovině textových jevů, tj. "nad úrovní popisu věty". Korpus obsahuje ruční anotace (1)textových konektorů, jejich argumentů a významů, (2) textové koreference a (3) tzv. bridging anafory, to vše na 50k větách korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the Prague Discourse Treebank
1.0, a collection of Czech texts annotated for
various discourse-related phenomena "beyond
the sentence boundary". The treebank contains
manual annotations of (1), discourse connectives,
their arguments and senses, (2), textual
coreference, and (3), bridging anaphora, all
carried out on 50k sentences of the treebank.
Contrary to most similar projects, the annotation
was performed directly on top of syntactic
trees (from the previous project of the Prague
Dependency Treebank 2.5), benefiting thus
from the linguistic information already existing
on the same data. In this article, we present
our theoretical background, describe the annotations
in detail, and offer evaluation numbers
and corpus statistics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce byl prezentován popis systémů strojového překladu mezi češtinou a ruštinou, a to statistických a pravidlových.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This talk described several Machine Translation systems between Czech and Russian that were developed or implemented by UFAL researchers.  We discuss which of the approaches - Statistical or Rule-Based is more suitable for MT between related morphologically rich languages. The focus is made on analyzing valency discrepancies between the two languages and their impact on MT.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku je popsana frekvence užití nefinitních konstrukcí s přechodníky a příčestí v češtině a ruštině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes one of the discrepancies between the languages that occur in non-finite clauses, namely in participial clauses and transgressives. We will concentrate mainly on
transgressives because they demonstrate greater discrepancies. The usage of Russian transgressives(gerunds) is very similar to that of
English, they are typical mostly of a written language rather than spoken, whereas in Czech they are nowadays archaic in any genres and generally not used at all. This discrepancy can pose some challenge for language learners as well as for translators or machine translation systems. We will present some examples of
Russian participial/gerund clauses and the respective translations into Czech from a parallel corpus of news and belletristic texts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Taxonomie strojového překladu
Online učení
Guided učení
Easy-First dekódování ve strojovém překladu
Guided učení ve strojovém překladu</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Machine Translation Taxonomy
Online Learning
Guided Learning
Easy-First Decoding in MT
Guided Learning in MT</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Treex je platforma pro nástroje zpracování přirozeného jazyka.
NIF je formát založený na RDF, který by měl sloužit k propojování takovýchto nástrojů.
Přednáška popisuje Treex a další nástroje a treebanky vyvíjené na ÚFAL. Zaměřuje se též na podobnosti a rozdíly s formátem NIF a iniciativou nlp2rdf.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Treex is a framework for NLP tools. NIF (NLP Interchange format) is an RDF/OWL-based format that aims to achieve interoperability between NLP tools, language resources and annotations. The presentation describes Treex and other tools/treebanks developed at ÚFAL. It focuses on its similarities and differences compared to NIF (and the NLP2RDF initiative).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>O parataktických syntaktických strukturách je známo, že jejich zachycení pomocí závislostních mechanismů je obtížné, což má bolestivé důsledky jako značné množství chyb v automatické syntaktické analýze, které souvisí s koordinacemi. Jinými slovy, koordinace je nevyřešeným problémem závislostní analýzy přirozených jazyků. Tento článek se snaží vnést trochu světla do této oblasti a přináší systematický přehled různých formálních prostředků vyvinutých k zachycení souřadných struktur. Přinášíme novou taxonomii takových přístupů a aplikujeme ji na treebanky z typologicky rozličné sady 26 jazyků. Dále prezentujeme empirická pozorování, pokud jde o vzájemnou převoditelnost mezi vybranými styly zachycení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Paratactic syntactic structures are notoriously
difficult to represent in dependency formalisms, which has painful consequences such as high frequency of parsing errors related to coordination. In other words, coordination is a pending problem in dependency analysis of natural languages. This paper tries to shed some light
on this area by bringing a systematizing view of various formal means developed for encoding coordination structures. We introduce a novel taxonomy of such approaches and apply it on treebanks across a typologically diverse range of 26 languages. Empirical observations on convertibility between selected styles of representations are shown too.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Praktický seminář představení platformu Treex, která slouží k integraci nástrojů pro zpracování přirozeného jazyka. Semináři předcházela přednáška o překladovém systému TectoMT vyvinutém v platformě Treex.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Hands-on tutorial on using Treex - a modular framework for Natural Language Processing. The tutorial was preceded by a presentation about TectoMT deep-syntactic MT system developed in Treex.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška představuje žákovský korpus češtiny a jeho anotační schéma sestavající ze propojených tří rovin. Každá rovina zachycuje a opravuje jiný typ chyb.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe a corpus of texts produced by non-native speakers of Czech. We discuss its annotation scheme, consisting of three interlinked levels to cope with a wide range of error types present in the input. Each level corrects different types of errors; links between the levels allow capturing errors in word order and complex discontinuous expressions. Errors are not only corrected, but also classified. The annotation scheme is tested on a doubly-annotated sample of approx. 10,000 words with fair inter-annotator agreement results. We also explore options of application of automated linguistic annotation tools (taggers, spell checkers and grammar checkers) on the learner text to support or even substitute manual annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje kompilaci a anotaci korpusu staré češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we describe our efforts to build a corpus of Old Czech. We report on tools, resources and methodologies used during the corpus development as well as discuss the corpus sources and structure, the tagset used, the approach to lemmatization, morphological analysis and tagging. Due to practical restrictions we adapt resources and tools developed for Modern Czech. However, some of the described challenges, such as the non-standardized spelling in early Czech and the form and lemma variability due to language change during the covered time-span, are unique and never arise when building synchronic corpora of Modern Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje nový způsob jak získat více morfologicky a syntakticky oanotovaných dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a new way to get more morphologically and syntactically annotated data. We have developed an annotation editor tailored to school children to involve them in text annotation. Using this editor, they practice morphology and dependency-based syntax in the same way as they normally do at (Czech) schools, without any special training. Their annotation is then automatically transformed into the target annotation schema. The editor is designed to be language independent, however the subsequent transformation is driven by the annotation framework we are heading for. In our case, the object language is Czech and the target annotation scheme corresponds to the Prague Dependency Treebank annotation framework.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku popisujeme Prague Markup Language (PML), generický a otevřený formát založený na XML vytvořený za účelem definice formátů lingvistických zdrojů, zvláště anotovaných korpusů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we describe the Prague Markup Language (PML), a generic and open XML-based format intended to define format of linguistic resources, mainly annotated corpora. We also provide an overview of existing tools supporting PML, including annotation editors, a corpus query system, software libraries, etc.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje žákovský korpus češtiny a jeho anotační schéma sestavající ze propojených tří rovin. Každá rovina zachycuje a opravuje jiný typ chyb.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes a corpus of texts produced by non-native speakers of Czech. We discuss its annotation scheme, consisting of three interlinked levels to cope with a wide range of error types present in the input. Each level corrects different types of errors; links between the levels allow capturing errors in word order and complex discontinuous expressions. Errors are not only corrected, but also classified. The annotation scheme is tested on a doubly-annotated sample of approx. 10,000 words with fair inter-annotator agreement results. We also explore options of application of automated linguistic annotation tools (taggers, spell checkers and grammar checkers) on the learner text to support or even substitute manual annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek shrnuje jednojazyčné přístupy k morfologické analýze a tagování nenáročné na zdroje.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article surveys resource-light monolingual approaches to morphological analysis and tagging. While supervised analyzers and taggers are very accurate, they are extremely expensive to create. Therefore, most of the world languages and dialects have no realistic prospect for morphological tools created in this way. The weakly-supervised approaches aim to minimize time, expertise and/or financial cost needed for their development. We discuss the algorithms and their performance considering issues such as accuracy, portability, development time and granularity of the output.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje, které typy volných slovesných doplnění vystupují jako obligatorní a jak často.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes which types of free verbal modifications may be obligatory and how often.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>The poster deals with the sentence information structure (functional sentence perspective) in Czech, focusing on the tendency of the particular sentence constituents to appear in Czech sentence as topic or focus.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Poster se týká aktuálního členění větného v češtině - prezentuje tendenci jednotlivých větných participantů objevovat se v české větě jako téma nebo réma.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Podle současných studií je okcipito-temporální ERP komponenta N170 levostranně modulována a posílena řetězci znaků, narozdíl od nelingvistických vstupů podobné vizuální komplexity. Tento výsledek je konzistentní s neurálními mechanismy, které zajišťují vizuální rozpoznávání slov. Avšak závěry o úrovních analýzy, které ovlivňují projevy N170, jsou nejednoznačné. V této studii jsme zkoumali časový průběh a kvalitu reakcí mozku na podněty předpokládané nižší a vyšší úrovně zpracování. Podněty nižší úrovně byly modulovány manipulací rotace písmen parametricky v 0 stupních, 22,5 stupních, 45 stupních, 67,5 stupních a 90 stupních. Vyšší úroveň zpracování byla modulována manipulací s lexikálnim statutem (slova vs. pseudoslova). Při zvyšování úhlu rotace písmen se amplituda N170 monotonicky zvyšovala až do úhlu 67,5, ale pak při úhlu 90 stupňů se opět snížila. Pseudoslova v porovnání se slovy zvětšila amplitudu N170 v levé okcipito-temporální oblasti. Tyto kombinované výsledky jsou konzistentní s kaskádovou, interaktivní architekturou, ve které nižší úroveň analýzy (např. rozpoznání tvaru slov) časově předchází vyšší úrovni analýzy (např. lexikální status), ale již v čase 170 ms reakce mozku na vizuální slovní podnět obsahuje paralelní, interaktivní zpracování na obou úrovních.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Recent studies report that the occipito-temporal N170 component of the ERP is enhanced by letter strings, relative to non-linguistic strings of similar visual complexity, with a left-lateralized distribution. This finding is consistent with underlying mechanisms that serve visual word recognition. Conclusions about the level of analysis reflected within the N170 effects, and therefore the timecourse of word recognition, have been mixed. Here, we investigated the timing and nature of brain responses to putatively low- and high-level processing difficulty. Low-level processing difficulty was modulated by manipulating letter-rotation parametrically at 0 degrees, 22.5 degrees, 45 degrees, 67.5 degrees, and 90 degrees. Higher-level processing difficulty was modulated by manipulating lexical status (words vs. word-like pseudowords). Increasing letter-rotation enhanced the N170 led to monotonic increases in P1 and N170 amplitude up to 67.5 degrees but then decreased amplitude at 90 degrees. Pseudowords enhanced the N170 over left occipital-temporal sites, relative to words. These combined findings are compatible with a cascaded, interactive architecture in which lower-level analysis (e.g., word-form feature extraction) leads higher-level analysis (e.g., lexical access) in time, but that by approximately 170 ms, the brain's response to a visual word includes parallel, interactive processing at both low-level feature extraction and higher-order lexical access levels of analysis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá vlastnostmi nejčastějších konektivních prostředků kauzálního vztahu v Pražském závislostním korpusu (konkrétně jde o výrazy protože, neboť, proto, takže, tak, tedy a totiž).  Některé vlastnosti těchto konektivních prostředků popisuje nezávisle na kontextu (např. pozici ve větě, kombinovatelnost s jinými spojovacími výrazy), u většiny vlastností si všímá i charakteristik frekvenčních (jak často spojují dané výrazy části textu v rámci jednoho větného celku, jak často přes jeho hranice, jak často se užívají ke spojování verbálních frází a jak často jinde apod.). Jeho cílem je ukázat, jaké vlastnosti tyto prostředky sdílejí a které vlastnosti jsou naopak pro jednotlivé prostředky specifické. Zároveň chce přispět k diskusi o slovnědruhové charakteristice zkoumaných výrazů – pozorované vlastnosti jsou vztaženy k charakteristikám uváděným ve vybrané odborné literatuře a jsou uvedena kritéria, podle kterých je možné zkoumané výrazy slovnědruhově popisovat. Jako materiálová základna výzkumu byla zvolena data uvolněná v Pražském závislostním korpusu pro lingvistický výzkum (43 955 vět (740 532 slov)), protože současná podoba značkování v tomto korpusu umožňuje měřit vlastnosti zkoumaných výrazů v rámci jednoho větného celku i přes jeho hranice.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This contribution deals with properties of most common connective means of the causal relation in the Prague Dependency Treebank (namely with expressions protože (because), neboť (because), proto (therefore), takže (so), tak (so), tedy (thus) and totiž (because, namely, sometimes untranslatable)). Some properties of these connectives are described independently of context (e.g. position in the sentence, compatibility with other connectives), the majority of properties is characterized also with regard to frequency (how often these expressions connect parts of one sentence and how often parts of text across sentence boundaries, how often they are used to connect verbal phrases and how often something else, etc.). Its general aim is to show which properties are shared by all these connectives (or by some of them) and which are, on the contrary, specific for each of them. This paper also wants to contribute to the discussion of the part-of-speech characteristics of these connectives – a relation of the observed properties of connectives to the characteristics reported in the relevant Czech literature is described and some criteria for their part-of-speech characteristics are stated. As the material base, data released in the Prague Dependency Treebank for linguistic research (43,955 sentences (740,532 words)) were chosen, because the current state of tagging in this corpus enables us to measure the properties of all connectives regardless of whether they connect parts of one sentence or parts of text across sentence boundaries.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek prezentuje analýzu nejčastějších anotátorských chyb v projektu anotace mezivýpovědních textových vztahů v PDT</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an analysis of the inter-annotator discrepancies of the Czech discourse annotation in the Prague Dependency Treebank 2.0. Having finished the annotation of the inter-sentential semantic discourse relations with explicit connectives in the treebank, we report now on the results of the evaluation of the parallel (double) annotations, which is an important step in the process of checking the quality of the data. After we shortly describe the annotation and the method of the inter-annotator agreement measurement, we present the results of the measurement and, most importantly, we classify and analyze the most common types of annotators’ disagreement.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje a vyhodnocuje proces poloautomatické anotace vnitrovětných textových vztahů v Pražském závislostním korpusu, což je část projektu jinak především manuální anotace všech (vnitro- i mezi-větných) diskurzních vztahů s explicitními konektory v tomto korpusu. Bohatá anotace korpusu nám umožnila ve většině případů automaticky najít vnitrovětné diskurzní vztahy, jejich konektory a argumenty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the present paper, we describe in detail and evaluate the process of semi-automatic annotation of intra-sentential discourse relations in the Prague Dependency Treebank, which is a part of the project of otherwise mostly manual annotation of all (intra- and inter-sentential) discourse relations with explicit connectives in the treebank. The rich annotation of the treebank allowed us to automatically detect the intra-sentential discourse relations, their connectives and arguments in most of the cases.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V posledních letech se výzkum v oblasti strojového překladu zaměřil na hybridní strojový překlad a kombinaci překladových systémů za účelem vylepšení překladové kvality. Jako první krok ke splnění tohoto cíle jsme vytvořili paralelní korpus vět přeložených několika různými systémy opatřenými různými metadaty a anotacemi získanými při překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In recent years, machine translation (MT) research has focused on investigating how hybrid machine translation as well as system
combination approaches can be designed so that the resulting hybrid translations show an improvement over the individual “component”
translations. As a ﬁrst step towards achieving this objective we have developed a parallel corpus with source text and the corresponding
translation output from a number of machine translation engines, annotated with metadata information, capturing aspects of the
translation process performed by the different MT systems. This corpus aims to serve as a basic resource for further research on
whether hybrid machine translation algorithms and system combination techniques can beneﬁt from additional (linguistically motivated,
decoding, and runtime) information provided by the different systems involved. In this paper, we describe the annotated corpus we have
created. We provide an overview on the component MT systems and the XLIFF-based annotation format we have developed. We also
report on ﬁrst experiments with the ML4HMT corpus data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato kapitola popisuje metody učení z dat vhodné pro vývoj systémů porozumění mluvené řeči pro dialogové systémy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This chapter described data-driven methods suitable for development of spoken language understanding components in dialogue systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku představím kroky vedoucí k spolehlivým klasifikátorům polarity založeným na českých datech. Popíšu metodologické základy anotování českých struktur a unigramové klasifikátory trénované na třech druzích dat. Rovněž zanalyzuji existující výsledky pro manuální i automatickou anotaci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this talk I present work towards building a reliable polarity classifiers based on Czech data. I will describe a method for annotating Czech evaluative structures and introduce a standard unigram-based Naive Bayes classifier together with the lexicon-based classifier, both trained on three different types of data. Also, I will analyze existing results for both manual and automatic annotation.

Joint work with Jan Hajič, jr. and Jana Šindlerová.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Reportáž z první mezinárodní konference v závislostní lingvistice spojená se zamyšlením nad jejím současným stavem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Report on the first international conference in dependency grammar and reflexion of its current state.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku je prezentována počáteční fáze výzkumu postojové analýzy v češtině: metoda anotování evaluativních struktur a první výsledky manuální i automatické anotace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents initial research in the area of sentiment analysis in Czech. We will describe a method for annotating Czech evaluative structures and analyze existing results for both manual and automatic annotation of the plain text data which represents the basis for further subjectivity clues learning.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek má dvojí cíl: jednak chceme prezentovat tu část anotačního schématu nedávno vydaného korpusu PCEDT2.0, která je spjata s anotací anglického zájmene “it” na tektogramatické rovině, jednak představujeme experimenty týkající se automatické identifikace anglického “it” a jeho českého protějšku. Navrhli jsme soubor pravidel pro stromové struktury, která v rámci anglické části korpusu kombinujeme se současnými statistickými systémy, což v důsledku vede ke zlepšení automatické detekce. Mimoto jsme také navrhli a úspěšně aplikovali pravidla, která využívají informace z paralelních českých struktur.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we have two goals. First, we want to present a part of the annotation scheme of the recently released Prague Czech-English Dependency Treebank 2.0 related to the annotation of personal pronoun it on the tectogrammatical layer of sentence representation. Second, we introduce experiments with the automatic identification of English personal pronoun it and its Czech counterpart. We design sets of tree-oriented rules and on the English side we combine them with the state-of-the-art statistical system that altogether results in an improvement of the identification. Furthermore, we design and successfully apply rules, which exploit information from the other language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje první kroky vedoucí ke spolehlivé automatické klasifikaci větné polarity v češtině. Objasňujeme metodu anotace českých evaluativních struktur a trénování základního bayesovského klasifikátoru na třech typech dat, včetně analýzy dosažených výsledků, z nichž některé jsou srovnatelné s výsledky, jichž bylo dosud docíleno v oblasti postojové analýzy jako takové (viz např. Cui, 2006).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the first steps towards reliable polarity classification based on Czech data. We describe a method for annotating Czech evaluative structures and build a standard unigram-based Naive Bayes classifier on three different types of annotated texts. Furthermore, we analyze existing results for both manual and automatic annotation, some of which are promising and close to the state-of-the-art performance, see Cui(2006).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nerízená závislostní analýza je alternativní zpusob urcování vztahu mezi slovy ve vete. Nepotrebuje žádný anotovaný závislostní korpus, je nezávislý na jazykové teorii a univerzální pro velké množství jazyku. Jeho nevýhodou je ale zatím relativne nízká úspešnost.
V této práci diskutujeme nekteré predchozí práce a predstavujeme novou metodu nerízenéhé analýzy.
Náš závislostní model se skládá ze ctyr podmodelu: (i) hranový model, který rídí rozdelení dvojic rídících a závislých clenu, (ii) model plodnosti, který rídí pocet clenu závislých na uzlu, (iii) model vzdálenosti, který rídí délku závislostních hran a (iv) model vypustitelnosti. Tento model je založen na
predpokladu, že slovau která se mohou z vety vypustit, aniž by se porušila její gramaticnost jsou v závislostním slove listy.
Odvození závislostních struktur provádíme pomocí Gibbsova vzorkovace. Predstavujeme vzorkovací
algoritmus, který zachovovává projektivitu závislostních stromu, cože je velmi užitecnou vlastností. V našich experimentech na 30 jazycích srovnáváme výsledky pro ruzné parametry modelu.
Naše metoda prekonávvá dríve publikované výsledky pro vetšinu zkoumaných jazyku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Unsupervised dependency parsing is an alternative approach to identifying relations
between words in a sentence. It does not require any annotated treebank, it
is independent of language theory and universal across languages. However, so far
quite low parsing quality is its main disadvantage.
This thesis discusses some previous works and introduces a novel approach to
unsupervised parsing. Our dependency model consists of four submodels: (i) edge
model, which controls the distribution of governor-dependent pairs, (ii) fertility
model, which controls the number of node's dependents, (iii) distance model, which
controls the length of the dependency edges, and (iv) reducibility model. The reducibility
model is based on a hypothesis that words that can be removed from a
sentence without violating its grammaticality are leaves in the dependency tree.
Induction of the dependency structures is done using Gibbs sampling method.
We introduce a sampling algorithm that keeps the dependency trees projective,
which is a very valuable constraint.
In our experiments across 30 languages, we discuss the results of various settings
of our models. Our method outperforms the previously reported results on a
majority of the test languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>The possibility of deleting a word from a sentence without violating its syntactic correctness belongs to traditionally known manifes
tations of syntactic dependency. We introduce
a novel unsupervised parsing approach that is
based on a new n-gram reducibility measure.
We perform experiments across 18 languages
available in CoNLL data and we show that
our approach achieves better accuracy for the
majority of the languages then previously reported results.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Predstavujeme novy pristup k nerizenemu zavislostnimu parsingu zalozenemu na mire vypustitelnosti jednotlivych sekvenci slov.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>This paper describes a system for unsuper-
vised dependency parsing based on Gibbs
sampling algorithm. The novel approach in-
troduces a fertility model and reducibility
model, which assumes that dependent words
can be removed from a sentence without vio-
lating its syntactic correctness.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Popis systemu nerizeneho zavislostniho parsingu zalozeneho na Gibbsove samplingu. Novy pristup predstavuje vlastnosti vypusttelnsti a fertility.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jedním z nejvýznamnějších nedávných vylepšení anglicko-českého překladače TectoMT je systematická a teoreticky podložená revize formémů. Formémy jsou anotací mofo-synaktických rysů plnovýznamových slov v hloubkově syntaktické závislostní syntaxi založené na tektogramatice. Naše úpravy měly za cíl snížit řídkost dat, zvýšit konzistenci anotace napříč jazyky a otevřít nové oblasti využití formémů. Formémy jsou využívány ve strojovém překladu i v dalších úlohách zpracování přirozeného jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>One of the most notable recent improvements of the TectoMT English-to-Czech translation is a systematic and theoretically supported revision of formemes—the annotation of morpho-syntactic features of content words in deep dependency syntactic structures based on the Prague tectogrammatics theory. Our modifications aim at reducing data sparsity, increasing consistency across languages and widening the usage area of this markup. Formemes can be used not only in MT, but in various other NLP tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V disertační práci podáváme návrh reprezentace lexikálně-sémantických konverzí
ve valenčním lexikonu. Lexikálně-sémantické konverze chápeme jako vztahy mezi
sémanticky blízkými syntaktickými konstrukcemi tvořenými odlišnými lexikálními
jednotkami lexematicky totožného slovesa. Tyto vztahy jsou spojeny se změnami
ve valenční struktuře slovesa, které vyplývají ze změn ve vzájemném přiřazení situačních
participantů a valenčních doplnění. Tyto změny mohou zasahovat počet, typ valenčních
doplnění, jejich obligatornost i morfematickou formu. Na základě sémantické a syntaktické
analýzy dvou typů lexikálně-sémantické konverze, lokativní konverze a konverze Nositel
děje-Místo, navrhujeme, aby lexikálním jednotkám vytvářejícím syntaktické varianty
ve vztahu lexikálně-sémantické konverze v datové části lexikonu odpovídaly odlišné
valenční rámce propojené atributem -conv, jehož hodnotou bude typ lexikálně-sémantické
konverze. Součástí pravidlové komponenty valenčního lexikonu pak budou pravidla
určující změny ve vzájemné korespondenci situačních participantů a valenčních doplnění.
Ačkoli reprezentaci lexikálně-sémantické konverze primárně navrhujeme pro účely popisu
valence českých sloves ve valenčním slovníku VALLEX, předpokládáme, že poznatky
uváděné v této práci mohou být využity při popisu tohoto typu vztahu i v dalších
lexikálních zdrojích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this thesis, we provide an adequate lexicographic representation of lexicalsemantic
conversion. Under the term lexical-semantic conversion, the relation between
semantically similar syntactic structures which are based on separate lexical units of the
same verb lexeme is understood. These relations are associated with various changes in
valency structure of verbs – they may involve a number of valency complementations, their
type, obligatoriness as well as morphemic forms. These changes arise from differences in
the mapping of situational participants onto valency complementations. On the basis of
semantic and syntactic analysis of two types of Czech lexical-semantic conversions, the
locative conversion and the conversion Bearer of action-Location, we propose to represent
lexical units creating syntactic variants in the relation of lexical semantic conversion by
separate valency frames stored in the data component of the lexicon. The special attribute
-conv whose value is a type of lexical-semantic conversion is assigned to relevant valency
frames. Then the rule component of the lexicon consists of general rules determining
changes in the correspondence between situational participants and valency
complementations. This proposal is primarily designed for the valency lexicon of Czech
verbs, VALLEX. However, we suppose that this representation can be applied in other
lexical resources as well.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku jsme se zabývali syntaktickými konstrukcemi typu Včely se hemží na zahradě –- Zahrada se hemží včelami, které chápeme jako
specifický typ lexikálně-sémantické konverze, u níž dochází ke změnám
v korespondenci dvou situačních participantů (s rolí Nositel děje a Místo)
a valenčních doplnění. Tyto změny mají za následek změny v povrchově syntaktické realizaci daných participantů, které zasahují prominentní pozici subjektu,
v níž je jednou vyjádřen Nositel děje (Včely se hemží na zahradě) a podruhé Místo (Zahrada se hemží včelami). Zatímco prvně zmíněná syntaktická varianta vyjadřuje neakční elementární děj, druhá varianta označuje procesuálně pojatý
stav. Předpokládáme potom, že holistická interpretace participantu Místo
ve druhé syntaktické variantě vyplývá přímo ze stavového významu dané konstrukce. Dále příspěvek pojednává o bezpomětových konstrukcích a konstrukcích
s formálním subjektem to, které vytvářejí formální paralelu k probíraným konstrukcím. Na závěr předkládáme možnou lexikografickou reprezentaci lexikálně-sémantické konverze daného typu, v níž informaci přerozdělujeme mezi
datovou a pravidlovou část lexikonu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we focus on the relation between Czech syntactic constructions Bees are swarming in the garden –- The garden is swarming with bees. We refer to
this relation as a lexical-semantic conversion. This lexical-semantic conversion is associated with changes in the correspondence between two situational participants –- Bearer
of action and Location –- and valency complementations. These changes result in
a permutation of these participants which affects the prominent surface syntactic position
of subject. We demonstrate that whereas the first syntactic variant refers to a simple action, the latter expresses rather a state arising from this simple action. In the latter case,
the participant Location is usually holistically interpreted. We assume that the holistic ef-
fect of this participant follows from the fact that a state is generally attributed to
each participant expressed in the subject position as a whole. Furthermore, syntactic variants with formal subject “to” and subjectless constructions (usually conceived as formal
paraphrases of the pairs of the observed constructions) are discussed. Finally,
a lexicographic representation of these syntactic structures is presented</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Termínem gramatické alternace označujeme změny ve valenční stuktuře sloves, které odpovídají rozdílným povrchově syntaktickým strukturám tvořeným jedinou lexikální jednotkou slovesa. České gramatikalizované alternace jsou vyjadřovány buď (i) morfologickými prostředky (diateze), nebo (ii) prostředky převážně syntaktickými (reciprocita). Změny ve valenčním rámci slovesa jsou v tomto případě omezeny na změny v morfematické realizaci valenčních doplnění, které jsou natolik pravidelné, že mohou být zachyceny formálními syntaktickými pravidly.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Under the term grammaticalized alternations, we understand changes in valency frames of verbs corresponding to different surface syntactic structures of the same lexical unit of a verb. Czech grammaticalized alternations are expressed either (i) by morphological means (diatheses),
or (ii) by syntactic means (reciprocity). These changes are limited to changes in morphemic form(s) of valency complementations; moreover,
they are regular enough to be captured by formal syntactic rules.
In this paper a representation of Czech grammaticalized alternations and their possible combination is proposed for the valency lexicon of Czech verbs, VALLEX.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku popisujeme projekt zaměřený na obohacení valenčního lexikonu českých sloves, VALLEX, o sémantické informace z FrameNetu. V tomto projektu bylo celkem osm skupin českých sloves anotováno sémantickými rámci z FrameNetu. Na základě sémantických rámců z horních pater relace dědičnosti byla tato slovesa rozdělena do koherentních sémantických tříd. Valenčním doplněním daných sloves byly dále přiděleny elementy sémantických rámců z horních pater relace dědičnosti jako sémantické role.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this article, we introduce a project aimed at enhancing a valency lexicon of Czech verbs
with semantic information. For this purpose, we make use of FrameNet, a semantically ori-
ented lexical resource. At the present stage, semantic frames from FrameNet have been mapped
to eight groups of verbs with various semantic and syntactic properties. The feasibility of this
task has been verified by the achieved inter-annotator agreement measured on two semantically
and syntactically different groups of verbs –- verbs of communication and exchange (85.9% and
78.5%, respectively). Based on the upper level semantic frames from the relation of ‘Inheritance’
built in FrameNet, the verbs of these eight groups have been classified into more coherent se-
mantic classes. Moreover, frame elements from these upper level semantic frames have been
assigned to valency complementations of the verbs of the listed groups as semantic roles. As
in case of semantic frames, the achieved interannotator agreement concerning assigning frame
elements measured on verbs of communication and exchange has been promising (95.6% and
91.2%, respectively).


As a result, 1 270 lexical units pertaining to the verbs of communication, mental action,
psych verbs, social interaction, verbs of exchange, motion, transport and location (2 129 Czech
verbs in total if perfective and imperfective verbs being counted separately) have been classified
into syntactically and semantically coherent classes and their valency complementations have
been characterized by semantic roles adopted from the FrameNet lexical database.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku přinášíme popis alternačního modelu valenčního lexikonu českých sloves, VALLEX. V práci rozlišujeme celkem dva základní typy alternací (změn ve valenční struktuře sloves): (i) gramatikalizované a (ii) lexikalizované alternace. Jak gramatikalizované, tak lexikalizované alternace mohou být konverzní či nekonverzní povahy. Zatímco gramatikalizované alternace spojují rozdílné povrchové strukturace téže lexikální jednotky, lexikalizované alternace charakterizují syntaktické struktury tvořené rozdílnými lexikálními jednotkami. Pro účely reprezentace alternací rozdělujeme lexikon na datovou a pravidlovou část. V datové části je každá lexikální jednotka reprezentována jedním valenčním rámcem. Pravidlová část obsahuje dva typy pravidel: (i) formální syntaktická pravidla zachycující gramatikalizované alternace a (ii) obecná pravidla určující změny v korespondenci participantů a valenčních doplnění typické pro lexikalizované alternace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, alternation based model of the valency lexicon of Czech verbs, VALLEX, is described.  Two types of alternations (changes in valency frames of verbs) are distinguished on the basis of used linguistic means: (i) grammaticalized alternations and (ii) lexicalized alternations. Both grammaticalized and lexicalized alternations are either conversive, or non-conversive. While grammaticalized alternations relate different surface syntactic structures of a single lexical unit of a verb, lexicalized alternations relate separate lexical units. For the purpose of the representation of alternations, we divide the lexicon into data and rule components. In the data part, each lexical unit is characterized by a single valency frame and by applicable alternations. In the rule part, two types of rules are contained: (i) syntactic rules describing grammaticalized alternations and (ii) general rules determining changes in the linking of situational participants with valency complementations typical of lexicalized alternations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato kapitola prezentuje současné poznatky v oblasti vývoje statistických dialogových systému založených na POMDP modelu řízení komunikace s uživatelem. Postupně jsou vysvětleny základy POMDP a vhodné metody pro implementaci v oblasti dialogových systémů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This chapter presents current knowledge about statistical dialogue systems based on POMDP model of interaction with human users. The chapter introduces basic concepts of POMDP and suggest appropriate methods for implementation it the context of dialogue systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme univerzální nástroj pro segmentaci a tokenizaci textů, který uživateli dovoluje nadefinovat potenciální hranice vět a slov a na základě trénovacích dat se naučí hranice hledat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a universal data-driven tool for segmenting and tokenizing text. The presented
tokenizer lets the user define where token and sentence boundaries should be considered.
These instances are then judged by a classifier which is trained from provided tokenized data.
The features passed to the classifier are also defined by the user making, e.g., the inclusion
of abbreviation lists trivial. This level of customizability makes the tokenizer a versatile tool
which we show is capable of sentence detection in English text as well as word segmentation
in Chinese text. In the case of English sentence detection, the system outperforms previous
methods. The software is available as an open-source project on GitHub</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příprava trénovacích dat je pracná a představuje hlavní překážku ve vývoji zpracování přirozeného jazyka (NLP). Mezi hlavní aplikace NLP patří strojový překlad, který se v současnosti opírá o dostupnost velkého množství dat. Sestavování těchto dat je finančně náročné a současně náchylné k chybám. Nově se objevující technologie jako sociální sítě a "seriózní" hry nabízejí jedinečnou příležitost změnit způsob přípravy trénovacích dat. Hry s účelem byly zkonstruovány pro větnou segmentaci, značkování obrázků a rozpoznávání koreference. Tyto hry fungují na třech úrovních: poskytují zábavu hráčům, hráči se při nich učí a současně poskytují data pro výzkum. Většina těchto systémů se potýká s nedostatkem účastníků. V tomto článku předkládáme sadu lingvisticky orientovaných her zaměřených na sestrojení paralelního korpusu pro několik jazyků a umožňujících hráčům zlepšení jejich slovní zásoby v těchto jazycích. První zveřejněná verze GlobeOtter je dostupná na Facebooku. Jedním z cílů je zde získat dostatečné množství hráčů i mimo řady lingvistů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Building training data is labor-intensive and presents a major obstacle to the advancement of Natural Language Processing (NLP)
systems. A prime use of NLP technologies has been toward the construction machine translation systems. The most common form of
machine translation systems are phrase based systems that require extensive training data. Building this training data is both expensive
and error prone. Emerging technologies, such as social networks and serious games, offer a unique opportunity to change how we
construct training data. These serious games, or games with a purpose, have been constructed for sentence segmentation, image labeling,
and co-reference resolution. These games work on three levels: They provide entertainment to the players, the reinforce information
the player might be learning, and they provide data to researchers. Most of these systems while well intended and well developed, have
lacked participation.
We present, a set of linguistically based games that aim to construct parallel corpora for a multitude of languages and allow players to
start learning and improving their own vocabulary in these languages. As of the first release of the games, GlobeOtter is available on
Facebook as a social network game. The release of this game is meant to change the default position in the field, from creating games
that only linguists play, to releasing linguistic games on a platform that has a natural user base and ability to grow.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek ukazuje závislostní analýzu s využitím kombinace parseru a self-trainingu pro jazyky s malým množstvím jazykových dat. Ověřili jsme, že pro jazyk s malým množstvím dat je využití ladicích dat pro meta-klasifikátor efektivnější než jejich přidání do zbývajících trénovacích dat jednotlivých analyzátorů. Tento mete-klasifikátor vytváří kombinovaný závislostní parse a zvyšuje úspěšnost analýzy v průměru o 4.92% a o 1.99% ve srovnání s jednotlivým nejlepším systémem. Meta-klasifikátor se může přizpůsobit rostoucím dostupným datům. Využitím self-trainingu společně s kombinací několika parseru vzniká další zlepšení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We also show ensemble dependency parsing and self training approaches applicable to under-resourced languages using our manually annotated dependency structures. We show that for an under-resourced language, the use of tuning data for a meta classifier is more effective than using it as additional training data for individual parsers. This meta-classifier creates an ensemble dependency parser and increases the dependency accuracy by 4.92% on average and 1.99% over the best individual models on average. As the data sizes grow for the the under-resourced language a meta classifier can easily adapt. To the best of our knowledge this is the first full implementation of a dependency parser for Indonesian. Using self-training in combination with our Ensemble SVM Parser we show additional improvement. Using this parsing model we plan on expanding the size of the corpus by using a semi-supervised approach by applying the parser and correcting the errors, reducing the amount of annotation time needed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Fokus hodně závislostní analýzy se na vytváření nových modelovacích technik a zkoumání nových funkcí sady pro stávající závislost modelů. Často jsou tyto nové modely mají to štěstí, aby dosahovaly rovnocenných výsledků s aktuálním stavem
že umělecké výsledky a často vedou hůř. Tyto přístupy jsou pro jazyky, které jsou často zdrojem bohaté a mají dostatek tréninková data k dispozici pro závislost rozebrat. Z tohoto důvodu, přesnost výsledky jsou často dosti vysoká. To, ze své podstaty, je to docela obtížné vytvořit výrazně velký nárůst v současném state-of-the-art. Výzkum v této oblasti se často zabývá malými změnami přesnosti nebo velmi specifickým lokalizovaných změn, jako je zvýšení přesnosti a zejména jazykovou konstrukci. S tolika technik modelování jsou k dispozici pro jazyky s velkým zdroji problém existuje o tom, jak využít stávající techniky s použitím kombinace, nebo souborem, techniky spolu s tímto množstvím dat.

Závislost analyzátory jsou téměř všude se vyskytující hodnoceny na jejich přesnost výsledků, tyto výsledky nemluvě o složitosti a užitečnosti výsledných struktur. Tyto struktury mohou mít větší složitost vzhledem k hloubce jejich co-
koordinační nebo podstatné jméno věty. Jako závislost analyzuje jsou základní struktury, ve kterých jsou ostatní systémy postavené na, by se zdálo rozumné posouzení těchto parserů dolů potrubí NLP. Typy parsování chyb, které způsobují významné
problémy v jiných aplikacích NLP je v současné době znám.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The focus of much of dependency parsing is on creating new modeling techniques and examining new feature sets for existing dependency models. Often these new models are lucky to achieve equivalent results with the current state of
the art results and often perform worse. These approaches are for languages that are often resource-rich and have ample training data available for dependency parsing. For this reason, the accuracy scores are often quite high. This, by its very nature, makes it quite difficult to create a significantly large increase in the current state-of-the-art. Research in this area is often concerned with small accuracy changes or very specific localized changes, such as increasing accuracy of a particular linguistic construction. With so many modeling techniques available to languages with large resources the problem exists on how to exploit the current techniques with the use of combination, or ensemble, techniques along with this plethora of data.

Dependency parsers are almost ubiquitously evaluated on their accuracy scores, these scores say nothing of the complexity and usefulness of the resulting structures. The structures may have more complexity due to the depth of their co-
ordination or noun phrases. As dependency parses are basic structures in which other systems are built upon, it would seem more reasonable to judge these parsers down the NLP pipeline. The types of parsing errors that cause significant
problems in other NLP applications is currently an unknown.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V posledních letech dosáhla závislostní syntaktická analýza podstatného zlepšení, zejména pro angličtnu. Existuje několik závislostních analyzátorů, které dosahují srovnatelné úspěšnosti, přičemž produkují různé typy chyb. Tento článek se zabývá vytvářením nové závislostní struktury složením výstupů různých analyzátorů. Všechny výstupy sloučíme do váženého grafu, který je vstupem pro kombinační systém založený na algoritmu 
minimální kostry orientovaného grafu.

Výsledná kombinace založená na pěti různých analyzátorech dosahuje úspěšnosti 92,58 %, čímž překonává všechny jednotlivé analyzátory.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Dependency parsing has made many advancements in recent years, in particular for English. There are a few dependency parsers that achieve comparable accuracy scores with each other but with very different types of errors. This paper examines creating a new dependency structure through ensemble learning using a hybrid of the outputs of various parsers. We combine all tree outputs into a weighted edge graph, using 4 weighting mechanisms. The weighted edge graph is the input into our ensemble system and is a hybrid of very different parsing techniques (constituent parsers, transition-based dependency parsers, and a graph-based parser). From this graph we take a maximum spanning tree. We examine the new dependency structure in terms of accuracy and errors on individual part-of-speech values.

The results indicate that using a greater number of more varied parsers will improve accuracy results. The combined ensemble system, using 5 parsers based on 3 different parsing techniques, achieves an accuracy score of 92.58%, beating all single parsers on the Wall Street Journal section 23 test set. Additionally, the ensemble system reduces the average relative error on selected POS tags by 9.82%.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Závislostní syntaktická analýza pomáhá některým aplikacím z oblasti zpracování přirozeného jazyka dosáhnout vyšší úspěšnosti, zejména pokud jde o jazyky s relativně volným slovosledem. Pro morfologicky bohaté jazyky typicky existuje jen malé množství trénovacích dat, přičemž kvůli větší velikosti slovníku by jich naopak bylo potřeba více. Tento článek se zabývá novými přístupy pro analýzu morfologicky bohatých jazyků s malým množstvím dat.

Testovacím jazykem je v našich experimentech tamilština. Vytvořili jsme 9 modelů pro závislostní syntaktickou analýzu, které byly natrénovány na malém množství dat. S využitím těchto modelů jsme natrénovali klasifikátor SVM, který jako rysy používá pouze informaci o shodě jednotlivých analyzátorů, díky čemuž lze tento přístup považovat za jazykově nezávislý.

Experimentálně jsme prokázali statisticky signifikantní zlepšení 5,44 % oproti průměrnému modelu a statisticky signifikantní zlepšení 0,52 % oproti nejlepšímu jednotlivému systému.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Dependency parsing has been shown to improve NLP systems in certain languages and in many cases helps achieve state of the art results in NLP applications, in particular applications for free word order languages. Morphologically rich languages are often short on training data or require much higher amounts of training data due to the increased size of their lexicon. This paper examines a new approach for addressing morphologically rich languages with little training data to start.  

Using Tamil as our test language, we create 9 dependency parse models with a limited amount of training data. Using these models we train an SVM classifier using only the model agreements as features. We use this SVM classifier on an edge by edge decision to form an ensemble parse tree. Using only model agreements as features allows this method to remain language independent and applicable to a wide range of morphologically rich languages.

We show a statistically significant 5.44% improvement over the average dependency model and a statistically significant 0.52% improvement over the best individual system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Segmentácia na tématicky koherentné úseky je dôležitou súčasťou vyhľadávania informácií. Vhodná segmentácia môže zlepšiť výsledky vyhľadávania a pomôcť užívateľom pri rýchlejšom hľadaní relevantného úseku. Segmentácia je zvlášť dôležitá pri audiovizuálnych nahrávkach, v ktorých je navigácia zvlášť zložitá. V článku popisujeme niekoľko prístupov ku tématickej segmentácii, založených na textovej, zvukovej a vizuálnej informácii. V článku je tiež prezentovaný náš návrh prístupu k tématickej segmentácii, založený na fúzii zvukových a vizuálnych dát.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Segmentation into topically coherent segments is one of the crucial points in information retrieval (IR). Suitable segmentation may improve the results of IR system and help users to find relevant passages faster. Segmentation is especially important in audiovisual recordings, in which the navigation is difficult. We present several methods used for topic segmentation, based on textual, audio and visual information. The proposition of our approach to topic segmentation based on the fusion of audio and visual data is presented in the article.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Česko-slovenský paralelný korpus, vytvorený z voľne dostupných zdrojov. Korpus je prístupný v textovom formáte a s morfologickou anotáciou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Czech-Slovak parallel corpus consisting of several freely available corpora. Corpus is given in both plaintext format and with an automatic morphological annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Anglicko-slovenský paralelný korpus, vytvorený z voľne dostupných zdrojov. Korpus je prístupný v textovom formáte a s morfologickou anotáciou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>English-Slovak parallel corpus consisting of several freely available corpora. Corpus is given in both in plaintext format and with an automatic morphological annotation</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Množstvo trénovacích dát je pre kvalitu štatistického strojového prekladu rozhodujúce. V článku popisujeme, akým spôsobom je možné zlepšiť kvalitu prekladu pre daný jazykový pár pomocou využitia paralelných dát v príbuznom jazyku. Konkrétne sme vylepšili en→sk preklad pomocou využitia veľkého česko-anglického paralelného korpusu a cs→sk prekladového systému založeného na pravidlách. Preskúmaných je niekoľko možností konfigurácie použitých systémov.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The amount of training data in statistical machine translation is critical for translation quality. In this paper, we demonstrate how to increase translation quality for one language pair by bringing in parallel data from a closely related language. In particular, we improve en→sk translation using a large Czech–English
parallel corpus and a shallow (rule-based) MT system for cs→sk. Several setup options are explored in order to identify the best possible configuration.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Automatický preklad z češtiny do slovenčiny pre prvých 50 viet testovacej sady WMT 2010 pomocou piatich prekladových systémov (Česílko, Česílko 2, Google Translate a Moses s rôznymi nastaveniami). V prekladoch boli ručne označené chyby, ktoré boli ďalej klasifikované podľa schémy danej v článku (David Vilar, Jia Xu, Luis Fernando D’Haro, Hermann Ney: Error Analysis of Statistical Machine Translation Output, Proceedings of LREC-2006, 2006).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Outputs of five Czech-Slovak machine translation systems  (Česílko, Česílko 2, Google Translate and Moses with different settings) for first 50 sentences of WMT 2010 testing set. The translations were manually processed and the errors were marked and classified according to the scheme by Vilar et al. (David Vilar, Jia Xu, Luis Fernando D’Haro, Hermann Ney: Error Analysis of Statistical Machine Translation Output, Proceedings of LREC-2006, 2006)</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Automatický preklad z angličtiny do slovenčiny 50 viet náhodne vybraných z testovacej sady WMT 2011 pomocou troch prekladových systémov. V prekladoch boli ručne označené chyby, ktoré boli ďalej klasifikované podľa schémy danej v článku (David Vilar, Jia Xu, Luis Fernando D’Haro, Hermann Ney: Error Analysis of Statistical Machine Translation Output, Proceedings of LREC-2006, 2006).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Outputs of three English-Slovak machine translation systems for 50 sentences randomly selected from WMT 2011 test set. The translations were manually processed and the errors were marked and classified according to the scheme by Vilar et al. (David Vilar, Jia Xu, Luis Fernando D’Haro, Hermann Ney: Error Analysis of Statistical Machine Translation Output, Proceedings of LREC-2006, 2006)</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Testovacia sada z workshopu WMT 2011 manuálne preložená z češtiny a angličtiny do slovenčiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>WMT 2011 Workshop testing set manually translated from Czech and English into Slovak.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článok popisuje postup, ktorý sme použili v úlohe Search and Hyperlinking  v MediaEval 2012 Multimedia Benchmark. Pri riešení bol použitý systém Terrier, ktorý bol aplikovaný na automatické prepisy video nahrávok. Nahrávky boli segmentované na menšie časti, ktoré boli ďalej prehľadané. Pri segmentácii boli použité dva prístupy: jeden založený na pravidelnej segmentácii podľa času a druhý založený na sémantickej segmentácii pomocou algoritmu TextTiling. Najlepší výsledok bol dosiahnutý pomocou pravidelnej segmentácie, pomocou modelov Hiemstra a TF-IDF na prepisoch LIMSI.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes the Charles University setup used in the Search and Hyperlinking task of the MediaEval 2012 Multimedia Benchmark. We applied the Terrier retrieval system to the automatic transcriptions of the video recordings segmented into shorter parts and searched for those relevant to given queries. Two strategies were applied for segmentation of the recordings: one based on regular segmentation according to time and the second based on semantic segmentation by the TextTiling algorithm. The best results were achieved by the Hiemstra and TF-IDF models on the LIMSI transcripts and various segmentation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článok sa zaoberá vyhodnocovaním vyhľadávania v nesegmentovanej hovorenej reči. Zameriavame sa na metriku Mean Generalized Average precision, ktorá sa vyhodnocovanie vyhľadávania v reči často využíva. Táto metrika je navrhnutá tak, aby tolerovala určitú odchýlku medzi automaticky vyhľadanými výsledkami (začiatočnými bodmi relevantných segmentov) a  ručne označenými výsledkami. Na tento účel používa metrika penalizačnú funkciu, ktorá určuje kvalitu vyhľadaných úsekov na základe ich vzdialenosti ku najbližšiemu začiatku ručne označeného relevantného úseku. Výber penalizačnej funkcie však nemusí zodpovedať kvalite vyhľadávania z pohľadu užívateľa. V článku popisujeme užívateľskú štúdiu, ktorou sme skúmali spokojnosť užívateľa pri vyhľadávaní informácií. Na základe výsledkov sme navrhli optimálnu penalizačnú funkciu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper deals with evaluation of information retrieval from unsegmented speech. We focus on Mean Generalized Average Precision, the evaluation measure widely used for unsegmented speech retrieval. This measure is designed to allow certain tolerance in matching
retrieval results (starting points of relevant segments) against a gold standard relevance assessment. It employs a Penalty Function which evaluates non-exact matches in the retrieval results based on their distance from the beginnings of their nearest true relevant segments. However, the choice of the Penalty Function is usually ad-hoc and does not necessary reﬂect users’ perception of the speech retrieval quality. We perform a lab test to study satisfaction of users of a speech retrieval system to empirically estimate the optimal shape of the Penalty Function.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>(Not yet available. English version repeated)
In this paper, we study the effect of incorporating morphological information on an Indonesian (id) to English (en) Statistical Machine Translation (SMT) system as part of a preprocessing module. The linguistic phenomenon that is being addressed here is Indonesian cliticized words. The approach is to transform the text by separating the correct clitics from a cliticized word to simplify the word alignment. We also study the effect of applying the preprocessing on different SMT systems trained on different kinds of text, such as spoken language text. The system is built using the state-of-the-art SMT tool, MOSES. The Indonesian morphological information is provided by MorphInd. Overall the preprocessing improves the translation quality, especially for the Indonesian spoken language text, where it gains 1.78 BLEU score points of increase.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we study the effect of incorporating morphological information on an Indonesian (id) to English (en) Statistical Machine Translation (SMT) system as part of a preprocessing module. The linguistic phenomenon that is being addressed here is Indonesian cliticized words. The approach is to transform the text by separating the correct clitics from a cliticized word to simplify the word alignment. We also study the effect of applying the preprocessing on different SMT systems trained on different kinds of text, such as spoken language text. The system is built using the state-of-the-art SMT tool, MOSES. The Indonesian morphological information is provided by MorphInd. Overall the preprocessing improves the translation quality, especially for the Indonesian spoken language text, where it gains 1.78 BLEU score points of increase.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>This paper describes the creation process of an Indonesian-English parallel corpus (IDENTIC). The corpus contains 45,000 sentences collected from different sources in different genres. Several manual text preprocessing tasks, such as alignment and spelling correction, are applied to the corpus to assure its quality. We also apply language specific text processing such as tokenization on both sides and clitic normalization on the Indonesian side. The corpus is available in two different formats: ‘plain’, stored in text format and ‘morphologically enriched’, stored in CoNLL format. Some parts of the corpus are publicly available at the IDENTIC homepage</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the creation process of an Indonesian-English parallel corpus (IDENTIC). The corpus contains 45,000 sentences collected from different sources in different genres. Several manual text preprocessing tasks, such as alignment and spelling correction, are applied to the corpus to assure its quality. We also apply language specific text processing such as tokenization on both sides and clitic normalization on the Indonesian side. The corpus is available in two different formats: ‘plain’, stored in text format and ‘morphologically enriched’, stored in CoNLL format. Some parts of the corpus are publicly available at the IDENTIC homepage.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>(Not yet available. English version repeated)
This paper presents a method to improve a word alignment model in a phrase-based Statistical Machine Translation system for a low resourced language using a string similarity approach. Our method captures similar words that can be seen as semi-monolingual across languages, such as numbers, named entities, and adapted/loan words. We use several string similarity metrics to measure the monolinguality of the words, such as Longest Common Subsequence Ratio (LCSR), Minimum Edit Distance Ratio (MEDR), and we also use a modified BLEU Score (modBLEU).
Our approach is to add intersecting alignment points for word pairs that are orthographically similar, before applying a word alignment heuristic, to generate a better word alignment.
We demonstrate this approach on Indonesian-to-English translation task, where the languages share many similar words that are poorly aligned given a limited training data.
This approach gives a statistically significant improvement by up to 0.66 in terms of BLEU score.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a method to improve a word alignment model in a phrase-based Statistical Machine Translation system for a low resourced language using a string similarity approach. Our method captures similar words that can be seen as semi-monolingual across languages, such as numbers, named entities, and adapted/loan words. We use several string similarity metrics to measure the monolinguality of the words, such as Longest Common Subsequence Ratio (LCSR), Minimum Edit Distance Ratio (MEDR), and we also use a modified BLEU Score (modBLEU).
Our approach is to add intersecting alignment points for word pairs that are orthographically similar, before applying a word alignment heuristic, to generate a better word alignment.
We demonstrate this approach on Indonesian-to-English translation task, where the languages share many similar words that are poorly aligned given a limited training data.
This approach gives a statistically significant improvement by up to 0.66 in terms of BLEU score.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>(Not yet available. English version repeated)
This paper describes a work on preparing an Indonesian-English Statistical Machine Translation (SMT) System. It includes the creation of Indonesian morphological analyzer, MorphInd, and the composing of an Indonesian-English parallel corpus, IDENTIC. We build an SMT system using the state-of-the-art phrase-based SMT system, MOSES. We show several scenarios where the morphological tool is used to incorporate morphological information in the SMT system trained with the composed parallel corpus.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes a work on preparing an Indonesian-English Statistical Machine Translation (SMT) System. It includes the creation of Indonesian morphological analyzer, MorphInd, and the composing of an Indonesian-English parallel corpus, IDENTIC. We build an SMT system using the state-of-the-art phrase-based SMT system, MOSES. We show several scenarios where the morphological tool is used to incorporate morphological information in the SMT system trained with the composed parallel corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>In this paper, we focus on improving part-of-speech (POS) tagging for Urdu by using existing tools and data for the language. In our experiments, we use Humayoun’s morphological
analyzer, the POS tagging module of an Urdu Shallow Parser and our own SVM Tool tagger trained on CRULP manually annotated data. We convert the output of the taggers
to a common format and more importantly unify their tagsets. On an independent test
set, our tagger outperforms the other tools by far. We gain some further improvement
by implementing a voting strategy that allows us to consider not only our tagger but also
include suggestions by the other tools. The ﬁnal tagger reaches the accuracy of 87.98%.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>V článku popisujeme, jak jsme zlepšili značkování slovních druhů pro urdštinu pomocí kombinace dostupných taggerů a dostupných ručně značkovaných dat. V prvním kroku sjednocujeme sady značek užívané v jednotlivých zdrojích. Dále náš vlastní tagger natrénovaný na dostupných datech funguje výrazně lépe než dostupné nástroje. A konečně tento výsledek je možné ještě mírně zlepšit za použití návrhů od ostatních taggerů.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Svědectví pamětníka je nezastupitelným a velice hodnotným doplňkem běžné výuky dějepisu a dalších předmětů na 2. stupni ZŠ i na různých typech středních škol. Svědectví umožňuje personalizaci „velké historie“, žáci a studenti se s konkrétním člověkem a jeho prožitky mohou lépe identifikovat, do vzdělávacího procesu vstupují emoce, což prokazatelně zvyšuje jeho účinnost. Nevyhnutelně se však blíží doba, kdy už nebude možné zvát pamětníky přímo do škol a dalších institucí – už dnes zbývá jen velice málo lidí, kteří v dospělém věku prožili 2. světovou válku a události s ní spjaté. Je tedy třeba zjišťovat možnosti a metody alternativních postupů, které mohou alespoň částečně přítomnost živého pamětníka ve třídě v budoucnu nahradit. Archiv vizuální historie Institutu USC Shoah Foundation je unikátní sbírkou takřka 52 000 audiovi­zuálních záznamů rozhovorů s pamětníky a přeživšími holocaustu, které byly natočeny během 2. poloviny 90. let v 56 zemích a 32 jazycích (v češtině a slovenštině je přes 1 000 rozhovorů). Archiv je možno prohledávat pomocí propracovaného uživatelského rozhraní: uživatelé mohou najít konkrétní úseky svědectví podle svého zájmu díky tezauru 55 000 hierarchicky uspořádaných klíčových slov, témat, událostí, jmen osob, míst atd. Praha je jedním z pěti evropských měst, kde se lze k obsahu licencovaného archivu připojit, a to z počítačů v Centru vizuální historie Malach při Matematicko-fyzikální fakultě Univerzity Karlovy. Také práce s podobným on-line archivem v reálném čase je dalším specifickým vzdělávacím postupem, který u žáků rozvíjí samostatnost, myšlení v souvislostech a kritický přístup k pramenům a informačním zdrojům. Za tímto účelem vznikl také nový edukační portál IWitness, který bude v příspěvku rovněž představen.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Witness testimonies are unsubstitutable and very important addition to the classic lessons of history and other subjects in primary and secondary education. The testimony allows a personalization of the „great history“, students can easily identify with a specific person and his/her experience, and it brings emotions into the educational process, which demonstrably improves its efficiency. However, the day will come, that there will be no more witnesses alive for certain historical events. It is thus necessary to explore and create new alternatives, that could – at least partially – substitute the presence of an eye-witness in the future classroom. The USC Shoah Foundation Institute Visual History Archive is a unique collection of almost 52 000 testimonies of the Holocaust survivors and witnesses, conducted in 56 countries and 32 languages during the late 90's (over 1 000 interviews are in Czech and Slovak language). The Visual History Archive provides complex tools for users to identify whole testimonies of relevance, as well as specific segments within testimonies that relate to their area of interest. Prague is currently one of five European cities with full access to the on-line licensed archive content from the Malach Visual History Centre at the Faculty of Mathematics and Physics of the Charles University. Real-time work with the archive is a valuable part of educational process as well, developing student's inde­pendence and critical thinking in context. This is a main purpose of the new educational portal IWitness, which is also going to be presented at the conference.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Khresmoi je integrační projekt financovaný Evropskou Unií, který se zabývá multilingválním a multimodálním vyhledávání v medicínských datech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Khresmoi is a European Integrated Project developing a multilingual multimodal search and access system for medical and health information and documents. It addresses the challenges of searching through huge amounts of medical data, including general medical information available on the internet, as well as radiology data in hospital archives. It is developing novel semantic search and visual search techniques for the medical domain. At the MIE Village of the
Future, Khresmoi proposes to have two interactive demonstrations of the system under development, as well as an overview oral presentation and potentially some poster presentations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Systémy pro detekci a korekci pravopisných chyb jsou obvykle založeny na třech komponentách: slovníku, chybovém modelu a jazykovém modelu. Zatímco většina ostatních prací se zaměřuje především na jazykový model, v této práci se ukazujeme, že vylepšení jakékoliv z těchto komponent může vést k významnému zlešení celého systému.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A spelling error detection and correction application is based on three main components: a
dictionary (or reference word list), an error model and a language model. While most of the
attention in the literature has been directed to the language model, we show how improvements in
any of the three components can lead to significant cumulative improvements in the overall
performance of the system. We develop our dictionary of 9.3 million fully inflected Arabic words
from a morphological transducer and a large corpus, cross validated and manually revised. We
improve the error model by analysing error types and creating an edit distance re-ranker. We also
improve the language model by analysing the level of noise in different sources of data and
selecting the right subset to train the system on. Testing and evaluation experiments show that
our system significantly outperforms Microsoft Word 2010, OpenOffice Ayaspell and Google
Document.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>(Not yet available. English version repeated)
We describe the development of a bidirectional rule-based machine translation system between Indonesian and Malaysian (id-ms), two closely related Austronesian languages natively spoken by approximately 35 million people. The system is based on the re-use of free and publicly available resources, such as the Apertium machine translation platform and Wikipedia articles. We also present our approaches to overcome the data scarcity problems in both languages by exploiting the morphology similarities between the two.}</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe the development of a bidirectional rule-based machine translation system between Indonesian and Malaysian (id-ms), two closely related Austronesian languages natively spoken by approximately 35 million people. The system is based on the re-use of free and publicly available resources, such as the Apertium machine translation platform and Wikipedia articles. We also present our approaches to overcome the data scarcity problems in both languages by exploiting the morphology similarities between the two.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme Korektor – flexibilní statistický nástroj pro opravu českých textů, jehož schopnosti přesahují tradiční nástroje pro kontrolu pravopisu. Korektor využívá kombinace jazykových modelů a chybového modelu jak k tomu, aby setřídil pořadí nabízených náhrad pro neznámé slovo podle pravděpodobnosti výskytu na daném místě v textu, tak také, aby nalezl i překlepy, které se nahodile shodují s existujícím českým slovním tvarem. Prostou náhradou chybového modelu náš pracuje Korektor také jako systém pro doplnění diakritiky („oháčkování textu“) s nejvyšší publikovanou úspěšností. Systém neobsahuje žádné významné jazykově specifické komponenty s výjimkou natrénovaných statistických modelů. Je tedy možné jej snadno natrénovat i pro jiné jazyky. Ukážeme, jakých zlepšení náš systém dosahuje v porovnání se stávajícími českými korektory pravopisu i systémy pro doplnění diakritiky. Ukážeme také, že kombinace těchto schopností pomáhá při anotaci chyb v korpusu češtiny jako druhého jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present Korektor – a flexible and powerful purely statistical text correction tool for Czech that goes beyond a traditional spell checker. We use a combination of several language models and an error model to offer the best ordering of correction proposals and also to find errors that cannot be detected by simple spell checkers, namely spelling errors that happen to be homographs of existing word forms. Our system works also without any adaptation as a diacritics generator with the best reported results for Czech text. The design of Korektor contains no language-specific parts other than trained statistical models, which makes it highly suitable to be trained for other languages with available resources. The evaluation demonstrates that the system is a state-of-the-art tool for Czech, both as a spell checker and as a diacritics generator. We also show that these functions combine into a potential aid in the error annotation of a learner corpus of Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Knížka přináší aktuální souborný popis problematiky strojového překladu, různých přístupů k němu a jejich výhod a nevýhod a zabírá se i otázkami vyhodnocování kvality překladu. Čeština jako podkladový jazyk potvrzuje svou vynikající roli: pro podchycení českých jazykových jevů je zapotřebí bohaté teorie, přitom je pro ale češtinu dostatek nástrojů a anotovaných i neanotovaných textových dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An up-to-date broad study of various approaches of machine translation and translation evaluation techniques, highlighting their advantages and disadvantages. The book is written in Czech and also primarily focuses on aspects of Czech, documenting that Czech is not only interestingly complex but it is also well supplied with linguistic tools and data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška o strojovém překladu spojená se zamyšlením, k jaké regeneraci či degeneraci díky překladu spějeme.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A talk on machine translation and a deliberation on the progress and regress it can lead to.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Seznámení zájemců o informatiku s úlohou strojového překladu, současnými metodami a jejich omezeními.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An introduction to the task of machine translation, current methods and their inherent limitations for those interested in computer science.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Stručný článek představuje problematiku strojového překladu širšímu publiku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This is short article describing the field of machine translation to a broad audience.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Seznámení studentů středních škol a jejich pedagogů s problematikou strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An introduction to machine translation for high school students and their teachers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dáta z troch zdrojov (časť korpusu Acquis, časť testovacej sady z WMT a vety vybrané z kníh) automaticky preložené pomocou piatich systémov (Česílko, Česílko 2, Google Translate a Moses s rôznymi nastaveniami) z češtiny do slovenčiny a ohodnotené troma anotátormi. Preklady boli ručne zoradené podľa ich kvality.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Data from three sources (part of Acquis, WMT test set and sentences selected from the set of books) translated by 5 machine translation systems (Česílko, Česílko 2, Google Translate and Moses with different settings) from Czech to Slovak and evaluated by three annotators. The translations were manually ordered according to their quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme taxonomii pro faktorové modely frázového překladu a provádíme sérii experimentů s konfiguracemi z navržené taxonomie. Odhalujeme přitom řadu chyb v návrhu překladových modelů, jichž je vhodné se vyvarovat. Článek slouží rovněž jako popis našich systémů CU-BOJAR a CU-POOR-COMB v soutěži WMT12.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a taxonomy of factored phrase based
translation scenarios and conduct a
range of experiments in this taxonomy. We
point out several common pitfalls when designing
factored setups. The paper also describes
our WMT12 submissions CU-BOJAR
and CU-POOR-COMB.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>HMEANT (Lo, Wu, 2011) je technika ručního hodnocení kvality strojového překladu založená na predikátově-argumentové struktuře věty. V článku dáváme HMEANT do souvislosti se zavedeným Funkčním generativním popisem a současně poprvé aplikujeme HMEANT na jiný jazyk než angličtinu, konkrétně na češtinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>HMEANT (Lo and Wu, 2011a) is a manual MT evaluation technique that focuses on
predicate-argument structure of the sentence.
We relate HMEANT to an established lin-
guistic theory, highlighting the possibilities of
reusing existing knowledge and resources for
interpreting and automating HMEANT. We
apply HMEANT to a new language, Czech
in particular, by evaluating a set of English-
to-Czech MT systems. HMEANT proves to
correlate with manual rankings at the sentence
level better than a range of automatic met-
rics. However, the main contribution of this
paper is the identification of several issues
of HMEANT annotation and our proposal on
how to resolve them.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Závěrečná technická zpráva grantu EuroMatrixPlus popisující strojový překlad prostřednictvím stromových struktur s bohatou anotací.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The final technical report on rich tree-based translation for the EuroMatrixPlus project.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CzEng 1.0 je aktualizovaná verze česko-anglického paralelního korpusu, volně použitelného pro nekomerční použití. Oproti předchozí verzi je velikost korpusu dvojnásobně zvětšena na 15 milionů větných párů (řádově 200 milionů slov pro každý jazyk). Data jsou pečlivě profiltrována, aby se omezil výskyt neodpovídajících si větných párů apod. CzEng 1.0 je automaticky zarovnán po větách i po slovech. Krom čistě textové verze dáváme k dispozici anotaci korpusu na několika rovinách: morfologické, větně členské (analytické, povrchová závislostní syntax) a tektogramatické (hloubková syntax). Obsažena je také automatická anotace koreference pro oba jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CzEng 1.0 is an updated release of our Czech-English parallel corpus, freely
available for non-commercial research or educational purposes. In this
release, we approximately doubled
the corpus size, reaching 15 million sentence
pairs (about 200 million tokens per language). More importantly, we carefully
filtered the data to reduce the amount of non-matching sentence pairs.
CzEng 1.0 is automatically aligned at the level of sentences as well as words.
We provide not only the plain text representation, but also automatic
morphological tags, surface syntactic as well as deep syntactic dependency parse
trees and automatic co-reference links in both English and Czech.
This paper describes key properties of the released resource including the
distribution of text domains,
the corpus data formats, and a toolkit to handle the provided rich annotation. We also
summarize the procedure of the rich annotation (incl. co-reference
resolution) and of the automatic filtering. Finally, we provide some suggestions
on exploiting such an automatically annotated sentence-parallel corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tři přídavné české referenční překlady celé datové sady WMT 2011 (http://www.statmt.org/wmt11/test.tgz), přeložené z němčiny. Původní segmentace dat z WMT 2011 byla zachována.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Additional three Czech reference translations of the whole WMT 2011 data set (http://www.statmt.org/wmt11/test.tgz), translated from the German originals. Original segmentation of the WMT 2011 data is preserved.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Bílá kniha prezentuje stav podpory jazykových technologií pro češtinu. Je částí série, která analyzuje dostupné jayzkové zdroje pro 30 evropských jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This white paper presents the state of language technology support for the Czech language. It is a part of a series that analyses the available language resources and technologies for 30 European languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Eman („experimentální manažer“) je softwarový nástroj, který umožňuje řídit rozsáhlé soubory vzájemně provázaných experimentů, při kterých se zpracovávají velké datové soubory, typicky na výpočetním clusteru. Byl vyvinut jako infrastruktura pro statistický strojový překlad, ale uplatní se i v jiných úlohách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Eman (“experimental manager”) is a software tool capable of maintaining large networks of mutually interconnected experiment processing large datasets, typically on a computational cluster. Eman was developed as an infrastructure for statistical machine translation but it can be used for other tasks as well.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Návod, jak provádět širokou sérii experimentů (se zaměřením na strojový překlad).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A tutorial on conducting a very broad range of experiments (for MT in particular).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>ÚFAL je pracovištěm, které je přátelsky nakloněné jak lingvistice, tak strojovému učení. Protože statistika je klíčovým pojmem strojového učení, je ÚFAL přátelsky spřízněn i se statistikou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>UFAL is a department well disposed to both linguistics and machine learning. It's well disposed to statistics as well because statistics is a key part of machine learning.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme významnou aktualizaci Pražského česko-anglického závistostního korpusu (PCEDT), paralelního korpusu, který je ručně anotován na hloubkové syntaktické rovině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a substantial update of the Prague Czech-English Dependency Treebank, a parallel corpus manually annotated at the deep
syntactic layer of linguistic representation. The English part consists of the Wall Street Journal (WSJ) section of the Penn Treebank.
The Czech part was translated from the English source sentence by sentence. This paper gives a high level overview of the underlying
linguistic theory (the so-called tectogrammatical annotation) with some details of the most important features like valency annotation,
ellipsis reconstruction or coreference.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku se klade otázka, zda jsme oprávněni u typů (i) - (iii) mluvit o asymetrickém vztahu "hypotaktické koordinace". Jde o typy (i) asociativ vs slučovací koordinace, (ii) tzv. nepravé věty vedlejší, (iii) konstrukce uvozené výrazy "místo, kromě, mimo". Dochází se k názoru, že na hypotaktickou koordinaci kandiduje část příkladů typu (a).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The question about boundaries of so-called "hypotactical coordination" is discussed in connection with three types of constructions: (i) asociative constructions vs simple conjunctions, (ii) so-called "false" subordinated clauses, (iii) constructions introduced by the expressions "místo" (instead), "kromě" (beside/with exception).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jde o nekrology věnované prof. Rudolfu Růžičkovi z Univerzity v Lipsku, předním představiteli teoretické a formální lingvistiky v Německu (1920-2011) a prof. Milce Ivicové z Univerzity v Nověm Sadu a z Institutu srbského jazky SAVU v Bělehradě (1923-2011).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>These obituaries are devoted to couple of famouse linguists, first of all Slavists: prof. Rudolf Růžička from the University of Leipzig (1920-2011) and to prof. Milka Ivic, professor of the University in Novi Sad and Institute of Serbian Language in Beograd (1923-2011).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve stati se analyzují české konstrukce srovnávací a konstrukce uvedené výrazy kromě/mimo. Navrhuje se jejich hloubková (tektogramatická) reprezentace rozvinutá v zapuštěnou predikaci a označovaná jako gramatikalizovaná elipsa. Zavádí se dva nové syntaktické vztahy (adice - ADDIT a výjimka - EXC) související s víceznačností konstrukcí s kromě.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Czech constructions of comparison and the constructions introduced by kromě/mimo are analyzed. For their deep structure it is necessary to expand them into the embedded predication, which is considered to be a grammaticalized ellipsis. The two new syntactic relations are introduced (ADDIT - an addition, EXC - an exception). They are connected with the ambiguity of kromě constructions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku se hodnotí metodologické směry užívané v české syntaxi od 40. let minulého století. Jako nejvlivnější modely se hodnotí Danešova Dvourovinná valenční syntax a Sgallův Funkční generativní popis.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The metodological approaches used in Czech syntactic research since the 40-ies of the last century are analyzed. The Daneš´s Two-Level Valency Syntax and Sgall´s Functional Generative Description are evaluated as most comprehensive models in the Czech syntax.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Korpusová anotace je důležitou součástí lingvistické analýzy a počítačového zpracování jazyka. Tento článek se zabývá problémy spojenými se syntaktickou anotací mluvených textů na pozadí syntaktické anotace ČAKu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Corpus annotation plays an important role in linguistic analysis and computa-tional processing of both written and spoken language. Syntactic annotation of spoken texts becomes clearly a topic of considerable interest nowadays, driven by the desire to improve auto-matic speech recognition systems by incorporating syntax in the language models, or to build language under-standing applications.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce představím software, který jsem vyvinul pro převod Kodaňského závislostního korpusu (CDT) do systému Treex. CDT je vícejazyčný korpus vyvinutý v Copenhagen Business School. Treex je multilinguální softwarový systém vyvinutý na Karlově univerzitě v Praze. Treex se používá mimo jiné pro vývoj systémů strojového překladu.

Přednáška bude mít dvě části. V první představím konverzní proceduru. Ve druhé části předvedu možné způsoby, jak s daty v novém formátu pracovat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I would like to present my recent efforts on converting the Copenhagen Dependency Treebank (CDT) data into Treex. CDT is a multilingual treebank developed at CBS. Treex is a multi-purpose Natural Language Processing software framework developed at Charles University in Prague. Treex is used in a number of projects aimed at building language data resources as well as at developing NLP applications such as Machine Translation systems.
                     
The talk will have two parts. In the first part I will give an overview of the conversion procedure, with focus on pecularities of CDT that made the conversion a challenging task. In the second part I will show how the CDT data in the new format can be browsed and further processed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>The present work focuses on using tree-shaped syntactic structures as an intermediate sentence representation in an experimental English-Czech machine translation system.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Předložená práce je zaměřená na využití stromových syntaktických struktur jakožto reprezentace věty přirozeného jazyka v experimentálním systému pro anglicko-český strojový překlad.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládáme detailní studii automatické lexikální disambiguace na pilotním vzorku
třiceti anglických sloves za použití lexikonu vzorů slovesných užití (patterns), který vychází
z Corpus Pattern Analysis (CPA). Tato inovátorská lexikografická metoda namísto na
abstraktních definicích jednotlivých významů staví na souhře morfosyntaktické, lexikální a
sémantické/pragmatické podobnosti slovesných užití. Natrénovali jsme několik statistických
klasifikátorů na rozpoznávání těchto vzorů. Klasifikátory využívají jak morfosyntaktických,
tak sémantických rysů. V naší studii se soustředíme na procedury pro extrakci rysů, jejich
výběr a jejich evaluaci. Ukazujeme, že rysy na míru uzpůsobené jednotlivým slovesům, jež
jsou implicitně obsaženy v definici každého vzoru v lexikonu, mají potenciál významně zvýšit
přesnost statistických klasifikátorů s učitelem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We give a report on a detailed study of automatic lexical disambiguation of 30 sample English
verbs. We have trained and evaluate several statistical classifiers that use both morphosyntactic
and semantic features to assign semantic patterns according to a pattern lexicon.
Our system of semantic classification draws on the Corpus Pattern Analysis (CPA) — a novel
lexicographic method that seeks to cluster verb uses according to the morpho-syntactic, lexical
and semantic/pragmatic similarity of their contexts rather than their grouping according to
abstract semantic definitions. In this paper we mainly concentrate on the procedures for feature
extraction and feature selection. We show that features tailored to particular verbs using
contextual clues given by the CPA method and explicitly described in the pattern lexicon have
potential to significantly improve accuracy of supervised statistical classifiers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozbor různých přístupů k pojetí a popisu stupňovitosti z hlediska aktuálního členění.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Analysis of different approaches to the concept and the description of hierarchy of element from the point of view of topic-focus articulation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Hlavní principy explicitního pražského závislostního modelu jazyka postupující od funkce k formě.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article is about main principles of the explicit description of language based on dependency syntax.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Srovnání dvou teoretických přístupů k aktuálnímu členění a uplatnění pražského přistupu v anotovaném počítačovém korpusu češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A comparison of two theoretical approaches to the information structure and the application of the Praguian approach in an annotated corpus of Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Srovnání přístupu k aktuálnímu členění v české lingvistice, od základní práce Mathesiovy až po práce současné.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A comparison of different Praguian approaches to information structure, from Vilem Mathesius up today.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek shrnuje lingvistické problémy, na které jsme narazili a které řešili anotátoři při anotování informační struktury ve větách Pražského závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Linguistic issues that had to be analyzed and resolved during the annotation of information structure in the Prague Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Akademický korpus češtiny zpracovaný v ÚJČ AV byl jedním z prvních anotovaných počítačových korpusů přirozeného jazyka, a to na rovině morfosyntaktické. Odtud vedla cesta k bohatě anotovanému Pražskému závislostnímu korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Academic corpus of the Czech language created in the Institute of Czech Language belongs to the first annotated corpora of natural languages ever in existence. This activity was one of the stimuli for the development of the richly annotated corpus of the Prague Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento software je modifikací a konfigurací Dspace 1.6, která umožňuje centrům Clarin, která používají Dspace jako svůj software pro repozitáře, využít službu Handle od konzorcia EPIC, doporučenou Clarinem, v rámci Dspace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This software is a modification and configuration of Dspace 1.6 that allows Clarin centres that employ Dspace as their repository software to use Clarin recommended Handle Service from EPIC Consortium within Dspace.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci se zabýváme doménovou adaptací statistického strojového překladu s využitím dat automaticky stažených z internetu. Experimenty jsou provedeny na doménách životního prostředí a pracovní legislativy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We tackle the problem of domain adaptation of Statistical Machine Translation by exploiting domain-specific data acquired by domain-focused web-crawling. We design and evaluate a procedure for automatic acquisition of monolingual and parallel data and their exploitation for training, tuning, and testing in a phrase-based Statistical Machine Translation system. We present a strategy for using such resources depending on their availability and quantity supported by results of a large-scale evaluation on the domains of Natural Environment and Labour Legislation and two language pairs: English--French, English--Greek. The average observed increase of BLEU is substantial at 49.5% relative.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Současné systémy statistického strojového překladu jsou založeny na logaritmicko-lineárních modelech, které pro hodnocení překladových hypotéz ve fázi dekódování kombinují sadu příznakových funkcí. Tyto modely jsou parametrizovány vektorem vah, které se optimalizují na tzv. vývojových datech, tj. množině vět a jejich referenčních překladů. V tomto článku se zabýváme (častou a pro průmyslové nasazení relevantní) situací, kdy je třeba překladový systém natrénovaný na datech z obecné domény adaptovat na nějakou specifickou doménu, pro kterou jsou k dispozici paralelní data jen ve velice omezeném (či žádném) množství. Ukazujeme, že takové systémy mohou být vhodně adaptovány pomocí optimalizace parametrů za použití jen překvapivě malého množství paralelních doménově-specifických dat nebo tzv. křížovou optimalizací. Možností je také nepoužití optimalizace vůbec. Jednotlivé přístupy analyzujeme a porovnáváme jejich cekovou náročnost. Dále se zabýváme analýzou systémových hyperparametrů (např. maximální délkou frází a velikostí vývojových dat) a jejich optimalizací.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Current state-of-the-art Statistical Machine Translation systems are based on log-linear models
that combine a set of feature functions to score translation hypotheses during decoding. The
models are parametrized by a vector of weights usually optimized on a set of sentences and
their reference translations, called development data. In this paper, we explore a (common
and industry relevant) scenario where a system trained and tuned on general domain data
needs to be adapted to a specific domain for which no or only very limited in-domain bilingual
data is available. It turns out that such systems can be adapted successfully by re-tuning model
parameters using surprisingly small amounts of parallel in-domain data, by cross-tuning or no
tuning at all. We show in detail how and why this is effective, compare the approaches and
effort involved. We also study the effect of system hyperparameters (such as maximum phrase
length and development data size) and their optimal values in this scenario.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek se zabývá vztahem korpusu a valenčního slovníku. Slovník vznikl jako vedlejší produkt anotace Pražského závislostního korpusu, stal se důležitým zdrojem jak pro další lingvistický výzkum, tak pro počítačové zpracování češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In our contribution, we relate the development of a richly annotated corpus and a computational valency dictionary. Our valency dictionary has been created as a “byproduct” of the annotation of the Prague Dependency Treebank (PDT) but it became an important resource for further linguistic research as well as for computational processing of the Czech language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se věnuje valenčním vlastnostem českých neproduktivně tvořených (dějových) substantiv odvozených od sloves s předmětovým genitivem. Prvotní motivací bylo zjistit, zda si tato substantiva (např. obava, dotaz, dotek) uchovávají původní adverbální genitivní vazbu, tj. GenAdnom (← GenAdverb). Valenční slovníky ani odborná literatura tuto vazbu u daných substantiv neuvádějí. V subkorpusech ČNK se u některých substantiv vyskytuje, např. naděje úspěchu, jeho dotek puku; vedle ostatních forem daného participantu (např. předložkových skupin) se jedná spíše o okrajovou možnost vyjádření. Za možností / nemožností užití GenAdnom (← GenAdverb) u dějových substantiv stojí různorodé faktory, související patrně především s příslušností daných substantiv k určité sémantické skupině, příp. podskupině (srov. pozitivní vs. negativní duševní stavy, např. odvaha spolupráce vs. *obava zkoušky, snaha vyhnout se strukturní homonymii u substantiv mluvení, např. *dotaz kamaráda.ADDR vs. dotaz kamaráda.ACT).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the present paper, adnominal counterparts of adverbal objects expressed by prepositionless Genitive (Gen; e.g. obávat se čeho ‘to-be-afraid of-sth’, dotázat se koho ‘to-ask of-sb’) are studied. The intention was to find out if Czech nouns derived from verbs by non-productive means can be modified by Patient (PAT) or Addressee (ADDR) expressed by Gen as well; neither other papers nor valency dictionaries mention this form of the participants. It has turned out that several nouns modified by PAT expressed by Gen can rarely be found in the corpus (e.g. CNC SYN2005, cf. odvaha spolupráce ‘courage of-cooperation’, jeho dotek puku ‘his touch of-the-puck’; other forms of PAT, i.e. prepositional groups, sometimes also infinitive or an embedded objective clause, are more frequent). Factors that influence possibility or impossibility to be modified by PAT or ADDR expressed by Gen are typically connected with the type of the semantic group the noun belongs to (cf. the difference between nouns denoting “positive” vs. “negative” mental state or dispositions, e.g. naděje úspěchu ‘hope of-success’ vs. *obava následků ‘fear of-consequences’, or the tendency to avoid syntactic homonymy of Actor (ACT) and ADDR expressed by Gen, which is typical of nouns of saying, cf. dotaz kamaráda.ACT vs. *dotaz kamaráda.ADDR ‘question of-the-friend’).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Systém Dialogy.Org je softwarový nástroj určený především k podpoře vytváření a sdílení anotovaných audio-video dat lingvistickou komunitou. Dialogy.Org umožňuje formou webového rozhraní editaci a vyhledávání v přepisech audio-visuálních nahrávek dialogů. Vyhledané úseky textu je možné přehrávat a analyzovat pomocí webového prohlížeče.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Dialogy.Org system is a software tool designed primarily to support the creation and sharing of annotated audio-visual data by linguistic community. The software tool allows editing and searching in the transcripts of audio-visual recordings of dialogues. The dynamic web application provides access for registered users to the digitised archive. Playing and exploring of selected parts is possible in the web browser.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku hodnotíme přínos, který představují syntacticko-sémantické stromy
(tektogramatická rovina anotace) a celá bohatá anotace Pražského závislostního korpusu pro
anotaci diskurzní struktury textu, tedy pro anotaci diskurzních vztahů, jejich konektorů a
argumentů. Rozhodnutím anotovat diskurzní strukturu přímo na stromech se náš přístup liší od
většiny podobně zaměřených projektů, které jsou obvykle založeny na anotaci lineárního textu.
Naším základním předpokladem je, že některé syntaktické rysy větné analýzy odpovídají jistým
rysům z roviny diskurzní struktury. Proto využíváme některé vlastnosti rozsáhlého závislostního
korpusu češtiny k ustanovení nezávislé diskurzní anotační vrstvy. V tomto příspěvku
odpovídáme na otázku, jaké výhody tento přístup přináší.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the following paper, we discuss and evaluate the benefits that deep syntactic trees
(tectogrammatics) and all the rich annotation of the Prague Dependency Treebank bring to the
process of annotating the discourse structure, i.e. discourse relations, connectives and their
arguments. The decision to annotate discourse structure directly on the trees contrasts with the
majority of similarly aimed projects, usually based on the annotation of linear texts. Our basic
assumption is that some syntactic features of a sentence analysis correspond to certain discourselevel
features. Hence, we use some properties of the dependency-based large-scale treebank of
Czech to help establish an independent annotation layer of discourse. The question that we
answer in the paper is how much did we gain by employing this approach.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje systém použitý pro závislostní syntaktickou analýzu hindských dat v rámci soutěže na Colingu 2012.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes our system used for dependency parsing of Hindi data during the shared task at Coling 2012.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme naše pokusy s frázovým strojovým překladem pro soutěž WMT 2012. Natrénovali jsme jeden systém pro 14 překladových párů mezi angličtinou nebo češtinou na jedné straně a angličtinou, češtinou, němčinou, španělštinou nebo francouzštinou na druhé straně. Popisujeme sadu výsledků s různými velikostmi trénovacích dat a jejich podmnožin.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe our experiments with phrase-based machine translation for the WMT 2012 Shared Task. We trained one system for 14 translation directions between English or Czech on one side and English, Czech, German, Spanish or French on the other side. We describe a set of results with different training data sizes and subsets.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Postřehy z letošních pokusů na ÚFALu v rámci mezinárodní soutěže ve strojovém překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Observations from this year's experiments at ÚFAL for the WMT shared translation task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme HamleDT – harmonizovaný mnohojazyčný závislostní korpus. HamleDT je kolekce existujících závislostních korpusů (nebo do závislostí převedených jiných syntakticky anotovaných korpusů), transformovaných tak, aby odpovídaly jednotnému anotačnímu stylu. Licenční podmínky nám nedovolují dále šířit samotné korpusy, většina z nich je ovšem pro vědecké účely snadno dostupná a my nabízíme software, který tato data převede do normalizované podoby.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We propose HamleDT – HArmonized Multi-LanguagE Dependency Treebank. HamleDT is a compilation of existing dependency treebanks (or dependency conversions of other treebanks), transformed so that they all conform to the same annotation style. While the license terms prevent us from directly redistributing the corpora, most of them are easily acquirable for research purposes. What we provide instead is the software that normalizes tree structures in the data obtained by the user from their original providers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Organizátoři každoročního Semináře o strojovém překladu (WMT) připravují a šíří paralelní korpusy, které lze použít při trénování systémů pro soutěžní úlohy. Mezi hlavní typy korpusů patří korpusy News Commentary a Europarl. Oba jsou k dispozici v několika jazykových párech, vždy mezi angličtinou a dalším evropským jazykem: cs-en, de-en, es-en a fr-en. Tyto korpusy nejsou paralelní přes více než dva jazyky. Pocházejí ze stejného zdroje a významně se překrývají, přesto však jsou některé věty přeloženy jen do některých jazyků. Dvojjazyčné paralelní podmnožiny nemají všechny stejný počet párů vět. Takové korpusy nemůžeme přímo nasadit při trénování systému pro např. de-cs (němčina-čeština). Můžeme nicméně použít angličtinu jako pivotní jazyk. Pokud rozpoznáme průnik anglických částí cs-en a de-en, můžeme použít jejich neanglické protějšky a vytvořit z nich paralelní korpus de-cs. Tuto úlohu plní tento software.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The organizers of the annual Workshop on Machine Translation (WMT) prepare and distribute parallel corpora that can be used to train systems for the shared tasks. Two core types of corpora are the News Commentary corpus and the Europarl corpus. Both are available in several language pairs, always between English and another European language: cs-en, de-en, es-en and fr-en. The corpora are not multi-parallel. They come from the same source and there is significant overlap but still some sentences are translated to only a subset of the languages. The bi-parallel subsets do not all have the same number of sentence pairs. Such corpora cannot be directly used to train a system for e.g. de-cs (German-Czech). However, we can use English as a pivot language. If we identify the intersection of the English parts of cs-en and de-en, we can take the non-English counterparts of the overlapping English sentences to create a de-cs parallel corpus. That is what this software does.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valenční slovník českých sloves VALLEX, verze 2.6 
obsahující přibližně 2730 záznamů o lexémech pokrývajících cca. 6460 lexikálních jednotek (významů). Slovník je volně k dispozici pro účely výzkumu. Nová verze slovníku je - oproti starší verzi - kvalitativně i kvantitativně obohacena (zejm. jsou pro vybraná slovesa doplněny korpusové příkladů).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Valency Lexicon of Czech Verbs, Version 2.6 (VALLEX 2.6), is a collection of linguistically annotated data and documentation, resulting from an attempt at formal description of valency frames of Czech verbs. VALLEX 2.6 has been developed at the Institute of Formal and Applied Linguistics, Faculty of Mathematics and Physics, Charles University, Prague. 

VALLEX 2.6 provides information on the valency structure (combinatorial potential) of verbs in their particular senses. VALLEX 2.6 is a successor of VALLEX 1.0, extended in both theoretical and quantitative aspects (including corpus evidence).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se soustřeďuje na popis kolekce vět, u kterých byla ručně analyzována větná struktura. Ukazuje, že koncept založený na lineárních segmentech, které lze snadno automaticky detekovat, slouží jako dobrý základ pro identifikaci klauzí v češtině. Anotace segmentů zahrnuje takové jevy jako je závislost kauzí, jejich koordinace, apozice či parenteze.  
Anotace větné struktury doplňuje závislostní přístup k popisu jazyka o explicitní informaci o vztazích mezi klauzemi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The focus of this article is on the creation of a collection of sentences manually annotated with respect to their sentence structure. We show that the concept of linear segments—linguistically motivated units, which may be easily detected automatically—serves as a good basis for the identification of clauses in Czech. The segment annotation captures such relationships as subordination, coordination, apposition and parenthesis; based on segmentation charts, individual clauses forming a complex sentence are identified. The annotation of a sentence
structure enriches a dependency-based framework with explicit syntactic information on relations among complex units like clauses. We have gathered a collection of 3,444 sentences from the Prague Dependency Treebank, which were annotated
with respect to their sentence structure (these sentences comprise 10,746 segments forming 6,341 clauses). The main purpose of the project is to gain a development data—promising results for Czech NLP tools (as a dependency parser or a machine translation system for related languages) that adopt an idea of clause segmentation have been already reported. The collection of sentences with annotated sentence structure provides the possibility of further improvement of such tools.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popularizační prezentace pro studenty gymnázia Na Zatlance v rámci Dne profesí. Cílem bylo přiblížit studentům matematickou lingvistiku jako zajímavý obor a obecně propagovat studium přírodních věd.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presentations to high school students within the "Day of professions". The aim was to introduce students to mathematical linguistics as an interesting field, and generally promote the study of natural sciences.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V češtině i v ruštině existuje množina předpon, jejichž připojením k nedokonavému slovesu a přidáním zvratného zájmena pozměníme význam původního slovesa vždy téměř stejným způsobem. Toho lze využít při automatickém rozpoznávání slovních tvarů, aniž by bylo třeba je ukládat do morfologických slovníků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>There is a set of prefixes in Czech as well as in Russian, which, added to imperfective verbs together with a reflexive pronoun, change the meaning of the verb in the same manner. This feature is so regular that could help automatically recognize words without using morphological dictionaries.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se věnuje problematice sémantické analýzy sloves a lexikálnímu popisu sloves se zřetelem k vytvoření trénovacích dat pro automatický sémantický analyzátor. Vychází z metody Corpus Pattern Analysis a zkoumá její uplatnění v lexikálním popisu sloves pro účely NLP pomocí měření mezianotátorské shody.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>There is no objectively correct way to create a monolingual entry of a polysemous verb. By structuring a verb into readings, we impose our conception onto lexicon users, no matter how big a corpus we use in support. How do we make sure that our structuring is intelligible for others? 

We are performing an experiment with the validation of the fully corpus-based Pattern Dictionary of English Verbs (Hanks &amp; Pustejovsky, 2005), created according to the lexical theory Corpus Pattern Analysis (CPA). The lexicon is interlinked with a large corpus, in which several hundred randomly selected concordances of each processed verb are manually annotated with numbers of their corresponding lexicon readings (“patterns”). It would be interesting to prove (or falsify) the leading assumption of CPA that, given the patterns are based on a large corpus, individual introspection has been minimized and most people can agree on this particular semantic structuring. We have encoded the guidelines for assigning concordances to patterns and hired annotators to annotate random samples of verbs cotained in the lexicon. Apart from measuring the interannotator agreement, we analyze and adjudicate the disagreements. The outcome is offered to the lexicographer as feedback. The lexicographer revises his entries and the agreement can be measured againg on a different random sample to test whether or not the revision has brought an improvement of the interannotator agreement score. A high interannotator agreement suggests that lexicon users are likely to find a pattern corresponding to a random verb use of which they seek explanation. A low agreement score gives a warning that there are patterns missing or vague.  

We focus on machine-learning applications, but we believe that this procedure is of interest even for quality management in human lexicography.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme VPS-30-En, malý lexikální zdroj, který obsahuje následujících 30 sloves: access, ally, arrive, breathe,
claim, cool, crush, cry, deny, enlarge, enlist, forge, furnish, hail, halt, part, plough, plug, pour, say, smash, smell, steer, submit, swell,
tell, throw, trouble, wake and yield. 
VPs-30-En jsme vytvořili a používáme jej k výzkumu potenciálu mezianotátorské shody u sémantického značkování podle anotačního schématu CPA.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We are presenting VPS-30-En, a small lexical resource that contains the following 30 English verbs: access, ally, arrive, breathe,
claim, cool, crush, cry, deny, enlarge, enlist, forge, furnish, hail, halt, part, plough, plug, pour, say, smash, smell, steer, submit, swell,
tell, throw, trouble, wake and yield. We have created and have been using VPS-30-En to explore the interannotator agreement potential
of the Corpus Pattern Analysis. VPS-30-En is a small snapshot of the Pattern Dictionary of English Verbs (Hanks and Pustejovsky,
2005), which we revised (both the entries and the annotated concordances) and enhanced with additional annotations. It is freely
available at http://ufal.mff.cuni.cz/spr. In this paper, we compare the annotation scheme of VPS-30-En with the original PDEV. We
also describe the adjustments we have made and their motivation, as well as the most pervasive causes of interannotator disagreements.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nízká mezianotátorská shoda (IAA) je známým problémem v sémantickém značkování. IAA koreluje s granularitou lexií a oboje koreluje s množstvím informace i s její spolehlivosí. Představujeme Reliable Gain (RG), míru, která optimalizuje sémantickou granularitu se zřetelem ke spolehlivosti informace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Low interannotator agreement (IAA) is a
well-known issue in manual semantic tagging
(sense tagging). IAA correlates with
the granularity of word senses and they
both correlate with the amount of information
they give as well as with its reliability.
We compare different approaches to semantic
tagging in WordNet, FrameNet, Prop-
Bank and OntoNotes with a small tagged
data sample based on the Corpus Pattern
Analysis to present the reliable information
gain (RG), a measure used to optimize the
semantic granularity of a sense inventory
with respect to its reliability indicated by
the IAA in the given data set. RG can also
be used as feedback for lexicographers, and
as a supporting component of automatic semantic
classifiers, especially when dealing
with a very fine-grained set of semantic categories.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Experimenty se sémantickou anotací založenou na Corpus Pattern Analysis a lexikálním zdroji PDEV (Hanks a Pustejovsky, 2005) ukázaly potřebu evaluační míry, která by identifikovala optimální vztah mezi sémantickou granularitou sémantických kategorií v lexikálním  popisu slovesa a spolehlivostí anotace, která se měří pomocí mezianotátorské shody. Představujeme takovou míru.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Experiments with semantic annotation based on the Corpus pattern Analysis and the lexical resource PDEV (Hanks and Pustejovsky, 2005), revealed a need of an evaluation measure that would identify the optimum relation between the semantic granularity of the semantic categories in the description of a verb and the reliability of the annotation expressed by the interannotator agreement (IAA). We have introduced the Reliable Information Gain (RG), which computes this relation for each tag selected by the annotators and relates it to the entry as a whole, suggesting merges of unreliable tags whenever it would increase the information gain of the entire tagset (the number of semantic categories in an entry). The merges suggested in our 19-verb sample correspond with common sense. One of the possible applications of this measure is quality management of the entries in a lexical resource.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje přístu k automatické a manuální anotaci žákovského korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an approach to building a learner corpus of Czech, manually corrected and annotated with error tags using a complex grammar-based taxonomy of errors in spelling, morphology, morphosyntax, lexicon and style. This grammar-based annotation is supplemented by a formal  classification of errors based on surface alternations. To supply additional information about non-standard or ill-formed expressions, we aim at a synergy of manual and automatic annotation, deriving information from the original input and from the manual annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá volným slovosledem z pohledu redukční analýzy. Využívá jejích formálních základů a datové struktury. Navrhujeme několik variant míry volnosti slovosledu založených na operaci posouvání (shift). tedy operaci spočívající ve slovosledných změnách zachovávajících syntaktickou správnost věty. Tyto míry umožňují pochopit a studovat rozdíl mezi komplexitou slovosledu (jak složité je analyzovat věty s komplexním slovosledem) a volností slovosledu (jak je možné měnit slovosled při zachování závislostních relací ve větě).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper focuses on a phenomenon of free word order through the analysis by reduction.
It exploits its formal background and data types and studies the word order freedom
of a (natural) language. We propose and discuss several variants of a measure based on
a number of word order shifts (i.e., word order changes preserving syntactic correctness,
individual word forms, their morphological characteristics and their surface dependency
relations). Such measure helps to understand the difference between word order complexity
(how difficult it is to parse sentences with more complex word order) and word
order freedom (to which extent it is possible to change the word order without causing a
change of individual word forms, their morphological characteristics and/or their surface
dependency relations).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek se zabývá formalizací popisu volného slovosledu přirozených jazyků. Využívá mechanismu redukční analýzy a definuje míru volnosti slovosledu na základě počtu přesunů provedených v průběhu analýzy. Tato míra umožňuje rozlišit složitost slovosledu (jak obtížné je analyzovat věty se složitějším slovosledem) a volnost slovosledu (do jaké míry je možné měnit slovosled, aniž by došlo ke změně jednotlivých slovních tvarů, jejich morfologické charakteristiky a / nebo jejich povrchově syntaktických vztahů). Tento rozdíl je ilustrován na pilotní studii českých vět s klitikami.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper contains an attempt to formalize the degree of word order freedom for natural languages. It exploits the mechanism of the analysis by reduction and defines a measure based on a number of shifts performed in the course of the analysis. This measure helps to understand the difference between the word order complexity (how difficult it is to parse sentences with more complex word order) and word order freedom in Czech (to which extent it is possible to change
the word order without causing a change of individual word forms, their morphological characteristics and/or their surface dependency relations). We exemplify this distinction on a pilot study on Czech sentences with clitics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek zkoumá fenomén volného slovosledu přirozeného jazyka metodou redukční analýzy. Využívá přitom formální rámec a datový typ této analytické metody a uplatňuje ho na zkoumání minimálního počtu slovosedných posunů (tj. slovosledných změn zachovávajících syntaktickou správnost, jednotlivé slovní tvary, jejicj morfologickou a syntaktickou charakteristiku).
Zkoumání se soustřeďují na dva vzájemně se ovlivňující jevy související se slovosledem: (ne)projektivitu a počet přesunů. Tyto jevy jsou ilustrovány na vzorlu českých vět obsahujících klitiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper investigates a phenomenon of free word order through the analysis by reduction. It exploits its formal background and data types and studies the word order freedom by means of the minimal number of word order shifts (word order
changes preserving syntactic correctness, individual word forms, their morphological characteristics and/or their surface dependency relations).
The investigation focuses upon an interplay of two phenomena related to word order: (non-)projectivity of a sentence and number of word order shifts within the analysis by reduction. This interplay is exemplified on a sample of Czech sentences with clitics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kapitola popisuje historii pokusů s automatickým překladem mezi příbuznými jazyky, použité metody a dosažené výsledky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This chapter describes the history of experiments in the field of machine translation among related Slavic languages, methods used and results achieved.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nejnovější verzi Pražského závislostního treebanku PDT 2.5, který bude poprvé vydán pod veřejnou licencí. Výhody PDT 2.5 ukážeme na srovnání s nejmodernějšími treebanky. Představíme nové vlastnosti verze 2.5, popíšeme, jak byly anotovány i jak spolehlivá tato anotace je. Ukážeme, jakými dotazy lze nové jevy hledat a jak se zobrazují v nástrojích dodávaných spolu s treebankem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the Prague Dependency Treebank 2.5, the newest version of  PDT and the first to be released under a free license. We show the benefits of PDT 2.5 in comparison to other state-of-the-art treebanks. We present the new features of the 2.5 release, how they were obtained and how reliably  they are annotated. We also show how they can be used in queries and how they are visualised with tools released alongside the treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Od řady českých adjektiv se tvoří jak adverbium s příponou -e, tak adverbium s příponou -o, př. deštivý – deštivě/deštivo, drahý – draze/draho. Deadjektivní adverbia s příponou -o často vystupují jako součást verbonominálního predikátu se slovesem být v neosobních konstrukcích, některá z nich se na tuto větněčlenskou funkci dokonce omezují (př. na horách bylo deštivo), jiná plní kromě této funkce i funkce jiné (srov. je tam draho vs. přišlo ho to draho). Predikativní funkce deadjektivních adverbií na -o se stala hlavním argumentem pro úvahy o vyčlenění těchto adverbií (spolu s dalšími výrazy) jako samostatného slovního druhu, tzv. predikativ. V příspěvku probíráme syntaktické a sémantické vlastnosti těchto adverbií na -o na základě aktuálního korpusového materiálu. Jako výhodnější se ukazuje popisovat tato adverbia jako samostatné lexikální jednotky, nikoli jako skupinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>From a number of Czech adjectives, both an adverb with the suffix -o and an adverb with the suffix -e are derived, cf. deštivý `rainy’ – deštivo/deštivě, drahý `expensive’ – draho/draze. These pairs of adverbs have often the same meaning but the adverb with the suffix -o occurs as a part of the predicate (with the verb být `to be’; ráno bylo deštivo `in the morning it was rainy’) while the adverb with the suffix -e is often specialized for the adverbial function (e.g. ráno vypadalo deštivě/*deštivo `the morning looked rainy’). On the functional specialization Komárek’s (1954) proposal to separate the adverbs with the suffix -o as an autonomous part of speech was based. Since the actual corpus material indicates that both the adverb with the suffix -o and the adverb with -e are acceptable in the same context (cf. je tam draho/draze `it is an expensive place to live’, draho/draze prodat `to sell dear’ VS. přišlo ho to draho/*draze ` it cost him dear’, draze/*draho za to zaplatil `he paid for this’) but the (non)acceptability cannot be probably explained by a grammatical principle, it seems to be appropriate to consider these words further as adverbs and to describe them as separate lexical units rather than a homogenous group.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá anotací větné modality v Pražském závislostním korpusu (PDT). Větná modalita je v češtině vyjádřena kombinací několika faktorů, především slovesným způsobem a koncovou interpunkcí. V PDT 2.0 byla větná modalita přiřazena poloautomaticky kořeni každé věty (stromu) a dále kořenům stromů reprezentujících vsuvku nebo přímou řeč. Tento přístup byl příliš zjednodušující pro adekvátní zachycení daného jevu, proto byla metoda přiřazení větné modality pro příští vydání treebanku (PDT 3.0) zrevidována a rozpracována.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper focuses on the annotation of sentence modality in the Prague Dependency Treebank (PDT). Sentence modality is expressed by a combination of several means in Czech, from which the category of verbal mood and the final punctuation of the sentence are the most important ones. In PDT 2.0, sentence modality was assigned semi-automatically to the root node of each sentence (tree) and further to the roots of parenthesis and direct speech subtrees. As this approach was too simple to adequately represent the linguistic phenomenon in question, the method for assigning the sentence modality has been revised and elaborated for the forthcoming version of the treebank (PDT 3.0).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zpětno vazební učení již bylo úspěšně použito k optimalizaci statisktických dialogových systémů. Typicky zpětnovazební učení se učí online on-policy tj. v přímé interakci s uživatelem. Alternativou k tomuto přístupu je off-policy učení kdy otimální strategie řízení je určena z korpusu již dříve pořízených dialogů. Tento článek prezentuje a nový zpětnovazební algoritmus založený na přirozených gradientech a vhodné adaptaci samplování dat. Experimenty ukazují, že prezentovaný algoritmus je schopen se naučit strategii řízení, která je lepší než manuálně vytvořená strategie řízení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Reinforcement learning methods have been successfully used
to optimise dialogue strategies in statistical dialogue systems.
Typically, reinforcement techniques learn on-policy i.e., the
dialogue strategy is updated online while the system is interacting
with a user. An alternative to this approach is off-policy
reinforcement learning, which estimates an optimal dialogue
strategy offline from a fixed corpus of previously collected
dialogues.
This paper proposes a novel off-policy reinforcement
learning method based on natural policy gradients and importance
sampling. The algorithm is evaluated on a spoken
dialogue system in the tourist information domain. The experiments
indicate that the proposed method learns a dialogue
strategy, which significantly outperforms the baseline handcrafted
dialogue policy</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme značkovač závislostních vztahů založený na metodě MIRA, vyladěný na češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a MIRA-based labeller designed to assign dependency relation labels to edges in a dependency parse tree, tuned for Czech language. The labeller was created to be used as a second stage to unlabelled dependency parsers but can also improve output from labelled dependency parsers. We evaluate two existing techniques which can be used for labelling and experiment with combining them together. We describe the feature set used. Our final setup significantly outperforms the best results from the CoNLL 2009 shared task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme vylepšenou verzi systému DEPFIX pro automatický post-editing výstupu z anglicko-českého strojového překladu, který se snaží zlepšovat jeho plynulost. Rozšířili jsme sadu pravidel použitou původním systémem DEPFIX a změřili výkon jednotlivých pravidel. Navíc jsme dvěma způsoby upravili parser McDonalda et al. (2005) pro zvýšení kvality parsingu výstupu strojového překladu. Ukazujeme, že náš systém je schopný zlepšit výstup nejmodernějších překladových systémů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an improved version of DEPFIX, a system for automatic rule-based post-processing of English-to-Czech MT outputs designed to increase their fluency. We enhanced the rule set used by the original DEPFIX system and measured
the performance of the individual rules.
We also modified the dependency parser of
McDonald et al. (2005) in two ways to adjust
it for the parsing of MT outputs. We show that
our system is able to improve the quality of the
state-of-the-art MT systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme dvě metody pro trénování závislostního parseru vhodného pro parsing výstupů strojového překladu. Upravili jsme MST parser použitím dalších rysů ze zdrojového jazyka a zavedením umělých gramatických chyb do trénovacích dat parseru, takže tato více odpovídají výstupu storjového překladu. Upravený parser evaluujeme na systému DEPFIX, který zlepšuje výstupy anglicko-českého strojového překladu automatickými opravami založenými na pravidlech. Obě úpravy parseru vedou ke zvýšení skóre BLEU; jejich kombinace byla evaluována manuálně a vykazuje statisticky signifikantní zlepšení kvality překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present two dependency
parser training methods appropriate for parsing outputs of statistical machine translation (SMT), which pose problems to standard
parsers due to their frequent ungrammaticality. We adapt the MST parser by exploiting
additional features from the source language,
and by introducing artificial grammatical errors in the parser training data, so that the
training sentences resemble SMT output.
We evaluate the modified parser on DEPFIX, a system that improves English-Czech
SMT outputs using automatic rule-based corrections of grammatical mistakes which requires parsed SMT output sentences as its input. Both parser modifications led to improvements in BLEU score; their combination was evaluated manually, showing a statistically significant improvement of the translation quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Množstvo pozornosti v MT komunitě bylo v nedávne době věnováno zlepšování lexikálního výběru v cílovém jazyce pomocí zachycení kontextu širšího nežli jedna věta. V této přednášce prezentuji náš příspěvek k těmto snahám, konkrétně pokrok v obohacování překladových modelů pro překlad z angličtiny do češtiny systémem TectoMT.
Ze všeho nejdřív jsme provedli čistě techický krok. Nahradili jsme doteď používaný modul MaxEnt na strojové učení nástrojem Vowpal Wabbit. Ten nám nejenom umožňuje trénovat modely rychleji a tím využít víc trénovacích příkladů a rysů, ale také nabízí bohaté možnosti parametrizace, co dohromady může vést k zlepšení našeho MT systému.
Nicméně, hlavním cílem tohoto pořád běžícího projektu je prozkoumat potenciál lexikálních kontextových rysů na vylepšení překladu. Činíme tak přidáním standardních bag-of-words rysů a zavedením nových rysů reprezentujících koncepty z Explicitní Sémantické Analýzy, co je metoda původně vyvynuta v oboru Dobývaní znalostí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Much of the attention in the MT community has recently been devoted to improving the lexical choice in the target language by capturing a context wider than just a single sentence.
In this talk I will present our contribution to these efforts, particularly the progress in enriching translation models for English to Czech translation within the TectoMT system.
First of all, we performed a pure technical step. We replaced the MaxEnt learning module used so far with the Vowpal Wabbit learning toolkit. It
not only allows us to train our models faster, to exploit more training examples and features, but it also offers rich parametrization options, which can together lead to improvement of our MT system.
However, the main objective of this ongoing work is to explore the potential of lexical context features to improve the translation. We do so by including the standard bag-of-words features and by introducing novel features representing concepts coming from Explicit Semantic
Analysis, which was originally developed in the field of Information Retrieval.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této přednášce jsem prezentoval posun témy mojeho výzkumu z využití koreference na využití textového kontextu všeobecně v úloze strojového překladu. Představil jsem lexikální diskriminativní překladové modely a prvotní experimenty s nimi v rámci překladu z angličtiny do češtiny systémem TectoMT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this talk I presented the shift of my research from utilization of coreference to utilization of text context in general in the task of Machine Translation. I introduced lexical discriminative translation models and the initial experiments with them integrated within the English to Czech translation scenario in TectoMT system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Identifikace jazyku psaného textu se zkoumá již několik desetiletí. Navzdory tomuto faktu se většina vůzkumů soustředila pouze na pár nejčastěji používaných jazyků zatímco ty ostatní jsou ignorovány. Při identifikaci velkého množství jazyků je nutné řešit jiné problémy než u identifikace malého množství jazyků, protože v opačném případě nastává pokles přesnosti. Cílem tohoto článku je prozkoumat důvody tohoto poklesu. Aby bylo možné izolovat jednotlivé faktory použili jsme 5 různých algoritmů a 3 různé počty jazyků. SVM algoritmus dosáhl úspěšnosti 98% pro 90 jazyků a YALI algoritmus založená na ohodnocující funkci dosáhl úspěšnosti 95,4%. YALI algoritmus je sice nepatrně horší, ale jazyky identifikuje 17x rychleji a jeho trénování je dokonce 4000x rychlejší.

Připravili jsme také 3 různé datasety s různým počtem jazyků a velikostí vzorků, abychom překonali nedostatek veřejně dostupných trénovacích dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Language identification of written text has been studied for several decades. Despite this fact, most of the research is focused on a few most spoken languages, whereas the minor ones are ignored. The identification of a larger number of languages brings new difficulties that do not occur for a few languages. These difficulties are causing decreased accuracy. The objective of this paper is to investigate the sources of such degradation. In order to isolate the impact of individual factors, 5 different algorithms and 3 different number of languages are used. The Support Vector Machine algorithm achieved an accuracy of 98% for 90 languages and the YALI algorithm based on a scoring function had an accuracy of 95.4%. The YALI algorithm has slightly lower accuracy but classifies around 17 times faster and its training is more than 4000 times faster.

Three different data sets with various number of languages and sample sizes were prepared to overcome the lack of standardized data sets. These data sets are now publicly available.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vytvořili jsme korpus obsahující texty ve 106 jazycích z dokumentů, které jsou dostupné na Internetu a Wikipedii. W2C Web Corpus obsahuje 54,7 GB textu a W2C Wiki Corpus obsahuje 8,5 GB textu. W2C Web Corpus obsahuje více než 100 MB textu pro 75 jazyů a alespoň 10 MB textu pro 100 jazyků. Tyto korpusy jsou jedinečným zdrojem dat pro lingvistiku, protože překonávají všechny dosud publikované práce, jak v množství nashromážděných textů, tak i v množství obsažených jazyků. Tento zdroj dat může být především užitečný pro vědce specializujícící se na vývoj vícejazyčných technologií. Také jsme vyvinuli software, který výrazně usnadňje tvorbu korpusů pro libovolný jazyk z textů volně dostupných na Internetu. Při vývoji jsme se hlavně zaměřili na komponenty pro filtrovaní a odstraňování duplicit, což nám umožnilo dosáhnout vysoké kvality výsledných dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We have built a corpus containing texts in 106 languages from texts available on the Internet and on Wikipedia. The W2C Web Corpus contains 54.7 GB of text and the W2C Wiki Corpus contains 8.5 GB of text. The W2C Web Corpus contains more than 100 MB of text available for 75 languages. At least 10 MB of text is available for 100 languages. These corpora are a unique data source for linguists, since they outclass all published works both in the size of the material collected and the number of languages covered. This language data resource can be of use particularly to researchers specialized in multilingual technologies development. We also developed software that greatly simplifies the creation of a new text corpus for a given language, using text materials freely available on the Internet. Special attention was given to components for filtering and de-duplication that allow to keep the material quality very high.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Různé experimenty z literatury naznačují, že ve statistickém strojového překladu (SMT), předzpracování nebo závěrečné úpravy pro morfologicky bohaté jazyky vedou k lepší kvalitě překladu. V této práci jsme se zaměřili na jazykový pár angličtina-tamilština.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Various experiments from literature suggest that in statistical machine translation (SMT), applying either pre-processing or post-processing to morphologically rich languages leads to better translation quality. In this work, we focus on the English-Tamil language pair. We implement suffix-separation rules for both of the languages and evaluate the impact of this
preprocessing on translation quality of the phrase-based as well as hierarchical model in terms of BLEU score and a small manual evaluation. The results confirm that our simple suffix-based morphological processing helps to obtain better translation performance. A by-product of our 
efforts is a new parallel corpus of 190k sentence pairs gathered from the web.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Anotované korpusy jako treebanks jsou důležité pro vývoj analyzátorů, jazykové aplikace, stejně jako porozumění
Jazyk sám. Jen velmi málo jazyků mají tyto omezené zdroje. V tomto článku si popíšeme naše úsilí v syntakticky anotace
malé korpusy (600 vět) z Tamil jazyce. Naše anotace je podobný Pražského závislostního korpusu (PDT) a skládá se z
Anotace na 2 podlažích či vrstev: (i) Morfologická rovina (m-layer) a (ii) analytické vrstvy (vrstvy). U obou vrstev, uvádíme
anotace programů, tj. poziční značení pro m-layer a vztahy závislosti na několika vrstev. Na závěr budeme diskutovat některé otázky v
korpus vývoj pro Tamil.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Annotated corpora such as treebanks are important for the development of parsers, language applications as well as understanding of the
language itself. Only very few languages possess these scarce resources. In this paper, we describe our efforts in syntactically annotating
a small corpora (600 sentences) of Tamil language. Our annotation is similar to Prague Dependency Treebank (PDT) and consists of
annotation at 2 levels or layers: (i) morphological layer (m-layer) and (ii) analytical layer (a-layer). For both the layers, we introduce
annotation schemes i.e. positional tagging for m-layer and dependency relations for a-layers. Finally, we discuss some of the issues in
treebank development for Tamil.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Morph délka je jedním z orientační funkce
, která pomáhá učit morfologii jazyků,
zejména aglutinační jazyky.
V tomto článku vám představíme jednoduchý bez dozoru
model pro morfologické segmentace
a pozorujete, jak se znalosti morph
Délka ovlivnit výkon segmentaci
úkol v rámci bayesovského rámce.
Model je založen na (Goldwater et
al., 2006) unigram slovo segmentace modelu
a předpokládá jednoduchou distribuci přes předchozí
morph délka. Jsme experimentovat tento model
na dvou velmi příbuzných a aglutinační jazyky
Tamil a Telugu to, a porovnávat
naše výsledky s nejmodernější Morfessor
systém. Ukazujeme, že znalost
morph délka má pozitivní vliv a poskytuje
konkurenceschopných výsledků, pokud jde o celkový
výkon.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Morph length is one of the indicative feature
that helps learning the morphology of languages,
in particular agglutinative languages.
In this paper, we introduce a simple unsupervised
model for morphological segmentation
and study how the knowledge of morph
length affect the performance of the segmentation
task under the Bayesian framework.
The model is based on (Goldwater et
al., 2006) unigram word segmentation model
and assumes a simple prior distribution over
morph length. We experiment this model
on two highly related and agglutinative languages
namely Tamil and Telugu, and compare
our results with the state of the art Morfessor
system. We show that, knowledge of
morph length has a positive impact and provides
competitive results in terms of overall
performance.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce se zabývá využitím manuálně post-editovaných automatických překladů pro iterativní trénování systémů statistického strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The post-editing of machine translated content has proved to be more productive than translating from
scratch in the localisation industry. A study carried out at Autodesk shows that Machine Translation (MT) and post-editing of technical documentation by professional translators provides a sustained productivity increase across different languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce se zabývá evaluací výkonosti větných a slovních zarovnávačů, což je velice významný aspekt pro průmyslové nasazení, a ukazuje, že zdroje vyžadované různými nástroji se velice liší.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a novel efﬁciencybased evaluation of sentence and word aligners. This assessment is critical in order to make a reliable use in industrial scenarios. The evaluation shows that the resources required by aligners differ rather broadly. Subsequently, we establish limitation mechanisms on a set of aligners deployed as web services. These results, paired with the quality expected from the aligners, allow providers to choose the most appropriate aligner according to the task at hand.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme TerrorCat, metriku kvality strojového překladu s níž jsme se účastnili soutěže WMT12. TerrorCat se opírá o automaticky odhadnuté počty několika kategorií překladových chyb a předpovídá tak, která ze dvou kandidátských vět je lepší.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present TerrorCat, a submission to the
WMT’12 metrics shared task. TerrorCat uses
frequencies of automatically obtained translation
error categories as base for pairwise comparison
of translation hypotheses, which is in
turn used to generate a score for every translation.
The metric shows high overall correlation
with human judgements on the system
level and more modest results on the level of
individual sentences.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V nedávné době se objevily první automatické metody identifikace chyb v překladu. Představujeme materiál, na němž lze takové metody vyhodnocovat, konkrétně kolekci textů, kde lidé ručně překladové chyby označili.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Recently the first methods of automatic diagnostics of machine translation have emerged; since this area of research is relatively young,
the efforts are not coordinated. We present a collection of translation error-annotated corpora, consisting of automatically produced trans-
lations and their detailed manual translation error analysis. Using the collected corpora we evaluate the available state-of-the-art methods
of MT diagnostics and assess, how well the methods perform, how they compare to each other and whether they can be useful in practice.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme komplexní open-source nástroj pro detailní analýzu chyb strojového překladu, který umožňuje automatickou detekci a klasifikaci chyb, poskytuje několik algoritmů pro jednojazyčné zarovnání (alignment) slov a procházení trénovacích a testovacích korpusů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a complex, open source tool for detailed machine translation error analysis providing the user with automatic error detection
and classification, several monolingual alignment algorithms as well as with training and test corpus browsing. The tool is the result of
a merge of automatic error detection and classification of Hjerson (Popović, 2011) and Addicter (Zeman et al., 2011) into the pipeline
and web visualization of Addicter. It classifies errors into categories similar to those of Vilar et al. (2006), such as: morphological, reordering, missing words, extra words and lexical errors. The graphical user interface shows alignments in both training corpus and
test data; the different classes of errors are colored. Also, the summary of errors can be displayed to provide an overall view of the MT
system’s weaknesses. The tool was developed in Linux, but it was tested on Windows too.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje Korpus starších českých textů a ukazuje možnosti jeho využití v lingvistice.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article describes the Corpus of older Czech texts and demonstrates how it is possible to use it in linguistics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V publikované verzi přednášky se syntax hodnotí jako poměrně nová disciplina jazykovědy, která však má v české lingvistice důstojnou reprezentaci od 40. let minulého století. Byla zahájena Šmilauerovou Novočeskou skladbou a pokračuje brněnskými syntaxemi Grepla a Karlíka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The two authors, scolars suscribing to two different syntactic streams, present their views on classical syntactic handbooks from the 1940s and 1950s. The two main streams are focused here (two-level valency syntax and functional generative description).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek prezentuje systém pro kombinování lidských přepisů s automatickým rozpoznáváním řeči pro vytvoření kvalitního přepisu velkého korpusu v dobrém čase. Systém používá web jako rozhraní pro přehrávání audia, synchronní zobrazování automaticky získaného přepisu a umožnění uživateli opravovat chyby v přepisu. Lidmi zaslané opravy jsou poté použity ve statistickém rozpoznávání mluvené řeči pro zdokonalení akustického i jazykového modelu a přegenerování celého přepisu. Systém je v současnosti vyvíjen.

Článek prezentuje návrh systému, zpracovaný korpus, jakož i možnosti použití systému na jiných datech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents a system for combining human transcriptions with automated speech recognition to create a quality transcription of a large corpus in good time. The system uses the web as interface for playing back audio, displaying the automatically-acquired transcription synchronously, and enabling the visitor to correct errors in the transcription. The human-submitted corrections are then used in the statistical ASR to improve the acoustic as well as language model and re-generate the bulk of transcription. The system is currently under development.

The paper presents the system design, the corpus processed as well as considerations for using the system in other settings.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Arabština je známá svojí bohatou a složitou morfologií. Tato práce se zabývá problémem generování arabských slovních forem za účelem kontroly textu. Námi vytvořený slovník obsahuje 9 milionů povrchových forem slov. Práce dále popisuje  konstrukci a evaluaci znakového jazykového modelu pro kontrolu pravopisu arabských slov.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Arabic is a language known for its rich and complex morphology. Although many research projects have focused on the problem of 
Arabic morphological analysis using different techniques and approaches, very few have addressed the issue of generation of fully 
inflected words for the purpose of text authoring.  Available open-source spell checking resources for  Arabic are too small and 
inadequate. Ayaspell, for example, the official resource used with OpenOffice applications, contains only 300,000 fully inflected 
words. We try to bridge this critical gap by creating an adequate, open-source and large-coverage word list for Arabic containing 
9,000,000 fully inflected surface words. Furthermore, from a large list of valid forms and invalid forms we create a character-based 
tri-gram language model to approximate knowledge about permissible character clusters in Arabic, creating a novel method for 
detecting spelling errors. Testing of this language model gives a precision of 98.2% at a recall of 100%. We take our research a step 
further by creating a context-independent spelling  correction tool using a finite-state automaton that measures the edit distance 
between input words and candidate corrections, the  Noisy Channel Model, and knowledge-based rules. Our  system performs 
significantly better than Hunspell in choosing the best solution, but it is still below the MS Spell Checker.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje soutěž “Shared Task on Applying Machine Learning Techniques to Optimise the Division of Labour in Hybrid Machine
Translation” (ML4HMT), zaměřenou na výzkum kombinování systémů pro strojový překlad. Úkolem soutěže bylo zkombinovat výstupy několika systémů pro strojový překlad a vylepšit tak výslednou kvalitu překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe the “Shared Task on Applying Machine Learning Techniques to Optimise the Division of Labour in Hybrid Machine
Translation” (ML4HMT) which aims to foster research on improved system combination approaches for machine translation (MT).
Participants of the challenge are requested to build hybrid translations by combining the output of several MT systems of different types.
We ﬁrst describe the ML4HMT corpus used in the shared task, then explain the XLIFF-based annotation format we have designed for
it, and brieﬂy summarize the participating systems. Using both automated metrics scores and extensive manual evaluation, we discuss
the individual performance of the various systems. An interesting result from the shared task is the fact that we were able to observe
different systems winning according to the automated metrics scores when compared to the results from the manual evaluation. We
conclude by summarising the ﬁrst edition of the challenge and by giving an outlook to future work.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V poslední době se výzkum v oblasti strojového překladu zaměřuje mimo jiné na hybridní a kombinavané systémy, které často vedou ke zlepšení kvality výsledného překladu.  V této práci popisujeme paralelní korpus vytvořený speciálně pro tuto úlohu. Korpus obsahuje metadata a anotace poskytované různými systémy strojového překladu, které lze v hybridních a kombinovaných systémech využít.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In recent years, machine translation (MT) research focused on investigating how hybrid MT
as well as MT combination systems can be designed so that the resulting translations give an
improvement over the individual translations.
As a ﬁrst step towards achieving this objective we have developed a parallel corpus with
source data and the output of a number of MT systems, annotated with metadata information,
capturing aspects of the translation process performed by the diﬀerent MT systems.
As a second step, we have organised a shared task in which participants were requested
to build Hybrid/System Combination systems using the annotated corpus as input. The main
focus of the shared task is trying to answer the following question: Can Hybrid MT algorithms
or System Combination techniques beneﬁt from the extra information (linguistically motivated, decoding and runtime) from the diﬀerent systems involved?
In this paper, we describe the annotated corpus we have created. We provide an overview
on the participating systems from the shared task as well as a discussion of the results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvěk se zabývá otázkou, jaké jazykové prostředky mohou být zahrnuty do anotace diskurzních vztahů v Pražském závislostním korpusu (PDT), a pokouší se za tímto účelem prozkoumat tzv. alternativní lexikální vyjádření diskurzních konektorů (altlexů) v češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper concentrates on which language means may be included into the annotation of discourse relations in the Prague Dependency
Treebank (PDT) and tries to examine the so called alternative lexicalizations of discourse markers (AltLex’s) in Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Práce se zabývá otázkou, jaké jazykové prostředky mohou být zahrnuty do anotace
diskurzních vztahů Pražského závislostního korpusu (PDT). Jejím cílem je prozkoumat
alternativní vyjádření diskurzních konektorů (tzv. altlexů) v češtině. Analýza vychází
z anotovaných dat PDT, jejím předmětem je mimo jiné srovnání českých altlexů
vyskytujících se v PDT a anglických altlexů z PDTB (anotovaného pensylvánského korpusu
Penn Discourse Treebank). Práce přináší lexikálně-syntaktickou a sémantickou klasifikaci
českých altlexů a analýzu jejich současné anotace v PDT. V současné době PDT obsahuje 306
vyjádření (v 43 955 větách), která byla anotátory označena jako altlexy. Jak ovšem tato práce
dokládá, toto číslo není konečné. Předpokládáme, že počet altlexů se po důkladném
zpracování podstatně zvýší, protože altlexy nejsou syntakticky ani lexikálně omezeny a
některé z nich vykazují velký stupeň variability.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper concentrates on which language means may be included into the annotation of
discourse relations in the Prague Dependency Treebank (PDT) and tries to examine the
so called alternative lexicalizations of discourse markers (AltLex’s) in Czech. The analysis
proceeds from the annotated data of PDT and tries to draw a comparison between the Czech
AltLex’s from PDT and English AltLex’s from PDTB (the Penn Discourse Treebank). The
paper presents the lexico-syntactic and semantic characterization of the Czech AltLex’s and
comments on the current stage of their annotation in PDT. In the current version, PDT
contains 306 expressions (out of the total 43,955 of sentences) that were labeled by annotators
as being an AltLex. However, as the analysis demonstrates, this number is not final. We
suppose that it will increase after the further elaboration, as AltLex’s are not restricted to a
limited set of syntactic classes and some of them exhibit a great degree of variation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Studujeme vliv různých metod výběru dat na anglicko-český strojový překlad. Vyhodnocujeme kvalitu nové paralelního korpusu CzEng 1.0, popisujeme jednoduchou metodu jak zlepšit pokrytí slovníku extrahovaného z paralelních dat a zkoumáme několik metod filtrace paralelních dat pro lepší překlad. Příspěvek zároveň slouží jako popis našeho systému CU-TAMCH-BOJ v soutěži WMT12.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We provide a few insights on data selection for
machine translation. We evaluate the quality
of the new CzEng 1.0, a parallel data source
used in WMT12. We describe a simple technique
for reducing out-of-vocabulary rate after
phrase extraction. We discuss the benefits
of tuning towards multiple reference translations
for English-Czech language pair. We
introduce a novel approach to data selection
by full-text indexing and search: we select
sentences similar to the test set from a large
monolingual corpus and explore several options
of incorporating them in a machine translation
system. We show that this method can
improve translation quality. Finally, we describe
our submitted system CU-TAMCH-BOJ.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek se zabývá dvěma aspekty českého žákovského korpusu: (1) vyhodnocuje praktičnost anotačního schématu na základě výsledku iaa, (2) zkoumá možnosti automatické anotace pomocí taggerů, kontrolorů pravopisu a gramatiky</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Using an error-annotated learner corpus as the basis, the goal of this paper is two-fold: (i) to evaluate the practicality of the annotation scheme by computing inter-annotator agreement on a non-trivial sample of data, and (ii) to find out whether the application of automated linguistic annotation tools (taggers, spell checkers and grammar checkers) on the learner text is viable as a substitute for manual annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje problémy anotace Czesl, korpusu češtiny nerodilých mluvčích, jeho anotační schéma a popis anotačního procesu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents the issues of annotation of the Czesl, a Czech learner corpus, the concept of its annotation scheme and a description of the annotation process.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V našem článku prezentujeme hlavní výsledky českého grantového projektu Web jako jazyková korpus, jehož cílem bylo vybudovat korpus českých webových textů a vyvinout a dát veřejnosti k dispozici příslušné softwarové nástroje.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In our paper, we present main results of the Czech grant project Internet
as a Language Corpus, whose aim was to build a corpus of Czech web
texts and to develop and publicly release related software tools. Our
corpus may not be the largest web corpus of Czech, but it maintains very
good language quality due to high portion of human work involved in the
corpus development process. We describe the corpus contents (2.65 billions
of words divided into three parts -- 450 millions of words from news and
magazines articles, 1 billion of words from blogs, diaries and other
non-reviewed literary units, 1.1 billion of words from discussions
messages), particular steps of the corpus creation (crawling, HTML and
boilerplate removal, near duplicates removal, language filtering) and its
automatic language annotation (POS tagging, syntactic parsing). We also
describe our software tools being released under an open source license,
especially a fast linear-time module for removing near-duplicates on a
paragraph level.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek zkoumá skupinu výrazů strukturujících text typu předložka + ukazovací zájmeno; věnuje se tak vztahu koreference a diskurzu. Ukazovací zájmeno zpravidla odkazuje na antecedent, zatímco celý výraz může, ale nemusí, nést diskurzní význam ve smyslu diskurzního konektoru. Popisujeme vlastnosti těchto výrazů ve vztahu k jejich antecedentům, pozici mezi dalšími způsoby strukturování textu a jejich vlastnosti typické pro "funkci konektoru" ve srovnání s jejich "nekonektorovou funkcí". Analýza je provedena na českých datech Pražského závislostního korpusu 2.0.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This contribution explores the subgroup of text structuring expressions with the form preposition + demonstrative pronoun, thus it is devoted to an aspect of the interaction of coreference relations and relations signaled by discourse connectives in a text. The demonstrative pronoun typically signals a referential link to an antecedent, whereas the whole expression can, but does not have to, carry a discourse meaning in sense of discourse connectives. We describe the properties of these phrases/expressions with regard to their antecedents, their position among the text-structuring language means and their features typical for the “connective function” of them compared to their “non-connective function”. The analysis is carried out on Czech data from the Prague Dependency Treebank 2.0.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato prírucka slouží jako anotacní manuál pro zachycování mezivýpovedních textových
vztahu (diskurzu) na materiálu Pražského závislostního korpusu (PDT) verze 2.5. V rámci
tektogramatické roviny (TR) byla podrobne popsána podkladová syntaktická struktura vety
vcetne aktuálního clenení a základních koreferencních vztahu. Anotace
textových vztahu se na tektogramatické rovine zakládá a TR je v nekterých aspektech pro úcely zachycování
textových vztahu prejímána. Tento manuál proto zachovává pojmový aparát popisující
tektogramatickou reprezentaci a predpokládá alespon základní znalost anotace na této rovine.
Nove jsou zavedeny a vysvetleny pojmy z oblasti analýzy textových vztahu, zejména pak ty,
které jsou inspirovány partnerským projektem Penn Discourse Treebank 2.0.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The report serves as an annotation manual for the portrayal of interclausal textual relations (or discourse relations) on the material of the Prague Dependency Treebank (PDT) version 2.5. Within the framework of the tectogrammatical representation (TR), the underlying syntactic structure of sentences including topic-focus articulation and basic coreference relations, has been described in detail. The annotation of discourse relations is based on the tectogrammatical representation and, in some aspects, TR is adopted for the portrayal of discourse relations. This manual maintains the terminology describing the tectogrammatical representation and presupposes at least a basic knowledge of annotation on this layer. There are some newly introduced and explained terms from the field of analysis of discourse relations, especially those that are inspired by a partnership project Penn Discourse Treebank 2.0.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CD obsahuje data PDT 2.5 s anotací mezi- i vnitrovětných diskurzních vztahů s explicitními konektory, rozšířené textové koreference a asociační anafory. Představuje novou vrstvu anotace nad již existujícími vrstvami PDT a zachycuje lingvistické jevy z pohledu struktury a koherence textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The CD contains the data of PDT 2.5 with the annotation of inter- and intra-sentential discourse relations with explicit connectives, extended textual coreference and bridging anaphora. It represents a new layer of manual annotation, above the existing layers of the PDT and it portrays linguistic phenomena from the perspective of the text structure and coherence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku porovnávám valenci českých a ruských sloves v rámci jejich sémantických tříd.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we have described the dissimilarities in Czech and Russian Valency based on the material of the Czech lexicon Vallex. Our main hypothesis is that the differences in valency structure might be explained by the semantics of verbs , so we have exploited the classification of the semantic classes provided by Vallex. In almost in each verb class we have found some regular dissimilarity that is typical of this class.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku jsou popsané některé syntaktické rozdíly mezi češtinou a ruštinou důležité zejména pro strojový překlad.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a comparative study of some constructions in Czech and Russian. Though Czech and Russian are closely related Slavic languages, they have a few differences at the level of syntax, morphology and their semantics. We discuss incongruities that we found in a parallel Czech-Russian corpus, mainly reflecting differences in the sentence structure. The linguistic evidence presented in the paper will be used while constructing the transfer
module of a rule-based machine translation system between Czech and Russian.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nové webové rozhraní česko-ruského paralelního korpusu UMC 0.1.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe the Czech-Russian parallel corpus that was initially created as training data for Machine Translation systems. The corpus has been available in a format suitable for the
computer processing, but theoretical linguists interested in the resources would not benefit much from them. So we decided to put the corpus into a user-friendly environment.
They are now accessible via a simple www interface, based on the Manatee backend. Both parts of each corpus can be queried using full CQL
syntax with regular expression based search of wordforms, lemmas and POS/morphological tags.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Feat je nástroj pro víceúrovňovou chybouvou anotaci žákovských korpusů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Feat is an environment for layered error annotation of learners corpora.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Morph - systém pro morfologickou analýzu a značkování, včetně podpůrných nástrojů pro konverzi korpusů, evaluaci, atd. Oproti verzi z r.2010 byly přidány nové algoritmy pro analýzu, vylepšen formát zadávání dat, přidána podpora obecných poz.tagsetů, atd.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Morph - system for morphological analysis and tagging, including supporting tools for corpus conversion, evaluation, etc. In comparison with version from 2010, this version contains additional analysis algorithms, supports better data format and general positional tagsets, etc.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Čapek (dříve Elysium) je anotační editor tvaroslovných a větných rozborů pro školáky. Je jazykově nezávislý.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Capek (Elysium formerly) is an annotation editor of morphology and syntax for school children. The editor is language independent.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje tagger pro staročeštinu (1200-1500 AD), flektivní jazyk s bohatou morfologií. Praktická omezení (žádní rodilí mluvčí, limitované korpusy a slovníky, limitované možnosti financování) dělají ze staročeštiny ideální objekt metody vývoje morfologických taggerů nenáročných na zdroje, kterou vyvíjíme (např. Hana et al., 2004; Feldman and Hana, 2010). Jako aproximaci neexistujících staročeských zdrojů používáme zdroje pro současnou češtinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes a tagger for Old Czech (1200-1500 AD), a fusional language with rich morphology. The practical restrictions (no native speakers, limited corpora and lexicons, limited funding) make Old Czech an ideal candidate for a resource-light cross-lingual method that we have been developing (e.g. Hana et al., 2004; Feldman and Hana, 2010). We use a traditional supervised tagger. However, instead of spending years of effort to create a large annotated corpus of Old Czech, we approximate it by a corpus of Modern Czech. We perform a series of simple transformations to make a modern text look more like a text in Old Czech and vice versa. We also use a resource-light morphological analyzer to provide candidate tags. The results are worse than the results of traditional taggers, but the amount of language-specific work needed is minimal.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá postavením patientu a způsobového volného slovesného doplnění v povrchové struktuře výpovědi. Rozlišuje přitom obligatorní a fakultativní výskyty způsobového doplnění a zohledňuje jeho formu (adverbiální/jmenné vyjádření).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Paper deals with the position of Patient and free verbal modification expressing Manner in the surface structure of the utterance. It distinguishes obligatory and facultative tokens of Manner and also its form (adverb, noun).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Poster a abstrakt popisují obvyklý slovosled vybraných volných slovesných doplnění. Berou přitom v úvahu vliv působení slovesné valence. Popisují zvlášť umístění obligatorních a fakultativních doplnění.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Poster and abstract describe the usual word order of chosen free verbal modifications. They take into account the influence of verbal valency. They desribe the position of obligatory and facultative modifications.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje slovosledné tendence vnitřních participantů - aktoru a patientu. Na základě širokého vzorku dat z Pražského závislostního korpusu vyvozuje, jaké postavení ve větě tyto participanty mají v závislosti na působení určitých slovosledných faktorů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Paper describes tendencies in word order of inner participants - of Actor and Patient. On the basis of large sample of data from Prague Dependency Treebank, it demonstrates what position in sentence these participants take (depending on particular factors influencing word order).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Poster popisuje vztah sémantických vlastností funktorů a valence.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Poster describes the relation between the categories of language meaning and valency.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku se zabýváme základními výrazy s textově strukturační funkcí, které mohou v češtině signalizovat příčinné vztahy (např. protože, proto, tedy). Na základě dat shromážděných v Pražském závislostním korpusu zkoumáme charakteristické rysy těchto výrazů s cílem podpořit nebo zpochybnit vyčleňování různých tříd pro tyto výrazy, které je běžné nejen v naší, ale i v zahraniční literatuře.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper is focused on basic expressions with text-structuring function which can indicate the causal meaning in Czech (e.g. protože (because), proto (therefore), tedy (thus)). Based on the data collected in the Prague Dependency Treebank, this contribution explores those features of these expressions which characterize differences in their properties and it aims at supporting or questioning the establishing of different classes of them which is common in literature.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku je nejprve stručně představen projekt anotace mezivýpovědních významových vztahů a jeho místo v práci na Pražském závislostním korpusu (Prague Dependency Treebank, PDT). Hlavní část příspěvku se zabývá konektivními prostředky, které mezivýpovědní významové vztahy signalizují. Konkrétně jsou představeny tři jevy – (i) významová diverzita (a nejednoznačnost) výrazu tedy, (ii) některé problematické stránky pojímání rematizátorů jako mezivýpovědních konektivních prostředků ilustrované na rematizátoru také a (iii) různé interpretace výrazu tak jako prostředku textové koherence.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper is focused on some problematic aspects of connective means in discourse annotation in the Prague Dependency Treebank (PDT). First, the contribution introduces some basic principles of discourse annotation and its position in the PDT. It also describes expressions which can act as connectives of discourse relations. The main part is devoted to three problematic aspects of connective means: (i) semantic diversity of connective tedy (roughly: thus), (ii) ambiguous role of rhematizators among discourse connectives (illustrated in rhematizator také (also)) and (iii) various roles of expression tak (so, thus) in text coherence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek reprezentuje první krok v porovnání vlastností syntakticko-sémantických vztahů přítomných ve struktuře věty a jejich ekvivalentů ve struktuře diskurzu. Rozlišujeme řadu typů vztahů, které mohou být vyjádřeny jak v jedné větě (tj. ve stromu), tak i v delším textu, napříč hranicemi vět (mezi stromy). Tvrdíme, že tyto vztahy na jednu stranu zachovávají své sémantické vlastnosti jak uvnitř věty, tak v delším textu (např. příčinný vztah zůstává příčinným vztahem), na druhou stranu ale v souladu se sémantickými vlastnostmi vztahů je jejich distribuce uvnitř vět či mezi větami velice rozličná. V této studii toto pozorování ověřujeme na dvou případech (na podmínkovém a specifikačním vztahu) a dokládáme podobným chováním anglických dat projektu Penn Discourse Treebank.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present contribution represents the first step in comparing the nature of syntactico-semantic relations present in the sentence structure to their equivalents in the discourse structure. We distinguish various types of relations that can be expressed both within a single sentence (i.e. in a tree) and in a larger
text, beyond the sentence boundary (between trees). We suggest that, on the one hand, each type of these relations preserves its semantic nature both within a sentence and in a larger text (e.g. a causal relation remains a causal relation) but, on the other hand, according to the semantic properties of the relations, their distribution in a sentence or between sentences is very diverse. In this study, this observation is analyzed for two cases (relations of condition and specification) and further supported by similar behaviour of the English data from the Penn Discourse Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Analýza realizace členské negace v češtině z pohledu konstrukčně-gramatického.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A constructional analysis of the constituent negation in Czech sentences.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kronika: o mezinárodním setkání komputačních lingvistů zaměřeném na problematiku anotace závislostních korpusů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Chronicle: on the international meeting of computational linguists focused on the dependency treebanks annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek bude věnován problematice členské negace a způsobům jejího vyjadřování v současné češtině, respektive v datech Českého národního korpusu (především SYN2005) a Pražského závislostního korpusu (verze 2.0). Za členskou negaci byla dosud ve většině českých příruček považována taková negace, která neoperuje na predikátu. Autorka na základě analýzy dat obou korpusů a studia české i zahraniční odborné literatury týkající se dané problematiky navrhne nové, primárně sémantické vymezení členské negace v češtině a prozkoumá možnosti zachycování tohoto jevu ve strukturách tektogramatické roviny PDT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk is focused on constituent negation and ways of its expressing in contemporary Czech, or more precisely in Czech National Corpus (SYN2005) and Prague Dependency Treebank (version 2.0). In most of the Czech linguistic handbooks, constituent negation was treated as such a negation that is not a part of verb. The author offers a new primarily semantic definition of constituent negation in Czech, based on data research and study of relevant literature concerning syntactic negation. Further, the author explores the possibility to express the findings on the tectogrammatical level of PDT.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rubrika Horizonty: zamyšlení nad smyslem a tématy pravidelného setkání odborníků z oboru psychologie, filozofie, neurověd, lingvistiky, kulturní antropologie a umělé inteligence.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Section Horizons: analysis of sense and themes of the annual meeting of psychologists, philosophers, neurologists, linguists, cultural anthropologists and artificial intellingence specialists.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek prezentuje předběžný průzkum o možných vztazích mezi
syntaktická struktura a polarita české věty pomocí tzv.
sentiment analýzu počítačového korpusu.Hlavním cílem analýzy je cit
detekce pozitivní nebo negativní polaritou, nebo neutralita věty (nebo, více
široce, text). Nejčastěji se tento proces probíhá tím, že hledá polarity položek,
tj. slova nebo fráze neodmyslitelně opatřené kladné nebo záporné hodnoty. Tato slova
(věty) jsou shromažďovány v subjektivity slovníků a implementovány do počítače
korpus. Nicméně, při použití věty jako základní jednotky, na které sentiment analýza
je aplikován, je vždy důležité podívat se na jejich sémantické a morfologické analýzy,
od polarity položky mohou být ovlivněny jejich morfologické kontextu. Očekává se,
že některé syntaktické (a hypersyntactic) vztahy jsou užitečné pro identifikaci
věta polarity, jako negace, diskursivní vztahů a úroveň integrace se
z polarity položky ve struktuře. Budeme tedy navrhnout takovou analýzu
vhodný zdroj dat, bohatě anotovaný Pražský závislostní korpus.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents a preliminary research on possible relations between the
syntactic structure and the polarity of a Czech sentence by means of the so-called
sentiment analysis of a computer corpus. The main goal of sentiment analysis is the
detection of a positive or negative polarity, or neutrality of a sentence (or, more
broadly, a text). Most often this process takes place by looking for the polarity items,
i.e. words or phrases inherently bearing positive or negative values. These words
(phrases) are collected in the subjectivity lexicons and implemented into a computer
corpus. However, when using sentences as the basic units to which sentiment analysis
is applied, it is always important to look at their semantic and morphological analysis,
since polarity items may be influenced by their morphological context. It is expected
that some syntactic (and hypersyntactic) relations are useful for the identification of
sentence polarity, such as negation, discourse relations or the level of embeddedness
of the polarity item in the structure. Thus, we will propose such an analysis for a
convenient source of data, the richly annotated Prague Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pro mnoho jazyků nemůžeme natrénovat parser, protože nemáme k dispozici žádná ručně anotovaná data. Tento problém můžeme vyřešit použitím paralelního korpusu s angličtinou naparsováním amglické strany a projekcí závislostí do druhého jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>For many languages, we are not able to train any supervised parser, because there are
no manually annotated data available. This problem can be solved by using a parallel corpus
with English, parsing the English side, projecting the dependencies through word-alignment
connections, and training a parser on the projected trees. In this paper, we introduce a
simple algorithm using a combination of various word-alignment symmetrizations. We prove
that our method outperforms previous work, even though it uses McDonald's maximum-spanning-tree
parser as it is, without any "unsupervised" modifications.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Několik experimentů pro neřízený parsing češtiny s různou inicializací, s různými modely a samplovací procedurou. Nejlepší konfigurace byla aplikována na 19 jazyků z CoNLL 2006 a 2007. Výsledky jsou porovnatelné ze state-of-the-art.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a work in progress on
the task of unsupervised parsing, following the main stream approach of optimizing the overall probability of the corpus. We evaluate a sequence of experiments for Czech with various modifications of corpus initiation, of dependency edge probability model and of sampling procedure, stressing especially the treeness constraint. The best configuration is then applied to 19 languages from CoNLL-2006 and CoNLL-2007 shared tasks. Our best achieved results are comparable to the state of the art in dependency parsing and
outperform the previously published results for many languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Technická zpráva shrnuje výzkum v oblasti neřízeného závislostního parsingu na ÚFAL v roce 2011.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This technical report summarizes the research on unsupervised dependency parsing at the Institute of Formal and Applied Linguistics, Faculty of Mathematics and Physics, Charles University in Prague, in the year 2011. It describes projective and non-projective approaches of sampling of dependency trees, possibility to employ reducibility feature of dependent words, and reports results obtained across various languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Je zde popsán experiment, který automaticky opravuje chyby v gramatické shodě.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes an experiment in which
we try to automatically correct mistakes in
grammatical agreement in English to Czech
MT outputs. We perform several rule-based
corrections on sentences parsed to dependency
trees. We prove that it is possible to improve
the MT quality of majority of the systems participating
in WMT shared task. We made both
automatic (BLEU) and manual evaluations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>HamleDT je sada softwarových nástrojů pro konverzi (harmonizaci) syntakticky anotovaných korpusů, které byly vytvořeny pro různé jazyky a na základě odlišných lingvistických teorií, do jednotného anotačního schématu. Účelem je usnadnění vývoje multilinguálních technologií. V současné verzi HamleDT pokrývá 28 jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>HamleDT is a set of software tools for conversion (hamonizing) of treebanks, which were created for different languages and based on various linguistic formalisms, into the same annotation framework. The main aim is to facilitate development of multilingual technology. HamleDT covers as many as 28 languages in its current version.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V prezentovaném článku se zaměřujeme na kontrolu jako podtyp anafory. Pracujeme s teorií kontroly v závislostním rámci Funkční generativní popisu, ve kterém je kontrola definována jako vztah mezi controllerem (antecedent – sémantický argument hlavní klauze) a controllee (anafor – prázdný subjekt nefinitního doplňku (kontrolované klauze)). Nejprve tento článek představí rekonstrukci controllee na základě pravidel, pak projedná určování antecedentů od controllee na základě perceptronu. Naše řešení jsme vyhodnocovali na datech Pražského závislostního korpusu 2.0. Nicméně předpokládáme, že pravidla a atributy našeho systému jsou jazykově nezávislé a mohou být testovány v budoucnu na jiných jazycích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the present paper we focus on control as a subtype of anaphora. We work with the theory
of control present within the dependency-based framework of Functional Generative Description (FGD1), in which control is defined as a relation of a referential dependency between a controller (antecedent – semantic argument of the main clause) and a controllee (anaphor – empty subject of the nonfinite complement (controlled clause)). First this paper presents the rule-based reconstruction of controllees, then, it discusses the perceptron-based determination of the controllees’ antecedent. We evaluated our approach on data from the Prague Dependency Treebank 2.0, however, the rules and features of our system are supposed to be language independent and can be tested on other languages in the future.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku se snažíme automaticky identifikovat subjekty, které nejsou vyjádřeny ale přesto rozuměny v českých větách. Náš systém využívá metodu maximální entropie k rozeznání různých druhů nevyjádřených subjektů. Systém byl trénován a testován na Pražském závislostním korpusu. Výsledky našich experimentů přináší dále úvahu nad vhodností vybraného korpusu pro naši úlohu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we aim to automatically identify subjects, which are not expressed but nevertheless understood in Czech sentences. Our system uses the maximum entropy method to identify different types of unstated subjects and the system has been trained and tested on the Prague Dependency Treebank 2.0. The results of our experiments bring out further consideration over the suitability of the chosen corpus for our task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato technická zpráva zahrnuje výsledky obdržené během výzkumů na analýze koreference v Ústavu formální a aplikované lingvistiky, Matematicko-fyzikální fakulta, Univerzita Karlova v Praze, během 2009 - 2011. Obsahuje stručný popis zahraničních prací vztahujících k tomuto tématu, popis manuální anotace koreference v Pražském závislostním korpusu a možnosti automatické anotace koreference.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This technical report summarizes results obtained during the research on coreference resolution at the Institute of Formal and Applied Linguistics, Faculty of Mathematics and Physics, Charles University in Prague, during 2009 - 2011. It contains a brief description of foreign approaches to this topic, a description of manual coreference annotation in the Prague Dependency Treebank 2.0 and an account of possibilities of automatic coreference annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku popisujeme z lexikografického hlediska lokativní sémantickou diatezi v češtině. Navrhujeme adekvátní lexikálně-sémantickou reprezentaci sloves založenou na predikátové dekompozici (tzv. lexikálně-konceptuálních strukturách).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we deal with locative semantic diathesis in Czech from a lexicographic point of view. We distinguish semantic diatheses from grammatical diatheses as two types of  changes in valency structure of verbs. In case of grammatical diatheses, these changes arise from changes in correspondence of valency complements and surface syntactic positions and they are regular enough to be described by formal syntactic rules. In contrast, semantic diatheses are  characterized by the changes in mapping of semantic participants and valency complements. Changes in valency structure of verbs associated with semantic diatheses vary even within one type of diathesis so that they cannot be described by formal syntactic rules. For the purpose of the description of semantic diatheses, we propose to set separate valency frames corresponding to the members of semantic diatheses and to capture the relevant changes in valency structure by general lexical rules based on an appropriate formal representation of meaning of verb. In this contribution, we focus primarily on the lexical-semantic representation of verbs which represents a first part of the description of semantic diatheses. The second part of this task, i.e., formulating lexical rules is left aside here. On the example of locative semantic diathesis, we demonstrate advantage of the lexical-sematic representation based on predicate decomposition (usually called lexical-conceptual structure) over those formulated in terms of unordered set of semantic roles. Namely, we use the lexical-conceptual structure built on the subeventual analysis in which event complexity, i.e., the fact whether an event is simple, or complex, plays a key role. On this basis, we attempt to formulate the appropriate lexical-conceptual structures reflecting semantic difference between the members of locative semantic diathesis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku navrhujeme reprezentaci českých diatezí ve valenčním lexikonu českých sloves, VALLEX. Diatezemi rozumíme vztahy mezi syntaktickými strukturami založenými na témže slovese, které jsou spojeny se specifickými změnami v jeho valenční struktuře. Rozlišujeme celkem tři typy diatezí: gramatické, syntaktické a sémantické. Gramatické a syntaktické diateze jsou spojeny s natolik pravidelnými změnami ve valenčním rámci slovesa, že mohou být popsány formálními syntaktickými pravidly. V případě sémantických diatezí jsou změny natolik rozmanité, že je výhodnější je zachytit pomocí lexikálních pravidel. Závěrem ukazujeme, že jednotlivé typy diatezí mohou být vzájemně kombinovány.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we propose a method of the representation ofCzech diatheses in the valency lexicon of Czech verbs, VALLEX. Underthe term diatheses, specific relations between uses of the same verb lexeme are considered here. These relations are associated with changes in valency frames of verbs which stem from the changes in the linking of situational participants, valency complementations and surface syntactic positions. We distinguish three types of Czech diatheses according to which linguistic means they are based on: (i) grammatical, (ii) syntactic and (iii) semantic diatheses. We demonstrate that in case of grammatical and syntactic diatheses, the changes in valency structure of verbs are regular enough to be captured by formal syntactic rules whereas the changes associated with semantic diatheses can be represented rather by lexical rules. In conclusion, we show that on certain conditions the different types of diatheses can be combined together.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Závislost analýzy byla hlavním terčem výzkumu NLP pozdě kvůli jeho schopnosti pomáhat analyzovat jazyky s volným slovosledem. Závislost analýzy bylo prokázáno, že zvýšení NLP systémů v některých jazycích av mnoha případech je považována za nejmodernější v oboru. Použití analýzy závislosti je většinou omezen na bezkontextových jazyků slovosledem, ale užitečnost závislosti staveb může přinést zlepšení v mnoha Word 6000 + jazycích. Dám přehled v oblasti závislosti rozebrat přičemž Mé cíle pro budoucí výzkum. Mnoho NLP aplikací do značné míry závisí na kvalitě závislosti rozebrat. Z tohoto důvodu budu zkoumat, jak různé analyzátory a anotace programů ovlivnit celkovou NLP potrubí, pokud jde o strojový překlad, stejně jako základní analýzu přesnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Dependency parsing has been a prime focus of NLP research of late due to its
ability to help parse languages with a free word order. Dependency parsing has been shown
to improve NLP systems in certain languages and in many cases is considered the state of
the art in the field. The use of dependency parsing has mostly been limited to free word
order languages, however the usefulness of dependency structures may yield improvements
in many of the word’s 6,000+ languages.
I will give an overview of the field of dependency parsing while giving my aims for
future research. Many NLP applications rely heavily on the quality of dependency parsing.
For this reason, I will examine how different parsers and annotation schemes influence the
overall NLP pipeline in regards to machine translation as well as the the baseline parsing
accuracy.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Byt jmenná fráze struktura byla, až do nedávné doby,
standard v anotaci na Penn
Treebanks. S nedávným Kromě vnitřních
jmenná fráze anotace, závislost parsování
dolů a aplikace na potrubí NLP
jsou pravděpodobně ovlivněny. Některé strojový překlad
systémy, jako TectoMT, používat hluboké syntaxi
jako jazyková převodu vrstvy. Navrhuje se,
že změny v závislosti jmenné fráze
analýze bude mít dominový efekt se
NLP potrubí a na konci, zlepšit stroj
Překlad výstup i při snížení
v parseru přesností, že jmenná fráze
Struktura může způsobit. Tato práce zkoumá
Toto podstatné jméno věta STRUKTURY vliv na závislost
analýze, v angličtině, s maximální
Spanning Tree parser a ukazuje 2,43%, 0,23
Bleu skóre, zlepšení pro angličtiny do češtiny
strojový překlad.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Flat noun phrase structure was, up until recently,
the standard in annotation for the Penn
Treebanks. With the recent addition of internal
noun phrase annotation, dependency parsing
and applications down the NLP pipeline
are likely affected. Some machine translation
systems, such as TectoMT, use deep syntax
as a language transfer layer. It is proposed
that changes to the noun phrase dependency
parse will have a cascading effect down the
NLP pipeline and in the end, improve machine
translation output, even with a reduction
in parser accuracy that the noun phrase
structure might cause. This paper examines
this noun phrase structure’s effect on dependency
parsing, in English, with a maximum
spanning tree parser and shows a 2.43%, 0.23
Bleu score, improvement for English to Czech
machine translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení pražské anotace textových vztahů a koreference v PDT</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Introduction of the Prague annotation of discourse and coreference in PDT</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předhledová zpráva o projektu anotace textových vztahů v Pražském závislostím korpusu pro tým z University of Pennsylvania</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Overview of the annotation project of discourse relations in the Prague Dependency Treebank for Penn discourse team</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článok popisuje možné zdroje pre vytvorenie česko-slovenského paralelného korpusu, vrátane postupu, ktorý bol použitý pri vytvorení neanotovaného textového korpusu z rôznych dátových zdrojov. Popisuje výhody a nevýhody týchto zdrojov, zvlášť v
prípade, že je vytvorený korpus použitý v automatickom preklade. V článku sú tiež uvedené výsledky automatického prekladu, ktorý používa
takto vytvorený korpus.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes suitable sources for creating Czech-Slovak
parallel corpora, including our procedure of creating plain text parallel
corpora from various data sources. We attempt to address the pros and
cons of various types of data sources, especially when they are used in
machine translation. Some results of machine translation from Czech to
Slovak based on the acquired corpora are also given.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Syntax přirozeného jazyka je v hledáčku lingvistiky již několik desítek let. Teorie komplexních sítí nabízí nový pohled na syntaktické vlastnosti jazyka. Přes řadu dílčích úspěchů ale zatím ponechává některé zásadní problémy nevyřešené. Přestože statistické vlastnosti typické pro komplexní sítě lze pozorovat ve všech syntaktických sítích, vliv syntaxe samotné na tyto vlastnosti zůstává nejasný. Předložená studie je zaměřená na vliv syntaktické funkce slovesa ve větě na strukturu komplexní sítě. Slovesa hrají rozhodující roli ve větné struktuře (lokální významnost), z čehož vyvozujeme, že budou významná i z hlediska komplexní sítě. Hypotézu testujeme na šesti jazycích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Syntax of natural language has been the focus of linguistics for decades. The complex network theory, being one of new research tools, opens new perspectives on syntax properties of the language. Despite numerous partial achievements, some fundamental problems remain unsolved. Specifically, although statistical properties typical for complex networks can be observed in all syntactic networks, the impact of syntax itself on these properties is still unclear. The aim of the present study is to shed more light on the role of syntax in the syntactic network structure. In particular, we concentrate on the impact of the syntactic function of a verb in the sentence on the complex network structure. Verbs play the decisive role in the sentence structure (“local” importance). From this fact we hypothesize the importance of verbs in the complex network (“global” importance). The importance of verb in the complex network is assessed by the number of links which are directed from the node representing verb to other nodes in the network. Six languages (Catalan, Czech, Dutch, Hungarian, Italian, Portuguese) were used for testing the hypothesis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje robustní konečněstavový morfologický analyzátor pro indonéštinu (MorphInd), který se stará jak o morfologickou analýzu, tak o lematizaci daného slovního tvaru. MorphInd má větší pokrytí indonéské derivační a flexivní morfologie než existující indonéský morfologický analyzátor.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes a robust finite state morphology tool
for Indonesian (MorphInd), which handles both morphological analysis
and lemmatization for a given surface word form so that it is suitable
for further language processing. MorphInd has wider coverage on han-
dling Indonesian derivational and in
ectional morphology compared to
an existing Indonesian morphological analyzer [1], along with a more de-
tailed tagset. MorphInd outputs the analysis in the form of segmented
morphemes along with the morphological tags. The implementation was
done using finite state technology by adopting the two-level morphology
approach implemented in FOMA. It acheived 84.6% of coverage on a
preliminary stage Indonesian corpus where it mostly fails to capture the
proper nouns and foreign words as expected initially.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje možnosti začlenění morfosyntaktické informace do frázového a syntaktického statistického strojového překladu. Soustředíme 
se na těžší směr, do jazyka morfologicky bohatšího.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the integration of morpho-syntactic information in
phrase-based and syntax-based Machine Translation systems. We mainly focus on
translating in the hard direction which is translating from morphologically
poor to morphologically richer languages and also between language pairs that
have significant word order differences. We intend to use hierarchical or
surface syntactic models for languages of large vocabulary size and improve the
translation quality using two-step approach \citep{fraser}. The two-step scheme
basically reduces the complexity of hypothesis construction and selection by
separating the task of source-to-target reordering from the task of generating
fully inflected target-side word forms. In the first step, reordering is
performed on the source data to make it structurally similar to the target
language and in the second step, lemmatized target words are mapped to fully
inflected target words. We will first introduce the reader to the detailed
architecture of the two-step translation setup and later its further proposed
enhancements for dealing with the above mentioned issues. We plan to conduct
experiments for two language pairs: English-Urdu and English-Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zkoumáme frázový statistický strojový překlad mezi angličtinou a urdštinou, dvěma indoevropskými jazyky s výrazně odlišnými preferencemi slovosledu. Přeskládání slov a frází je tudíž nezbytnou součástí překladové procedury. Zatímco lokální přeskládání lze ve frázových systémech modelovat elegantně, přesuny na velkou vzdálenost jsou problematické. Provádíme pokusy s překladovým systémem Moses a probíráme přeskládávací modely, kterými Moses disponuje. Potom předkládáme náš nový, urdštiny znalý, avšak zobecnitelný přístup založený na přeskládávání frází v syntaktickém stromu zdrojové anglické věty. Naše metoda významně zlepšuje kvalitu anglicko-urdského překladu s Mosesem, měřeno jak automatickým BLEU skórem, tak subjektivním lidským hodnocením.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We investigate phrase-based statistical machine translation between English and Urdu, two Indo-European languages that differ significantly in their word-order preferences. Reordering of words and phrases is thus a necessary part of the translation process. While local reordering is modeled nicely by phrase-based systems, long-distance reordering is known to be a hard problem. We perform experiments using the Moses SMT system and discuss reordering models available in Moses. We then present our novel, Urdu-aware, yet generalizable approach based on reordering phrases in syntactic parse tree of the source English sentence. Our technique significantly improves quality of English-Urdu translation with Moses, both in terms of BLEU score and of subjective human judgments.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Archiv vizuální historie Institutu USC Shoah Foundation je unikátní sbírkou takřka 52 000 audiovi­zuálních záznamů rozhovorů s pamětníky a přeživšími holocaustu, které byly natočeny během 2. poloviny 90. let v 56 zemích a 32 jazycích (v češtině a slovenštině je přes 1 000 rozhovorů). Praha je jedním ze tří evropských měst, kde se lze ke kompletnímu obsahu licencovaného archivu připojit on-line pomocí vysokorychlostní sítě Internet 2, a to z počítačů v Centru vizuální historie Malach při Matematicko-fyzikální fakultě Univerzity Karlovy. V průměru dvouhodinové rozhovory byly vedeny podle jednotné metodiky a přeživší v nich hovoří jak o zkušenostech z období druhé světové války, tak o svém dětství a životě po válce. Vzniklý archiv je hodnotným souborem ohromujícího množství individuálních vzpomínek na celé 20. století. Institut USC Shoah Foundation se v současné době soustředí na vzdělávací využití tohoto materiálu. Archiv lze prohledávat pomocí velmi sofistikovaného uživatelského rozhraní: uživatelé mohou najít konkrétní úseky svědectví podle svého zájmu díky tezauru 55 000 hierarchicky uspořádaných klíčových slov, témat, událostí, časových úseků, jmen osob, míst atd. I samotná práce s archivem v reálném čase se proto může stát hodnotnou vzdělávací činností, rozvíjející počítačovou a informační gramotnost spolu se širším historickým a kulturním povědomím. Archiv vizuální historie je cenným zdrojem nejen pro historiky, ale lze jej využít i v mnoha dalších oblastech od lingvistiky přes psychologii až k sociologii, politologii či gender studies. K ilustraci možností vyhledávání a využití poslouží i praktická ukázka práce s archivem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The USC Shoah Foundation Institute Visual History Archive is a unique collection of almost 52 000 testimonies of the Holocaust survivors and witnesses, recorded in 56 countries and 32 languages during the late 90's (over 1 000 interviews are in Czech and Slovak language). Prague is one of the three European access points to the on-line licensed archive content, connected by the high-speed Internet 2 from the Malach Visual History Centre at the Faculty of Mathematics and Physics of the Charles University. With average 2 hours duration, the interviews have been conducted with standardised methodology. The interviewees speak about their childhood, the time before World War II, the war-related experiences and also their life after the war. The Archive is thus a valuable collection of countless individual recollections of entire 20th century. In the present time, the USC Shoah Foundation Institute is focused on using the archive testimonies for educational purposes. The Visual History Archive provides sophisticated tools for users to identify whole testimonies of relevance, as well as specific segments within testimonies that relate to their area of interest. Searching is possible using a thesaurus containing more than 55 000 geographic and experiential keywords, time intervals, names, places etc. Even the real-time work with the Archive is a valuable educational experience, connecting computer and informational literacy with broader historic and cultural knowledge. The Visual History Archive is a relevant source not only for the historians, but is applicable to many more fields of study including linguistics, political science, sociology or gender studies. As an example of the Archive possibilities, a practical illustration will be included in the presentation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek pojednává o Centru vizuální historie Malach při Univerzitě Karlově, které od ledna 2010 zpřístupňuje unikátní soubor 52 000 audiovizuálních svědectví - rozhovorů s pamětníky, pořízených v letech 1994-1999 v 56 zemích a 32 jazycích. Českojazyčná část archivu obsahuje přes 550 rozhovorů, podobné množství nahrávek je k dispozici ve slovenštině. Archiv vizuální historie je v současnosti dostupný z přístupových míst v rámci smluvních institucí, mezi něž patří i UK, která se tak řadí mezi pět evropských univerzit s plným přístupem do Archivu. I když Archiv vznikal jako dokumentace konkrétní historické události (zaměřen je na přeživší a svědky holocaustu za 2. světové války), způsoby využití nashromážděných svědectví pokrývají řadu humanitních, společenských i exaktních věd. V článku jsou nastíněny možnosti využití archivního materiálu v badatelské či pedagogické praxi s důrazem na oblast aplikované lingvistiky. Přiblížen je také projekt MALACH, který proběhl mezi lety 2002-2007 za participace UK.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article presents the Malach Centre and the USC Shoah Foundation Institute Visual History Archive. The Visual History Archive, which contains almost 52 000 witness testimonies covering the history of entire 20th century, is fully accessible through an on-line interface. The users can search for and view testimonies of interest by using more than 55 000 keywords or database of 1.1 million names. It is possible to identify relevant interviews or specific segments by using the sophisticated user search interface. Almost 25 000 interviews in English is easily accessible. The testimonies available in Malach Centre were recorded in 56 countries and in 32 languages. The content of testimonies includes material that is applicable to many different scientific fields.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>MorphCon je nový softwarový nástroj pro automatickou konverzi českých morfologických taggovacích sad (tagsetů). Software umožňuje konverzi dvou základních tagsetů češtiny: pražského pozičního a brněnského atributivního systému. MorphCon pracuje se třemi formáty dat (SimpleTag-Conversion, KWIC/Tag-Format, WPL-Format), tagsety jsou v aplikaci implementovány jako "drivery" (s funkcemi enkódování a dekódování). Klíčovou roli převodníku při konverzi plní univerzální knihovna DZ Interset. MorphCon je budován jako univerzální konvertor díky modularitě a možnosti přidat další tagsety či formáty dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>MorphCon is a new software tool for automatic conversion of Czech morphological tagsets. This software enables converting of two basic tagsets of Czech: Prague positional system and Brno's attributive system. There are three basic Input/Output (I/O) formats of data (SimpleTag-Conversion, KWIC/Tag-Format, WPL-Format). Tagsets are implemented into the MorphCon as "drivers" with "encode" and "decode" function as well as an "universal library" called DZ-Interset plays key role for the process of conversion as a transcoder. The MorphCon software is thus built as an universal converter: modularity, the Interset as a transcoder, possibility of adding of another tagsets (not only Czech ones) and I/O formats.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Termín valence označuje počet, typ a tvar argumentů, které jsou vázány k danému slovu. Jedná se o charakteristickou vlastnost jednotlivých slov, která tím pádem musí být zachycena formou slovníků. Tento článek je rešeršní studií vytvořenou s úmyslem vytvořit valenční slovník českých podstatných jmen. Na vzorku několika českých, anglických a německých valenčních slovníků (jak těch, které jsou určeny pro běžné uživatele, tak zdrojů pro počítačové zpracování přirozeného jazyka), které všechny zahrnují podstatná jména, mapujeme možné odpovědi na následující otázky:
* Komu je slovník určen?
* Které aspekty valenčního chování zahrnout?
* Jak uspořádat materiál slovníku?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The term valency refers to the number, type and form of arguments that are bound to a word. Valency is specific to any given lexical unit and therefore is covered by lexicons. This is a preliminary survey conducted with the creation
of a valency lexicon of Czech nouns in mind. The authors of such a lexicon have
to decide who will be the intended users, how the material will be presented and
which aspects of valency behaviour will be covered; we present the choices made
by the authors of several Czech, English and German resources that cover the
valency of nouns, both machine readable and printed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci představujeme jeden z možných modelů zpracovaní rozšířené textové koreference a asociační anafory na velkém
korpusu textů, který dále používáme pro anotaci daných vztahů v textech Pražského závislostního korpusu. Na základě literatury z oblasti teorie reference, diskurzu a některých dalších poznatků teoretické lingvistiky na jedné straně a s použitím existujících anotačních metodik na straně druhé jsme vytvořili detailní klasifikaci textově koreferenčních vztahů a typů vztahů asociační anafory. 
V rámci textové koreference rozlišujeme dva typy textově koreferenčních vztahů – koreferenční vztah mezi jmennými frázemi se specifickou referencí a koreferenční vztah mezi jmennými frázemi s~nespecifickou, především generickou
referencí. 
Pro asociační anaforu jsme stanovili šest typů vztahů: vztah PART mezi částí a celkem, vztah SUBSET mezi množinou a podmnožinou/prvkem množiny, vztah FUNCT mezi entitou a určitým objektem, který má vzhledem k této entitě jedinečnou funkci, vztah CONTRAST vyjadřující sémantický a kontextový protiklad, vztah ANAF označující anaforické odkazování mezi nekoreferenčními entitami a vztah REST pro jiné případy asociační anafory. 
Jedním z úkolů výzkumu bylo vytvořit systém teoretických principů, které je nutno dodržovat při anotaci koreferenčních vztahů a asociační anafory. V rámci tohoto systému byl zaveden například princip důslednosti anotace, princip
dodržování maximálního koreferenčního řetězce, princip kooperace se syntaktickou strukturou tektogramatické roviny, princip preference koreferenčního vztahu před asociační anaforou a další. 
Vypracovanou klasifikaci jsme aplikovali na koreferenční a anaforické vztahy v~Pražském závislostním korpusu (Prague Dependency Treebank, PDT). Anotace těchto vztahů byla provedena na polovině korpusu PDT (cca 25 tis. vět). Srovnání shody mezi anotátory při navazování vztahů a určování typů těchto vztahů ukázalo, že použitá klasifikace při daném rozsahu materiálu je spolehlivá zejména pro účely teoretického
výzkumu; pro počítačové aplikační účely (strojový překlad, automatické učení atd.) je nutné rozšíření materiálové základny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This book aims to present one of the possible models of processing extended textual coreference and bridging anaphora in a large textual corpora, which we then use for annotation of certain relations in texts of the Prague Dependency Treebank (PDT). We compare our annotation scheme to the existing ones with respect to the language to which the scheme is applied. We identify the annotation principles and demonstrate their application to the large-scale annotation of Czech texts. We further present our classification of coreferential relations and bridging relations types and discuss some problematic aspects in this area. An automatic pre-annotation and some helpful features of the annotation tool, such as maintaining coreferential chain, underlining candidates for antecedents, etc. are presented and discussed. Statistical evaluation is performed on the already annotated part of the Prague Dependency Treebank. We also present the first results of the inter-annotator agreement measurement and explain the most frequent cases of disagreement.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato technická zpráva popisuje projekt manuální anotace rozšířené textové koreference a asociačních vztahů, probíhající na Ústavu formální a aplikované lingvistiky, Matematicko-fyzikální fakultě Univerzity Karlovy v Praze od roku 2009. Obsahuje typologii koreferenčních a asociačních vztahů, klasifikaci jednotek anotovaných koreferencí a aplikaci anotace na PDT 2.0.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This technical report describes the project of manual annotation of extended textual coreference and bridging relations, which runs at the Institute of Formal and Applied Linguistics, Faculty of Mathematics and Physics, Charles University in Prague, since 2009. It contains the
typology of coreference and bridging relations, classification of elements that are annotated for
coreference and the application of the annotation on PDT 2.0.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CD obsahuje data PDT 2.0 s anotací rozšířené textové koreference a asociační anafory. Představuje novou vrstvu anotace nad již existujícími vrstvami PDT a zachycuje lingvistické jevy z pohledu struktury a koherence textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The CD includes the data of PDT 2.0 with the annotation of extended textual coreference and bridging relations. It represents a new layer of manual annotation, above the existing layers of the PDT and it portrays linguistic phenomena from the perspective of the text structure and coherence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek zkoumá dvě metody ručního vyhodnocení kvality překladu, které mohou pomoci identifikovat nejčastější typy chyb.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper examines two techniques of manual evaluation that can be used to identify error
types of individual machine translation systems. The first technique of “blind post-editing” is
being used in WMT evaluation campaigns since 2009 and manually constructed data of this
type are available for various language pairs. The second technique of explicit marking of errors
has been used in the past as well.
We propose a method for interpreting blind post-editing data at a finer level and compare
the results with explicit marking of errors. While the human annotation of either of the techniques
is not exactly reproducible (relatively low agreement), both techniques lead to similar
observations of differences of the systems. Specifically, we are able to suggest which errors in
MT output are easy and hard to correct with no access to the source, a situation experienced by
users who do not understand the source language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení výsledků experimentů s kombinováním několika systémů strojového překladu za účelem lepšího pokrytí cílových slovních tvarů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The results of my experiments with MT system combination aimed at better coverage of target-side word forms.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Seznámení zájemců o informatiku s úlohou strojového překladu, současnými metodami a jejich omezeními.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An introduction to the task of machine translation, current methods and their inherent limitations for those interested in computer science.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení úlohy slovního zarovnání pro kolegy z Katedry aplikované matematiky s cílem odhalit kombinatorickou povahu problému. Hlavním účelem bylo společně se přiblížit vhodné rigorózní formulaci problému tak, aby bylo možné použít známé algoritmy z diskrétní matematiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An introduction to the task of word alignment for colleagues at the Department of applied mathematics (KAM).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článok sa zaoberá strojovým prekladom medzi blízkymi
jazykmi, najmä medzi češtinou a slovenčinou. Popisuje špecifiká
vyhodnocovania strojového prekladu medzi blízkymi jazykmi. Cieľom
článku je potvrdenie staršieho predpokladu, že prekladové systémy
založené na pravidlách sú pre preklad medzi blízkymi jazykmi stále
vhodnejšie ako najnovšie systémy založené na štatistike, a to aj
napriek tomu, že je k dispozícii veľké množstvo dát.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We focus on machine translation between closely related
languages, in particular Czech and Slovak. We mention the specifics of
evaluating MT quality for closely related languages. The main
contribution is the test and confirmation of the old assumption that
rule-based systems with shallow transfer still work better than
current state-of-the-art statistical systems in this setting, unless
huge amounts of data are available</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>WMT (Workshop on Statistical Machine Translation) konaný každoročně od roku 2006, se stal jedním z nejvýznamnějších workshopů ACL. Součástí tohoto workshopu je úloha (soutěž) s vyhodnocením systémů strojového překladu, a to zejména na základě lidského hodnocení kvality překladů.
Závěrečná správa z každého workshopu uvádí přehled hlavních dosažených výsledků, ovšem nezbývá v ní místo pro hlubší analýzu. Cílem tohoto článku je prozkoumat a vysvětlit některé zajímavosti a rozpory v uváděných výsledcích. Provedená analýza ukazuje, jak by měly (a neměly) být interpretovány tyto výsledky. Článek také uvádí některá doporučené do budoucna pro organizátory WMT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Workshop on Statistical Machine Translation (WMT) has become one of ACL's
flagship workshops, held annually since 2006.  In addition to soliciting papers
from the research community, WMT also features a shared translation task for
evaluating MT systems.  This shared task is notable for having manual evaluation
as its cornerstone.
The Workshop's overview paper, playing a descriptive and administrative role, reports
the main results of the evaluation without delving deep into analyzing those results.
The aim of this paper is to investigate and explain some interesting idiosyncrasies
in the reported results, which only become apparent when performing a more thorough
analysis of the collected annotations.  Our analysis sheds some light on how the
reported results should (and should not) be interpreted, and also gives rise to some helpful
recommendation for the organizers of WMT.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nové vydání paralelního korpusu CzEng, tentokrát zaměřené na odstranění nesprávných větných párů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A new release of the parallel corpus CzEng, this time with a focus on the removal of bad sentence pairs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Navrhujeme a vyhodnocujeme jednoduchou techniku "zpětné samouky" pro strojový překlad. Technika využívá jednojazyčná data v cílové řeči k obohacení překladového slovníku existujícího statistického systému.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We propose and evaluate a simple technique of "reverse self-training" for
statistical machine translation. The technique allows to extend target-side
vocabulary of the MT system using target-side monolingual data and it is
especially aimed at translation to morphologically rich languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Používáme jednojazyčná data na cílové straně k tomu, abychom obohatili překladový model ve statistickém strojovém překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We use target-side monolingual data to extend
the vocabulary of the translation model
in statistical machine translation. This method
called “reverse self-training” improves the decoder’s
ability to produce grammatically correct
translations into languages with morphology
richer than the source language esp. in
small-data setting. We empirically evaluate
the gains for several pairs of European
languages and discuss some approaches of
the underlying back-off techniques needed to
translate unseen forms of known words. We
also provide a description of the systems we
submitted to WMT11 Shared Task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Od konce ledna 2010 je na MFF UK zpřístupněn archív nahrávek přeživších holokaustu, poskytnutý VHI  University of Southern California. Tento archív je přístupný nejen badatelům z humanitních oborů, ale i pro další výzkum v oboru zpracování mluvené řeči a analýzy textu počítačem. Článek představuje nové výsledky v obou těchto směrech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Since the beginning of 2010, the Shoa Foundation archive of Holocaust survivors memories is accessible at MFF UK thanks to a licence from University of Southern California. The archive serves research needs for scientists in the humanities as well as technologists in the field of computational linguistics and speech processing. The article presents new developments in both these areas.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pražský česko-anglický závislostní korpus  2.0 (PCEDT 2.0) nahrazuje starší verzi Pražského závislostního korpusu  1.0 (LDC2004T25). Je to manuálně parsovaný česko-anglický paralelní korpus o velikosti přes 1,2 miliónu tokenů v téměř padesáti tisících vět v každé jazykové paralele.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Prague Czech-English Dependency Treebank 2.0 (PCEDT 2.0) is a major update of the Prague Czech-English Dependency Treebank 1.0 (LDC2004T25). It is a manually parsed Czech-English parallel corpus sized over 1.2 million running words in almost 50,000 sentences for each part.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Na tomto CD se nachází víceúčelový korpus mluvených českých dialogů. Data jsou uspořádána do tří rovin: zvukový záznam, doslovný manuální přepis a editovaný přepis odpovídající standardům spisovného jazyka (tzv. rekonstrukce mluvené řeči).
Tématem dialogů je vzpomínání a konverzace nad osobní sbírkou fotografií jednoho ze zúčastněných. Dialogy byly nahrávány v rámci projektu Companions. Cílem tohoto projektu bylo vytvoření virtuálního společníka, který by si s lidským uživatelem povídal o jeho fotoalbu. Rozhovory byly nahrávány v nastavení Wizard of Oz.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This CD brings you a multi-purpose corpus of spoken dialog Czech which has been recorded, manually transcribed and manually edited in three interlinked layers.
The domain is reminiscing about personal photograph collections, which were recorded within the Companions project. The goal of this project was to create virtual companions that would be able to have a natural conversation with humans. The setup is Wizard of Oz.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Na tomto CD se nachází víceúčelový korpus mluvených anglických dialogů. Data jsou uspořádána do tří rovin: zvukový záznam, doslovný manuální přepis a editovaný přepis odpovídající standardům spisovného jazyka (tzv. rekonstrukce mluvené řeči). 

Tématem dialogů je vzpomínání a konverzace nad osobní sbírkou fotografií jednoho ze zúčastněných. Dialogy byly nahrávány v rámci projektu Companions. Cílem tohoto projektu bylo vytvoření virtuálního společníka, který by si s lidským uživatelem povídal o jeho fotoalbu. Rozhovory byly částečně nahrávány v nastavení Wizard of Oz, zčásti se jedná o spontánní konverzaci dvou lidí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This CD brings you a multi-purpose corpus of spoken dialog English. 145 469 tokens, 12 203 sentences and 864 minutes of spontaneous dialog speech have been recorded, manually transcribed and manually edited in three interlinked layers.

The domain is reminiscing about personal photograph collections, which were recorded within the Companions project. The goal of this project was to create virtual companions that would be able to have a natural conversation with humans. The setup is partly Wizard of Oz, but mostly conversation of two humans.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této kapitole se ověřuje na datech z anotovaného korpusu PDT hypotéza o posunu referenčního bodu pro časovou orientaci dějů v souvětí obsahujícícm vedlejší větu obsahovou. Dále se studuje výskyt přechodníkových konstrukcí v několika subkorpusech ČNK.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The hypothesis about the tense shifts in the complex sentences where the content clauses are included is examined on the data from the annotated corpus PDT. The occurence of transgressives in several subcorpora of the CNC is studied as well.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Infinitivní konstrukce s atributivní funkcí se vyskytují převážně jako součást analytických predikátů. Ty jsou v kapitole tříděny z hlediska vlastností kontroly na "uzavřené" a "otevřené". Zpravidla jde o typ gramatické kontroly.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Czech corpus data are studied from the point of view which infinitival attributive constructions are grammatically controlled and how to determine their controller (usually omitted on the surface). The analyzed constructions enter the analytical predicates which are classified into the "closed" and "opened" class.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slovesná adjektiva produktivně tvořená příponami 
-ící, -vší, -ný/-tý jsou chápána jako syntaktické deriváty základového slovesa. Studuje se jejich distribuce, časové vztahy nominalizace a řídícího děje a konkurence nominalizovaných vazeb a vedlejších vět.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The verbal adjectives derived by suffixes -ící, -vší, -ný/-tý are analyzed as forms of their respective basic verbs. Their distrubution, temporal relations between nominalization and governing predication are described. The competition between nominalization and subordinated clause is studied as well.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rezultativ je chápán jako typ diateze (morfologické kategorie). Analyzuje se jeho blízkost k jiným gramatickým diatezím a popisují se podmínky vytváření rezultativní konstrukce obejektové a posesivní v češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Resulative constructions in Czech are analyzed as a type of grammatical diathesis included in the morphological pardigm of the verb. The derivation of so-called obejective and posessive resultative is studied here.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Stať se věnuje českým infinitivním konstrukcím v pozici větného subjektu. Analyzují se slovesa s infinitivním podmětem, adjektiva přísudková (včetně verbonominálních přísudků) a sledují se vlastnosti koreference infinitivního subjektu a členu řídídcí predikace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper the Czech infinitival constructions  in the position of subject are studied. The coreference between (innere) infinitival subject and particular member of the governor are analyzed on the material from the corpora.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Na základě teorie valence navržené ve FGP jsou zkoumány funkce předložkového pádu o + Akuzativ. Podle typu slovesa plní tato předložková skupina funkce Překážky, Rozdílu, Patientu nebo Efektu. Tyto funkce jsou zachyceny u příslušného slovesa ve valenčním rámci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The theory of valency developed within the Functional Generative Description is applied for the prepositional group o + Accusative. The Obstacle complement, the Difference modification, the Patient, or the Effect are assigned to this form according to the valency frames of the respective verbs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V stati se analyzují speciální typy příslovečných určení, které se z hlediska významové struktury (tektogramatické reprezentace) jeví jako gramatikalizované elipsy zapuštěných predikací. (jde o příslovečná určení srovnání (podobnosti i rozdílu)a příslovečné určení omezení a slučování výjimkou. Navrhují se "konstrukty", jak lze tato určení parafrázovat v hloubkové struktuře.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article deals with special types of adverbials, which represent from the point of view of the level of deep structure grammaticalized elliptical constructions of the embedded clauses. The adverbials of comparison, restriction and addition are concerned. The way,how to reconstruct them on the level of deep structure is proposed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá příslovečnými určeními srovnání, výjimky a slučování v češtině, které jsou sémanticky komplikované a jejich hloubková struktura vyžaduje doplnění sdílených členů řídící a závislé (rekonstruované) predikace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The special types of Czech adverbials (comparison, exception and addition) are analyzed. Due to their semantic complexity the reconstruction of their members shared with the governing predication in  is needed. This reconstructed deep representation is presented as grammaticalized deletion.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek přispívá k dlouhodobé linvistické diskuzi o hranicích mezi gramatikou a slovníkem, konkrétně se zaměřuje na čtyři témata z české gramatiky: valence, modalita v obsahových závislých klauzích, gramatické diateze a souborový význam.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper contributes to the long-term linguistic discussion on the boundaries between grammar and lexicon by analyzing four related issues from Czech. The analysis is based on the theoretical framework of Functional Generative Description (FGD), which has been elaborated in Prague since 1960’s. First, the approach of FGD to the valency of verbs is summarized. The second topic, concerning dependent content clauses, is closely related to the valency issue. We propose to encode the information on the conjunction of the dependent content clause as a grammatical feature of the verb governing the respective clause. Thirdly, passive, resultative and some other constructions are suggested to be understood as grammatical diatheses of Czech verbs and thus to be a part of the grammatical module of FGD. The fourth topic concerns the study of Czech nouns denoting pair body parts, clothes and accessories related to these body parts and similar nouns. Plural forms of these nouns prototypically refer to a pair or typical group of entities, not just to many of them. Since under specific contextual conditions the pair/group meaning can be expressed by most Czech concrete nouns, it is to be described as a grammaticalized feature.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá gramatickou kategorií čísla v češtině. Základní opozice singuláru a plurálu, která tuto kategorii konstituuje, navrhujeme zavést novou sémantickou distinkci prostého většího množství jednotlivin vs. souborovosti. Souborovost je v češtině typicky vyjadřována substantivy jako oči, vlasy, sirky apod. Uvádíme také stručné srovnání s dalšími slovanskými i neslovanskými jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present contribution deals with grammatical category of number of nouns in Czech. On the basis of empirical investigations, an introduction of a new semantic distinction is being proposed within the formal forms of nouns, namely the distinction of a simple quantitative meaning versus a pair/group meaning. There are nouns in Czech that refer typically to a pair or to a (usual) group of entities and not just to a large amount of these entities (e.g. ruce ‘arms’, vlasy ‘hair’, or sirky ‘matches’). These nouns are combined with set numerals rather than with the basic ones. In the paper, arguments are presented supporting the view that the above mentioned subcategorization is grammaticalized. A brief outline of the expressions of the pair/group meaning in Czech as compared to some other languages (German, English, Slavonic languages) is also given.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška podá přehled o dostupných korpusech pro češtinu a poukáže na výhody a nevýhody jednotlivých typů korpusů pro různé lingvistické úkoly.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk present available corpora of Czech and gives examples how they can be used in linguistic research.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zkoumá se, čím je ovlivněn pád  českého adjektivního doplňku, který závisí na infinitivu. Nastávají dva případy: pád se shoduje s pádem antecedentu (CT), nebo je nezávislý na antecedentu (CI), v češtině je to nominativ.  Po některých slovesech je obtížné určit, zda jde o CT nebo o CI.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The features which influence the category of a case in the deeply embedded adjectives modified the infinitive construction are studied. Several hypothesis (raising verbs, verbs of control, the form and function of the antecedent) are studied on the Czech data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce bude představen otevřený modulární systém Treex, určený k vývoji aplikací z oblasti zpracování přirozeného jazyka. V systému je integrována řada nástrojů pro morfologickou a syntaktickou analýzu a syntézu několika jazyků. Nejpokročilejší aplikací systému Treex je anglicko-český překladač, který bude popsán podrobněji.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Treex, which is an open-source modular framework tailored for developing natural language processing applications, will be presented in the talk. Numerous tools for morphological and syntactical analysis and synthesis are integrated in the framework. The most advanced application of the framework is English-Czech machine translation system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje systém Treex (dříve TectoMT). Jde o víceúčelový otevřený softwarový rámec pro vývoj aplikací z oblasti zpracování přirozeného jazyka. Treex usnadňuje vývoj nových aplikací díky integraci řady existujících modulů, např. pro segmentaci textu, morfologickou analýzu, značkování slovních druhů, syntaktickou analýzu, rozpoznávání pojmenovaných entit, rozpoznávání anafor, syntézu vět atd. Treex je využíván také pro výukové účely.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper describes Treex (formerly TectoMT), a multi-purpose open-source
framework for developing Natural Language Processing applications. It facilitates the development  by exploiting a wide range of software modules already integrated in Treex, such as tools for  sentence segmentation, tokenization, morphological analysis,
 part-of-speech tagging, shallow and deep syntax parsing,  named entity recognition, anaphora resolution, sentence synthesis,  word-level alignment of parallel corpora, and other tasks.
The most elaborate application of Treex is
an English-Czech machine translation system with transfer on deep syntactic (tectogrammatical) layer. Besides research, Treex is used
for teaching purposes and helps students to implement morphological and syntactic analyzers of foreign languages in a very short time.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Treex::Core je soubor modulů v jazyce Perl určený pro obecné zpracování syntaktických stromových struktur. Datová reprezentace je v maximální míře jazykově nezávislá. Treex::Core se v současnosti používá zejména jako základní datová reprezentace ve strojovém překladu využívajícím syntaxi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Treex::Core is a set of Perl modules aimed at a general processing of syntactic tree structures. The data structures are designed to be as language independent as possible. Treex::Core is currently used especially for developing syntax-based Machine Translation systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Soubor vybraných česky psaných prací předního českého lingvisty zejména o češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A set of selected works, written in Czech, by a famous Czech linguist, mainly about the Czech language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Před sto lety byla v Praze založena Kancelář Slovníku jazyka českého, ze které pak vznikl akademický Ústav pro jazyk český. Ten se stal (za vedení B. Havránka, pak Fr. Daneše ad.) nejvýznamnějším bohemistickým pracovištěm.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Hundred years ago, the Office of the Czech Language Dictionary was founded in Prague. Later it evolved into the academic Institute of the Czech Language. Under the leadership of B. Havránek, F. Daneš and others it became the most important research institute for the Czech language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek nebude hlášen do RIV, proto není třeba uvádět abstrakt.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article will not be reported to RIV, thus the abstract is not mandatory.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek nebude hlášen do RIV, proto není třeba uvádět abstrakt.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article will not be reported to RIV, thus the abstract is not mandatory.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Na několika vybraných lingvistických problémech chceme ukázat, že klasická strukturní a funkcionální lingvistika dokonce se svými zdánlivě tradičními přístupy má co nabídnout formálnímu popisu jazyka a jeho aplikacím ve zpracování přirozeného jazyka, a krátkým odkazem na Funkčně-generativní gramatiku (na teoretické straně CL) a Pražský závislostní treebank (na straně aplikační) ilustrovat možnou interakci mezi lingvistikou a CL.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We want to demonstrate on some selected linguistic issues that classical structural and functional linguistics even with its seemingly traditional approaches has something to offer to a formal description of language and its applications in natural language processing  and to illustrate by a brief reference to Functional Generative Grammar (on the theoretical side of CL) and Prague Dependency Treebank (on the applicational side) a possible interaction between linguistics and CL.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pojednání o vzájemné souhře informační struktury věty, anaforických a koreferenčních vztahů a různých aspektů struktury diskurzu, založené na dokladech z velkého anotovaného korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Examination of the interplay of the information structure of the sentence, the anaphoric and coreferential relations and of different aspects
of discourse structure, based on the evidence from a large annotated corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabýval přístupy FGP k aktuálnímu členění a popisoval nové poznatky o systémovém uspořádání.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presentation dealt with the approaches to the sentence information structure and it described some new findings about systemic ordering.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem našeho příspěvku je představit korpusově založené srovnání češtiny a angličtiny a zvážit, které aspekty jevu informační struktury jsou univerzální povahy a které jsou jazykově závislé.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of our contribution is to present a corpus-based comparison of Czech and English and to consider which aspects of the phenomenon of information structure are of a universal nature and which are language specific.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>TrEd 2.0 je nově refaktorovaná verze editoru stromů TrEd. TrEd je používán jak v Ústavu formální a aplikované lingvistiky, tak na několika zahraničních výzkumných pracovištích v projektech zaměřených na práci se syntaktickými strukturami. Výrazné změny v architektuře TrEdu podstatně usnadní jeho budoucí vývoj, údržbu, testování a distribuci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>TrEd 2.0 is a newly refactored version of the TrEd tree editor. TrEd is used in a number of research projects, at the Institute of Formal and Applied Linguistics as well as in several foreign research institutions. Recent changes in its design will substantially facilitate its future development, testing and distribution.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Recenze odborné knihy: Syntax-Based Collocation Extraction od Violety Seretan</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A review of a book: Syntax-Based Collocation Extraction by Violeta Seretan</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato disertační práce popisuje valenci sloves v rámci anotace Pražského závislostního korpusu (PDT) a jejím hlavním cílem je popsat valenční slovník PDT-Vallex. Tento slovník vznikl při anotaci PDT a díky svému charakteru se stal významným zdrojem valenční informace využitelné jak pro lingvistický výzkum, tak pro počítačové zpracování přirozeného jazyka. V práci popisujeme nejen koncepci slovníku, která úzce souvisí s pojetím valence v rámci Funkčně generativního popisu, ale i vztah slovníku k PDT. Právě na základě tohoto vztahu - úzkého propojení slovníku s korpusem - věnujeme zvláštní pozornost popisu formálních prostředků diatezí. Navrhujeme transformační pravidla pro sekundární diateze, s jejichž pomocí se dokážeme vyrovnat s případy, kdy formy slovesných valenčních doplnění ve slovníku neodpovídají formám slovesných doplnění v korpusových textech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This dissertation describes PDT-Vallex, a valency lexicon of Czech verbs and its relation to the annotation of the Prague Dependency Treebank (PDT). The PDT-Vallex lexicon was created during the annotation of the PDT and it is a valuable source of verbal valency information available both for linguistic research and for computerized natural language processing. In this thesis, we describe not only the structure and design of the lexicon (which is closely related to the notion of valency as developed in the Functional Generative Description of language) but also the relation between the PDT-Vallex and the PDT. The explicit and full-coverage linking of the lexicon to the treebank prompted us to pay special attention to diatheses; we propose formal transformation rules for diatheses to handle their surface realization even when the canonical forms of verb arguments as captured in the lexicon do not correspond to the forms of these arguments actually appearing in the corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Práce popisuje valenční slovník PDT-Vallex. Tento slovník vznikl při anotaci PDT a díky svému charakteru se stal významným zdrojem valenční informace využitelné jak pro lingvistický výzkum, tak pro počítačové zpracování přirozeného jazyka. Na základě úzkého propojení slovníku s korpusem věnujeme zvláštní pozornost popisu formálních prostředků diatezí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This book describes PDT-Vallex, a valency lexicon of Czech verbs and its relation to the annotation of the Prague Dependency Treebank. The PDT-Vallex lexicon was created during the annotation of the PDT and it is a valuable source of verbal valency information available both for linguistic research and for computerized natural language processing. The book describes also the relation between the PDT-Vallex and the PDT. The explicit and full-coverage linking of the lexicon to the treebank prompted us to pay special attention to diatheses; we propose formal transformation rules for diatheses to handle their surface realization.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kniha zachycuje slovesnou valenci v Pražském závislostním korpusu. Popisuje vztah mezi anotací a jednotlivými významy sloves,které se v korpusu vyskytly. Slovesa (resp. jejich významy) jsou popsána svými valenčními rámci. Popis valenčních rámců je plně formalizovaný, zachycuje lema, valenční doplnění a jejich povrchové realizace (povrchové formy). Valenční rámce obsahují také ilustrativní příklady a poznámky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The book captures verbal valency in the Prague dependency treebank. It describes the relation between the annotation and the individual verbal meanings described by their valency frames. The description is fully formalized in the form of a full list of valency frames, together with their lemmas and full description of valency slots, their surface realization, notes and examples. It thus forms a special extract from the Prague Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Systém Dialogy.Org je softwarový nástroj určený  především k podpoře vytváření a sdílení anotovaných audio-video dat lingvistickou komunitou. Dialogy.Org umožňuje formou webového rozhraní editaci a vyhledávání v přepisech audio-visuálních nahrávek dialogů. Vyhledané úseky textu je možné přehrávat a analyzovat pomocí webového prohlížeče.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Dialogy.Org system is a software tool designed primarily to support the creation and sharing of annotated audio-visual data by linguistic community.
The software tool allows editing and searching in the transcripts of audio-visual recordings of dialogues. The dynamic web application provides access for registered users to the digitised archive. Playing and exploring of selected parts is possible in the web browser.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme naše pokusy s hierarchickým frázovým strojovým překladem v rámci soutěže WMT 2011. Natrénovali jsme systém pro všech 8 překladových směrů mezi angličtinou na jedné straně a češtinou, němčinou, španělštinou nebo francouzštinou na druhé. Poněkud více jsme se soustředili na směr z angličtiny do češtiny. Poskytujeme podrobný popis naší konfigurace a dat, aby bylo možné pokusy zopakovat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe our experiments with hierarchical phrase-based machine translation for the WMT 2011 Shared Task. We trained a system for all 8 translation directions between English on one side and Czech, German, Spanish or French on the other side, though we focused slightly more on the English-to-Czech direction. We provide a detailed description of our configuration and data so the results are replicable.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Addicter je zkratka za Automatic Detection and DIsplay of Common Translation ERrors. Je to soubor nástrojů (především skriptů napsaných v Perlu), které pomáhají s analýzou chyb strojového překladu. Ve druhé verzi byl podstatně rozšířen prohlížeč a přibyla řada modulů pro automatickou detekci a klasifikaci chyb.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Addicter stands for Automatic Detection and DIsplay of Common Translation ERrors. It is a set of tools (mostly scripts written in Perl) that help with error analysis for machine translation. The second version contains a greatly improved viewer and many new modules for automatic detection and classification of errors.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme Addicter, nástroj pro automatickou detekci a zobrazování chyb strojového překladu. Tento nástroj automaticky identifikuje a klasifikuje chyby a umožňuje prohlížet testovací a trénovací korpus a slovní párování; umožňuje také propojení s dalšími lingvistickými nástroji. Klasifikace chyb je inspirována Vilarem et al. (2006), některé jejich kategorie jsou ovšem pro současnou verzi našeho systému nedosažitelné.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce Addicter, a tool for Automatic Detection and DIsplay of Common Translation
ERrors. The tool allows to automatically identify and label translation errors and browse the test
and training corpus and word alignments; usage of additional linguistic tools is also supported.
The error classification is inspired by that of Vilar et al. (2006), although some of their higherlevel
categories are beyond the reach of the current version of our system. In addition to the
tool itself we present a comparison of the proposed method to manually classified translation
errors and a thorough evaluation of the generated alignments.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popíšeme naše nedávné experimenty se závislostní syntaktickou analýzou v mnoha jazycích. Víme o více než 30 jazycích, pro které jsou k dispozici treebanky (většinou závislostní) za přijatelných licenčních podmínek. Tyto treebanky však mají mnoho různých anotačních stylů. Aby byly výsledky pokusů porovnatelné, je třeba jednotlivé anotační styly navzájem co nejvíce přiblížit. Zajímavou otázkou je, jak by měl společný anotační styl vypadat a jaká kritéria použít k vyhodnocení vhodnosti jednotlivých přístupů.

V první části přednášky představíme data, která máme. Rozličnost anotačních stylů předvedeme na různých syntaktických jevech, jejich reprezentaci v korpusech a naše transformace do společného anotačního schématu. Ve druhé části se soustředíme konkrétně na koordinační struktury – jeden z nejobtížnějších jevů jak z pohledu autorů treebanků, tak parserů. Představíme klasifikaci možných reprezentací, vyhodnotíme jejich teoretickou vyjadřovací sílu i praktický dopad na úspěšnost syntaktické analýzy za použití dvou předních závislostních parserů: Maltu a MST.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We will present our recent parsing experiments with dependency treebanks in multiple languages. We identified more than 30 languages for which treebanks (mostly dependency-based) are available under acceptable licensing terms. However, the treebanks adhere to many different annotation styles. To make our results comparable, we need to make the annotation styles as similar as possible. An interesting question is, how should the common annotation style look like, and what criteria should we use to evaluate suitability of the various approaches.

In the first part of the talk we will present the data we have. We will demonstrate the diversity of annotation styles by giving an overview of various syntactic phenomena, their representation in treebanks and our effort to transform the representation to one common scheme. In the second part we will focus specifically on coordinating structures – one of the most difficult phenomena both for treebank designers and parsers. We will classify the possible annotation styles along several dimensions and we will evaluate both their theoretical expressive power and practical impact on parsing accuracy, using two state-of-the-art dependency parsers: Malt and MST.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládaným příspěvkem bezprostředně navazujeme na stať Proměna Českého akademického korpusu, která byla publikována ve Slově a slovesnosti v roce 2006 (Hladká – Králík, 2006). Nyní popisujeme zkušenosti ze syntaktické proměny Českého akademického korpusu, při které byly původní syntaktické anotace ponechány stranou a texty byly nově anotovány dle koncepce Pražského závislostního korpusu. Proměna, neboli anotace, byla zahájena tři roky poté, co byla dokončena syntaktická anotace již zmíněného Pražského závislostního korpusu – největšího anotovaného korpusu psané češtiny. Tento tříletý časový odstup je do jisté míry kuriózní; neznáme jiný jazyk, pro který by po anotování velkého objemu dat (více než jeden milion slov) proběhla anotace dalších dat, sice objemu menšího, ale rovněž nezanedbatelného (statisíce slov). 
Syntaktickou anotací Českého akademického korpusu jsme vstoupili podruhé do stejné řeky. Doufáme, že zkušenost, kterou si odnášíme, bude přínosná pro všechny jazykovědce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The idea of the Czech Academic Corus (CAC) came to life in 1971 thanks to the Department of Mathematical Linguistics within the Institute of Czech Language. By the mid 1980s, a total of 540,000 words were morphologically and syntactically manually annotated.
After the Prague Dependency Treebank (PDT) – the largest treebank of Czech written texts – has been built, a conversion from the CAC to the PDT format has started. The main goal was to make the CAC and the PDT compatible thus to enable integration of the CAC into the PDT. The second version of the CAC presents such a complete conversion of the internal format and the annotation schemes.
Conversion of syntactic annotation has started three years after the syntactic annotation of PDT has been finished. Such a situation is exceptional since, at least to our knowledge, there is no other language for the annotation of indispensable amount of data is being done in two subsequent annotation projects.
This article summarizes the experience acquired during the CAC syntactic annotation conversion.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zabýváme se pojmem crowdsorcingu v souvislosti se zpracováním textových dat. Implementovali jsme on-line hry s texty a detailně je popisujeme; jde o hru s koreferencí (PlayCoref) a o hry se slovy a s mezerami mezi slovy (Shannon Game a Place the Space). Pravidla her jsou jazykově nezávislá a hry jsou v současné době hratelné s českými a anglickými daty.
Po dosud odehraných partiích poopravujeme poněkud svůj pohled na možnost atraktivity her s texty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We review a notion of crowdsourcing, namely we turn our attention to crowdsourcing projects that manipulate the textual data. We carry out the implementation of the on-line games with texts. We introduce a game on coreference, PlayCoref, and games with words and white spaces in the sentence, Shannon Game and Place the Space, in great details. The game rules are designed to be language independent and the games are playable with both Czech and English texts by default. After a number of sessions played so far we revise our initial expectations and enthusiasm to design an attractive annotation game with the document.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Práce přináší odpověď na otázku, co je a co není elipsa, a stanovuje kritéria určování eliptických vět. Analýzu jednotlivých typů elips podává z pohledu významové (sémanticko-syntaktické) reprezentace vět. Nezabývá se podmínkami a příčinami vzniku elips (kdy a proč je možné něco elidovat), zaměřuje se výhradně na identifikaci eliptických míst (zda je ve větě něco elidováno a co) a na jejich významovou reprezentaci, konkrétně na jejich zachycení na tektogramatické rovině pražských závislostních korpusů. Strukturní přístup závislostní (uplatňovaný v pražských závislostních korpusech) je v práci srovnán s přístupem složkovým (užitým v americkém korpusu PennTreebank). Tohoto srovnání je možné využít při (automatickém) převodu složkových stromů na závislostní.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper answers the question what is and what is not ellipsis and specify criteria for identification of elliptical sentences. It reports on an analysis of types of ellipsis from the point of view of semantic (semantico-syntactic) representation of sentences. It does not deal with conditions and causes of constitution of elliptical positions in sentences (when and why it is possible to omit something in a sentence) but it focuses exclusively on identification of elliptical positions (if there is something omitted and what) and on their semantic representation, specifically on their representation on the tectogrammatical level of the Prague Dependency Treebanks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rekonstrukce standardizovaného textu z mluvené řeči v Pražském závislostním korpusu mluvené češtiny odstraňuje z přepisu mluvených projevů specifické rysy mluvenosti (prvky nespisovné z obecné češtiny i nářečí, nadbytečná „ukazovací“ zájmena a navazovací konektory, „vycpávky“, subjektivní slovosled, opakování a opravy, restarty aj.). Např. z mluveného „takže jako tam byla dobrá parta a dlouho tedy no“ vzniká standardizované „Byla tam dlouho dobrá parta.“ To umožňuje názorné a zajímavé porovnávání autentických mluvených projevů a textů standardizovaných, u nichž je otázka, jak je kategorizovat: jsou to stále ještě texty mluvené – ale korektní, spisovné? Nebo standardizace transformuje mluvený projev do podoby textu psaného? Necháme-li stranou jevy hláskoslovné a tvaroslovné a soustředíme-li se na proměny syntaxe při standardizaci, nabízí se otázka, v čem vlastně je syntaktická identita českého mluveného a psaného textu, jaké syntaktické celky/ jednotky/ konstrukce jsou „přirozené“ v mluveném a naproti tomu v psaném textu, jaký je rozdíl v hustotě kohezních spojů, v explicitní a implicitní návaznosti mezi jednotkami atd. Vede odstraňování specifických rysů mluvené syntaxe ke vzniku syntakticky „přirozeného“ psaného textu? Jaký je rozdíl mezi „světem psanosti“ a „světem mluvenosti“, jaké estetické hodnoty jsou s nimi spojeny? Vznikají standardizací skutečně „hezké“ české věty, „hezčí“ než ty mluvené? Lze vůbec mluvený projev „přeložit“ do psaného?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The reconstruction of standardized text from the Prague Dependency Treebank of Spoken Czech removes specific aspects of spoken language (non-standard elements from Common Czech and dialects, superfluous demonstrative pronouns and connectors, filler words, subjective word order, repetitions and repairs, restarts etc.). For example, the spoken utterance "takže jako tam byla dobrá parta a dlouho tedy no" ("so like there was a good crowd and for a long time yeah") becomes the standardized sentence "Byla tam dlouho dobrá parta" ("For a long time, there was a good crowd there."). This enables a vivid and interesting comparison of authentic spoken expressions and standardized texts. The question of how to categorize the standardized texts thus arises; they are still spoken texts - but they are correct, standard? Or does standardization transform spoken utterances into written texts? If we leave aside phonetic and morphological phenomena and concentrate on the syntactic transformations which occur during standardization, the following questions arise: What does the syntactic identity of the Czech spoken and written texts consist of? What syntactic wholes/units/constructions are natural in the spoken text and also in written text? What is the difference in the density of cohesive conjuctions, in the explicit and implicit relationship between units, etc.? Does the removal of specific aspects of spoken syntax lead to syntactically natural wtritten texts? What is the difference between the written world and the spoken world, what aesthetic values are assigned to them? Does standardization create nice Czech sentence, nicer than those that are spoken? Is even possible to translate a spoken utterance into a written text?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zpracování přirozeného jazyka je poměrně nový, rychle se rozvíjející obor, který nachází široké uplatnění v mnoha aplikacích. Tato přednáška bude věnována počítačovému zpracování (zejména) češtiny. Přiblížíme si problematiku zachycení 	významu a představíme kolekce jazykových dat – slovníky a korpusy –, které se 	vytvářejí na MFF UK. Krátce se též seznámíme s některými projekty, které tato data využívají, zejména s projektem strojového překladu mezi češtinou a angličtinou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Natural Language Processing is a modern field which is used in many applied tasks. The lecture focuses on language meaning and its representation. Data collections (lexicons and corpora) and automatic tools (esp. machine translation systems) will be introduced.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Informace o syntaktických a sémantických vlastnostech sloves hraje zásadní roli v mnoha NLP aplikacích. V přednášce se soustředím na lexikografické zpracování sloves ve Valenčním slovníku češkých sloves.
Prototypicky jedna syntaktická struktura odpovídá jednomu významu slovesa. Nicméně je řada příkladů, kdy jsou sémanticky blízká slovesa různě syntakticky strukturována - takové změny ve strukturaci se obvykle nazývají alternace. Druhá část přednášky se bude věnovat typologii českých alternací a možností jejich zachycení ve slovníku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Information on syntactic and semantic properties of verbs, which are traditionally considered to be the center of a sentence, plays a key role in many rule-based NLP tasks such as automated semantic role labeling, semantic parsing, machine translation, etc. 
The talk will focus on a lexicographic description of syntactic-semantic features of verbs. I will present the valency lexicon of Czech verbs VALLEX (Lopatková et al., 2007), which is closely related to the Prague Dependency Treebank project (Hajič et al., 2006). VALLEX provides information on the combinatorial potential of verbs in their individual senses: for each verb sense, the lexicon lists a number of its syntactic-semantic complementations labeled with their semantic roles and their possible morphological forms. 
Prototypically, a single syntactic structure corresponds to a single meaning of verb. However, in many cases semantically related uses of verbs can be syntactically structured in different ways.
Such changes in syntactic structure are usually referred to as `alternations’, see esp. (Levin, 1993). In the second part of my talk, I will address two basic types of alternations for Czech verbs and I will present a method of their representation in a valency lexicon (Kettnerová, Lopatková, 2011).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Budu mluvit o jazykových korpusech, různých úrovních lingvistického značkování, elektronických slovnících, parsování, krátce potom o hlavních aplikacích, např. strojový překlad, automatické odpovědi, analýza a syntéza řeči. Příspěvek bude ilustrován na příkladech, hlavně z českého jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I will discuss language corpora, several levels of linguistic tagging, electronic dictionaries, parsing of sentences, briefly about main applications, for instance machine translation, question answering, speech analysis and synthesis. Examples will be taken mainly from the Czech language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zápis slov v~mnoha jazycích není jednoznačný, existují různé varianty. Někdy se jedná o~varianty rovnocenné, jindy jsou některé nářeční, nespisovné či jinak příznakové. Při automatickém zpracování jazyka však chceme umět rozpoznat všechny, a současně jim přiřadit stejný základní tvar, tzv. lemma. Na druhou stranu ale potřebujeme všechny varianty od sebe nějakým způsobem odlišit, abychom např. mohli při automatické syntéze zvolit tu správnou. Příspěvek se zabývá možným řešením tohoto problému, a to zavedením tzv. vícenásobného lemmatu. Uvedeme možnosti jeho využití při konkrétních aplikacích, zejména v~korpusové lingvistice.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In some languages, some words may be written in several ways. Sometimes the variants are equivalent, sometimes not. There can be standard, nonstandard, dialectical or otherwise marked variants. During automatic processing we need to recognize them all, but at the same time we need a means how to distinguish them, because during synthesis, it is important to select the right variant. One of the solutions is introduction of so called multiple lemma. We present its possible usa for concrete applications, especially in the field of corpus linguistics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje a porovnává automatické metody pro rozpoznávání předpon. Ukazuje výsledky experimentů s češtinou a angličtinou a porovnává rozdíly v závislosti na velikosti korpusů a na výběru typu vstupních dat (slovní tvar nebo lemma).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper deals with automatic methods for prefix extraction and their comparison. We present experiments with Czech and English and compare the results with regard to the size and type (wordforms vs. lemmas) of input data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá problémem analýzy českých souvětí na základě manuálně anotovaných dat. Takováto data explicitně popisující vztahy mezi segmenty a klauzemi v souvětí poskytují základnu pro další lingvistický výzkum.
Příspěvek prezentuje kvantitativní, lingvistická a strukturní pozorování, na jejichž základě je možné navrhnout algoritmus pro automatickou analýzu struktury souvětí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper deals with the problem of an analysis of complex sentences in Czech on the basis of manually annotated data. The availability of a specialized corpus explicitly describing mutual relationships between segments and clauses 
in Czech complex sentences, together with the availability of a thoroughly syntactically annotated corpus, the Prague Dependency Treebank, provide a solid background for linguistic 
investigation. The paper presents quantitative, linguistic and structural observations which provide a number of clues for building an algorithm for analyzing a structure of complex 
sentences in the future.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Závislostní korpus anotovaný na rovině morfologické (2 miliony slov), syntaktické (1,5 milionu slov) a sémantické (0,8 milionu slov). Oproti verzi 2.0 přibyly opravy morfologie, anotace víceslovných výrazů, souborovosti a segmentace na větné klauze.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Dependency corpus annotated at morphological, syntactic, and deep syntactic levels. Previous version 2.0 has been enriched by annotation of multiword expressions, pair/group annotation, sentence clause segmentation. Morphological level has been improved.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Víceslovné výrazy (VSV) jsou důležitou lingvistickou jednotkou, která v mnoha aplikacích počítačového zpracování přirozených jazyků vyžaduje zvláštní zacházení. Je proto vhodné, abychom je dokázali rozpoznat automaticky. Sémanticky anotované korpusy by měly vyznačovat VSV jasně a zřetelně, čímž se usnadní vývoj nástrojů pro automatické rozpoznávání.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Multiword Expressions (MWEs) are important linguistic units that require special treatment in many NLP applications. It is thus desirable to be able to recognize them automatically. Semantically annotated corpora should mark MWEs in a clear way that facilitates development of automatic recognition tools.

In the present paper we discuss various corpus design decisions from this perspective. We propose guidelines that should lead to MWE-friendly annotation and evaluate them on numerous sentence examples. Our experience of identifying MWEs in the Prague Dependency Treebank provides the base for the discussion and examples from other languages are added whenever appropriate.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Morfologická kategorie čísla je v češtině konstituována protikladem singuláru (s významem jedné jednotliviny) a plurálu (s významem množství jednotlivin). Substantiva "ruce", "boty", "vlasy", "sirky" apod. ovšem svými plurálovými formami prototypicky odkazují nikoli k pouhému množství jednotlivin, ale k jejich páru nebo obvyklému souboru, popř. k několika párům nebo obvyklým souborům. Párový nebo šíře souborový význam proto navrhujeme chápat jako další význam plurálové formy českých substantiv. V přednášce popíšeme průběh a výsledky anotace zaměřené právě na identifikaci souborového významu u substantiv obsažených v datech Pražského závislostního korpusu. Uvedeme příklady kontextů, v nichž se paralelní anotace shodovala, i kontexty problematické a navrhneme postup, jak souborový význam včlenit do stávající tektogramatické anotace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The grammatical category of number is constituted by the singular vs. plural opposition in Czech. Nouns such as "hands", "shoes", "matches" etc. prototypically denote a pair (or group) of entities, not just many of them. This meaning considered as a grammaticalized pair/group meaning and is to be annotated in the data of Prague Dependency Treebank. The talk focuses on the annotation process and its results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá zachycením gramatické kategorie čísla v připravované verzi Pražského závislostního korpusu (PDT 3.0). Představen je nový sémantický rys úzce související s kategorií čísla (tzv. souborový význam) a jeho anotace v datech PDT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper focuses on the way how the grammatical category of number of nouns will be
annotated in the forthcoming version of Prague Dependency Treebank (PDT 3.0), concentrating
on the peculiarities beyond the regular opposition of singular and plural. A new semantic
feature closely related to the category of number (so-called pair/group meaning) was introduced.
Nouns such as ruce ‘hands’ or klíče ‘keys’ refer with their plural forms to a pair or to a
typical group even more often than to a larger amount of single entities. Since pairs or groups
can be referred to with most Czech concrete nouns, the pair/group meaning is considered as a
grammaticalized meaning of nouns in Czech. In the present paper, manual annotation of the
pair/group meaning is described, which was carried out on the data of Prague Dependency
Treebank. A comparison with a sample annotation of data from Prague Dependency Treebank
of Spoken Czech has demonstrated that the pair/group meaning is both more frequent and
more easily distinguishable in the spoken than in the written data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek navrhuje způsoby, jakými využít znalost o anaforických vztazích při automatickém překladu z angličtiny do češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Majority of present machine translation systems do not address the retaining of text coherency, they translate just isolated sentences. On the other hand, the authors of anaphora resolvers rarely integrate these tools into more complex scenarios, e.g. the task of machine translation. We propose the ways how machine translation systems can utilize the knowledge of anaphoric relations both in the source as well as in the target language in order to improve the quality of
translation. Specifically, we present how to incorporate anaphora resolution into the process of English to Czech translation using the TectoMT system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Práca prezentuje první výsledky rozpoznávání koreference substantivních frází v češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this work, we present first results on noun phrase coreference resolution on Czech data. As the data resource for our experiments, we employed yet unfinished and unpublished extension of Prague Dependency Treebank 2.0, which captures noun phrase coreference and bridging relations. Incompleteness of the data influenced one of our motivations – to aid annotators with automatic pre-annotation of the data. Although we introduced several novel tree features and tried different machine learning approaches, results on a growing amount of data shows that the selected feature set and learning methods are not able to sufficiently exploit the data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vytvořili jsme korpus se 106 jazyky z wikipedie a webových stránek. W2C Wiki Corpus obsahuje 8.5 GB textu a W2C Web Corpus obsahuje 54.7 GB textu. Součástí je také software pro distribuované stahování a zpracovávání webových stránek.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We built corpus containing 106 languages from texts available on the Wikipedia and on the Internet. The W2C Wiki Corpus contains 8.5 GB of text and the W2C Web Corpus contains 54.7 GB of text. The software part contains tools for distributed crawling and processing of web pages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>W2C je kolekce software a dat. Softwarová část slouží k vytvoření rozsáhlého textového korpusu pro zvolený jazyk. Využívány jsou textové materiály volně dostupné na WWW. Významnou částí jsou komponenty pro filtrování dat, které umožní odstranit materiál s nízkou kvalitou. Datová část obsahuje již vytvořené jazykové korpusy pro více než 100 jazyků, pro každý z nich ve velikosti přibližně 10 milionů slov. Tento zdroj jazykových dat usnadní řadě pracovišť vývoj multilinguálních technologií.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>W2C is a collection of software and data. The software part radically facilitates creating a new text corpora for a given language, using text materials freely available on the Internet. A special attention was given to components for filtering that allow to keep the material quality very high. The data part contains corpora for more than 100 languages, with around 10 million words in each. This language data resource can be used especially by researchers specialized at developing multilingual technologies.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato diskuse je zaměřena na prezentaci naší neustálé snahy o vybudování PDT styl závislostní korpus pro Tamil jazyk. Přednáška nastíní anotace programu a anotací na morfologické a povrchové vrstvy syntax. Různé otázky, jako jsou nejednoznačné konstrukce, NP směsí, koordinace jevů a clitics s ohledem na anotace korpus se bude diskutovat. Naším konečným cílem tohoto projektu je vyvinout nabitý funkcemi analýzy rámce pro Tamil, tak my také prezentovat výsledky jsme získali v automatické analýzy (norma na základě a na základě souboru) pomocí vyvinuté prostředky. Některé problematické otázky v analýze Tamil se bude rovněž projednávat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This talk is aimed at presenting our ongoing effort to build a PDT style dependency treebank for Tamil language. The talk will outline the annotation scheme and annotation at morphological and surface syntax layers. Various issues such as ambiguous structures, NP compounding, coordination phenomena and clitics with respect to the treebank annotation will be discussed. Our ultimate goal in this project is to develop a feature rich parsing framework for Tamil, thus we also present the results we obtained in automatic parsing (rule based &amp; corpus based) using the developed resources. Some problematic issues in Tamil parsing will also be discussed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Anotovaná korpusy jako treebanks jsou důležité pro rozvoj
analyzátorů, jazyk aplikace, stejně jako znalost jazyka.
Jen velmi málo jazyků, mají tyto omezené zdroje. V tomto příspěvku popisujeme
naše eort v syntakticky anotace malé korpusy (600 vět) tamilského
jazyk. Naše poznámka je podobný Pražský závislostní korpus (PDT 2.0)
a skládá se ze 2 úrovní nebo vrstev: (i) morfologické vrstvy (m-layer) a (ii) analytické
vrstva (vrstvy). Pro obě vrstvy, jsme zavedli anotace programů, tj. polohy
Tagging pro m-vrstvě a vztahy závislosti (a jak závislost struktury
by měla být vypracována) pro vrstev. Nakonec jsme se zhodnotit naše korpusy v označování a
analýze úlohy pomocí známých značkovače a analyzátory a diskutovat některé obecné otázky
V anotaci na Tamil jazyce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Annotated corpora such as treebanks are important for the development
of parsers, language applications as well as understanding of the language itself.
Only very few languages possess these scarce resources. In this paper, we describe
our effort in syntactically annotating a small corpora (600 sentences) of Tamil
language. Our annotation is similar to Prague Dependency Treebank (PDT 2.0)
and consists of 2 levels or layers: (i) morphological layer (m-layer) and (ii) analytical
layer (a-layer). For both the layers, we introduce annotation schemes i.e. positional
tagging for m-layer and dependency relations (and how dependency structures
should be drawn) for a-layers. Finally, we evaluate our corpora in the tagging and
parsing task using well known taggers and parsers and discuss some general issues
in annotation for Tamil language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Lze nalézt jen velice málo informací o syntaktické analýze tamilštiny. V tomto článku popisujeme prvotní experimenty zaměřené na syntaktickou analýzu tamilštiny na základě pravidel a s pomocí korpusu. Anotační schéma bylo odvozeno od Pražského závislostního treebanku, proběhla ruční anotace cca 3000 slov. Pro analýzu založenou na korpusu používáme MST parser a Malt parser. Pro pravidlový přístup jsme implementovali řadu gramatických pravidel. V obou přístupech překročila úspěšnost analýzy 74%.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Very few attempts have been reported in the literature on dependency parsing for Tamil. In this paper, we report results obtained for Tamil dependency parsing with rule-based and corpus-based approaches. We designed annotation scheme partially based on Prague Dependency Treebank (PDT) and manually annotated Tamil data (about 3000 words) with dependency relations. For corpus-based approach, we used two well known parsers MaltParser and MSTParser, and for the rule-based approach, we implemented series of linguistic rules (for resolving coordination, complementation, predicate identification and so on) to build dependency structure for Tamil sentences. Our initial results show that, both rule-based and corpus-based approaches achieved the accuracy of more than 74% for the unlabeled task and more than 65% for the labeled tasks. Rule-based parsing accuracy dropped considerably when the input was tagged automatically.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tamilský závislostní treebank (TamilTB) je
pokusem vytvořit syntakticky anotovaný korpus pro tamilštinu. TamilTB
obsahuje 600 vět obohacených o ruční morfologickou anotaci a
závislostní syntax ve stylu Pražského závislostního korpusu (PDT). TamilTB
byl vytvořen v Ústavu formální a aplikované lingvistiky, Univerzity Karlovy v Praze. Anotační pravidla jsou podrobně popsána a ilustrována příklady.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Tamil Dependency Treebank (TamilTB)  is
an attempt to develop a syntactically annotated corpora for Tamil. TamilTB
contains 600 sentences enriched with manual annotation of morphology and
dependency syntax in the style of Prague Dependency Treebank. TamilTB
has been created at the Institute of Formal and Applied Linguistics, Charles
University in Prague. This report serves the purpose of how the annotation has been done at morphological level and syntactic level. Annotation scheme has been elaborately discussed with examples.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>TamilTB je první zveřejněný syntakticky anotovaný korpus tamilštiny, který umožní rozvoj jazykových technologií (zejména morfologické analýzy a parsingu) pro tento jazyk.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>TamilTB is a first published syntactically annotated corpus of Tamil. TamilTB will allow a more rapid development of language technologies for Tamil (especially parsing and morphological analysis).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nástroj pro extrakci překladových párů z korpusu zarovnaného po slovech. Nástroj již během extrakce málo četné páry ignoruje a šetří tak čas následného zpracování a diskový prostor.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a tool that extracts phrase pairs from a word-aligned parallel corpus and filters
them on the fly based on a user-defined frequency threshold. The bulk of phrase pairs to be
scored is much reduced, making the whole phrase table construction process faster with no
significant harm to the ultimate phrase table quality as measured by BLEU. Technically, our
tool is an alternative to the extract component of the phrase-extract toolkit bundled with Moses
SMT software and covers some of the functionality of sigfilter.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku představujeme pokus o zlepšení strojového překladu pojmenovaných entit s využitím Wikipedie. Pojmenované entity rozpoznáváme na základě kategorií anglických článků Wikipedie, následně extrahujeme jejich potenciální překlady z odpovídajících českých článků a přidáváme je jako nové možnosti překladu do statistického systému pro strojový překlad. Automatická metrika kvality překladu značí její zhoršení, avšak podle ruční anotace se naše překlady jeví jako lepší. Docházíme k závěru, že tento přístup vede v řadě chyb v překladu a měl by být proto vždy kombinován se standardním statistickým překladovým modelem. Měla by mu také být přiřazena přiměřená váha.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we present our attempt to improve machine translation of named entities by using Wikipedia. We recognize named entities based on categories of English Wikipedia articles, extract their potential translations from corresponding Czech articles and incorporate them into a statistical machine translation system as translation options. Our results show a decrease of translation quality in terms of automatic metrics but positive results from human annotators. We conclude that this approach can lead to many errors in translation and therefore should always be combined with the standard statistical translation model and weighted appropriately.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje žákovský korpus češtiny, který je kompilací krátkých prací napsaných studenty češtiny jako druhého jazyka. Věnujeme se pozadí projektu, základním požadavkům, procesu sběru textů, přepisu a anotaci. Anotace spočívá v několika vzájemně propojených rovinách, které zachycujou široké spektrum druhů chyb v textu. Ruční anotace je doplněna automatickou identifikací některých chyb. Navíc původní i opravený text je otegován morfologickými značkami. Anotační schéma je otestováno na vzorku o velikosti cca 10.000 slov oanotovaném dvěma nezávislými skupinami anotátorů s vyhovující iaa.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes a learner corpus of Czech, compiled from short essays written by students of Czech as a second or foreign language. We discuss the project’s background assumptions, the process of text acquisition, transcription and mark-up, 
and finally focus on the annotation scheme, consisting of multiple interlinked levels to 
cope with a wide range of error types present in the input. Manual annotation is complemented by automatic error identification wherever possible and morphosyntactic tags for all word forms both in the emended and the original text. The annotation schema is tested on a doubly-annotated sample of approx. 10,000 words with fair inter-annotator agreement results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Navrhujeme metodu pro automatickou identifikaci různých typů chyb ve výstupu strojového překladu. Přístup je převážně založen na jednojazyčném slovním zarovnání hypotézy a referenčního překladu. Kromě běžných lexikálních chyb se zjišťují také chyby slovosledu. Předkládáme srovnání s ručně klasifikovanými chybami MT. Naše klasifikace je inspirovaná klasifikací Vilara et al. (2006), přestože rozlišení některých jejich kategorií přesahuje možnosti současné verze našeho systému.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We propose a method of automatic identification of various error types in machine translation output. The approach is mostly based on monolingual word alignment of the hypothesis and the reference translation. In addition to common lexical errors misplaced words are also detected.
A comparison to manually classified MT errors is presented. Our error classification is inspired by that of Vilar et al. (2006), although distinguishing some of their categories is beyond the reach of the current version of our system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek navrhuje novou metodu ručního vyhodnocování kvality překladu, tzv. evaluaci pomocí otázek.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper proposes a new method of manual evaluation for statistical machine translation,
the so-called quiz-based evaluation, estimating whether people are able to extract information
from machine-translated texts reliably. We apply the method to two commercial and two experimental
MT systems that participated in WMT 2010 in English-to-Czech translation. We
report inter-annotator agreement for the evaluation as well as the outcomes of the individual
systems. The quiz-based evaluation suggests rather different ranking of the systems compared
to the WMT 2010 manual and automatic metrics. We also see that overall, MT quality is becoming
acceptable for obtaining information from the text: about 80% of questions can be answered
correctly given only machine-translated text.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>SemPOS je automatická metrika kvality strojového překladu založená na hloubkové syntaxi. Poměrně dobře koreluje s lidským hodnocením, ale je výpočetně drahá. V příspěvku zkoumáme její odlehčené verze a zapojujeme je i do iterativní optimalizace překladového modelu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>SemPOS is an automatic metric of machine
translation quality for Czech and English focused
on content words. It correlates well
with human judgments but it is computationally
costly and hard to adapt to other languages
because it relies on a deep-syntactic
analysis of the system output and the reference.
To remedy this, we attempt at approximating
SemPOS using only tagger output and
a few heuristics. At a little expense in correlation
to human judgments, we can evaluate
MT systems much faster. Additionally, we describe
our submission to the Tunable Metrics
Task in WMT11.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabýval možnostmi rozlišení explikativních a příčinných vztahů pro potřeby anotace diskurzu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presentation disscused the possibilities of of dictinction of explicative and causal relations for the purposes of discourse annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje, kterak lze využít potenciálu webových diskusních fór coby porovnatelných korpusů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>As the title suggests, our paper deals with web discussion fora, whose
content can be considered to be a special type of comparable corpora. We
discuss the potential of this vast amount of data available now on the
World Wide Web nearly for every language, regarding both general and common
topics as well as the most obscure and specific ones. To illustrate our
ideas, we propose a case study of seven wedding discussion fora in five
languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ruský tagger vyvinutý nad minimálním taggerem (Hana et al 2004).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Russian tagger developed on top of the resource-light tagger (Hana et al 2004). Available either as part of the Morph system or as a simple model for the TnT tagger.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku popisujeme jak překlad předložek ovlivňuje kvalitu strojového překladu a navrhujeme způsob jeho vylepšení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this article we explore differences in preposition usage in Czech and Russian languages and how the Machine Translation (MT) system between the languages deals with prepositions. We focus on the errors that occur in preposition phrases. Our study involves research on a parallel corpus for theoretical evidence and analysis of the output of the rule-based Machine
Translation (RBMT) system for closely related languages Česílko and the Statistical MT (SMT) system Joshua from Czech into Russian.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Strojový překlad z angličtiny do češtiny implementovaný v systému TectoMT sestává ze tří fází: analýzy, transferu a syntézy. Transfer se provádí na tektogramatické rovině upravené pro účely překladu. Každá fáze je rozčleněna do bloků, které řeší konkrétní lingvisticky interpretovatelný úkol (např. přiřazení morfologických značek statistickým taggerem, či přesun klitik podle ručně psaných pravidel). Systém TectoMT je navržen modulárně - bloky je možné zaměňovat za alternativní implementace a také využít i pro jiné aplikace než strojový překlad. Přednáška představí základní průběh celého překladu a zaměří se na popis vylepšení, která byla provedena v posledním roce, zejména: (a) využití tektogramatického jazykového modelu a Hidden Markov Tree Models, (b) nový systém slovníků natrénovaných na paralelním korpusu CzEng pomocí metody Maximum Entropy. Volitelným dodatkem je interaktivní tutoriál o platformě Treex.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>English to Czech machine translation as it is implemented in the TectoMT system consists of three phases: analysis, transfer and synthesis. The system uses tectogrammatical (deep-syntactic dependency) trees as the transfer medium. Each phase is divided into so-called blocks, which are processing units that solve linguistically interpretable tasks (e.g., statistical part-of-speech tagging or rule-based reordering of clitics). I will shortly introduce the linguistic layers of language description which are used for the translation and I will describe basic concepts of the NLP framework Treex in which the MT system is implemented. I will explain the whole translation process by step by step examples. After showing statistics on translation errors and their sources, I will describe recent improvements of the system. One of the most helpful improvements was the utilization of Maximum Entropy rich-feature translation models and Hidden Markov Tree Models in the transfer phase of translation, which is interpretable as labeling nodes of dependency trees. The translation results are evaluated using both automatic metric BLEU and human judgments from the WMT 2011 evaluation. Discussion on the manual evaluation (and meta evaluation) of MT is welcomed. For those interested in the Treex framework, I have another talk as well as a hands-on tutorial.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Úspěšnost závislostních parserů je jedním z hlavních faktorů, které určují kvalitu závislostního strojového překladu. Tento článek zkoumá vliv různých přístupů k závislostnímu parsingu (a také různých velikostí trénovacích dat) na výslednou kvalitu anglicko-českého statistického překladového systému implementovaného ve frameworku Treex. Článek též rozebírá vztah mezi intrinsickou evaluací parsingu (UAS) a extrinsickou (BLEU).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Accuracy of dependency parsers is one of the key factors limiting the quality of dependency-based machine translation. This paper deals with the influence of various dependency parsing approaches (and also different training data size) on the overall performance of an English-to-Czech dependency-based statistical translation system implemented in the Treex framework. We also study the relationship between parsing accuracy in terms of unlabeled attachment score and machine translation quality in terms of BLEU.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Feat je prostředí pro víceúrovňovou chybovou anotaci studenských korpusů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Flexible Error Annotation Tool (feat) is an environment for layered error annotation of learners corpora.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Lexical Annotation Workbench (LAW) je IDE pro morfologickou anotaci. Umožňuje prostou anotaci, slučování a porovnáná různých anotací téhož textu, vyhledávání, atd. Tato verze přidává několik vstupních filtrů, podporu tagsetů (vkládání tagů, nápověda pro tagy, atd), and opravuje mnoho chyb.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Lexical Annotation Workbench (LAW) is an integrated environment for morphological annotation. It supports simple morphological annotation, integration and comparison of different annotations of the same text, serching for particular word, tag etc. This version adds several new input filters, adds tagset support (guided tag entry, tag help), and fixes many bugs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Morph - systém pro morfologickou analýzu a značkování, včetně podpůrných nástrojů pro konverzi korpusů, evaluaci, atd.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Morph - system for morphological analysis and tagging, including supporting tools for corpus conversion, evaluation, etc.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje studentský korpus češtiny. Korpus zachycuje češtinu používanou nerodilými mluvčími. Popisujeme jeho strukturu, více-úrovňovou chybovou anotaci a proces anotace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The  paper  describes  a  learner  corpus  of Czech, currently under development.  The corpus  captures  Czech  as  used  by  non-native speakers.  We discuss its structure, the layered annotation of errors and the annotation process.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Flektivní jazyky mají bohaté skloňování, takže tagsety popisující jejich morfologické vlasnosti musí být nutně velké. Pro takové tagsety je vhodné použít strukturované tagy. V tomto článku popisujeme poziční tagset zachycující morfologii ruštiny. Vychází z Českého pozičního tagsetu (Hajič 2004) a rozvíjí předběžnou verzi tohoto tagsetu, kterou jsme používali v naší dosavadní práci (např. Hana et al. (2004, 2006); Feldman (2006); Feldman and Hana (2010)). Zde jednak prezentujeme systematičtější a úplnejší verzi (přidali jsme informaci o životnosti, vidu a zvratnosti) a jednak ho podrobněji popisujeme a srovnáváme s Českým pozičním tagsetem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Fusional languages have rich inflection. As a consequence, tagsets capturing their morphological features are necessarily large. A natural way to make a tagset manageable is to use a structured system. In this paper, we present a positional tagset for describing morphological properties of Russian. The tagset was inspired by the Czech positional system (Hajic 2004). We have used preliminary versions of this tagset in our previous work (e.g., Hana et al. (2004, 2006); Feldman (2006); Feldman and Hana (2010)). Here, we both systematize and extend these preliminary versions (by adding information about animacy, aspect and reflexivity); give a more detailed description of the tagset and provide comparisons with the Czech system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku popisujeme problémy tvorby zdrojů pro resource-light tagger flektivních jazyků (Feldman and Hana, 2010). Omezení dostupných zdrojů (čas, odbornost a finance) s sebou přinášejí problémy, které se nevyskytují při tvorbě zdrojů tradičním způsobem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We  describe  the  challenges  of  resource creation  for  a  resource-light  system  for morphological  tagging  of  fusional  languages (Feldman and Hana, 2010).   The constraints on resources (time, expertise, and money) introduce challenges that are not present in development of morphological tools and corpora in the usual, resource intensive way.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kurz vysvětluje lingvistické a komputační základy morfologické analýzy a značkování flektivních jazyků. Důraz je kladen na analýzu a značkování nenaročné na zdroje.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This course lays out the linguistic and computational foundations of morphological analysis and tagging of highly inflected languages, with an emphasis on resource-light analysis and tagging of fusional languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Korpus obsahující cca 6000 slov z Orwelova 1984, ručně označkovaný pomocí ruského pozičního tagsetu</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Corpus containing circa 6000 words from Orwel's 1984; manually annotated using Russian positional tagset</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Srovnání části systémového uspořádání v české a německé větě (slovosled volných slovesných doplnění).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Comparison of a part of system ordering in Czech and German sentence (word order of free verbal modifications).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přezkoumání systémového uspořádání volných slovesných doplnění na datech z Pražského závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Review of system ordering of the free verbal modifications on data from the Prague Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace představuje korpus starších českých textů a jeho využití k lingvistickým účelům - ke zkoumání tzv. systémového uspořádání.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presentation of the corpus of older Czech texts and its use for linguistic purposes - for studying the so-called system ordering.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Náplní tohoto článku je ověřit možnost řešit úkol závislostního parsingu pomocí nástrojů pro sekvenční značkování. Uvádíme algoritmus pro převod závislostních stromů na značky vhodné pro algoritmus sekvenčního značkování a vyhodnocujeme několik nastavení parametrů na standardních datech z treebanku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of this paper is to explore the feasibility of solving the
dependency parsing problem using sequence labeling tools. We introduce an
algorithm to transform a dependency tree into a tag sequence suitable for
a sequence labeling algorithm and evaluate several parameter settings on the
standard treebank data. We focus mainly on Czech, as a high-inflective
free-word-order language, which is not so easy to parse using traditional
techniques, but we also test our approach on English for comparison.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Velké korpusy jsou základem pro moderní metody počítačové lingvistiky. V tomto článku popisujeme probíhající projekt, jehož cílem je vytvořit největší korpus českých textů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Large corpora are essential to modern methods of computational linguistics
and natural language processing. In this paper, we describe an ongoing
project whose aim is to build a largest corpus of Czech texts. We are
building the corpus from Czech Internet web pages, using (and, if needed,
developing) advanced downloading, cleaning and automatic linguistic
processing tools. Our concern is to keep the whole process language
independent and thus applicable also for building web corpora of other
languages. In the paper, we briefly describe the crawling, cleaning, and
part-of-speech tagging procedures. Using a prototype corpus, we provide a
comparison with a current corpora (in particular, SYN2005, part of the
Czech National Corpora). We analyse part-of-speech tag distribution, OOV
word ratio, average sentence length and Spearman rank correlation
coefficient of the distance of ranks of 500 most frequent words. Our
results show that our prototype corpus is now quite homogenous. The
challenging task is to find a way to decrease the homogeneity of the text
while keeping the high quality of the data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce je věnována problematice členské negace a způsobům jejího vyjadřování v současné češtině, respektive v datech Českého národního korpusu (SYN2005) a Pražského závislostního korpusu (verze 2.0). Za členskou negaci byla dosud ve většině českých příruček považována taková negace, která neoperuje na predikátu. Autorka na základě analýzy dat a studia české i zahraniční odborné literatury týkající se syntaktické negace definuje nové vymezení členské negace v češtině, navrhuje formalismy pro zaznamenání struktur se členskou negací v systému konstrukční gramatiky a zkoumá možnosti zachycování tohoto jevu v PDT.

Klíčová slova: členská negace, adverzativní koordinace, konstrukční gramatika, funkční generativní popis, Pražský závislostní korpus</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The thesis is focused on constituent negation and ways of its expressing in contemporary Czech, or more precisely in Czech National Corpus (SYN2005) and Prague Dependency Treebank (version 2.0). In most of the Czech linguistic handbooks, constituent negation was treated as such a negation that does not operate on predicate. The author offers a new definition of constituent negation in Czech, based on data research and study of relevant literature concerning syntactic negation. Further, the author proposes a formalization of the structure of constituent negation within the framework of construction grammar and she also explores the possibility to express the findings in PDT.

Keywords: constituent negation, adversative coordination, Construction Grammar, Functional Generative Description, Prague Dependency Treebank</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek navrhuje přímý (forward) překladový model sestávající z množiny maximum entropy klasifikátorů: pro každé (dostatečně časté) zdrojové lemma je natrénován jeden klasifikátor. Tímto způsobem mohou být překladové pravděpodobnosti modelovány s ohledem na mnoho rysů (features) ze zdrojové věty, včetně nelokálních rysů využívajících syntaxe věty. Po zapojení do anglicko-českého překladového systému TectoMT bylo dosaženo signifikantního zlepšení oproti původnímu (baseline) překladovému modelu - měřeno metrikou BLEU.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Maximum Entropy Principle has been
used successfully in various NLP tasks. In
this paper we propose a forward translation
model consisting of a set of maximum
entropy classifiers: a separate classifier
is trained for each (sufficiently frequent)
source-side lemma. In this way
the estimates of translation probabilities
can be sensitive to a large number of features
derived from the source sentence (including
non-local features, features making
use of sentence syntactic structure,
etc.). When integrated into English-to-
Czech dependency-based translation scenario
implemented in the TectoMT framework,
the new translation model significantly
outperforms the baseline model
(MLE) in terms of BLEU. The performance
is further boosted in a configuration
inspired by Hidden Tree Markov Models
which combines the maximum entropy
translation model with the target-language
dependency tree model.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku ukazujeme na příkladu lokativní sémantické diateze možnosti lexikografického popisu sémantických diatezí. Odlišujeme sémantické diateze od gramatických diatezí. Sémantické diateze charakterizují změny v korespondenci situačních participantů a valenčních doplnění, které vedou k takovým změnám ve valenční struktuře sloves, které se liší i v rámci jednoho typu diateze. Sémantické diateze navrhujeme zachycovat pomocí oddělených valenčních rámců propojených určitým typem sémantické diateze. Změny ve valenční struktuře spojené se sémantickými diatezemi jsou pak zachyceny pomocí lexikálních pravidel vycházejících z lexikálně sémantické reprezentace sloves založené na predikátové dekompozici (tzv. lexikálně-konceptuálních strukturách).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this contribution, we demonstrate the description of semantic diatheses on the example of Czech locative semantic diathesis. We distinguish semantic diatheses from grammatical diatheses as two types of  changes in valency structure of verbs. Semantic diatheses are  characterized by the changes in mapping of situational participants and valency complementations. Changes in valency structure of verbs associated with semantic diatheses vary even within one type of diathesis so that they cannot be described by formal syntactic rules. For the purpose of the description of semantic diatheses, we propose to set separate valency frames corresponding to the members of semantic diatheses and to capture the relevant changes in valency structure by general lexical rules based on an appropriate formal representation of meaning of verb based on predicate decomposition (usually called lexical-conceptual structure).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku se zabýváme vybranými změnami ve valenční struktuře českých sloves z pohledu lexikografie. Nejprve vymezujeme dva typy slovesného významu, situační a strukturní význam, které společně tvoří komplexní významovou strukturu slovesa. Na základě tří typů asymetrie v korespondenci mezi komponentami situačního a strukturního významu vyčleňujeme tři typy změn ve valenční struktuře českých sloves a navrhujeme jejich zachycení ve valenčním slovníku VALLEX.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we deal with some changes in valency structure of Czech verbs from a lexicographic point of view. As a first step,
two parts of verbal meaning are stipulated: situational and structural meaning. These two parts of meaning constitute complex meaning structure of verbs. On the basis of three types of asymmetry in the correspondence between
components of situational and structural meaning, we distinguish three typologically different
changes in valency structure of Czech verbs. We propose a method of their description in the valency lexicon of Czech verbs, VALLEX. We show that two of them can be described by lexical rules stored in the grammar component of the lexicon whereas the third one can be treated in a special attribute in the data component of the lexicon.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku se zabýváme diatezemi v češtině z hlediska lexikografa. Navrhujeme metodu jejich popisu ve slovníku českých sloves VALLEX. Rozlišujeme dva typy diatezí, gramatické a sémantické, jako dvě typologicky odlišné typy změn ve valenční struktuře sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the present paper, we deal with diatheses in Czech from a lexicographic point of view. We propose a method of their description in
the valency lexicon of Czech verbs VALLEX. We distinguish grammatical and semantic diatheses as two typologically different changes in verbal
valency structure. In case of grammatical diatheses, these changes are regular enough to be described by formal syntactic rules. In contrast, the changes in valency structure of verbs associated with semantic diatheses vary even within one type of diathesis. Thus for the latter type, we propose to set separate valency frames corresponding to their members and to capture the changes in verbal valency structure by lexical rules based on an adequate lexical-semantic representation of verb meaning.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jazyk jako jeden z nejsložitějších systémů fascinoval vědce různých oborů po desetiletí. Ať už popisujeme jazyk z pohledu klasické lingvistiky, psychologie, komputační lingvistiky, medicíny nebo neurolingvistiky, stále se objevují otázky jako "Jak vlastně používáme a chápeme jazyk na úrovni našeho mozku?"

Nejzajímavější výsledky často vznikají ze spojeného úsilí několika vědeckých odvětví. V tomto článku ukážeme, jak statistika a informatika přispívají k zvládnutí zobrazení lidského mozku a jak tento pokrok zodpověděl některé z lingvistických otázek o lidském mozku.

Účelem tohoto článku není podat podrobný přehled o těchto výsledcích, ale spíše nabídnout srozumitelnou rešerši metod a technik na hranici neurověd a informatiky. Abychom tohoto cíle dosáhli, krátce se dotkneme základních a pokročilých metod zobrazování lidského mozku, od základní statistické analýzy a obecného lineárního modelu, přes bayesovské analýzy, až po rozpoznávání vzoru, a to z pohledu neurolingvistiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Human language as one of the most complex systems has fascinated scientists
from various fields for decades. Whether we consider language from a point of
view of a classical linguistics, psychology, computational linguistics,
medicine or neurolinguistics, it keeps bringing up questions such as "How do we
actually comprehend language in our brain?"

The most interesting achievements often result from a joined effort of multiple
scientific fields. In this paper, we will explore how statistics and
informatics contributed to human brain neuroimaging and how this answered some
of the linguistic questions about human brain.

The purpose of this paper is not to survey these achievements in detail, but
rather to offer a comprehensive coverage of methods and techniques on the
border of neuroimaging and informatics. To achieve this, we will touch on some
of the basic and advanced methods for neuroimaging techniques, ranging from
fundamental statistical analysis with the General Linear Model, Bayesian
analysis methods to multivariate pattern classification, in the light of
neurolinguistic research.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá vyhledávání informací v češtině za použití syntaktických jazykových modelů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we deal with information retrieval approach based on language model paradigm, which has been intensively investigated in recent years. We propose, implement, and evaluate an enrichment of language model employing syntactic dependency information acquired automatically from both documents and queries. By testing our model on the Czech test collection from Cross Language Evaluation Forum 2007 Ad-Hoc track, we show positive contribution of using dependency syntax in this context.
I</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kronika: o pátém ročníku konference Corpus Linguistics, pořádané jednou za dva roky ve Velké Británii.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Chronicle: on the fifth Corpus Linguistic Conference, held every other year in Great Britain</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jedním z projektů, které PDT 2.0 obohatí o novou vrstvu lingvistického popisu, je projekt anotace mezivýpovědních textových vztahů. Cílem tohoto projektu je zachytit sémantickou výstavbu textu (ve světovém lingvistickém povědomí „discourse structure“), zejména pak sémantické vztahy mezi jednotlivými výpověďmi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Short introduction of the discourse annotation project at the Institute of Formal and Applied Linguistics</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá problematikou segmentace slov. Ukáže, k čemu lze znalosti o segmentaci slov použít, a popíše několik metod pro automatickou segmentaci slov. Závěrem představí open source nástroj Affisix implementující představené metody.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article deals with automatic word segmentation. Several methods are presented which were implemented within the tool Affisix.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje nový pohled na valenci sloves, kdy se zkoumají vlastnosti sloves v rámci jejich skutečného užití. Termín "plná valence"  značí, že jsou uvažována všechna doplnění bez rozlišení obligatornosti. Jelikož lze očekávt, že plná valence odráží určité mechanismy, které řídí chování sloves v jazyce, jsou v článku testovány tři hypotézy: distribuce plných valenčních rámců, vztah mezi počtem rámců a frekvencí výskytu slovesa a vztah mezi počtem valenčních rámců a délkou slovesa. K otestování těchto hypotéz je užit Pražský závislostní korpus.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of the article is to introduce a new approach to verb valency analysis. This approach – full valency – observes properties of verbs which occur solely in actual language usage. The term ‘full valency’ means that all arguments, without distinguishing complements (obligatory arguments governed by the verb) and adjuncts (optional arguments directly dependent on the predicate verb), are taken into account. Because of an expectation that full valency reflects some mechanism which governs verb behaviour in a language, hypotheses concerning (1) the distribution of full valency frames, (2) the relationship between the number of valency frames and the frequency of the verb, and (3) the relationship between the number of valency frames and verb length were tested empirically. To test the hypotheses, a Czech syntactically annotated corpus – the Prague Dependency Treebank – was used.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>článek se zabývá problémy implementace systému automatického překladu mezi indonézštinou a malajštinou pomocí systému Apertium</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article describes issues encountered in the first implementation of an MT system between Indonesian and Malaysian.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jednou z obtíží, kterým čelí systémy statistického strojového překladu (SMT), jsou rozdíly ve slovosledu. Při překladu z jazyka, jako je angličtina, s pevným slovosledem typu SVO, do jazyka, jehož upřednostňovaný slovosled se dramaticky liší (např. slovosled typu SOV v urdštině, hindštině, korejštině, ...) se systém musí naučit přeuspořádávat slova a fráze na velkou vzdálenost v rámci celé věty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>One of the difficulties statistical machine translation (SMT) systems face are differences in word order. When translating from a language with rather fixed SVO word order such as English, to a language where the preferred word order is dramatically different (such as the SOV order of Urdu, Hindi, Korean, ...), the system has to learn long-distance reordering of the words.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Anglicko-urdský paralelní korpus slouží k trénování statistického strojového překladu mezi těmito dvěma jazyky. Skládá se ze čtyř částí: 1. Anglo-urdská část korpusu EMILLE; 2. texty z Wall Street Journalu (Penn Treebank); 3. překlady Koránu; 4. překlady Bible. Paralelní data, která existovala dříve (EMILLE) byla kompletně a nově ručně vyčištěna, opraveno zarovnání i řada vět na urdské straně.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>English-Urdu Parallel Corpus serves training of statistical machine translation between these two languages. It consists of four parts: 1. English-Urdu part of the EMILLE corpus; 2. texts from the Wall Street Journal (Penn Treebank); 3. translations of the Quran; 4. translations of the Bible. Parallel data that existed before (EMILLE) have been completely and newly manually cleaned, corrected alignment and many sentences on the Urdu side.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Výbor z díla Pavla Nováka (1932–2007) představuje vůbec první samostatné vydání vědeckých příspěvků českého jazykovědce – obecného lingvisty, bohemisty a albanisty, který spojil své odborné působení především s Filozofickou fakultou Univerzity Karlovy. Jeho podnětné myšlenky stále otevírají cesty k netriviálnímu uvažování o jazyce a jazykovědě, avšak dnes jsou čtenářům spíše neznámé, protože jsou rozesety po různých časopisech a sbornících nebo zůstaly ukryty v těžko dostupných výzkumných zprávách. Hlavním cílem knihy je tak umožnit současnému a budoucímu českému jazykovědnému publiku poznávat osobitý naturel lingvistického myšlení Pavla Nováka. Výbor je koncipován jako průřezový, editoři J. Křivan, J. Januška a J. Chromý do něj vybrali texty z různých tematických oblastí (metodologie lingvistiky, jazykový význam, pády, teorie syntaxe, dějiny novodobé české jazykovědy, popularizační články o jazyce aj.), s důrazem na ty příspěvky, které stále vynikají svojí aktuálností. Kniha je uvedena studiemi doc. Z. Starého, prof. J. Kořenského, prof. P. Sgalla a průvodním komentářem hlavního editora J. Křivana. Součástí výboru je rovněž Novákův životopis a nově sestavená bibliografie.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Selected works of Pavel Novák (1932-2007) is the first standalone publication of the scientific papers by Czech linguist, Bohemist and Albanist who bound his research to the Faculty of Arts of the Charles University in Prague. His stimulating thoughts keep leading us to non-trivial thinking about language and linguistics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku popisujeme Exodus, společný pilotní projekt generálního ředitelství pro překlad Evropské komise a generálního ředitelství pro překlad Evropského parlamentu, který zkoumá možnosti využití nových směrů strojového překladu v evropských institucích. Zúčastnili jsme se letošní soutěže WMT10 v překladu z angličtiny do francouzštiny s použitím trénovacích dat získaných z překladových pamětí institucí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we describe Exodus, a joint
pilot project of the European Commission’s
Directorate-General for Translation (DGT)
and the European Parliament’s Directorate-
General for Translation (DG TRAD) which
explores the potential of deploying new ap-
proaches to machine translation in European
institutions. We have participated in the
English-to-French track of this year’s WMT10
shared translation task using a system trained
on data previously extracted from large in-
house translation memories.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Výzkum na základě probíhajícího projektu anotace rozšířené textové koreference a asociační anafory na PDT 2.0. V článku se rozebírají problematické případy a předkládá se typologie neshod mezi anotátory.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The research is based on a developing project that aims to annotate nominal coreference and bridging anaphora in the syntactically annotated corpus of Czech texts, PDT 2.0. In the process of annotating coreferential and bridging relations it became evident that the relatively low inter-annotator agreement is, to a large extent, due to the fact that a text may have a variety of legitimate objective interpretations, rather than the annotators’ mistakes or carelessness. A classification of discrepancy types and possible causes of emergence thereof is presented. Typical examples of multiple interpretations of coreferencial and bridging relations in a text are given.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek zkoumá mapováné mezi dvěma sémantickými reprezentacemi, jmenovitě tektogramatickou rovinou Pražského závislostního korpusu (PDT) a (Robustní) Sémantikou minimální rekurze ((R)MRS). Jde o první pokus porovnat závislostní anotační schéma PDT a přístup založený na kompozicionální sémantice. Příspěvek představuje algoritmus pro konverzi PDT stromů do (R)MRS struktur.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper investigates the mapping between two semantic formalisms, namely the tectogrammatical layer of the Prague Dependency
Treebank 2.0 (PDT) and (Robust) Minimal Recursion Semantics ((R)MRS). It is a first attempt to relate the dependency-based annotation
scheme of PDT to a compositional semantics approach like (R)MRS. A mapping algorithm that converts PDT trees to (R)MRS structures
is developed, associating (R)MRSs to each node on the dependency tree. Furthermore, composition rules are formulated and the relation
between dependency in PDT and semantic heads in (R)MRS is analyzed. It turns out that structure and dependencies, morphological
categories and some coreferences can be preserved in the target structures. Moreover, valency and free modifications are distinguished
using the valency dictionary of PDT as an additional resource. The validation results show that systematically correct underspecified
target representations can be obtained by a rule-based mapping approach, which is an indicator that (R)MRS is indeed robust in relation
to the formal representation of Czech data. This finding is novel, as Czech, with its free word order and rich morphology, is typologically
different than languages analyzed with (R)MRS to date.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Seznámení s úlohou strojového překladu, její atraktivitou i problémy. Představení dvou velmi odlišných přístupů k překladu a jejich empirické porovnání.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A brief introduction to the task of machine translation. Two rather different approaches to MT are described and empirically compared.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace výsledků dvouměsíční stáže zaměřené na kombinaci systémů strojového překladu do češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The results of two-month stay devoted to the combination of machine translation outputs when translating to Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška pro frekventanty semináře Lucie Mladové.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An introduction to machine translation for the participants of seminar led by Lucie Mladová.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška podrobně představuje problémy strojového překladu specifické pro překlad z angličtiny do češtiny: otázky větného rozboru a bohaté morfologie na cílové straně.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk describes in detail the issues specific to English-to-Czech MT: sentence syntax and target-side rich morphology.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozhovor o přístupech ke strojovému překladu a jeho současných i budoucích možnostech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An interview about approaches to machine translation and its current and coming potence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CzEng 0.9 je třetí vydání velkého paralelního korpusu. V tomto vydání byl rozšířen o velké množství textů z různých typů zdrojů. Příspěvek popisuje a vyhodnocuje metody čištění paralelních dat a nabízí tak pohled na přínos jednotlivých typů zdrojů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CzEng 0.9 is the third release of a large parallel corpus of Czech and English. For the current release, CzEng was extended by significant
amount of texts from various types of sources, including parallel web pages, electronically available books and subtitles. This paper
describes and evaluates filtering techniques employed in the process in order to avoid misaligned or otherwise damaged parallel sentences
in the collection. We estimate the precision and recall of two sets of filters. The first set was used to process the data before their inclusion
into CzEng. The filters from the second set were newly created to improve the filtering process for future releases of CzEng. Given the
overall amount and variance of sources of the data, our experiments illustrate the utility of parallel data sources with respect to extractable
parallel segments. As a similar behaviour can be expected for other language pairs, our results can be interpreted as guidelines indicating
which sources should other researchers exploit first.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Část dat z korpusu CzEng 0.9 pro využití společností AppTek v jejích překladových systémech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A part of CzEng 0.9 corpus data released for the company AppTek.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Statistický strojový překlad do morfologicky bohatších jazyků je obtížná úloha, a to tím více, jestliže se zdrojový a cílový jazyk liší pořádkem slov. Nejlepší současné systémy proto neprodukují optimální výsledky. Mnohdy pomůže přidat paralelní data; pokud to nepomůže, může to být způsobeno různými problémy jako rozdílné domény, špatné párování slov nebo šum v nových datech. V tomto článku vyhodnocujeme úlohu strojového překladu z angličtiny do hindštiny z této datové perspektivy. Probíráme několik existujících zdrojů paralelních dat a poskytujeme výsledky křížových testů nad kombinacemi korpusů s použitím dvou volně dostupných statistických překladových systémů. Spolu s analýzou chyb také prezentujeme nový nástroj pro prohlížení spárovaných korpusů, díky čemuž je snadnější objevit problematické či obtížné pasáže v textech i pro vývojáře, který neovládá cílový jazyk překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Statistical machine translation to morphologically richer languages is a challenging task and more so if the source and target languages differ in word order. Current
state-of-the-art MT systems thus deliver mediocre results. Adding more parallel data often helps improve the results; if it doesn't, it may be caused by various problems such as different domains, bad alignment or noise in the new data.
In this paper we evaluate the English-to-Hindi MT task from this data perspective. We discuss several available parallel data sources and provide cross-evaluation results on their combinations using two freely available
statistical MT systems. Together with the error analysis, we also present a new tool
for viewing aligned corpora, which makes it easier to detect difficult parts in the data even for a developer not speaking the target
language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Korpus hindských textů z webu vhodný k jazykovému modelování: segmentovaný na věty a tokenizovaný.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A corpus of Hindi texts from the web suitable for language modelling: segmented into sentences and tokenized.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Anglicko-hindský paralelní korpus sestavený z několika zdrojů, tokenizovaný a zarovnaný po větách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>English-Hindi parallel corpus collected from several sources. Tokenized and sentence-aligned.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku studujeme potenciál a omezení nápadu budovat dvojjazyčný valenční slovník pomocí zarovnávání uzlů v paralelním závislostním korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we explore the potential and limitations of a concept of building a bilingual valency lexicon based on the alignment of nodes
in a parallel treebank. Our aim is to build an electronic Czech<->English Valency Lexicon by collecting equivalences from bilingual
treebank data and storing them in two already existing electronic valency lexicons, PDT-VALLEX and Engvallex. For this task a special
annotation interface has been built upon the TrEd editor, allowing quick and easy collecting of frame equivalences in either of the source
lexicons. The issues questioning the annotation practice encountered during the first months of annotation include limitations of technical
character, theory-dependent limitations and limitations concerning the achievable degree of quality of human annotation. The issues of
special interest for both linguists and MT specialists involved in the project include linguistically motivated non-balance between the
frame equivalents, either in number or in type of valency participants. The first phases of annotation so far attest the assumption that
there is a unique correspondence between the functors of the translation-equivalent frames. Also, hardly any linguistically significant
non-balance between the frames has been found, which is partly promising considering the linguistic theory used and partly caused by
little stylistic variety of the annotated corpus texts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje naše pokusy s anglicko-českým překladem pro soutěž WMT10: dvoukrokový překlad a optimalizaci na SemPOS.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes our experiments with
English-Czech machine translation for
WMT10 in 2010. Focusing primarily
on the translation to Czech, our additions
to the standard Moses phrase-based MT
pipeline include two-step translation to
overcome target-side data sparseness and
optimization towards SemPOS, a metric
better suited for evaluating Czech. Unfortunately,
none of the approaches bring a
significant improvement over our standard
setup.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ilustrujeme a vysvětlujeme problémy n-gramových metrik strojového překladu (např. BLEU), když jsou použity na jazyky s bohatou morfologií, jako je čeština. Nová metrika SemPOS problém omezuje a účinná je i angličtinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We illustrate and explain problems of
n-grams-based machine translation (MT)
metrics (e.g. BLEU) when applied to
morphologically rich languages such as
Czech. A novel metric SemPOS based
on the deep-syntactic representation of the
sentence tackles the issue and retains the
performance for translation to English as
well.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Česko-anglicko-ruský korpus UMC001 pro využití společností AppTek v jejích překladových systémech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Czech-English-Russian corpus UMC001 released for the company AppTek.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Senior Companion CZ kombinuje v jedné aplikaci poslední výsledky výzkumu v českém jazyce v oblasti: Automatické rozpoznávání řeči (ASR), Pochopení přirozeného jazyka (NLU), Generování přirozeného jazyka (NLG) a Text-to-Speech (TTS),
a magement dialogu (DM) pro přirozený dialog s člověkem. Některé součásti, speciálně vyvinuté v projektu Senior Companion CZ předčí nejlépší stávající výsledky, měřeno standardními kvantitativními metrikami používanými v této vědní oblasti (např. POS, morfologické taggery, atd.).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Czech Companion demonstrates that recent advances in Czech language: Automatic Speech Recognition (ASR), Natural Language Understanding (NLU), Natural Language Generation (NLG) and Text to Speech (TTS) synthesis can be combined with existing dialogue management (DM) techniques to build a dialogue system for natural-sounding dialogue. Certain components specifically developed in the Companions project surpassed the best existing results, as measured by the standard quantitative metrics used in the field (e.g., Czech and English full POS and morphological taggers, etc.).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve stati se analyzují české konstrukce s infinitivem ve funkci subjektu (typy Je příjemné poslouchat hudbu a Vadilo mu jít na koncert) a konfrontují se s Karlíkovými kritérii pro zjištění, zda jde o konstrukce typu "raising" nebo "control" popř. o konstrukce monoklauzální nebo biklauzální.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>P. Karlík's criteria for the distinction between raising and control constructions, and between monoclausal and biclausal constructions are tested on selecte Cyech examples where the infinitive fills the function of surface subject.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve stati se analyzuje Danešovo pojetí Mathesiova učení o funkční onomatologii a funkční syntaxi. Na jednoduchém českém příkladu se ukazuje, jak jsou ve Funkčním generativním popisu aplikovány všechny tři principy rozvíjené Danešem (jazykové pojmenování, usouvztažnění pomocí kategoríí syntakticky závisle proměnných a nezávislých).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Daneš's approach to Mathesius' principles of functional onomatolgy and functional syntax is used for the description of the procedure of senetence generation in Functional generative description. All three principles used there are exemplified by the simple Czech sentence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nutnost rozlišovat mezi ontologickým obsahem a jazykovým významem je zakotvena v evropské strukturní lingvistice. Příspěvek si neklade za cíl dospět k řešení této netriviální distinkce. Navzdory řadě problémů, které zmiňujeme, jsme ovšem přesvědčeni, že lingvistická analýza není bez zachování této distinkce možná.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The necessity to distinguish between ontological (cognitive, extralinguistic) content and linguistic (‚literal’) meaning has its sources in European structural linguistics. The present contribution cannot be aimed at the solution of these non-trivial distinctions; we only present some Czech examples as a challenge for consideration, which we believe to be useful for the determination of this boundary. Despite of these problems, we are convinced that without keeping the distinction between linguistic meaning and cognitive content during the analysis of language data the description of the language system is impossible.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V plenárním referátu na zasedání Komise pro gramatickou stavbu slovanských jazyků  
a Syntaxi srbštiny byl přednesen příspěvek o povaze gramatických diatezí. Z nich byla soustředěna pozornost na diatezi rezulatativní a její vlastnosti morfologické a syntaktické.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>On the plenary session within the Annual Meeting of the Committee of the Grammatical Structure of Slavonic Languages and the conference about Serbian Syntax the contribution analyzing the character of grammatical diathesis with the stress on resultative one was presented. Their morfological and syntactic features were discussed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Stať v rubrice  Kronika připomíná životní jubileum doc. dr. Adely Rechziegelové a její zásluhy o českou bohemistiku doma a zejména za jejího dlouholetého působení v Holandsku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the connection of the life anniversary of Dr. Adela Rechziegelová her achievements within Czech linguistcs at home and especially abroad (in Netherlands, where she has lived for a long time) are reminded.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku se porovnávají české reciproční konstrukce s jednotlivými charakteristikami, které z hlediska typologie jazyků s různou strukturou podává V. P. Nedjalkov. Konstatuje se, že homonymie mezi reflexivitou a reciprocitou je češtině rovněž vlastní, preference se řídí jinými principy. Shledávají se rysy odlišné od polštiny, porovnává se užití recipročních adverbií v češtině s francouzštinou a bulharštinou. "Lokální" reciproka jsou v češtině okrajová.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The features of Czech reciprocals are compared with their typological markers given by Nedjalkov. There are similar ambiguities between reflexives and reciprocs in Czech as in other languages, however with the different preferences. The occurence of "reciprocal" adverbs in Czech is different from French and Bulagarian. So-called "local" reciprocals are marginaly in Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Významy morfologických kategorií jsou nepostradatelnou součástí popisu větného významu. V Pražském závislostním korpusu 2.0 (PDT 2.0) je význam věty zachycován jako závislostní struktura sestávající z ohodnocených uzlů a hran. Významy morfologických kategorií jsou zachycovány v atributech těchto uzlů, v tzv. gramatémech. V příspěvku představujeme revizi slovesných gramatémů, revidovaná sada gramatémů bude použita při anotaci nové verze Pražského závislostního korpusu, verze PDT 3.0.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Meanings of morphological categories are an indispensable component of representation of sentence semantics. In the Prague Dependency
Treebank 2.0, sentence semantics is represented as a dependency tree consisting of labeled nodes and edges. Meanings of
morphological categories are captured as attributes of tree nodes; these attributes are called grammatemes. The present paper focuses on
morphological meanings of verbs, i.e. on meanings of the morphological category of tense, mood, aspect etc. After several introductory
remarks, seven verbal grammatemes used in the PDT 2.0 annotation scenario are briefly introduced. After that, each of the grammatemes
is examined. Three verbal grammatemes of the original set were included in the new set without changes, one of the grammatemes was
extended, and three of them were substituted for three new ones. The revised grammateme set is to be included in the forthcoming version
of PDT (tentatively called PDT 3.0). Rules for automatic and manual assignment of the revised grammatemes are further discussed in
the paper.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Připravovaný Pražský závislostní korpus 3.0 (PDT 3.0) se bude od stávající verze tohoto korpusu 
PDT 2.0 lišit jednak objemem anotovaných dat, jednak změnami v anotačním schématu. V přednášce představíme změny týkající se morfologických gramatémů. Morfologické gramatémy jsou atributy uzlů tektogramatického stromu, v těchto atributech jsou zachyceny významy sémanticky relevantních morfologických kategorií (např. kategorie substantivního čísla a slovesného způsobu). Podrobně popíšeme probíhající revizi slovesných gramatémů: zejména výsledky teoretického výzkumu slovesných diatezí a slovesného způsobu a promítnutí těchto výsledků do tektogramatické anotace. Naznačíme rovněž navrhované změny v zachycování kategorie čísla substantiv.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the present talk the revision of so-called grammatemes is introduced. Grammatemes are attributes of nodes of tectorammatical trees, in these attributes the meaning of morphological categories, such as tense or number, are captured. The revised set of grammatemes will be incorporated in the new version of Prague Dependency Treebank PDT 3.0.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška se zabývá elektronickýmmi jazykovými korpusy a jejich využitím, zejména pro lingvistickou práci. Zvláštní pozornost bude věnována Pražskému závislostnímu korpusu (PDT), představíme jeho strukturu, proces anotace a publikované verze (PDT 1.0, 2.0), zmíněny budou rovněž změny chystané pro další verzi tohoto korpusu PDT 3.0.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk deals with electronic language corpora and their usage, especially within linguistic research. Special attention is paid to Prague Dependency Treebank, its structure, annotation process and versions (PDT 1.0, 2.0), changes proposed for the new version (PDT 3.0) will be introduced.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předložená práce se skládá ze dvou částí. První část popisuje jeden z možných přístupů ke strojovému překladu, a sice překlad s využitím tektogramatické roviny jako roviny transferu. Druhá část je souborem vybraných článků autora z let 2000-2009.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presented work is composed of two parts.
In the first part we discuss one of the possible
approaches to using the annotation scheme of the Prague Dependency Treebank for the task of Machine Translation (MT), and demonstrate it in detail within our highly modular transfer-based MT
system called TectoMT.

The second part of the work consists of a  sample
of our publications, representing our research work from 2000 to 2009. Each article is accompanied with short comments on its context from a present day perspective. The articles
are classified into four thematic groups: nnotating Prague Dependency Treebank,
Parsing and Transformations of Syntactic Trees, Verb Valency, and Machine Translation.

The two parts naturally connect at numerous points, since most of the topics tackled in the second part---be it sentence analysis or synthesis, coreference resolution, etc.---have their obvious places in the mosaic of the translation process and are now in some way implemented in the TectoMT system
described in the first part.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška je věnována TectoMT, vysoce modulárnímu systému pro zpracovávání textů v přirozeném jazyce. Systém je implementován v Perlu. Primárně ja zaměřen na strojový překlad, využívá k tomu teoretický rámec a softwarové nástroje Pražského závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I will introduce TectoMT, which is a highly modular NLP (Natural Language Processing) software system implemented in Perl programming language under Linux. It is primarily aimed at Machine Translation, making use of the
ideas and technology created during the Prague Dependency Treebank project. At the same time, it is also hoped to significantly facilitate
and accelerate development of software solutions of many other NLP tasks, especially due to re-usability of the numerous integrated processing
modules (such as taggers, parsers, or named-entity recognizers), which are all equipped with uniform object-oriented interfaces.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pracuji jako výzkumný pracovník v Ústavu formální a aplikované lingvistiky (ÚFAL) Matematicko-fyzikální fakulty UK. Po absolvování Elektrotechnické fakulty ČVUT jsem tu v roce 2001 nastoupil na doktorské studium a dokončil jsem ho v roce 2005, nyní zde působím jako odborný asistent. Polovina mého úvazku je krytá projektem LC536 – Centrum komputační lingvistiky (2005–2010).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I work as a researcher in the Institute of Formal and Applied Linguistics, Charles University in Prague. After graduating at the Faculty of Electrical Engineering in 2001, I started my PhD study, which I have finished in 2005. Now I work here as an assistant professor.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>TectoMT je systém, který umožňuje rychlý a efektivní vývoj NLP aplikací. Využívá široké spektrum softwarových modulů, které jsou již integrovány do TectoMT, např. nástroje pro segmentaci textu na věty, tokenizaci, morfologickou analýzu a disambiguaci (tagging), parsing (povrchový i hloubkový), rozpoznávání pojmenovaných entit, rozpoznávání anafory, strojový překlad stromových struktur, generování vět z hloubkových struktur, slovní zarovnávání paralelních korpusů atd.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>TectoMT is a multi-purpose open-source NLP framework. It allows for fast and efficient development of NLP applications by exploiting a wide range of software modules already integrated in TectoMT, such as tools for sentence segmentation, tokenization, morphological analysis, POS tagging, shallow and deep syntax parsing, named entity recognition, anaphora resolution, tree-to-tree translation, natural language generation, word-level alignment of parallel corpora, and other tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Strojový překlad z angličtiny do češtiny implementovaný v systému TectoMT sestává ze tří fází: analýzy, transferu a syntézy. Pro transfer se využívají tektogramatické stromy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>English to Czech machine translation as it is implemented in the TectoMT system consists of three phases: analysis, transfer and synthesis. The system uses tectogrammatical (deep-syntactic dependency) trees as the transfer medium.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek obsahuje recenzi valenčního slovníku českých sloves autorů z Univerzity Karlovy v Praze, který před dvěma lety vydalo nakladatelství Karolinum.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article reviews the valency dictionary of Czech verbs by authors from the Charles University in Prague, published two years ago by Karolinum.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Výbor z díla doc. Pavla Nováka (1932–2007) představuje vůbec první samostatné vydání vědeckých příspěvků českého jazykovědce – obecného lingvisty, bohemisty a albanisty, který spojil své odborné působení především s Filozofickou fakultou Univerzity Karlovy. Jeho podnětné myšlenky stále otevírají cesty k netriviálnímu uvažování o jazyce a jazykovědě, avšak dnes jsou čtenářům spíše neznámé, protože jsou rozesety po různých časopisech a sbornících nebo zůstaly ukryty v těžko dostupných výzkumných zprávách. Hlavním cílem knihy je tak umožnit současnému a budoucímu českému jazykovědnému publiku poznávat osobitý naturel lingvistického myšlení Pavla Nováka. Výbor je koncipován jako průřezový, editoři J. Křivan, J. Januška a J. Chromý do něj vybrali texty z různých tematických oblastí (metodologie lingvistiky, jazykový význam, pády, teorie syntaxe, dějiny novodobé české jazykovědy, popularizační články o jazyce aj.), s důrazem na ty příspěvky, které stále vynikají svojí aktuálností. Kniha je uvedena studiemi doc. Z. Starého, prof. J. Kořenského, prof. P. Sgalla a průvodním komentářem hlavního editora J. Křivana. Součástí výboru je rovněž Novákův životopis a nově sestavená bibliografie.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Selected works of Pavel Novák (1932-2007) is the first standalone publication of the scientific papers by Czech linguist, Bohemist and Albanist who bound his research to the Faculty of Arts of the Charles University in Prague.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Namísto diskuse mezi lingvisty samotnými, zvláště mezi odpůrci a zastánci pozůstatků jazykového purismu, mnozí odborníci na češtinu se stále raději obracejí k českým mluvčím a odrazují je od používání jazykových variant, které nebyly monopolní kodifikací uznány za „spisovné“.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Instead of a discussion between linguists themselves, especially between adversaries and supporters of the standing remnants and consequences of purism, many specialists in Czech prefer to turn to speakers of Czech, reproaching them for their continuing usage of language varieties that have not been acknowledged as “literary” by the monopolistic codification. A long continuation of discussions among linguists thus will be needed to overcome the post-purist dilemma concerning Standard Czech. Conditions for a transition from the present majority approach based on traditions concerning French and German to an approach closer to that proper to Anglo-Saxon linguistics are strengthened by the existence of large text corpora, with which it is possible to get better knowledge of the actual usage of morphemic and other forms. The intuition of speakers, underlying their linguistic behaviour, should be distinguished from intuition of linguists, which served as the starting point of description of Czech up to the middle of the 20th century and was then overcome by approaches based on research. Hypercorrection stems in the differences between actual usage and codification, and it is important to distinguish between hypercorrect expressions and those that came into existence as hypercorrect, but have already penetrated into the Standard norm, or into its large borderland.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládaná Mluvnice současné češtiny autorů působících na různých ústavech Univerzity Karlovy je po téměř patnácti letech novým pokusem o stručný a srozumitelný popis našeho mateřského jazyka. Mluvnice je koncipována jako dvoudílná, přičemž první díl zahrnuje poučení mj. o zvukové stránce jazyka, slovní zásobě, slovotvorbě, tvarosloví, stylistice a psací soustavě (druhý díl pak bude věnován větné skladbě).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present Grammar of Current Czech by authors working at various institutes of the Charles University is the first attempt in almost fifteen years to concisely and comprehensibly describe our mother tongue. The grammar is split into two volumes and Volume 1 includes information on phonetics, vocabulary, morphology, stylistics and the writing system (Volume 2 will be devoted to sentence syntax).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Autor přišel v dobách německé okupace o otce a o většinu svých příbuzných a strávil posledních šest měsíců války v koncentračním táboře v Postoloprtech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The author lost his father and most of his relatives in the time of the German occupation, and spent the last six months of that time in the concentration camp of Postoloprty.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Sborník příspěvků. Z obsahu: Užívání jazyka jako kreativní proces. Pražská škola a empirický funkcionalismus: danešovské inspirace. Větná melodie v češtině v pohledu současného výzkumu. Fonetická a fonologická hlediska při zkoumání české intonace. Označování rodičů v českojazyčném prostředí. Český slovotvorný systém 21.století v databázích. Jazyk, jeho užívání a funkce při bohoslužbě. Komponentová analýza predikátu a hiearchizace propozice. Interpretace negativních zájmen v češtině. Rozvíjení tématu v akademickém a narativním textu. Perspektivy češtiny a spisovnost kolem nás i v nás. K řečové kultuře současnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The usage of language as a creative process. The Prague School and the empirical functionalism: Danešian inspirations. Sentence melody in Czech from the point of view of the current research. Phonetic and phonological aspects of the Czech intonation. Marking of parents in the Czech language environment. The Czech morphological system of the 21st century in databases. Language, its usage and function for a church service. Component analysis of predicate and hierarchization of proposition. Interpretation of negative pronouns in Czech. Topic development in academic and narrative text. The future of Czech and the language standard around us and in us. On the contemporary speech culture.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rematizátory jsou zvláštní třídou částic, které mají svou specifickou roli v informační struktuře věty. Nejsou však nutné indikátory jejiho rématu či ohniska.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Rhematizers belong to a specific class of particles that have a special function in the information structure of the sentence, but it is argued that in spite of the name of this class, they are not necessarily indicators of the focus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Autorka upozorňuje na tři otevřené problémy, především z hlediska minimalistické teorie, a to popis sémantické relevance aktuálního členění, rozdíl mezi významem</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Three open questions are formulated .concerning (i) the minimalist account of the semantic relevance of the topic-focus articulation, (ii) the minimalist account of the distinction between meaning and content, and (iii) how to represent  in corpus annotation syntactic ambiguities.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V kapitole se zabýváme anotací treebanků - jejich definicí, vlastnostmi, příklady existujících treebanků, jejich vztahem k lingvistické teorii a procesem anotace a kontroly kvality. Věnujeme se také použití treebanků, zvláště pojednáváme o vyhledávání lingvistické informace v treebancích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the chapter, we focus on treebank annotation – the definition of treebanks, their properties, examples of existing treebanks, their relation to linguistic theory and the process of annotation and quality control. We also refer the application of treebanks, and specifically discuss searching for linguistic information in them.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce se zabývá anotací víceslovných výrazů v Pražském závislostním korpusu 2.0. Popisuje datovou reprezentaci vyvinutou pro anotaci i anotační nástroj a další software vyvinutý pro anotaci i vizualizaci a prohledávání anotovaných dat. Práce také popisuje anotační slovník obsahující víceslovné výrazy nalezené v datech a zabývá se i analýzou anotací z hlediska kvality a efektivity.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This thesis explores annotation of multiword expressions in the Prague Dependency Treebank 2.0. We explain, what we understand as multiword expressions (MWEs), review the state of PDT 2.0 with respect to MWEs and present our annotation. We describe the data format developed for the annotation, the annotation tool, and other software developed to allow for visualisation and searching of the data. We also present the annotation lexicon SemLex and analysis of the annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek zkoumá datový formát CoNLL ST, jeho vlastnosti a možnosti jeho použití pro komplexní anotace. Tvrdíme, že CoNLL ST se možná navzdory původnímu záměru stal jedním z nejdůležitějších formátů syntakticky anotovaných dat současnosti. Ukazujeme meze tohoto formátu v jeho současné podobě a navrhujeme několik jednoduchých rozšíření, která je posunují dále a činí ho robustnějším a použitelnějším v budoucnosti. Analyzujeme několik lingvistických anotací různé složitosti jako příklady a ukazujeme, jak mohou být účinně reprezentovány ve formátu CoNLL ST.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we investigate the CoNLL Shared Task format, its properties and possibility of its use for complex annotations. We argue that, perhaps despite the original intent, it is one of the most important current formats for syntactically annotated data. 
We show the limits of the CoNLL-ST data format in its current form and propose several simple enhancements that push those limits further and make the format more robust and future proof. We analyse several different linguistic 
annotations as examples of varying complexity and show how they can be efficiently stored in the CoNLL-ST format.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce je věnovaná empirické studii lexikálních asociačních měr a jejich aplikaci v úloze automatické extrakce kolokací. Experimenty byly provedeny na třech referenčních datových množinách: závislostních bigramech z ručně anotovaného Pražského závislostního korpusu, povrchové bigramy ze stejného korpusu a instancích prvků předchozí množiny z Českého národního korpusu opatřeného automatickou lemmatizací a morfologickým značkováním. Kolokační kandidáti v referenčních  množinách byli manuálně anotováni jako kolokace nebo nekolokace. Použité evaluační schéma je založeno na měření kvality seřazení kolokačních kandidátů dle jejich pravděpodobnosti tvořit kolokaci. Metody jsou porovnány pomocí precision-recall křivek a hodnot mean average precision, které jsou převzaty  z oboru vyhle­dávání informací. Provedeny byly i testy signifikance výsledků. Dále je zkoumána možnost kombi­nování lexikálních asociačních měr a presentovány výsledky několika kombinačních metod, jejichž použití vedlo k výraznému zlepšení úspěšnosti řešení této úlohy. Dále je v práci navržen algoritmus významně redukující složitost použitých kombinačních modelů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an extensive empirical evaluation of collocation extraction methods based on lexical association measures and their combination. The experiments are performed on three sets of collocation candidates extracted from the Prague Dependency Treebank with manual morphosyntactic annotation and from the Czech National Corpus with automatically assigned lemmas and part-of-speech tags. The collocation candidates were manually labeled as collocational or non-collocational. The evaluation is based on measuring the quality of ranking the candidates according to their chance to form collocations. Performance of the methods is compared by precision-recall curves and mean average precision scores. The work is focused on two-word (bigram) collocations only. We experiment with bigrams extracted from sentence dependency structure as well as from surface word order. Further, we study the effect of corpus size on the performance of the individual methods and their combination.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valence je důležitou součástí PZK. Je zachycena ve slovníku, jenž obsahuje všechna slovesa, která se v korpusu vyskytla. Valenční rámec je popsán z formálního hlediska, obsahuje obligatorní volná doplnění a obligatorní a fakultativní argumenty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Prague Dependency Treebank (PDT) contains as its integral part a valency lexicon called PDT-Vallex. PDT-Vallex lists all the verbs that occur in the PDT and it also distinguishes their senses. A valency frame is formally described for each sense. This valency frame includes a list of verb complementations and their required surface form(s), for obligatory as well as optional arguments.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce je souborem detailních studií věnovaných syntakticky nebo sémanticky uceleným skupinám českých deverbativních substantiv (zejména substantiv s dativní valencí). Opírá se o bohatý korpusový materiál. Problematiku valence deverbativních substantiv představuje jako pružný mechanismus, který na jedné straně funguje na základě v systému pevně zakotvených a jasně stanovených primárních obecných principů, na druhé straně je ovšak ovlivněn několika různorodými principy sekundárními, umožňujícími rozličné nové, od slovesa nezděděné formy valenčních doplnění. Jako teoretický rámec pro popis valenčních vlastností zkoumaných substantiv byl zvolen funkční generativní popis.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The monograph deals with valency properties of deverbal nouns in Czech. After an overview of Czech and foreign approaches to the valency of nouns and a summary of current knowledge concerning some special issues of valency of Czech deverbal nouns, we present results consisting of our analysis of syntactically and semantically compact groups of deverbal nouns as well as theoretical conclusions following from the examined language material. We have focused our attention on Czech deverbal nouns that can be modified by a participant expressed by prepositionless dative. Such nouns were searched for in two Czech electronic corpora, Czech National Corpus (CNC) and Prague Dependency Treebank (PDT). The obtained occurrences were manually sorted and analysed and their valency behaviour is described within the theoretical framework of the Functional Generative Description (FGD).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Softwarový nástroj zprostředkující formou webového rozhraní editaci a vyhledávání v přepisech audio-visuálních nahrávek dialogů. Vyhledané úseky textu je možné přehrávat a analyzovat pomocí webového prohlížeče.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A software tool allowing editing and searching in the transcripts of audio-visual recordings of dialogues. The dynamic web application provides access for registered users to the digitised archive. Playing and exploring of selected parts is possible in the web browser.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nástroj pro anotaci sémantických mezivětných vztahů na tektogramatické rovině Pražského závislostního korpusu (PDT). Uvádíme vlastnosti nástroje, které pomáhají anotátorům, jako jsou možnost kombinace anotace na větách a na reprezentaci hloubkové syntaxe, možnost definovat, zobrazit a spojovat libovolné skupiny uzlů, dále kompaktní zobrazení stromů po klauzích apod. Pro studium rozdílů v paralelních anotací nástroj poskytuje současné zobrazení paralelních anotací jedněch dat více anotátory.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a tool for annotation of semantic inter-sentential discourse relations on the tectogrammatical layer of the Prague Dependency Treebank (PDT). We present the way of helping the annotators by several useful features implemented in the annotation tool, such as a possibility to combine surface and deep syntactic representation of sentences during the annotation, a possibility to define, display and connect arbitrary groups of nodes, a clause-based compact depiction of trees, etc. For studying differences among parallel annotations, the tool offers a simultaneous depiction of parallel annotations of the data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Uvádíme několik způsobů měření mezianotátorské shody v probíhající anotaci sémantických mezivětných diskurzních vztahů v Pražském závislostním korpusu (PDT). Jsou použity dva způsoby pro překonání nevýhod měření shody na přesné pozici počátečních a koncových bodů vztahů. Obě metody - přeskočení jedné úrovně stromu v počátečním či koncovém uzlu a míra založená na konektorech - jsou zaměřený spíše na rozpoznání existence vztahu a jeho typu než na přesnou pozici počátečních a koncových bodů spojujících šipek.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present several ways of measuring the inter-annotator agreement in the ongoing annotation of semantic inter-sentential discourse relations in the Prague Dependency Treebank (PDT). Two ways have been employed to overcome limitations of measuring the agreement on the exact location of the start/end points of the relations. Both methods – skipping one tree level in the start/end nodes, and the connective-based measure – are focused on a recognition of the existence and of the type of the relations, rather than on fixing the exact positions of the start/end points of the connecting arrows.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme anotační nástroj pro rozšířenou textovou koreferenci a bridging anaforu v Pražském závislostním korpusu 2.0. Popisujeme způsob pomoci anotátorům pomocí několika praktických vlastností implementovaných v anotačním nástroji, jako jsou možnost kombinovat povrchovou a hloubkovou syntaktickou reprezentaci vět během anotace, automatické zachovávání koreferenčního řetězce, podtrhávání kandidátů pro antecedent apod.. Pro studium rozdílů v paralelních anotacích nabízí nástroj současné zobrazení několika anotací jedněch dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an annotation tool for the extended textual coreference and the bridging anaphora in the Prague Dependency Treebank 2.0. We describe the way of helping the annotators by several useful features implemented in the annotation tool, such as a possibility to combine surface and deep syntactic representation of sentences during the annotation, an automatic maintaining of the coreferential chain, underlining candidates for antecedents, etc. For studying differences among parallel annotations, the tool offers a simultaneous depiction of several annotations of the same data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slova v nadpisu této kapitoly jako by k sobě překvapivě dobře pasovala. Nejen že závislost a funkce jsou ústřední pojmy mnoha moderních lingvistických teorií a že jsou ‚inherentní‘ v informatice a logice. Neméně zajímavý je i jejich vztah ke studiu arabského jazyka a jeho významu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The words in the title of this chapter seem to like each other to a surprising extent. Not only are the notions of dependency and function central to many modern linguistic theories and ‘inherent’ to computer science and logic. Their connection to the study of the Arabic language and its meaning is interesting, too.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>ElixirFM je vysokoúrovňová implementace Funkční arabské morfologie zdokumentovaná na http://elixir-fm.wiki.sourceforge.net/. Jádro ElixirFM je napsáno v Haskellu, zatímco rozhraní v Perlu podporuje údržbu slovníku a další interakce.
- rozšířený a zdokonalený slovník
- zdokonalená analýza sekvencí slov
- zdokonalené uživatelské rozhraní
- zdokonalené API</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>ElixirFM is a high-level implementation of Functional Arabic Morphology documented at http://elixir-fm.wiki.sourceforge.net/. The core of ElixirFM is written in Haskell, while interfaces in Perl support lexicon editing and other interactions.
- extended and improved lexicon
- improved analysis of word sequences
- improved user interface
- improved API</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Addicter je zkratka za Automatic Detection and DIsplay of Common Translation ERrors. Je to soubor nástrojů (především skriptů napsaných v Perlu), které pomáhají s analýzou chyb strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Addicter stands for Automatic Detection and DIsplay of Common Translation ERrors. It is a set of tools (mostly scripts written in Perl) that help with error analysis for machine translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Morfologické značky jsou důležitým prostředkem anotace ve velkém množství korpusů. V různých korpusech, dokonce i pro tentýž jazyk, se však používají různé sady značek. Konverze sad značek je obtížná a řešení bývají ušitá na míru konkrétní dvojici sad. V článku probíráme Interset, univerzální metodu, díky které se dají převodní nástroje používat opakovaně. Zatímco některé mluvnické kategorie jsou jasně definované a dají se snadno přenášet z jedné sady do druhé, existují také jevy, které je těžké zachytit kvůli překrývajícím se konceptům. Zaměřujeme se na některé takové problémy, probíráme jejich výskyt ve vybraných sadách značek a navrhujeme řešení, která sjednotí přístupy jednotlivých sad.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Part-of-speech or morphological tags are important means of annotation in a vast number of corpora. However, different sets of tags are used in different corpora, even for the same language. Tagset conversion is difficult, and solutions tend to be tailored to a particular pair of tagsets. We discuss Interset, a universal approach that makes the conversion tools reusable. While some morphosyntactic categories are clearly defined and easily ported from one tagset to another, there are also phenomena that are difficult to deal with because of overlapping concepts. In the present paper we focus on some of such problems, discuss their coverage in selected tagsets and propose solutions to unify the respective tagsets' approaches.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme naše pokusy s hierarchickým frázovým strojovým překladem v soutěži WMT 2010. Poskytujeme podrobný popis našich dat a konfigurace, aby bylo možné naše výsledky zopakovat. U překladu z angličtiny do češtiny jsme experimentovali s několika různě velkými soubory dat a s různými způsoby předzpracování. U zbývajících 7 směrů překladu předkládáme pouze výchozí výsledky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe our experiments with hierarchical
phrase-based machine translation
for WMT 2010 Shared Task. We provide
a detailed description of our configuration
and data so the results are replicable. For
English-to-Czech translation, we experiment
with several datasets of various sizes
and with various preprocessing sequences.
For the other 7 translation directions, we
just present the baseline results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>MD-make (multidimenzionální make) je nástroj, který umožňuje zpracování datových souborů v mnoha rozměrech jako jazyky, domény, způsoby předzpracování, alternativní zpracovatelské programy, vývojová/vyhodnocovací data atd.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>MD-make (multidimensional make) is a tool that enables processing of data files in many dimensions such as languages, domains, preprocessing styles, alternative processing programs, development/evaluation data etc.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V roce 2040, musí být ACL sborníku stroje přeloženy do 20 jazyků z původního čínského. (Kevin Knight)</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>By 2040, the ACL conference proceedings shall be machine translated into 20 languages from the original Chinese. (Kevin Knight)</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tagzplorer je nástroj pro průzkum anotovaných korpusů, sumarizaci morfologických značek, dohledávání příkladů výskytů a jejich snadnou vizualizaci pomocí hypertextu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Tagzplorer is a tool for exploring annotated corpora, summarization of POS and morphological tagsets, finding example occurrences and their easy visualization using hypertext.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Metoda a software pro romanizaci (tj. transliteraci do písma založeného na latince) urdského textu. Cílem je odrážet pokud možno původní výslovnost, aniž by byla porušena obnovitelnost původního pravopisu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Approach and software for Romanization (i.e. transliteration into a Latin-based alphabet) of Urdu text. My goal is to reflect the original pronunciation as well as possible, while not violating the requirement that the original spelling be restorable. To help the reader with the pronunciation, I want to insert missing short vowels and disambiguate a few other cases. I provide a Perl script that implements the deterministic part of the transliteration and marks positions where human decision is needed. Urdu uses a few characters that are not used in the original Arabic script. Moreover, some of the original Arabic letters might prefer a different Latin representation if the mapping were motivated by Arabic, instead of Urdu pronunciation. On the target side, no particular language was on my mind when modeling the pronunciation. See below for details.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme systematické srovnání předzpracovávacích metod pro dva jazykové páry: angličtina-čeština a angličtina-hindština. Přestože oba cílové jazyky patří do indoevropské jazykové rodiny, vykazují významné odlišnosti v tvarosloví, skladbě a slovosledu. Popisujeme TectoMT, úspěšné prostředí pro analýzu a syntézu jazyka, a ukazujeme, jak ho lze využít pro předzpracování dat pro frázový systém strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a systematic comparison of preprocessing techniques
for two language pairs: English-Czech and English-Hindi. The two target languages, although both
belonging to the Indo-European language family, show significant differences in morphology, syntax and word order. We describe how TectoMT, a successful framework for analysis and generation of language, can be used as preprocessor for a phrase-based MT system.
We compare the two language pairs
and the optimal sets of source-language transformations applied to them. The following transformations are examples of possible preprocessing steps: lemmatization; retokenization, compound splitting; removing/adding words lacking counterparts in the other language; phrase reordering to resemble the target word order; marking syntactic functions. TectoMT, as well as all other tools and data sets we use, are freely available on the Web.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Hráči je předložen text s odstraněnými mezerami mezi slovy. Jeho úkolem je mezery na správná místa vložit. Cílem hry je nejen nalákat hráče k závažnějším jazykovým hrám, ale pro jazyky jako je např. thajština i získat trénovací data.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The player fills in spaces between words in text, from which they have been previously removed. The aim of the game is not only to attract the players to more serious language games but for languages like Thai to obtain training data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Hráči hádají slova ve větách na základě předchozího kontextu. Cílem hry je nejen nalákat hráče k závažnějším jazykovým hrám, ale rovněž studovat entropii jazyka z pohledu rodilého mluvčího.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The players guess words in sentences on the basis of the previous context. The aim of the game is not only to attract the players to more serious language games but also to study entropy of the language from the native speaker's point of view.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>PlayCoref je internetová hra pro dva hráče s cílem zábavnou metodou obohatit holý text anotací koreference.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>PlayCoref is an internet game for two players whose aim is to enrich pure text with the annotation of coreference, using an entertaining method.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme hru PlayCoref, internetovou hru s cílem obohatit textová data anotací koreference. Uvádíme detailní popis hry, obzvláště její průběh a implementaci, a zmiňujeme se o procesu zpracování vstupních dat a o vyhodnocovací funkci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the PlayCoref game, an on-line
internet game, whose purpose is to enrich
text data with coreference annotation. We
provide a detailed description of the game,
especially of its course and its implementation,
and we mention the processing of
the data and the scoring function.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje několik způsobů měření a evaluace anotace a anotátorů, které byly navrženy a používány při anotaci české části Pražského česko-anglického závislostního korpusu: měření mezianotátorské shody, chybovosti a výkonu anotátorů. Měření shody je komplikováno faktem, že při anotaci bylo možné přidávat i mazat uzly, takže není snadné zjistit, které uzly si odpovídají. Míra chybovosti je zjišťována pomocí sady kontrolních procedur, které sledují platnost daných invariantů v datech. Výkon anotátorů je zaznamenáván pomocí "účetní" webové aplikace. Všechny tři metody jsou poté porovnány a uvedeny do souvislostí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents several ways to measure and evaluate the annotation and annotators, proposed and used during the building of the Czech part of the Prague Czech-English Dependency Treebank. At first, the basic principles of the treebank annotation project are introduced (division to three layers: morphological, analytical and tectogrammatical). The main part of the paper describes in detail one of the important phases of the annotation process: three ways of evaluation of the annotators - inter-annotator agreement, error rate and performance. The measuring of the inter-annotator agreement is complicated by the fact that the data contain added and deleted nodes, making the alignment between annotations non-trivial. The error rate is measured by a set of automatic checking procedures that guard the validity of some invariants in the data. The performance of the annotators is measured by a booking web application. All three measures are later compared and related to each other.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zpracování přirozeného jazyka je poměrně nový, rychle se rozvíjející obor, který nachází široké uplatnění v mnoha aplikacích. Jedním ze základních problémů zpracování jazyka je zachycení a popis významu. V této přednášce se budu věnovat zachycení významu, které vychází ze závislostní / valenční syntaxe. Krátce představím Funkční generativní popis, na jehož základě se vytvářejí soubory syntakticky a významově anotovaných dat (Pražský závislostní korpus) využívané pro takové úkoly jako je automatická syntaktická analýza či strojový překlad. Dále se budu věnovat problematice lexikálního významu a jeho popisu pomocí valenční charakteristiky (zejména) sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Natural language processing is a relatively new field, which has a strong impact on many applications. One of the fundamental problems of language processing is a description of meaning. In this talk I examine the importance of meaning description based on the dependency / valence syntax. First, I briefly introduce the Functional Generative Description and its application in a large syntactically annotated corpus, Prague Dependency Treebank. This corpus is used for many applied tasks such as parsing and machine translation. Second, I address the issue of lexical meaning and its description the valency lexicon VALLEX.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valencí se rozumí schopnost slovesa (příp. slova jiného slovního druhu) vázat na sebe určitý počet jiných, syntakticky závislých jazykových jednotek. Valenční informace se tedy vztahuje k jednotlivým lexémům, a jako takovou je nutno popsat ji pro jednotlivé lexémy ve formě slovníku. Bez valenčních slovníků se neobejdou komplexní aplikace pro zpracování přirozeného jazyka, které jsou založeny na explicitním popisu jazykových jevů; zároveň jsou takové slovníky nepostradatelné při vytváření jazykových dat, na nichž jsou založeny nástroje využívající strojového učení.

V předkládané habilatační práci shrnujeme výsledky dosažené při vytváření lexikální databáze českých sloves. Práce se soustřeďuje na tři základní okruhy. Prvním okruhem je formální zachycení valenčních vlastností českých sloves ve valenčním slovníku. Je zde představena logická stavba bohatě strukturovaných slovníkových dat. Druhým okruhem, kterému se práce věnuje, jsou nové teoretické aspekty, které přináší zpracování rozsáhlého jazykového materiálu -- je to především koncept kvazivalenčních doplnění a adekvátní zpracování slovesných alternací. Třetí okruh předkládané práce tvoří problematika formálního modelování přirozeného jazyka. Je zde představen nový formální model závislostní syntaxe založený na originálním konceptu restartovacích automatů. 

Hlavním aplikovaným výstupem předkládané práce je Valenční slovník českých sloves VALLEX, rozsáhlý a kvalitní veřejně dostupný slovník, který obsahuje významové a valenční charakteristiky nejčastějších českých sloves. Při navrhování jeho
koncepce byl kladen důraz na možnost všestranného využití pro člověka jako uživatele jazyka i pro aplikační účely při automatickém zpracování češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Valency refers to the capacity of verb (or a word belonging to another part of speech) to take a specific number and type of syntactically  dependent language units. Valency information is thus related to particular lexemes and as such it is necessary to describe valency characteristics for separate lexemes in the form of lexicon entries. A valency lexicon is indispensable for any complex Natural Language Processing application based on the explicit description of language phenomena. At the same time such lexicons
are necessary for building language resources which provide the basis for tools using machine learning techniques. 

The present habilitation work consists of a collection of already published scientific papers. It summarizes the results of building a lexical database of Czech verbs. It concentrates on three essential topics. The first of them is the formal representation of valency properties of Czech verbs in the valency lexicon. The logical organization of richly structured lexicon data is presented here. The second topic concerns new theoretical issues that result from the extensive processing of language material, namely the concept of quasi-valency complementation and adequate processing of verb alternations.
The third topic addresses questions of formal modeling of a natural language. A new formal model of dependency syntax based on a novel concept of restarting automata is introduced here.

The main applied product of the work presented here is the publicly available Valency Lexicon of Czech Verbs VALLEX, a large-scale, high-quality lexicon which contains semantic and valency characteristics for the most frequent Czech verbs. VALLEX has been designed with emphasis on both human and machine-readability. Therefore,
both linguists and developers of applications within the Natural Language Processing domain can use it.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku postupně vysvětlujeme pojem zlomového bodu introspekce (ZBI), pojem statistického zlomového bodu introspekce (SZBI) a způsob, jak a proč
jsme k těmto pojmům dospěli.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this article, we deal with the turning point of introspection (ZBI) and with the statistical turning point of introspection.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku se zavádí formální model pro závislostní a stratifikační popis přirozeného jazyka, který je motivován metodou postupné redukční analýzy. Model využívá konceptu obohacených restartovacích automatů, které zpracovávané větě přiřazují paralelní DR-struktury popisující vztah mezi jednotkami různých rovin popisu jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We provide a formal model of a stratificational
dependency approach to natural language description. This formal model is motivated by an elementary method of analysis by reduction, which serves for describing correct sentence analysis. The model is based on enhanced restarting
automata that assign a set of parallel dependency structures to every reduction of an input sentence. These structures capture the correspondence of dependency trees on
different layers of linguistic description, namely layer of surface syntax and layer of language meaning.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška se soustřeďuje na následující body:
(i) lingvistické zázemí FGD,
(ii) formální nástroje pro FGD a jejich adekvátnost,
(iii) porovnání FGD a přístupu `Abhangigkeitsgrammatik' (Jurgen Kunze), a
(iv) současné úkoly ve formálním modelování přirozeného jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk at the TheorieTag focuses on the following issues: 
(i) A linguistic background of FGD,
(ii) Formal tools connected with FGD and their adequacy,
(iii) A comparison between FGD and the    `Abhangigkeitsgrammatik' (developed by the group around Jurgen Kunze), and
(iv) Current tasks in formal models of natural languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku představujeme formální model pro stratifikační závislostní popis přirozeného jazyka. Tento model je motivován elementární metodou redukční analýzy, která umožňuje zachytit závislostní strukturu věty. Model je založen na obohacených restartovacích automatech, které dané větě (na základě možných redukcí) přiřazují množinu paralelnách závislostních struktur. Tyto struktury zachycují korespondenci mezi závislostními stromy pro různé roviny syntaxe, jmenovitě povrchové a hloubkové syntaxe.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We provide a formal model of a stratificational dependency approach to natural language description. This formal model is motivated by an elementary method of analysis by reduction,
which serves for describing correct sentence analysis. The model is based on enhanced restarting automata that assign a set of parallel dependency structures to every reduction of an input sentence. These structures capture the correspondence of dependency trees on different layers of linguistic description, namely the layer of surface syntax and the layer of language meaning.
The novelty of this contribution consists in the formal extension of restarting automata in
order to produce tree structures with several interlinked layers and in the application of these
automata to the stratificational description of a natural language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Restartovací automaty byly navrženy jako vhodný model pro redukční analýzu. V tomto příspěvku obohacujeme restartovací automaty o strukturovaný výstup v podobě závislostního stromu. Obohacené restartovací automaty slouží jako model Funkčního generativního popisu, který lze charakterizovat jako stratifikační závislostní popis přirozeného jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Restarting automata were introduced for modeling linguistically motivated analysis by reduction. In this paper we enhance these automata with a structured output in the form of a tree. Enhanced
restarting automata can serve as a formal framework for the Functional Generative Description. In this framework, a natural language is described at four layers. Working simultaneously with all these layers, tectogrammatical dependency structures that represent the meaning of the sentence are derived.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Verse Českého Wordnetu použitá pro lexikálně-semanticé anotace PDT s úpravami UFALu: http://hdl.­handle.­net/11858/00-097C-0000-0001-4880-3

Tato verze WordNetu obsahuje 23094 synsetů, z toho 203 synsety vznikly v ÚFALu a 22891 synsetů pochází z výchozí verze WordNetu vytvořené v Centru ZPJ FI MU.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A version of Czech WordNet that was used for lexico-semantic annotation of PDT: http://hdl.­handle.­net/11858/00-097C-0000-0001-4880-3 

The Czech WordNet was developed by the Centre of Natural Language Processing at the Faculty of Informatics, Masaryk University, Czech Republic. 
 
 The Czech WordNet captures nouns, verbs, adjectives, and partly adverbs, and contains 23,094 word senses (synsets). 203 of these were created or modified by UFAL during correction of the annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V kulturně, avšak nikoli strukturně si navzájem blízkých jazycích (pár čeština-švédština) existuje strukturní i sémantické kontinuum mezi vyjádřením účinku a vyjádřením účelu. Paralelní korpus umožňuje detailní pozorování těchto přechodů v překladech. 
Česká vedlejší věta účelová je typicky uvozena podřadicí spojkou aby, zatímco věta účinková je typicky uvozena podřadicí spojkou že. Vztah účelový vyjadřuje žádoucnost, zamýšlenost děje popsaného vedlejší větou: Odešli, aby mě nerušili. Už jsem raději mlčela, aby se nerozzlobil ještě víc. Vztah účinkový vyjadřuje „následek děje nebo vlastnosti, a to větou uvozenou spojkami že (tak že, tolik že, takový že…), až, takže (div že, taktak že): Tvářil se tak, že jsem raději mlčel. (Karlík et al., 1995, s. 353).  Za strukturní i významový přechod mezi oběma typy lze považovat určení zřetelově srovnávací – speciální případ, kdy se porovnává míra děje, stavu nebo vlastnosti s očekávaným následkem (Karlík et al., 1995, s. 465 a cf. s. 457-459). S výjimkou případů, kdy je nedostatečnost/přílišnost příznaku popsaného větou řídící vyjádřena příslovcem příliš (a synonymy), které dovoluje výhradně použití spojky aby, může mluvčí volit mezi aby a tak, přičemž aby (podle Karlík et al., 1995) implikuje, že účinek je žádoucí nebo očekávaný: Petr nebyl tak zkušený, že by to pochopil. Petr nebyl tak zkušený, aby to pochopil. 
Ve švédštině jsou účinkové věty uvozeny podřadicí spojkou så att („tak že“) a účelové věty jsou uvozeny spojkou för att („pro že“). Zřetelově srovnávací vztah se také vyjadřuje pomocí spojky för att. Definice účinkové, resp. účelové věty staví do popředí fakticitu děje popsaného vedlejší větou, resp. jeho záměrnost. Účinkové věty charakterizují faktické děje bez ohledu na to, zda jsou žádané nebo zamýšlené dějem popsaným řídící větou, zatímco účelové věty charakterizují žádané nebo zamýšlené děje bez ohledu na jejich fakticitu. Sémantický rozdíl mezi oběma typy se však stírá v případech, kdy sám řídící predikát účinkové věty neoznačuje faktický děj (například proto, že je v rozkazovacím způsobu) a zůstává pouze strukturní rozdíl u predikátu vedlejší věty (různost spojek a použití pomocného slovesa skola, které vyjadřuje budoucnost, vůli a někdy povinnost). 
Příspěvek porovnává páry švédských a českých vět účinkových, účelových a zřetelově srovnávacích a pozorování doplňuje o rešerše ve větších jednojazyčných korpusech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Three types of subordinate clauses express the dependence of the event they denote on an event that is denoted by the governing clause: these are the result clause (henceforth RC), the purpose clause (henceforth PC), and the minimum requirement clause (henceforth MRC). This paper analyzes the semantics of various forms of event consecutiveness expressed by these three clause types in Swedish (mainly based on Clausén et al., 2003, pp. 633-639) and Czech (besed on Karlík et al., 1995), respectively. Findings in a 2-million Swedish-Czech parallel corpus suggest that different languages may exhibit different clause-type preferences (RC/PC/MRC) when referring to consecutive events of the same type.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek je pilotní studií validace elektronického slovníku anglických sloves PDEV pro účely NLP.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Corpus Pattern Analysis (CPA) [4], coined and implemented
by Hanks as the Pattern Dictionary of English Verbs (PDEV) [3], appears
to be the only deliberate and consistent implementation of Sinclair’s
concept of Lexical Item [12]. In his theoretical inquiries [5] Hanks
hypothesizes that the pattern repository produced by CPA can also support
the word sense disambiguation task. Although more than 670 verb
entries have already been compiled in PDEV, no systematic evaluation
of this ambitious project has been reported yet.
Assuming that the Sinclairian concept of the Lexical Item is correct, we
started to closely examine PDEV with its possible NLP application in
mind. Our experiments presented in this paper have been performed on
a pilot sample of English verbs to provide a first reliable view on whether
humans can agree in assigning PDEV patterns to verbs in a corpus. As
a conclusion we suggest procedures for future development of PDEV.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek se zabývá automatickou extrakcí lexikálních realizací tzv. sémantických typů v elektronickém slovníku PDEV.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This contribution reports on an ongoing analysis of the Pattern Dictionary of English Verbs (PDEV, Hanks 2007b) with respect to both its consistency and reproducibility of use by different users. We address, in particular, the assign-ment of Semantic Type labels to noun collocates of verbs, in a series of experi-ments conducted at the Institute of Formal and Applied Linguistics of the Charles University in Prague.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje Reduktor, program, který automaticky odstraní nepoužívané části zdrojových kódů v jazyce Mercury.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of this paper is to introduce Reductor,
a program that automatically removes unused parts of the source code of valid programs written in the Mercury language. Reductor implements two main kinds of reductions: statical reduction and dynamical reduction. In the statical reduction, Reductor exploits semantic analysis of the Melbourne Mercury Compiler to nd routines which can be removed from the program. Dynamical reduction of routines additionally uses Mercury Deep Profiler and some sample input data for the program to remove unused contents of the program routines. Reductor modifies the sources of the program in a way, which keeps the formatting of the original program source so that the reduced code is further editable.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kapitola představuje diskriminativní techniky modelování, které opravují chyby vytvořené automatickým parserem. Model je podobný rerankingu, ale nevyžaduje generování n-best seznamů. Místo nich vytváří seznam potenciálně lepších výsledků parsování pomocí lokálních strukturálních transformací automaticky vytvořeného parsu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This chapter presents a discriminative modeling technique which corrects the errors made by an automatic parser.
The model is similar to reranking; however, it does not require the generation of k-best lists as in MCDonald et
al. (2005), McDonald and Pereira (2006), Charniak and Johnson (2005), and Hall (2007). The corrective strategy
employed by our technique is to explore a set of candidate parses which are constructed by making structurally–
local perturbations to an automatically generated parse tree.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek zavádí hybridní přístup v oblasti strojového překladu mezi příbuznými jazyky. Výsledky jsou uvedeny pro jazykové páry čeština-slovenština a čeština-ruština.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper introduces a hybrid approach to a very specific field in machine translation — the translation of closely related languages. It mentions previous experiments performed forclosely related Scandinavian, Slavic, Turkic and Romanic languagesand describes a novel method, a combination of a simple shallowparser of the source language (Czech) combined with a stochasticranker of (parts of) sentences in the target language (Slovak, Russian, Slovenian). The ranker exploits a simple stochastic model ofthe target language and its main task is to choose the best translation among those provided by the system. The last section of thepaper presents results indicating better translation quality comparedto the existing MT system for Czech and Slovak and compares themwith the results obtained by the translation from Czech to Russian using the same system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek podrobně popisuje využití grafových metod a datových struktur v jednoduchém systému strojového překladu pro blízké jazyky. Multigrafy použité v reprezentaci víceznačných částečných výsledků v různých fázích zpracování a syntaktické analýzy umožňují modifikaci jednoduché architektury vyvinuté před deseti lety při experimentech s automatickým překladem mezi slovanskými jazyky v systému Česílko.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes in detail the exploitation of chart-based methods and data structures in a simple system for the machine translation between related languages. The multigraphs used for the representation of ambiguous partial results in various stages of the processing and a shallow syntactic chart parser enable a modification of a simplistic and straightforward architecture developed ten years ago for MT experiments between Slavic languages in the system Česílko. The number of translation variants provided by the system inspired an addition of a stochastic ranker whose aim is to select the best translations according to a target language model.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje metodu kombinace dvou systémů strojového překladu k získání nového jazykového páru.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes a sophisticated method of combining two MT systems to obtain a new translation pair. Instead of a simple pipe, we use a complex data structure to pass the data from the first MT system to the second one. Evaluation results are reported for the language triplet Czech-Slovenian-Slovak.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá problémem skládání klauzí v českých souvětích z jednotlivých segmentů identifikovaných pomocí spojek a interpunkčních znamének. Množství segmentů je obvykle větší než
počet klauzí, proto je při syntaktické analýze souvětí nutné rozpoznat a spojit jednotlivé segmenty do klauzí a určit vzájemné postavení těchto klauzí. Článek navrhuje a předkládá k diskusi určitá pravidla, která vycházejí z české gramatiky a z lexikálně syntaktických vlastností českých slov. Tato pravidla se opírají o analýzu jevů, důležitých pro určení vzájemného vztahu českých klauzí, a o jejich frekvenci. Pravidla jsou vytvořena převážně na základě dat, získaných pro tento úkol z Pražského závislostního
korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article addresses the problem of an identification of individual sentence clauses from segments that are identified on a basis of subordinating conjunctions and punctuation marks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek přináší důmyslné prohledávání valenčních slovníků. Popisuje zobrazení slovníků v editoru (TrEd) se zabudovaným vyhledáváním (PML-TQ), ve kterém jsou dotazy kresleny uživatelem v grafickém režimu. Tyto universální metody článek demonstruje na českých slovnících VALLEX a PDT-VALLEX.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a sophisticated way to search valency lexicons. We provide a visualization of lexicons with such built-in searching that allows users to draw sophisticated queries in a graphical mode. We exploit the PML-TQ, a query language based on the tree editor TrEd. For demonstration purposes, we focus on VALLEX and PDT-VALLEX, two Czech valency lexicons of verbs. We propose a common lexicon data format supported by PML-TQ. This format offers easy viewing both lexicons, parallel searching and interlinking them. The proposed method is universal and can be used for other hierarchically structured lexicons.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme anotaci víceslovných výrazů v Pražském závislostním korpusu, k čemuž využíváme několikastupňové předanotace. Representaci víceslovných výrazů ve slovníku uchováváme ve formě podstromů tektogramatické roviny PDT a následující výskyty automaticky označujeme. Předkládáme způsob měření spolehlivosti takové anotace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe annotation of multiword expressions (MWEs) in the Prague dependency treebank, using several automatic pre-annotation steps. We use subtrees of the tectogrammatical tree structures of the Prague dependency treebank to store representations of the MWEs in the dictionary and pre-annotate following occurrences automatically. We also show a way to measure reliability of this type of annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato data obsahují anotaci PDT pomocí české verse WordNetu. Data jsou uložena ve formátu PML.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This dataset contains annotation of PDT using czech WordNet ontology. Data is stored in PML format.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato data doplňují anotaci víceslovných výrazů a víceslovných pojmenovaných entit k původním datům PDT 2.0. Anotace je ukládána "stand-off" ve stejném PML formátu jako data PDT 2.0. Jsou určena k použití spolu s PDT 2.0.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This dataset adds annotation of multiword expressions and multiword named entities to the original PDT 2.0 data. The annotation is stand-off, stored in the same PML format as the original PDT 2.0 data. It is to be used together with the PDT 2.0.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se pokouší ukázat , že se – oproti rozšířenému lingvistickému názoru – jako součást explicitních performativních formulí (EPF) uplatňuje nejen tvar 1. os. sg. nebo pl. indikativu prézenta nedokonavých sloves, ale běžně také tvar 1. os. sg. nebo pl. kondicionálu přítomného, a to jak sloves nedokonavých, tak dokonavých. Zaměřujeme se na kondicionál přítomný několika sloves konstatování v EPF s komunikační funkcí oznámení a na kondicionálové tvary několika sloves s výzvovými významy, které se uplatňují v EPF vyjadřujících některé typy výzvy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper focuses on the form of the first person present condition-al as a component of explicit performative formulae (EPF), which are considered to be a means directly expressing the communicative function of an utterance. We try to dem-onstrate that the performative usage of verbs is not limited to indicative forms of imper-fective verbs in Czech, as usually stated, but that also the form of the first person present conditional of imperfective as well as perfective verbs is used as an ordinary component of EPF. In Section 2, basic characteristics of EPF are briefly described. Two groups of verbs (verbs of assertion and verbs of appeal) are examined on the basis of language data from two corpora: from the Prague Dependency Treebank 2.0 and SYN2005 corpus (Sect. 3). In Section 4, the performative function of the conditional forms of these verbs is at-tested by means of Austin’s test and by some other criteria described in theoretical works. We further examine how the propositional content is expressed in analyzed utter-ances as well as what the difference between the examined EPF with the conditional verb form and the EPF with the indicative form is.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se věnuje gramatické kategorii čísla v češtině. Základní opozice singularity a plurality je obohacena rozdílem mezi významem prostého počtu a významem souborovým. Stručně uvádíme, jak je kategorie čísla reprezentována v několikarovinném anotačním schématu Pražského závislostního korpusu 2.0, poté probíráme možnost zavedení navrhovaného významového rozdílu do anotace. Článek uzavírá studie distribuce plurálových preferencí českých substantiv ve větším korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper deals with the grammatical category of number in Czech. The basic semantic opposition of singularity and plurality is proposed to be enriched with a (recently introduced) distinction between a simple quantitative meaning and a pair/group meaning. After presenting the current representation of the category of number in the multi-layered annotation scenario of the Prague Dependency Treebank 2.0, the introduction of the new distinction in the annotation is discussed. Finally, we study an empirical distribution of preferences of Czech nouns for plural forms in a larger corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme automatický dialogový systém Senior Companion. Vede dialog, jehož cílem není splnění předem daného společného úkolu, ale naopak nezávazný kratochvilný hovor s postarším uživatelem o jeho rodinných fotografiích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a real-time implementation of an automatic dialogue system called ‘Senior Companion’, which is not strictly task-oriented, but instead it is designed to ‘chat’ with elderly users about their family photographs. To a large extent, this task has lost the usual restriction of dialogue systems to a particular (narrow) domain, and thus the speech and natural language processing components had to be designed to cover a broad range of possible user and system utterances.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje jednotný dotazovací systém pro treebanky, schopný pracovat se závislostními i složkovými stromy v libovolném jazyce. Možnosti systému jsou předváděny na 11 různých treebankách. Dotazovací jazyk systému má mnoho rysů, které v ostatní systémech chybějí, ale zachovává si výkonnost. Článek popisuje konverzi různých datových formátů do formátu postaveného na XML, který systém používá. Následně jsou představeny některé lingvisticky zajímavé otázky, na které systém umí hledat odpovědi, např. prohlížení slovesných klauzí bez podmětu, generování gramatiky ze složkového treebanku, hledání neprojektivních hran v závislotních datech, nebo typologie jazyka podle SOV pořádku. Na závěr je provedeno měření výkonu různých implementací systému.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents a system for querying treebanks in a uniform way. The system is able to work with both dependency and constituency
based treebanks in any language. We demonstrate its abilities on 11 different treebanks. The query language used by the system
provides many features not available in other existing systems while still keeping the performance efﬁcient. The paper also describes
the conversion of ten treebanks into a common XML-based format used by the system, touching the question of standards and formats.
The paper then shows several examples of linguistically interesting questions that the system is able to answer, for example browsing
verbal clauses without subjects or extraposed relative clauses, generating the underlying grammar in a constituency treebank, searching
for non-projective edges in a dependency treebank, or word-order typology of a language based on the treebank. The performance of
several implementations of the system is also discussed by measuring the time requirements of some of the queries.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozpoznávání anafory je klíčové pro některé z úloh zpracování přirozeného jazyka (NLP), jako extrakce informací nebo dialogové systémy. Tato informace může byt hodnotná taky při strojovém překladu. Všechny předešlé práce týkající se rozpoznávání anafory v českém jazyce se soustředily především na zájmennou koreferenci. Díky nedávnemu projektu anotace širších anaforických vztahů v Pražském závislostním korpusu 2.0 však tato práce jde nad rámec zájmenné koreference. Pokouší se o rozpoznání koreference jmenných frází se specifickou referencí, generických jmenných frází a rozpoznání asociační anafory. Jsou v ní realizovány některé z nejúspěšnějších postupů v oblasti rozlišování anafor na základě strojového učení, konkrétně “ranking” a společné řešení úloh identifikace anaforu a nalezení antecedenta. Bylo vytvořeno množství rysů a analyzován jejích podíl na míře úspěšnosti. Nejlepší model koreference jmenných frází dosáhl F-hodnoty 39.4%.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Anaphora resolution is the key task for some of the Natural Language Processing (NLP) tasks like the information extraction or dialog systems. It can be also valuable in machine translation. All the previous works concerning the anaphora resolution in Czech language mostly focused on the pronoun coreference. Thanks to the recent project of the annotation of extended anaphoric relations in Prague Dependency Treebank 2.0 this work goes further. It attempts to resolve noun phrase coreference, identity-of-sense anaphora and part-whole bridging relations. It has adopted some of the state-of-the-art approaches in the area of machine learning approaches to anaphora resolution, particularly the ranking and the joint anaphor identification with the antecedent selection. It introduced a plenty of features and analyzed their contribution on the success rate. The best model of noun phrase coreference achieves the F-score of 39.4%.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozpoznávání anafory je klíčové pro některé z úloh zpracování přirozeného jazyka, jako extrakce informací nebo dialogové systémy. Tato informace může byt hodnotná taky při strojovém překladu. Všechny předešlé práce týkající se rozpoznávání anafory v českém jazyce se soustředily především na zájmennou koreferenci. Díky nedávnemu projektu anotace širších anaforických vztahů v Pražském závislostním korpusu 2.0 je však možné provádět výzkum v oblasti automatického rozpoznávání těchto vztahů. V mé přednášce budu prezentovat probíhající projekt rozpoznávání koreference jmenných frází a asociační anafory za použití řízeného strojového učení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Anaphora resolution is the key task for some of the Natural Language Processing tasks like the information extraction or dialog systems. It can be also valuable in machine translation. All the
previous works concerning the anaphora resolution in Czech language mostly focused on the pronoun coreference. Thanks to the recent project of the annotation of extended anaphoric relations in Prague Dependency Treebank 2.0 it is possible to carry out research in automatic resolution of these relations. In my talk I will present the
ongoing project of noun phrase coreference and bridging anaphora resolution using supervised machine learning.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V počítačové lingvistice je běžnou praxí používat výběrová omezení a hierarchie sémantických typů jako primární zdroj znalostí pro rozlišení významu slov (srov. Jurafsky a Martin 2000). Nejrozšířenějším přístupem je začít s ontologií typů (např. Wordnet, srov. Miller a Fellbaum 2007) a pokusit se využít jimi implikované konceptuální kategorie ke specifikaci kombinatorických omezení lexikálních jednotek.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>It is common practice in computational linguistics to attempt to use selectional constraints and semantic type hierarchies as primary knowledge resources to perform word sense disambiguation (cf. Jurafsky and Martin 2000). The most widely adopted methodology is to start from a given ontology of types (e.g. Wordnet, cf. Miller and Fellbaum 2007) and try to use its implied conceptual categories to specify the combinatorial constraints on lexical items. Semantic Typing information about selectional preferences is then used to guide the induction of senses for both nouns and verbs in texts. Practical results have shown, however, that there are a number of problems with such an approach. For instance, as corpus-driven pattern analysis shows (cf. Hanks et al. 2007), the paradigmatic sets of words that populate specific argument slots within the same verb sense do not map neatly onto conceptual categories, as they often include words belonging to different types. Also, the internal composition of these sets changes from verb to verb, so that no stable generalization seems possible as to which lexemes belong to which semantic type (cf. Hanks and Jezek 2008). In this paper, we claim that these are not accidental facts related to the contingencies of a given ontology, but rather the result of an attempt to map distributional language behaviour onto semantic type systems that are not sufficiently grounded in real corpus data. We report the efforts done within the CPA project (cf. Hanks 2009) to build an ontology which satisfies such requirements and explore its advantages in terms of empirical validity over more speculative ontologies.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek prezentuje pokus automatizace procesů k vytvoření systému strojového překladu založeného na pravidlech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article presents an attempt to automate
all data creation processes of a rule-based
shallow-transfer machine translation
system. The presented methods were
tested on two fully functional translation
systems Slovenian-Serbian and Slovenian-Macedonian.
An extensive range of evaluation
tests was performed to assess the applicability
of the methods.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku se věnujeme srovnání valenčních vlastností českých a anglických sloves. Materiál pro výzkum této oblasti pochází ze dvou paralelních korpusů: korpusu Prague Czech-English Dependency Treebank a korpusu Intercorp. Nejprve provedeme srovnání obou korpusů a vysvětlíme, z jakých hledisek jsou pro zkoumání valenčních vlastností sloves výhodné. Dále nastíníme několik oblastí, v nichž se vyskytují asymetrie valenčních vlastností překladových ekvivalentů. V případové studii se zaměříme zejména na slovesa vyžadující doplnění tzv. směrovými doplněními a blíže ukážeme konkrétní typy asymetrií na slovesech crawl, descend a travel.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we are concerned with the comparison of valency characteristics of Czech and English verbs. As a material we use two parallel corpora: PCEDT and Intercorp. First we compare the two corpora to one another with respect to their suitability for research in valency. Then we propose several areas of crosslingustic asymmetries in verbal valency. Finally we present a case study of three pairs of Czech and English motion verbs to exemplify the asymmetries mentioned.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme výsledky prvního vyhodnocení paralelních ručních anotací dirkurzu v Pražském závislostním korpusu. Uvádíme přehled vlastního anotačního procesu, popisujeme měření mezianotátorské shody a hlavně klasifikujeme a analyzujeme nejběžnější druhy anotátorské neshody a navrhujeme řešení pro další fázi anotací.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present results of the first evaluation of parallel manual annotations of discourse in the Prague Dependency Treebank. We give an overview of the process of the annotation itself, describe the inter-annotator agreement measurement, and, most importantly, we classify and analyze the most common types of annotators’ disagreement and propose solutions for the next phase of the annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Mezi nejúspì¹nìj¹í metody strojového pøekladu
se v souèasné dobì øadí relativnì velmi jednoduchý frázový
statistický pøeklad, který se opírá v podstatì pouze o posloupnosti slov bez ohledu na lingvistické rozbory. Z více
dùvodù kvalita strojového pøekladu stále není uspokojivá
a lze se domnívat, ¾e èást problémù by bylo mo¾né odstranit explicitním zapojením lingvistické anotace do frázového
pøekladu. Pro èe¹tinu a angliètinu jsou navíc rozsáhlá bohatì anotovaná paralelní data k dispozici. Cílem této práce
je proto pøipravit nástroj usnadòující experimenty s bohatou lingvistickou anotací v relativnì jednoduchém prostøedí
frázových statistických pøekladù. Popisujeme formát dat i
mo¾nosti implementovaného nástroje a souèasnì uvádíme
výsledky prvních experimentù. ©iroký prostor mo¾ností, jak
lingvistická data do modelu zapojit, je otevøen pro dal¹í výzkum.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a simple tool for the inclusion of rich linguistic annotation in phrase-based machine translation systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška pojednává o problémech anotace žákovského korpusu češtiny, tzn. čestiny jak ji používají nerodilí mluvčí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk describes a challenges of annotation of learners corpus of Czech, i.e. Czech as used by non-native speakers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek podává přehled o hlavních tématech, která zajímají autory jednojazyčných slovníků pro rodilé mluvčí. Diskutuje vztah mezi lexikální databází a jednojazyčným slovníkem, roli korpusových příkladů, historické lexikografické principy v porovnání se synchronními, nestabilitu významu slova, potřebu plného pokrytí slovní zásoby, prinicipy psaní definic a roli slovníků ve společnosti a návod ke správnému používání slov jako jeden z hlavních úkolů jednojazyčného slovníku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article gives a survey of the main issues confronting the compilers of monolingual
dictionaries in the age of the Internet. Among others, it discusses the relationship between a
lexical database and a monolingual dictionary, the role of corpus evidence, historical principles in
lexicography vs. synchronic principles, the instability of word meaning, the need for full vocabulary
coverage, principles of definition writing, the role of dictionaries in society, and the need for
dictionaries to give guidance on matters of disputed word usage. It concludes with some questions
about the future of dictionary publishing.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Elektronické slovníky budoucnosti budou velmi žádané — pro komputační, pedagogické a jiné aplikace — pokud bude možné je použít jako systematické zobrazení slovních významů na slovní výskyty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Electronic dictionaries of the future will be much in demand—for computational, pedagogical, and other applications—if they can be used as resources for mapping word meaning systematically onto word use. Research in computational linguistics and artificial intelligence over the past twenty years, despite many declarations of success, has shown that existing dictionaries, designed for human users, coupled with existing linguistic theory of a top-down, speculative nature, have failed to be suitable for this goal. Nor are results using hierarchical ontologies such as WordNet any better. Such resources are very plausible for human users, but they fail to meet the challenges of mapping meaning systematically onto words in use in ordinary text.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek je zvanou přednáškou na téma vztahu mezi slovy a významem. Autor zde představuje svoji metodu manuální analýzy jazykových pravidelností v korpusu. Popisuje zde, jakým způsobem analyzuje anglická sloevsa, aby získal přehled o významových změnách, které nastávají s výběrem různých slovesných doplnění, a nabádá lingvisty, aby při dokládání svých teorií o významu a používání slov nepoužívali introspekci, nýbrž korpus.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The title of the present paper, “How people use words to making meanings”, carries with it a number of theoretical assumptions, some of which are more controversial than others. It is, I suppose, uncontroversial that language, used fully and meaningfully, is a uniquely human phenomenon, and that the communications and thought processes of other animals, even chimpanzees, are different in kind from human language. It is people who use words. Language does not exist in a vacuum: it exists in the brains and the interactions of humans. Humans are social animals, and language is the instrument of their sociability, as well as the vehicle of their thought processes.
Equally uncontroversial is the assumption that language is composed of words, put together into some sort of structure, which has persistent attributes. By a “persistent attribute” I mean features like the rank scale of grammar – discourse (document or conversation), paragraph, sentence, clause, phrase or group, word, morpheme). Such structures are not the exclusive property of language, for they also govern other forms of human behaviour such as games and music. Syntax is not an exclusively linguistic phenomenon, but language is the most salient, the most prototypical example of rule-governed behaviour.
At the heart of this view of language lie words. Words are central to the activity of making meanings. Each content word in a language, like a great international airport, represents a place where things come together, meet, and go off in different directions—a huge set of interchange points with various sorts of connections: internal connections to beliefs in the mind of the speaker, interpersonal connections to the beliefs of other users of the same language, intralingual connections to other words in the same language, and putative connections to objects in the world.
Associated with this account of words is the assumption that each word has a meaning. This, however, is an assumption that I will question in the course of this paper.
Another assumption implicit in the title is that meanings are events—events that are for the most part only momentary, transitory, evanescent. A speaker makes a meaning—billions of people do it every minute of every day—but the next moment the meaning is gone: lost for ever, unless some assiduous Boswell, parliamentary reporter, law clerk, or linguist happens to have transcribed the utterance of the speaker and thus preserved it for posterity. Most meaningful spoken interactions are as evanescent as the mutual sniffing of dogs—or the passing of passengers through an airport. The notion that meanings are events is, nevertheless, one that I propose to defend, against the alternative
1
view that meanings are static abstract entities. It is a commonplace among philosophers of language that speaker’s meaning and hearer’s meaning are not necessarily identical, and the. As Wright (1976) put it:
“Speaker and hearer have only their mutual pragmatic satisfaction to rely on that they mean the same thing.”
The philosopher H. P. Grice (1957) regarded meanings as events, involving a particular kind of interaction between speaker and hearer—an interchange of beliefs. The core of Grice’s account of meaning is summarized by Bennett (1976) thus: If [an utterer] U does x, thereby meaning that P, he does it intending i. that some audience A should come to believe P
ii. that A should be aware of intention (i), and
iii. that the awareness mentioned in (ii) should be part of A’s reason for believing that P.
This account of meaning applies equally well to the interaction between writer and reader, with a displacement in time, which may be very large or quite small1.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku autor popisuje historický vývoj slovníků a hodnotí přínos knihtisku k rozvoji lexikografie, který porovnává s dopadem, který může mít na současnou lexikografii masívní rozšíření elektronického ukládání dat. Autor vysvětluje, jak došlo k nahrazení mnohajazyčných glosářů dvojjazyčnými slovníky. Opírá se o příklady těchto slovníků: Robert Cawdrey: Table Alphabeticall, Robert Estienne: Dictionarum, seu Thesaurus 
Linguae Latinae, Henri Estienne: Thesaurus Linguae Graecae.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Historians of lexicography in the English-speaking world have implied that Robert
Cawdrey's Table Alphabeticall (1604) is the first English dictionary. Landau (1984,
2001) makes this claim, adding that it is “the least inspiring of all seminal works”. In
this paper, I agree that the Table Alphabeticall is uninspiring, but I deny that it is a
seminal work. Landau overlooks the rich 16th-century tradition of Renaissance and
Humanist lexicography in Europe, in particular the Dictionarum, seu Thesaurus
Linguae Latinae of Robert Estienne (1531) and the Thesaurus Linguae Graecae of his
son Henri Estienne (1572). These seminal works are astonishing achievements—
breathtaking innovations—in terms of both scholarship and technology. They set
standards for subsequent European lexicography. Two technological innovations
made these great dictionaries possible: the invention of printing by Gutenberg in
Strasbourg in about 1440 and the typography of Nicolas Jenson in Venice in 1462.
These technological developments and the lexicographical achievements that were
made possible by them contributed, in the first place, to the Renaissance programme
of preserving the classical heritage of ancient Greece and Rome and, in the second
place, to the role of dictionaries in spreading Renaissance culture and Humanism
across Europe. The paper goes on to briefly outline the emergence of bilingual
lexicography, replacing the polyglot lexicography that was standard in the 16th
century. A comparison is made between the influence of printing technology on 16th
century lexicography and the potential influence of computer technology on 21st
century lexicography</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato obšírná a velice precizní recenze se zabývá knihou A. Deignan o analýze metafory pomocí metod korpusové lingvistiky. Autor recenze knihy využívá jako odrazového můstku k obecnějším úvahám o struktuře a využití metafory v jazyce. Poukazuje na nutnost lingvistické analýzy kognitivních jevů, které se manifestují v jazyce. Autor recenze se sám problematikou metafory zabýval již ve svých dřívejších původních dílech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>With the benefit of hindsight, it is now possible to see that one of the most important themes in the study of language to emerge in the 20th century was developed, not by linguists, but primarily by philosophers of language such as Wittgenstein and Grice and anthropologists such as Malinowski and Rosch.  This theme involves, among other things, rejection of sharply defined category boundaries and adoption instead of systems of categories built by analogy around prototypes. Central and typical examples of linguistic categories are usually easy to identify, but boundaries between categories are fuzzy grey areas on a cline, rather than sharp divisions.  Metaphor is the most prototypical example of linguistic analogy, so a corpus-based study of metaphor will be a theme of central interest.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>This paper explores two aspects of word use and word meaning in terms of Sinclair's (1991, 1998)
distinction between the open-choice principle (or terminological tendency) and the idiom principle
(or phraseological tendency). Technical terms such as strobilation are rare, highly domain-specific,
and of little phraseological interest, although the texts in which such word occur do tend to contain
interesting clusters of domain-specific terminology. At the other extreme, it is impossible to know the
meaning of ordinary common words such as the verb blow without knowing the phraseological context
in which the word is used.
Many words have both a terminological tendency and a phraseological tendency. In some cases the two
tendencies are in harmony; in other cases there is tension between them. The relationship between these
two tendencies is investigated, using examples from the British National Corpus.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Tento článek se zabývá dvěma aspekty užívání slov a významu slov ve smyslu Sinclairovy distinkce mezi tzv. principem volného výběru a idiomatického výběru. Technické pojmy jako např. strobilace jsou vzácné, vázané na určitou doménu a nejsou zajímavé po stránce frazeologické, i když texty, v nichž se tato slova vyskytují, mají tendenci obsahovat zajímavé shluky doménově specifické terminologie. Na druhou stranu, význam běžně užívaných slov nelze popsat, není-li k dispozici kontext.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje kombinační systém pro strojový překlad univerzity DCU použitý  na workshopu strojového překladu Joint Fifth Workshop on Statistical Machine Translation and Metrics in ACL-2010.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the augmented threepass system combination framework of the Dublin City University (DCU) MT group for the WMT 2010 system combination task. The basic three-pass framework includes building individual confusion networks (CNs), a super network, and a modiﬁed Minimum Bayes-risk (mConMBR) decoder. The augmented parts for WMT2010 tasks include 1) a rescoring component which is used to re-rank the N-best lists generated from the individual
CNs and the super network, 2) a new hypothesis alignment metric – TERp – that is used to carry out English-targeted hypothesis alignment, and 3) more different backbone-based CNs which are employed to increase the diversity of the mConMBR decoding phase. We took part in the combination tasks of Englishto-Czech and French-to-English. Experimental results show that our proposed combination framework achieved 2.17 absolute points (13.36 relative points) and
1.52 absolute points (5.37 relative points) in terms of BLEU score on English-toCzech and French-to-English tasks respectively than the best single system. We also achieved better performance on human evaluation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato kniha se dotýká řady témat: typologie, morfologie, korpusové lingvistiky, kontrastivní lingvistiky, lingvistické anotace, počítačové lingvistiky a zpracování přirozeného jazyka (NLP). Budou z ní mít prospěch výzkumníci a studenti se zajmem o tyto oblasti, stejně i o mezijazykové studie.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>While supervised corpus-based methods are highly accurate for different NLP tasks, including morphological tagging, they are difficult to port to other languages because they require resources that are expensive to create. As a result, many languages have no realistic prospect for morpho-syntactic annotation in the foreseeable future. The method presented in this book aims to overcome this problem by significantly limiting the necessary data and instead extrapolating the relevant information from another, related language. The approach has been tested on Catalan, Portuguese, and Russian. Although these languages are only relatively resource-poor, the same method can be in principle applied to any inflected language, as long as there is an annotated corpus of a related language available. Time needed for adjusting the system to a new language constitutes a fraction of the time needed for systems with extensive, manually created resources: days instead of years.

This book touches upon a number of topics: typology, morphology, corpus linguistics, contrastive linguistics, linguistic annotation, computational linguistics and Natural Language Processing (NLP). Researchers and students who are interested in these scientific areas as well as in cross-lingual studies and applications will greatly benefit from this work. Scholars and practitioners in computer science and linguistics are the prospective readers of this book.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zdroje pro morfologickou analýzu a značkování ruštiny, běloruštiny a staročeštiny. Vzory, frekventovaná slova a další obecná morfologicka, fonologická a grafémická pravidla.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Resources for morphological analysis and tagging of Russian, Belorussian and Old Czech. Paradigms, frequent forms and other morphological, phonological and graphemic rules.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku popisujeme výrobu česko-ruského paralelního závislostního treebanku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we describe initial steps in constructing Czech-Russian dependency treebank and discuss the perspectives of its development. Following the experience of Czech-English Treebank
we have taken syntactically annotated "gold standard" text for one language (Russian) and run an automatic annotation on the respective parallel text for the other language (Czech). Our treebank includes also automatic word-alignment.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku popisujeme rozdíly ve valenci mezi češtinou a ruštinou. Výsledkem je malý rozdílový valenční slovník, který bude využít ve strojovém překladu Česílko.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper analyzes the differences in ver-
bal valency frames between two related
Slavic languages, Czech and Russian, with
regard to their role in a machine translation
system. The valency differences are a fre-
quent source of translation errors. The re-
sults presented in the paper show that the
number of substantially different valency
frames is relatively low and that a bilingual
valency dictionary containing only the dif-
fering valency frames can be used in an
MT system in order to achieve a high pre-
cision of the translation of verbal valency.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>English to Czech machine translation as it is implemented in the TectoMT system consists of three phases:
 analysis, transfer and synthesis.
The system uses tectogrammatical (deep-syntactic dependency) trees as the transfer medium.
Each phase is divided into so-called blocks, which are processing units that solve linguistically interpretable tasks
 (e.g., statistical part-of-speech tagging or rule-based placement of clitics).

This paper shortly introduces linguistic layers of language description which are used for the translation
 and describes basic concepts of the TectoMT framework. 
The translation results are evaluated using both automatic metric BLEU and human judgments from the WMT 2010 evaluation.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Strojový překlad z angličtiny do češtiny implementovaný v systému TectoMT sestává ze tří fází: analýzy, transferu a syntézy. Pro transfer se využívají tektogramatické stromy. Každá fáze je rozdělena do tzv. bloků, což jsou jednotky kódu, které řeší jednotlivé lingvisticky interpretovatelné úlohy (např. statistický tagging či přemístění klitik podle ručně psaných pravidel).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Strojový překlad z angličtiny do češtiny implementovaný v systému TectoMT sestává ze tří fází: analýzy, transferu a syntézy. Transfer se provádí na tektogramatické rovině upravené pro účely překladu. Každá fáze je rozčleněna do bloků, které řeší konkrétní lingvisticky interpretovatelný úkol (např. přiřazení morfologických značek statistickým taggerem, či přesun klitik podle ručně psaných pravidel). Systém TectoMT je navržen modulárně - bloky je možné zaměňovat za alternativní implementace a také využít i pro jiné aplikace než strojový překlad.
Přednáška představí základní průběh celého překladu a zaměří se na popis vylepšení, která byla provedena v posledním roce, zejména:
(a) využití tektogramatického jazykového modelu a Hidden Markov Tree Models,
(b) nový systém slovníků natrénovaných na paralelním korpusu CzEng pomocí metody Maximum Entropy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>English-to-Czech machine translation implemented in TectoMT system consists of three phases: analysis, transfer, and synthesis. Transfer is performed on the tectogrammatical layer which is modified for MT purposes. Each phase is divided into so-called blocks which solve particular linguistically interpretable tasks (e.g. tagging with statistic tagger or clitic shifting according to hand-written rules). TectoMT system is designed in a modular way - blocks can be substituted with alternative implementations. The talk presents basic steps of the whole translation and focuses on improvements implemented in the last year, especially:
(a) tectogrammatical LM and Hidden Markov Tree Models,
(b) new translation dictionaries trained on parallel corpus CzEng using Maximum Entropy.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Treex je víceúčelový open-source framework pro počítačové zpracovávání přirozeného jazyka. Je implementován v programovacím jazyku Perl. Umožňuje rychlý a efektivní vývoj aplikací s využitím široké škály softwarových integrovaných modulů, např. nástroje pro větnou segmentaci, tokenizaci, morfologickou analýzu, tagging, syntaktickou analýzu (parsing na analytickou i tektogramatickou rovinu), rozpoznávání pojmenovaných entit, strojový překlad apod.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Treex is a multi-purpose open-source natural language processing (NLP) framework implemented in Perl programming language. It allows for fast and efficient development of NLP applications by exploiting a wide range of software modules already integrated in Treex, such as tools for sentence segmentation, tokenization, morphological analysis, POS tagging, shallow and deep syntax parsing, named entity recognition, anaphora resolution, tree-to-tree translation, natural language generation, word-level alignment of parallel corpora, and other tasks. One of the most complex applications of Treex is the English-Czech machine translation system TectoMT. Several modules are available also for other languages (German, Russian, Arabic). Where possible, modules are implemented in a language-independent way, so they can be reused in many applications.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jazykové modely jsou klíčovou součástí mnoha aplikací jako rozpoznávání mluvené řeči či strojového překladu. Jazykové modely počítají pravděpodobnost řetězce slov jako součin P(w_i|h_i), kde h_i je kontext (historie) slova w_i. Většina jazykových modelů používá jako kontext předchozí slova. Tento článek popisuje dva alternativní přístupy: post-ngramové jazykové modely (které používají jako kontext následující slova) a závislostní jazykové modely (které využívají závislostní strukturu věty). závislostní jazykové modely. V porovnání s baseline trigramovým jazykovým modelem dosáhly oba navrhované přístupy signifikantně nižší perplexity pro všech sedm testovaných jazyků (arabština, katalánština, čeština, angličtina, maďarština, italština, turečtina).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Language models (LMs) are essential components of many applications such as speech recognition or machine translation. LMs factorize the probability of a string of words into a product of P(w_i|h_i), where h_i is the context (history) of word w_i. Most LMs use previous words as the context. The paper presents two alternative approaches: post-ngram LMs (which use following words as context) and dependency LMs (which exploit dependency structure of a sentence and can use e.g. the governing word as context). Dependency LMs could be useful whenever a topology of a dependency tree is available, but its lexical labels are unknown, e.g. in tree-to-tree machine translation. In comparison with baseline interpolated trigram LM both of the approaches achieve significantly lower perplexity for all seven tested languages (Arabic, Catalan, Czech, English, Hungarian, Italian, Turkish).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje víceúčelový open-source NLP framework TectoMT, který umožňuje rychlý a efektivní vývoj NLP aplikací. Využívá široké spektrum softwarových modulů, které jsou již integrovány do TectoMT, např. nástroje pro segmentaci textu na věty, tokenizaci, morfologickou analýzu a disambiguaci (tagging), parsing (povrchový i hloubkový), rozpoznávání pojmenovaných entit, rozpoznávání anafory, strojový překlad stromových struktur, generování vět z hloubkových struktur, slovní zarovnávání paralelních korpusů atd.
Jednou z nejkomplexnějších aplikací TectoMT je anglicko-český systém strojového překladu s transferem přes tektogramatickou rovinu. Moduly jsou dostupné i pro další jazyky (němčina, ruština, arabština,...).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the present paper we describe TectoMT, a multi-purpose open-source NLP framework. It allows for fast and efficient development of NLP applications
 by exploiting a wide range of software modules already integrated in TectoMT, such as tools for
 sentence segmentation, tokenization, morphological analysis, POS tagging, shallow and deep syntax parsing, named entity recognition, anaphora resolution, tree-to-tree translation, natural language generation, word-level alignment of parallel corpora, and other tasks. One of the most complex applications of TectoMT is the English-Czech machine translation system with transfer on deep syntactic (tectogrammatical) layer. Several modules are available also for other languages (German, Russian, Arabic). Where possible, modules are implemented in a language-independent way, so they can be reused in many applications.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Compost Dutch je nástroj pro morfologické značkování holandštiny založený na taggeru Morče. Je trénován na holandské části korpusu CGN. Výsledný tagger svojí úspěšností překonal dosavadní publikované taggery. Verze 2.0 přináší navíc integrovanou morfologickou analýzu, což usnadňuje použití. Aplikace je spustitelná pod Linuxem a Windows.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Compost Dutch is a tool for POS tagging of Dutch based on Morče tagger. It is trained on Dutch part of CGN corpus. The resulting tagger overcome other previously published taggers. Version 2.0 brings integrated morphological analysis, which makes use of the software easier. Application is executable under Windows and Linux.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Compost English je nástroj pro morfologické značkování angličtiny založený na taggeru Morče. Je trénován na korpusu WSJ. Výsledný tagger svojí úspěšností dosáhl výsledků doposud publikovaných taggerů. Verze 2.0 přináší navíc integrovanou morfologickou analýzu, což usnadňuje použití. Aplikace je spustitelná pod Linuxem a Windows.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Compost English is a tool for POS tagging of English based on Morče tagger. It is trained on WSJ corpus. The resulting tagger obtained results similar to other previously published taggers. Version 2.0 brings integrated morphological analysis, which makes use of the software easier. Application is executable under Windows and Linux.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Compost Icelandic je nástroj pro morfologické značkování islandštiny založený na taggeru Morče. Je trénován na korpusu IFD. Výsledný tagger svojí úspěšností výrazně překonal dosavadní publikované taggery. Pro použití je však nutné použít předzpracování morfologickou analýzou. Aplikace je spustitelná pod Linuxem a Windows.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Compost Icelandic is a tool for POS tagging of Icelandic based on Morče tagger. It is trained on IFD corpus. The resulting tagger highly overcome other previously published taggers. However, it is necessary to preprocess data with morphological analysis. Application is executable under Windows and Linux.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Compost Swedish je nástroj pro morfologické značkování švédštiny založený na taggeru Morče. Je trénován korpusu SUC 2.0. Výsledný tagger se svojí úspěšností blíží dosavadním publikovaným taggerům. Jeho výhodou však je snadné použití. Aplikace je spustitelná pod Linuxem a Windows.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Compost Dutch is a tool for POS tagging of Swedish based on Morče tagger. It is trained on corpus SUC 2.0. Results of the tagger are close to previously published taggers. Its advantage is in simple usage. Application is executable under Windows and Linux.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje značkovací experimenty se semi-supervised trénováním coby rozšířením algoritmu průměrovaného perceptronu (Collins02).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes POS tagging experiments with semi-supervised training as an extension to the (supervised) averaged perceptron algorithm, first introduced for this task by Collins02. Experiments with an iterative training on  standard-sized supervised (manually annotated) dataset (10^6 tokens) combined with a relatively modest (in the order of 10^8 tokens) unsupervised (plain) data in a bagging-like fashion showed significant improvement of the POS classification task on typologically different languages, yielding better than state-of-the-art results for English and Czech (4.12 % and 4.86 % relative error reduction, respectively; absolute accuracies being 97.44 % and 95.89 %).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zaměřuje na vytváření paralelního česko-anglického korpusu pro
účely strojového překladu vyhledáváním a stahováním paralelních textů z~Internetu. Navrhujeme a vyhodnocujeme několik vlastních metod pro nalezení
kandidátských webů, identifikaci jazyka stránek a především párování získaných
dokumentů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We examine methods for collecting parallel Czech-English corpora from the web. We propose and evaluate automatic methods for finding source web sites, language identification and most importantly the document alignment of obtained pages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek je věnován problematice členské negace a způsobům jejího vyjadřování v datech Českého národního korpusu (především SYN2005) a Pražského závislostního korpusu (verze 2.0). Za členskou negaci byla dosud ve většině českých příruček považována taková negace, která neoperuje na predikátu. Autorka na základě analýzy dat obou korpusů a studia české i zahraniční odborné literatury týkající se dané problematiky navrhne nové, primárně sémantické vymezení členské negace v češtině a prozkoumá možnosti zachycování tohoto jevu ve strukturách tektogramatické roviny PDT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This contribution is focused on constituent negation and ways of its expressing in contemporary Czech, or more precisely in Czech National Corpus (SYN2005) and Prague Dependency Treebank (version 2.0). In most of the Czech linguistic handbooks, constituent negation was treated as such a negation that is not a part of verb. The author offers a new primarily semantic definition of constituent negation in Czech, based on data research and study of relevant literature concerning syntactic negation. Further, the author explores the possibility to express the findings on the tectogrammatical level of PDT.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku se zabýváme rozdíly mezi klasickým zarovnáváním slov a zarovnání hloubkových syntaktických struktur. Hloubkovými strukturami rozumíme závislostní stromy obsahující pouze autosémantická slova. Ostatní (funkční) slova jako předložky nebo členy jsou schovány.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we describe differences between a classical word alignment on the surface (word-layer alignment) and an alignment of deep syntactic sentence representations (tectogrammatical alignment). The deep structures we use are dependency trees containing content (autosemantic) words as their nodes. Most of other functional words, such as prepositions, articles, and auxiliary verbs are hidden. We introduce an algorithm which aligns such trees using perceptron-based scoring function. For evaluation purposes, a set of parallel sentences was manually aligned. We show that using statistical word alignment (GIZA++) can improve the tectogrammatical alignment. Surprisingly, we also show that the tectogrammatical alignment can be then used to significantly improve the original
word alignment.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pokus o zlepšení kvality frázového překladu (nástroj Moses) tím že word-alignment, na kterém se překladač učí, vylepšíme použitím zarovnání tektogramatických stromů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we describe an experiment whose goal is to improve the quality of machine translation. Phrase-based machine translation, which is the state-of-the-art in the field of statistical machine translation, learns its phrase
tables from large parallel corpora, which have to be aligned on the word level. The most common word-alignment tool is GIZA++. It is very universal and language independent. In this text, we introduce a different approach – the tectogrammatical alignment. It works on content (autosemantic) words only, but on these words
it widely outperforms GIZA++. The GIZA++ word-alignment can be therefore improved using tectogrammatical alignment and if we use this improved alignment for training phrase-based automatic translators, the translation quality also slightly increases.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Česko-anglický ručně anotovaný paralelní korpus byl vytvořen za účelem testování kvalit česko-anglických automatických zarovnávačů na slova. Obsahuje 2500 paralelních vět, každý pár je anotován nezávisle dvěma anotátory.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Czech-English Manually Aligned Parallel Corpus was developed for testing qualities of Czech-English automatic aligners. It consists of 2500 parallel sentences (books, law, newspapers), each sentence is anotated independently by two annotators.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku popíšeme práci na transformaci ruského treebanku SynTagRus do pražského stylu PDT.
Zatímco v PDT tektogramatická anotace existuje, V treebanku SynTagRus žádná jí podobná není.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we report a work in progress on transforming syntactic structures from the SynTagRus corpus into tectogrammatical trees in the Prague Dependency Treebank (PDT) style. SynTagRus (Russian) and PDT (Czech) are both
dependency treebanks sharing lots of common features and facing similar linguistic challenges
due to the close relatedness of the two languages. While in PDT the tectogrammatical representation exists, sentences in SynTagRus are annotated on syntactic level only.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se věnuje teoretickým i praktickým problémům, které vyvstaly při přípravě anotace mezipropozičních vztahů v PDT 2.0. Na rovině teoretické se článek věnuje problematice jendotlivých rovin v PDT 2.0 a otázce, zda můžeme mezipropoziční vrstvu jazyka považovat za další rovinu ve smyslu koncepce rovin ve FGP. Na rovině praktické se pak věnuje především případům problematického rozsahu diskurzních argumentů (např. nepřekrývání t-stromu a propozice, přímá řeč, parcelace propozice, vsuvky ad.).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper focuses on the problems and possibilities of the annotation of interpropositional discourse relations in a dependency corpus of a natural language (Czech) such as PDT 2.0. Apart from the general introduction to the existing PDT 2.0 material and the relevant terminology (proposition, connective, argument, discourse layer etc.) it presents the main areas of problems arising during the IDR annotation that are the consequence of the specific form of the existing (especially tectogrammatical) PDT 2.0 annotation. As the degree of IDR is not an independent language layer but brings new information about the integration of tectogrammatical tree or its parts into its language context, the main emphasis is on recording the IDR on the basis of t-trees annotation. The main areas of problems do therefore touch the relation of a proposition and a tectogrammatical tree, that do not always have to correspond (the point is especially to mark out the argument, discourse parceling, multisentence direct speech and parenthesis) and the possibility of IDR classification. We can thus divide the IDR into syntactic discourse relations and non-syntactic discourse relations that comprise relations expressing the discourse producer`s motivation as well as relations expressing the receiver`s point of view.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje problém rekonstrukce mluvené řeči a představuje seznam nejčastějších chyb mluvčích. Ukazujeme, že pouhé smazaní chyb je nepostačující a je potřeba udělat více komplexní operace nad řetězmi. Dále popisujeme nejlepší metody, které se snažíproblém rekonstrukce řešit a představujeme nové nápady, které budeme v budoucnu implementovat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the speech reconstruction problem and lists the
most frequent speaker errors. We show that deletion of these errors is not sufficient
and more complex string operation should be done. We overview state-of-the-art
methods which try to solve this problem and present new ideas which we would
like implement in the near future.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku jsou srovnány dvě metody strojového učení pro určování anafory: konvenční systém založený na klasifikaci a nový systém založený na uspořádání kandidátů pomocí perceptronu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we compare two Machine Learning approaches to the task of pronominal anaphora
resolution: a conventional classification system based on C5.0 decision trees, and a novel perceptron-based ranker. We use coreference links annotated in the Prague Dependency Treebank~2.0 for training and evaluation purposes. The perceptron system achieves f-score 79.43% on recognizing coreference of personal and possessive pronouns, which clearly outperforms the classifier and which is the best result reported on this data set so far.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá konstrukcemi s rozpadem tématu a dikta v češtině. Tyto konstrukce jsou realizovány některými třídami sloves mluvení a sloves vyjadřujících duševní procesy. Typickým rysem těchto konstrukcí je realizace participantu sdělení ve dvou valenčních pozicích. V předloženém příspěvku se zabýváme zejména syntaktickými vlastnostmi těchto konstrukcí, při jejich popisu však bereme v úvahu i morfologické a sémantické rysy. Dále si tyto konstrukce zaslouží pozornost zejména z ohledem na koreferenci. Na základě korpusových vyhledávek dokazujeme, že téma představuje antecendent, ke kterému odkazuje/í koreferující výraz/y, který je součástí dikta. U těchto konstrukcí zjišťujeme dva druhy koreference: (i) textovou a (ii) 'obsahovou'. V případě 'obsahové' koreference chybí explicitní anaforická relace a vztah mezi koreferujícím a koreferovaným výrazem/y je založen na nejrůznějších sémantických vztazích. Z těchto důvodů nepovažujeme konstrukce bez rozpadu tématu a dikta a s rozpadem za synonymní.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper deals with Czech constructions with the splitting of the theme and dictum. They are realized by some classes of verbs of communication and verbs that express mental actions. The characteristic feature of these constructions lies in the fact that one of their participants − the participant message − occupies two valency slots. In the present contribution, syntactic properties, especially valency characteristics, of these constructions are of our main interest. However, both morphological and semantic features have to be taken into account within their description. Finally, these constructions deserve close attention with regard to coreference relations. 
On the basis of the corpus evidence, we demonstrate that the theme represents an antecendent to which the coreferring expression(s) that is a part of the dictum refers. In principle, two kinds of coreference occur in the examined constructions: (i) textual and (ii) 'content' coreference. In the latter case, an explicit anaphoric relation is missing, and the relation between the coreferred and the coreferring element(s) is based on various semantic relations. For these reasons, we assume that the constructions with the splitting of the theme and dictum are not synonymous with the constructions without the splitting, due to divergent truth conditions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku se zabýváme změnami ve valenční struktuře českých sloves z hlediska lexikografické. Zaměřujeme se pouze na syntaktické konstrukce, které se vztahují v zásadě stejné (zobecněné) situaci. Změny ve struktuře valence jsou chápány jako různé mapování mezi jednotlivými účastníky zevšeobecněné situace a valenčními sloty, včetně jejich morfematický realizace. Rozlišujeme dva typy změn ve valenční struktuře, takzvané gramatické diateze a sémantické diateze.

Představujeme základní typologii možných změn ve struktuře valence a navrhujeme způsob reprezentace těchto změn ve valenčním slovníku českých sloves VALLEX.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we deal with changes in valency structure of Czech verbs from a lexicographic point of view. We focus only on syntactic constructions that are related in principle to the same (generalized) situation. Changes in valency structure are understood as different mappings between individual participants of a generalized situation and valency slots, including their morphemic realization. We
distinguish two types of changes in valency structure, so-called grammatical diatheses
and semantic diatheses.

We introduce a basic typology of potential changes in valency structure and we propose a method of the representation of these changes in the valency lexicon of Czech verbs VALLEX.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme projekt zaměřený na obohacení valenčního slovníku českých sloves o chybějící sémantické informace - sémantické třídy a sémantické role. Pro tento účel jsme využili data z projektu FrameNet. Navrhli jsme způsob, jak překonat problém s jemnějším rozlišením významu ve FrameNet. Tato metoda je založena na relaci 'Inheritance'. Takto bylo zařazeno 6 skupin sloves do ucelenější sémantických tříd a jejich valenčním doplněním byly přiděleny sémantické role. Plánujeme studovat další skupiny sloves, zejména v souvislosti se zvýšením pokrytí sémantické informace ve FrameNetu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce the project aimed at enhancing the valency lexicon with missing semantic information – semantic classes and semantic roles. For this purpose, we made use of FrameNet data. We proposed a method of overcoming the problem with finer granularity of word sense disambiguation made in FrameNet. This method is based on the relation of 'Inheritance'. As a result, the 6 'supergroups' of verbs were classified into more coherent semantic classes and semantic roles were assigned to their valency complementations. As to future work, we intend to experiment with other groups of verbs and to increase coverage of semantic information following the progress made in FrameNet.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek pojednává o textových konektorech a principu jejich anotace plánované v Pražském závislostním korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents a preliminary study on discourse connectives (DC) in Czech. Aiming to build a computerized language corpus capturing discourse relations in Czech, we base our observations on current foreign projects with the same purpose. In this study, first, the different methods of linguistic analysis of the discourse structure and discourse connectives are described, next, the nature and properties of the group of DCs are analyzed and, finally, the procedure of the annotation of discourse connectives in Prague is presented.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek referuje o projektu anotace textových vztahů v Pražském závislostním korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper reports on a developing project concerning manual annotation of discourse relations for Czech. The aim of the project is to design a language corpus capturing Czech language material from the perspective of text structure and coherence, i.e. focusing on the description of inter-sententional relations. The outcome of the project will be a new annotation layer above the existing layers of annotation (morphology, surface syntax and underlying syntax) in the Prague Dependency Treebank. This discourse annotation should function as a unique source for linguistic research in the field of the discourse analysis and for computational experiments in text processing and summarization as well as machine translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem tohoto článku je empiricky testovat předpovědi formulované v rámci Hypotézy Tranzitivity. Jsou zde diskutovány metodologické problémy předchozích přístupů a nabídnuta některá řešení. Pro testování hypotéz byly použity dva korpusy (Pražský mluvený korpus a Pražský závislostní korpus). Výsledky zpochybňují jednak předpokládaný vliv jazykové formy na tranzitivitu a, co je důležitější, pojem Hypotézy tranzitivity obecně.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The  aim  of  the  article  is  to  test  empirically  predictions  formulated  in  the  Transitivity Hypothesis framework. Methodological problems of the original approach are discussed and some solutions  are  offered.  For  the  testing  of  the  hypotheses  two  corpora  of  Czech  were  used  (Prague Spoken Corpus and Prague Dependency Treebank). The results question both the predicted impact of the language form on transitivity and, more importantly, the concept of the Transitivity Hypothesis in general.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>AJAX-CAT je ukázka integrace strojového překladu do webového editoru. Pro danou vstupní větu AJAX-CAT ukazuje návrhy překladů jednotlivých frází a též několik variant dokončení překladu celé věty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>AJAX-CAT demonstrates an integration of machine translation system into a simple web text editor. For a given input sentence, AJAX-CAT presents a table of translation options of source phrases and also provides a suggestion box with output continuations when the user starts typing the translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nástroj pro podporu lidského překladu. Ve webovém rozhraní uživatelům systém nabízí nejen překlady jednotlivých úseků vstupní věty, ale též několik variant, jak v načaté věte pokračovat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A number of tools to support translators (computer-aided translation, CAT)
exist, as there are many systems of machine translation (MT). So far, the
integration of the two system types was little or none. The aim of this paper is
to examine a tighter coupling of MT and CAT.

We introduce our web-based CAT tool implemented using the modern AJAX technology
that
communicates with Moses MT system on the server side
to provide the translator with suggested translations of individual phrases of
the source sentence as well as several options of the complete continuation of
the output sentence. The suggested continuation is based on what has been
already translated and what the user has already written as the output.
Hopefully, the proposed user interface and the MT system at the back end will
accelerate and simplify the process of translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek studuje vliv příbuznosti jazyků na kvalitu frázového strojového překladu. Kontrastní dvojice jazyků jsou angličtina-ruština a čeština-ruština.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we describe an attempt to compare how relatedness of languages
can influence the performance of statistical machine translation (SMT). We
apply the Moses toolkit on the Czech-English-Russian corpus UMC 0.1 in order to
train two translation systems: Russian-Czech and English-Czech. The quality
of the translation is evaluated on an independent test set of 1000 sentences
parallel in all three languages using an automatic metric (BLEU score) as well
as manual judgments. We examine whether the quality of Russian-Czech is better
thanks to the relatedness of the languages and similar characteristics of word
order and morphological richness. Additionally, we present and discuss
the most frequent translation errors for both language pairs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>UMC003 je vyčištěná sada tokenizovaných vět určená pro evaluaci strojového překladu mezi češtinou, ruštinou a angličtinou. Tato testovací sada je vhodným doplňkem k trénovací sadě UMC 0.1.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>UMC003 is a cleaned tokenized development and test set to accompany the training data in UMC 0.1 aimed at Czech-English-Russian machine translation. For more information about the test set, see the README file in the UMC003 package.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace probihajiciho projektu anotace jmenne koreference a asociacni anafory na PDT se zvlastnim zretelem k lingvistickym problemum vznikajicim pri anotaci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the scheme of annotation of coreference in PDT. We represent the structure of annotation stages, which consist of grammatical coreference annotation (the antecedent can be calculated from the grammatical rules of  the language), textual pronominal coreference and the extended scheme of textual coreference, which includes noun anaphoric relations and bridging anaphora. We suggest and discuss coreference relations in the extended coreference scheme, examine some complications, which occur by annotation and present our first results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prehled anotace koreference a asociacni anafory na tektogramaticke rovine se zvlastnim zretelem k lingvistickym problemum vznikajicim pri anotaci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article describes the annotation of coreference and bridging anaphora on the tectogammatical level of PDT, with special reference to liguistic problems, which occur during the annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme probíhající projekt anotace rozšířené jmenné koreference a bridging anafory v Pražském závislostním korpusu. Popisujeme anotační schéma s přihlédnutím k jazykové klasifikaci koreferenčních a bridging vztahů a zaměřujeme se rovněž na podrobnosti průběhu anotace z technického hlediska. Představujeme metody usnadnění anotace - pomocí předanotace a několika praktických pomůcek implementovaných v anotačním nástroji. Naše metoda měření mezianotátorské shody je zaměřena na zdokonalení anotačních instrukcí; uvádíme výsledky tří měření této shody.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper outlines an ongoing project of annotation of the extended nominal coreference and the bridging anaphora in the Prague Dependency Treebank. We describe the annotation scheme with respect to the linguistic classification of coreferential and bridging relations and focus also on details of the annotation process from the technical point of view. We present methods of helping the annotators – by a pre-annotation and by several useful features implemented in the annotation tool. Our method of the inter-annotator agreement is focused on the improvement of the annotation guidelines; we present results of three subsequent measurements of the agreement.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje anotační schéma rozšířené jmenné koreference a bridging anafory v Pražském závislostním korpusu. Porovnáváme náš přístup s již existujícími přístupy ve vztahu k jazyku, pro který je to které schéma použito. Jmenujeme anotační principy a ukazujeme jejich aplikaci na rozsáhlou anotaci českých textů. Dále uvádíme vlastní rozdělení typů koreferenčních a bridging vztahů a věnujeme se některým problematickým otázkám této oblasti. Popisujeme rovněž automatickou předanotaci a několik pomocných vlastností anotačního nástroje, jako je zachovávání koreferenčních řetězců, zvýrazňování kandidátů pro antecedenty apod. Uvádíme řadu statistických údajů získaných na doposud anotované části Pražského závislostního korpusu. Rovněž přinášíme první výsledky měření mezianotátorské shody a objasňujeme nejčastější případy neshody.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper outlines the coding scheme for annotating extended nominal coreference and bridging relations in the Prague Dependency Treebank. We compare our annotation scheme to the existing ones with respect to the language to which the scheme is applied. We identify the annotation principles and demonstrate their application to the large-scale annotation of Czech texts. We further present our classification of coreferential relations and bridging relations types and discuss some problematic aspects in this area. An automatic pre-annotation and some helpful features of the annotation tool, such as maintaining coreferential chain, underlining candidates for antecedents, etc. are presented and discussed. Statistical evaluation is performed on the already annotated part of the Prague Dependency Treebank. We also present the first results of the inter-annotator agreement measurement and explain the most frequent cases of disagreement.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V posledních letech výzkumu v oblasti vyhledávání informací je
věnována značná pozornost metodám založeným na jazykovém modelování.
I přesto, že tento přístup dovoluje použití libovolného jazykového modelu,
většina publikovaných experimentů byla prováděna s klasickým n-gramovým
modelem (mnohdy pouze s unigramovým modelem). Cílem diplomové práce
je navrhnout, implementovat a vyhodnotit (na českých datech) metodu, která
by pravděpodobnostní model obohatila o použití syntaktické informace získané
automaticky (strojově) z dokumentů i dotazů. V předkládané práci se pokusíme
vhodným způsobem zavést syntaktickou informaci do jazykových modelů a ex-
perimentálně srovnáme navržený přístup s výsledky unigramového a bigramo-
vého povrchového modelu. Kromě využití syntaktické informace se zaměříme
také na vliv vyhlazování, stemmingu, lemmatizace, použití stopwords a me-
tody rozšiřování dotazů – pseudo relevance feedback. Provedeme také detailní
analýzu použitých systémů vyhledávání informace a podrobně popíšeme jejich
vlastnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the last years, application of language modeling in infor-
mation retrieval has been studied quite extensively. Although language models
of any type can be used with this approach, only traditional n-gram models
based on surface word order have been employed and described in published
experiments (often only unigram language models). The goal of this thesis is
to design, implement, and evaluate (on Czech data) a method which would
extend a language model with syntactic information, automatically obtained
from documents and queries. We attempt to incorporate syntactic information
into language models and experimentally compare this approach with uni-
gram and bigram model based on surface word order. We also empirically
compare methods for smoothing, stemming and lemmatization, eﬀectiveness
of using stopwords and pseudo relevance feedback. We perform a detailed ana-
lysis of these retrieval methods and describe their performance in detail.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek se zabývá rozpoznáváním pojmenovaných
entit v českých textech. Popisuje nový korpus s ručně značkovanými entitami ve dvouúrovňovém anotačním schématu. Data byla použita pro trénování rozpoznávače pojmenovaných entit, který je založen na klasifikátoru SVM.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper deals with recognition of named entities in Czech texts. We present a recently released corpus of Czech sentences with manually annotated named entities, in which a rich two-level classification scheme was used.  There are around 6000 sentences in the corpus with roughly 33000 marked named entity instances. We use the data for training and evaluating a named entity recognizer based on Support Vector Machine classification technique. The presented  recognizer outperforms the results previously reported for NE recognition in Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Czech Named Entity Corpus 1.0 je první veřejně přístupný korpus českých vět, v nichž byly ručně vyznačeny a klasifikovány pojmenované entity (nejčastěji se jedná o vlastní jména).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Czech Named Entity Corpus 1.0 is the first publicly available corpus providing a large body of manually annotated named entities in Czech sentences, including a fine-grained classification.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Špatné zprávy o dosavadní užitečnosti hloubkového rozboru pro MT, nápady rysů, jimiž by se dal obohatit mělký překlad.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Bad news about the utility of deep syntax in MT so far, ideas on adding more features to shallow MT systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Studie se zabývá vzájemným vztahem mezi lingvistickými teoriemi, daty a aplikacemi. Soustředuje se přitom na jednu konkrétní teorii, teorii Funkčního generativního popisu, jeden konkrétní typ dat, totiž slovesné valenční rámce, a jednu konkrétní aplikaci: strojový překlad z angličtiny do češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This study explores the mutual relationship between linguistic theories, data
and applications. We focus on one particular theory, Functional Generative
Description (FGD), one particular type of linguistic data, namely valency
dictionaries and one particular application: machine translation (MT) from
English to Czech.

First, we examine methods for automatic extraction of verb valency dictionaries
based on corpus data. We propose an automatic metric for estimating how much
lexicographers' labour was saved and evaluate various frame extraction
techniques using this metric.

Second, we design and implement an MT system with transfer at
various layers of language description, as defined in the framework of FGD. We
primarily focus on the tectogrammatical (deep syntactic) layer.

Third, we leave the framework of FGD and experiment with a rather direct,
phrase-based MT system. Comparing various setups of the system and
specifically treating target-side morphological coherence, we are
able to significantly improve MT quality and out-perform a commercial MT
system within a pre-defined text domain.

The concluding chapter provides a broader perspective on the utility of lexicons
in various applications, highlighting the successful features.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popularizační seminář pro korespondenční seminář Pralinka na téma strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A quick introduction to problems of machine translation for the participants of Pralinka corespondence series.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Korpus výstupů čtyř systémů strojového překladu s ručně vyznačenými chybami a klasifikací chyb.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A collection of outputs of 4 machine translation systems. Errors in the output are manually flagged and classified.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>QuickJudge je minimalistický nástroj pro rychlé a pohodlné ruční hodnocení libovolných řádkově-orientovaných výstupů. Primárně byl vyvinut pro účely hodnocení kvality srovnáním či počítáním chyb ve výstupech jednoho či více systémů strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>QuickJudge is a tiny tool to simplify the process of manual judging of string segments (e.g. sentences in machine translation output) of one or more competing systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška pro studenty předmětu Nástroje pro strojový překlad o problematice systémů plně automatického překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A talk for students of the subject Nástroje pro strojový překlad on fully automatic machine translation systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Stručný úvod k automatické bohaté anotaci pomocí TectoMT s cílem vyhýbat se bohatým datovým formátům, jak je to jen možné.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A quick introduction to the automatic rich annotation workflow using TectoMT -- while avoiding rich formats whenever possible.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Trénovatelný tokenizér je schopen tokenizovat a segmentovat většinu jazyků na základě dodané konfigurace a ukázkových dat. Není určen pro jazyky bez jasně odlišených slov, jako je např. čínština.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Trainable Tokenizer is able to tokenize and segment most languages based on supplied configuration and sample data. The tokenizer is not aimed e.g. for Chinese with no explicit delimitation of words.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme dva systémy strojového překladu z angličtiny do češtiny užité při soutěži WMT09.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe two systems for English-to-Czech machine translation that took part
in the WMT09 translation task. One of the systems is a tuned phrase-based system
and the other one is based on a linguistically motivated
analysis-transfer-synthesis approach.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Závěrečná zpráva o hloubkově-syntaktickém transferu pro projekt EuroMatrix.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Final technical report on deep syntactic transfer for the EuroMatrix Project.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zvláštní vydání časopisu PBML zaměřené na volně šiřitelné nástroje pro strojový překlad. Vydání bylo sestaveno u příležitosti týdenního kurzu MT Marathon 2009.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A special issue of the PBML journal focussed on open-source tools for machine translation, as presented during MT Marathon 2009.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme práce na paralelním česko-anglickém korpusu CzEng pro jeho třetí vydání (verze 0.9). V aktuální verzi korpus obsahuje 8.0 mil. paralelních vět (93 mil. anglických slov a 82 mil. českých slov).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe our ongoing efforts in collecting a Czech-English parallel corpus CzEng.
The paper provides full details on the current
version~0.9 and focuses on its new features: (1) data from new sources were added, most importantly a few hundred electronically available books, technical documentation and also some parallel web pages, (2) the full corpus has been automatically annotated up to the tectogrammatical layer (surface and deep syntactic analysis), (3) sentence segmentation has been refined, and (4) several heuristic filters to improve corpus quality were implemented. In total, we provide a sentence-aligned automatic parallel treebank of 8.0 million sentences, 93 English and  82 Czech words. CzEng~0.9 is freely available for non-commercial research purposes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CzEng 0.9 je třetí vydání paralelního česko-anglického korpusu. V aktuální verzi korpus obsahuje 8.0 mil. paralelních vět (93 mil. anglických slov a 82 mil. českých slov).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CzEng 0.9 is the third release of a sentence-parallel Czech-English corpus compiled at the Institute of Formal and Applied Linguistics (ÚFAL) freely available for non-commercial and research purposes.

CzEng 0.9 contains 8.0 million parallel sentences (93 million English and 82 million Czech tokens) from seven different types of sources automatically annotated at surface and deep (a- and t-) layers of syntactic representation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme naši snahu zlepšit dřívější výsledky strojového překladu z angličtiny do hindštiny. Využíváme dva frázové open-source systémy: Moses a Joshua. Testujeme několik přístupů k morfologickému značkování: od automatických slovních tříd přes segmentaci na kmen a sufix až k POS taggeru. Experimentujeme také s faktorizovanými jazykovými modely. Vyhodnocujeme různé kombinace trénovacích dat a dalších existujících anglicko-hindských jazykových zdrojů. Pokud je nám známo, BLEU skóre, kterého jsme dosáhli, je v současnosti nejlepší publikovaný výsledek na testovacích datech IIIT-TIDES.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe our attempt to improve on previous English to Hindi machine translation results, using two open-source phrase-based MT systems: Moses and Joshua. We use several approaches to morphological tagging: from automatic word classes, through stem-suffix segmentation, to a
POS tagger. We also experiment with factored language models. We evaluate various combinations of training data sets and other existing English-Hindi resources. To our knowledge, the BLEU score we obtained is currently the best published result for the IIIT-TIDES dataset.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Česko-anglických 515 vět, jak byly ručně zarovnány po slovech a použity v publikaci Bojar, Prokopová. LREC 2006. Data zveřejňujeme až v roce 2009.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Czech-English 515 parallel sentences, manually aligned at the word level, as used by Bojar and Prokopová in paper at LREC 2006. We release the data in 2009.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jedenáctý rok v řadě je konference CoNLL doprovázena společnou úlohou, jejímž úkolem je podpořit aplikace na zpracování přirozeného jazyka a ohodnotit je za standardních podmínek. V roce 2009 byla úloha zasvěcena spojenému parsingu syntaktických a sémantických závislostí v několika jazycích. Úloha kombinuje zadání z předchozích pěti let v jedinečném závislostním formalizmu podobném úloze z roku 2008. V tomto článku definujeme společnou úlohu, popisujeme, jak byly vytvářeny sady dat a jaké jsou jejich kvantitativní vlastnosti, oznamujeme výsledky a shrnujeme přístupy soutěžících systémů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>For the 11th straight year, the Conference
on Computational Natural Language Learn-
ing has been accompanied by a shared task
whose purpose is to promote natural language
processing applications and evaluate them in
a standard setting. In 2009, the shared task
was dedicated to the joint parsing of syntac-
tic and semantic dependencies in multiple lan-
guages. This shared task combines the shared
tasks of the previous five years under a unique
dependency-based formalism similar to the
2008 task. In this paper, we define the shared
task, describe how the data sets were created
and show their quantitative properties, report
the results and summarize the approaches of
the participating systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Česká testovací (dev i eval) data pro CoNLL 2009 Shared Task. Data jsou vygenerována z PDT 2.0.
LDC2009E35B</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Czech testing (development and evaluation) data for CoNLL 2009 Shared Task. The data are generated from PDT 2.0. LDC2009E35B</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Česká trénovací data pro CoNLL 2009 Shared Task. Data jsou vygenerována z PDT 2.0. LDC2009E34B</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Czech training data for CoNLL Shared Task. The data are generated from PDT 2.0. LDC catalog number: LDC2009E34B</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Česká ukázková data pro CoNLL 2009 Shared Task. Data jsou vygenerována z PDT 2.0.
LDC2009E32B</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Czech sample data for CoNLL 2009 Shared Task. The data are generated from PDT 2.0.
LDC2009E32B</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Projekt rekonstrukce mluvené řeči (pro češtinu a angličtinu) začal na pracovišti UFAL současně s projektem PIRE v roce 2005 a postupně se rozvinul z prvních návrhů k vytvoření software, anotačních manuálů a samotné anotaci dat. PDTSL je součástí korpusů a zdrojů kolem Pražského závislostního korpusu, ke kterému přidává úroveň mluveného jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The project of speech reconstruction of Czech and English has been started at UFAL together with the PIRE project in 2005, and has gradually grown from ideas to (first) annotation specification, annotation software and actual annotation. It is part of the Prague Dependency Treebank family of annotated corpus resources and tools, to which it adds the spoken language layer(s).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jedná se o data z anglického dialogového korpusu NAP, který vznikl v rámci projektu Companions. Jsou to rozhovory nad fotografiemi. Na tomto korpusu byla prováděna ruční anotace tzv. speech reconstruction, rekonstrukce mluvené řeči.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This release contains the manual speech reconstruction annotation of about 260k tokens of the NAP corpus. The NAP corpus is a corpus of dialogs over personal photograph collections, recorded for the Companions project.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Na CD se nachází 10 000 manuálně tektogramaticky anotovaných vět z Penn Treebanku, editor a dokumentace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This CD presents part of the Prague English Dependency Treebank (PEDT). PEDT is the manual tectogrammatical (syntactico-semantic) annotation of texts from the Wall Street Journal - Penn Treebank III. The present CD (PEDT 1.0) comprises approx. 10,000 annotated and checked trees, which is about 20% of the original WSJ-PTB. The following components are included:

    * manually annotated data, integrated valency lexicon Engvallex
    * the valency lexicon Engvallex in printable form (latest revision: January 2009)
    * the ready-to-install package of the tree editor/viewer TREd
    * documentation
    * specification of the annotation format (Prague Markup Language)</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku je navrženo zařazení české 2. osoby zdvořilé (vykání) jako paradigmatické kategorie českého slovesa, protože v některých tvarech vykazuje zvláštní shodu. Klade se otázka, zda v plurálu, kde se formální rozdíl neprojevuje, jde o homonymii nebo o neutralizaci. Na korpusových datech se zkoumají ukazatelé zdvořilostní formy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The author claims that the Czech polite forms (so-called „vykání“) for addressing the 2nd person should  be undestood as a legitimate part of the Czech conjugation paradigm. If we address a single person in a polite way, some Czech analytical verb forms exhibit „hybrid“ agreement (auxiliaries are in plural, while participle form is in singular). However, the paradigm for singular and plural polite forms is not symmetrical. The question, whether 2nd person plural polite forms are ambiguous (between  the polite meaning and 2nd plural non-polite), or whether the semantic distinction „polite – non-polite“ is neutralized in plural, is open for further discussion.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Český subjektový infinitiv se probírá jako typ kontrolované struktury. Analyzují se formy a distribuce kontroloru  v řídící predikaci jako žto antecedentu (nevyjádřeného) subjektového infinitivu. V češtině jsou předmětem analýzy tři typy konstrukcí (kromě sloves typu baví ho, zajímá ho jsou to věty s verbonominálním přísudkem substantivníma a djektivním).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Czech infinitive in the position of subject is analyzed as a type of controlled construction. In three types of constructions (verbs of evaluation of the state, verbonominal predicates with nouns and adjectives)the form and distribution of their respective controller is described.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Korpusová anotace je důležitou součástí lingvistické analýzy a počítačového zpracování jazyka. Tento článek se zabývá problémy spojenými se syntaktickou anotací mluvených textů na pozadí syntaktické anotace ČAKu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Corpus annotation plays an important role in linguistic analysis and computa-tional processing of both written and spoken language. Syntactic annotation of spoken texts becomes clearly a topic of considerable interest nowadays, driven by the desire to improve auto-matic speech recognition systems by incorporating syntax in the language models, or to build language under-standing applications. Syntactic anno-tation of both written and spoken texts in the Czech Academic Corpus was created thirty years ago when no other (even annotated) corpus of spoken texts has existed. We will discuss how much relevant and inspiring this annotation is to the current frameworks of spoken text annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>TectoMT je široce modulární nástroj pro zpracování přirozeného jazyka (NLP, natural language processing tool).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>TectoMT is a highly modular NLP (Natural Language Processing) software system implemented in Perl programming language under Linux. It is primarily aimed at Machine Translation, making use of the ideas and technology created during the Prague Dependency Treebank project. At the same time, it significantly facilitates and accelerates development of software solutions of many other NLP tasks, especially due to re-usability of the numerous integrated processing modules (called blocks), which are equipped with uniform object-oriented interfaces.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozšiřující modul pro TrEd, který umožní zanalyzovat zadanou českou větu na morfologické, analytické a tektogramatické rovině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Tred extension that provides a simple TectoMT-based Czech analyzer producing tectogrammatical, analytical and morphological layers for a given Czech sentence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek pojednává o možnosti využití skrytých stromových markovovských modelu ve fázi transferu ve strojovém překladu využívajícím tektogramatickou rovinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We would like to draw attention to Hidden Markov Tree Models (HMTM), which are to our knowledge still unexploited in the field of Computational Linguistics, in spite of highly successful Hidden Markov (Chain) Models. In dependency trees, the independence assumptions made by HMTM correspond to the intuition of linguistic dependency. Therefore we suggest to use HMTM and tree-modified Viterbi algorithm for tasks interpretable as labeling nodes of dependency
trees. In particular, we show that the transfer phase in a Machine Translation system based on tectogrammatical dependency trees can be seen
as a task suitable for HMTM. When using the HMTM approach for the English-Czech translation,
we reach a moderate improvement  over the baseline.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Stručně se připomínají zásady Pražské školy a ukazuje se jejich důležitost pro současné lingvistické bádání.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A brief characterization of the Prague School is presented on the background of the present day development of lingustic theories.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Probírají se nejrůznější aspekty autorovy vědecké dráhy v lingvistice.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Most different aspects of the author`s linguistic carreer are discussed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vztah mezi jádrem jazyka, relativně jednoduše strukturovaným, a jeho složitou periferií lze chápat na základě pojmu příznakovosti. Jádro můžeme vidět v aktuálním členění a v později vyvinuté syntaktické závislosti (valenci).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The distinction between the core of language, patterned in a relatively simple way, and its complex periphery can be expressed in terms of the notion of markedness. If the properties of the language core are to be explained as attributable to general principles, then the core of language may be seen as based on information structure, and syntactic dependency, or valency, might be taken to be a later development.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve stati je shrnut a vysoko hodnocen vědecký přínos jednoho z pionýrů strojového překladu nejen v České republice, Zdeňka Kirschnera, u příležitosti jeho úmrtí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The scientific contribution of one of the pioneers in the field of machine translation is summarized at the occasion of the scientists' death.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Několik námětů, které je třeba řešit v budoucím vývoji anotačních schémat pro korpusy textů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Some issues to be solved in the future development of annotation scenaria
for text corpora.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pražská lingvistická škola je jedním ze základních směru evropského strukturalismu, který se vyznačuje vedle strukturního pohledu na jazykový systém také zohledněním komunikativní funkce jazyka. Tyto dva principy jsou ve stati uplatněny na oblast formálního popisu jazyka a na anotování jazykových korpusů na různých rovinách popisu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Prague School of Linguistics is one of the main schools of linguistic thought distinguished first of all by its structural and functional approach to language system. In the paper these two viewpoints are discussed in their contribution to a formal description of language and to different levels of annotation of linguistic resources.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jedním ze základních principů přístupu Pražského lingvistického kroužku bylo důsledné rozlišování jazykové formy a funkce. Ve stati se nejprve shrnují klasické pohledy na tuto distinkci a poté se uplatňuje tento pohled na rozbor aktuálního členění větného.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>One of the main tenets of the Prague School of Linguistics was a clear distinction to be made between form and function. The paper first refers to the classical statements about this distinction and then discusses this opposition from the viewpoint of topic-focus articulation of the sentence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Aktuální členění věty je třeba zachycovat jako jeden ze základních aspektů hloubkové stavby věty, tj. jejího jazykového významu. Jeho primární výrazové prostředky jaou slovosled a větná intonace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Information structure has to be reflected as a fundamental aspect of the underlying structure of the setence, i.e. of its literal meaning. Its primary means of expression may be seen in the word order and sentence intonantion.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento softwarový balíček poskytuje vizualizaci pro data z CoNLL-2009-ST v TrEdu, konverzní skripty z CoNLL-2009-ST formátu do formátu PML a rozsšíření pro TrEd, umožňující anotaci predikátů a argumentů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This software package provides visualization of CoNLL-2009-ST data in TrEd, scripts (for Linux and similar systems) for converting from the CoNLL-2009-ST format to the PML format and a TrEd annotation mode for predicate-argument annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Sada nástrojů pro práci s datovým formátem PML pro reprezentaci lingvistické anotace jazykových dat. Sada zahrnuje validátor, sadu konverzních skriptů z jiných formátů, programy pro management dat, úpravy schématu, knihovny pro Perl, a další nástroje.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A set of tools for working with the PML data format used for capturing linguistic annotations. The toolkit consists of validators, conversion scripts for several other formats, data management tools, schema processing tools, Perl API, and various other tools.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>PML-TQ je systém pro vyhledávání nad lingvisticky anotovanými korpusy závislostních či složkových stromů ve formátu PML. Systém definuje velmi silný dotazovací jazyk. Je implementován pomocí client-server architektury, s použitím SQL databáze na straně serveru a zahrnuje webové a CLI rozhraní a grafické rozhraní zabudované do editoru TrEd. Jako rozsíření TrEdu je dále dostupný čistě klientský vyhledávací systém nad stejným dotazovacím jazykem, umožňující prohledávat lokální soubory.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>PML-TQ is a search system for linguistic TreeBanks in PML format. The system defines a powerfull query language, uses a client server architecture (with SQL database backend on the server side) and provides a command-line and a simple web-based search client. The system also includes a graphical client for PML-TQ and client-side PML-TQ search engine, allowing the users to use PML-TQ queries on their local data. The GUI and client-side search engine are distributed separately as extensions to the tree editor TrEd.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje systém pro dotazování nad treebanky. Systém se skládá ze silného dotazovacího jazyka s přímou podporou dotazů napříč anotačními rovinami, z klientského rozhraní s grafickým editorem dotazů a vizualizací výsledků, a ze dvou zaměnitelných dotazovacích nástrojů: velmi výkonného nástroje nad relační databází (vhodného pro statická data) a pomalejšího, ale paralelizovatelného nástroje, který pracuje přímo nad souboru treebanku (vhodného pro "živá" data).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a system for querying
treebanks. The system consists of a powerful query language with natural support for cross-layer queries, a client interface with a graphical query builder and visualizer of the results, a command-line client interface, and two substitutable query engines: a very efﬁcient engine using a relational database (suitable for large static data), and a slower, but paralel-computing enabled, engine operating on treebank ﬁles
(suitable for “live” data).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato kniha je věnovaná empirické studii lexikálních asociačních měr a jejich aplikaci v úloze automatické extrakce kolokací. Práce obsahuje vyčerpávající seznam 82 lexikálních asociačních měr ajejich evaluaci na celkem čtyřech referenčních datových množinách: závislostních bigramech z ručně anotovaného Pražského závislostního korpusu, povrchové bigramy ze stejného korpusu, instance prvků předchozí množiny z Českého národního korpusu opatřeného automatickou lemmatizací a morfologickým značkováním a vzdálenostními verbnominálními bigramy z automaticky značko­vaného švédského korpusu Parole. Kolokační kandidáti v referenčních  množinách byli manuálně anotováni jako kolokace nebo nekolokace. Použité evaluační schéma je založeno na měření kvality seřazení kolokačních kandidátů dle jejich pravděpodobnosti tvořit kolokaci. Metody jsou porovnány pomocí precision-recall křivek a hodnot mean average precision, které jsou převzaty  z oboru vyhle­dávání informací. Provedeny byly i testy signifikance výsledků. Dále je zkoumána možnost kombi­nování lexikálních asociačních měr a presentovány výsledky několika kombinačních metod, jejichž použití vedlo k výraznému zlepšení úspěšnosti řešení této úlohy. Dále je v práci navržen algoritmus významně redukující složitost použitých kombinačních modelů bez statisticky významného snížení jejich úspěšnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This publication is devoted to an empirical study of lexical association measures and their application to collocation extraction. It presents a comprehensive inventory of lexical association measures and their evaluation on four reference data sets of collocation candidates: Czech dependency bigrams from the Prague Dependency Treebank, surface bigrams from the same source, instances of the latter from the Czech National Corpus, and Swedish distance verb-noun combinations obtained from the PAROLE corpus. The collocation candidates in the reference data sets were manually annotated and labeled as collocations or non-collocations by expert linguists. The evaluation scheme applied in this work is based on measuring the quality of ranking collocation candidates according to their chance to form collocations. The methods are compared by precision-recall curves, mean average precision scores, and appropriate tests of statistical significance. Further, the study focuses on the possibility of combining lexical association measures and discusses empirical results of several combination methods that significantly improve state of the art in collocation extraction. The work is concluded by a description of a model reduction algorithm that significantly reduces the number of combined measures without any statistically significant difference in performance.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Důležitou součástí Pražského závislostního korpusu je valenční slovník. Slovník obsahuje 5300 sloves s 8200 valenčními rámci, které jsou propojeny s korpusem. Reprezentace valenčního rámce je plně formalizována. Tento příspěvek je orientován na formální popis forem, kterých nabývají argumenty sloves v různých sekundárních diatezích, jako je  například pasivizace (pasivum opisné a zvratné), rezultativ (sloveso mít, dostat), dispoziční modalita.  Článek detailně  popisuje fungování jednotlivých transformačních pravidel pro příslušné změny forem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>As an important part of the Prague Dependency Treebank project a valency lexicon is being distributed. In this lexicon, more than 5300 verb entries are fully formally represented, with more than 8200 valency frames for verb senses included. Moreover, the valency frames are interlinked with the Prague Dependency Treebank corpus, effectively providing a verb sense distinction and annotation for every occurrence of a verb in the corpus. More than 100,000 verb occurrences are annotated in this way. The valency frame representation is fully formalized. In this contribution, we will concentrate on the formal description of the form of the verb</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Softwarový nástroj zprostředkující formou webového rozhraní editaci a vyhledávání v přepisech audio-visuálních nahrávek dialogů. Vyhledané úseky textu je možné přehrávat a analyzovat pomocí webového prohlížeče.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A software tool allowing editing and searching in the transcripts of audio-visual recordings of dialogues. The dynamic web application provides access for registered users to the digitised archive. Playing and exploring of selected parts is possible in the web browser.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V knize studujeme anotaci Pražského závislostního korpusu 2.0 a vytváříme seznam požadavků kladených na dotazovací jazyk, který by umožnil vyhledávání a studium všech lingvistických jevů anotovaných v tomto korpusu. Navrhujeme rozšíření dotazovacího jazyka existujícího nástroje Netgraph 1.0 a ukazujeme, že takto rozšířený dotazovací jazyk splňuje definovaný seznam požadavků. Ukazujeme rovněž, jak je pomocí tohoto jazyka možno vyhledávat všechny zásadní lingvistické jevy v korpusu anotované. Navržený dotazovací jazyk byl rovněž implementován – představujeme vyhledávací nástroj a pojednáváme o jeho datovém formátu. Dotazovací jazyk je porovnán s několika dalšími dotazovacími jazyky. Ukazujeme rovněž, do jaké míry jsou vlastnosti tohoto jazyka využívány skutečnými uživateli a co tito uživatelé vyhledávají. Řada dalších informací je k dispozici v přílohách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the book, we study the annotation of the Prague Dependency Treebank 2.0 and assemble a list of requirements on a query language that would allow searching for and studying all linguistic phenomena annotated in the treebank. We propose an extension to the query language of the existing search tool Netgraph 1.0 and show that the extended query language satisfies the list of requirements. We also show how all principal linguistic phenomena annotated in the treebank can be searched for with the query language. The proposed query language has also been implemented – we present the search tool as well and talk about the data format for the tool. The query language is compared to several other query languages. We also show to what extent the features of the query language are put to use by the users and what the users really do search for. Much additional information can be found in Appendixes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Implementace originálního lingvistického modelu Funkční arabské morfologie, která zahrnuje jak deklarativní definici systému v jazyce Haskell, tak i rozsáhlý arabský morfologický slovník. Publikováno pod licencí GNU GPL.
- první verze online interface
- základní textové a API rozhraní
- základní analýza sekvencí slov</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An implementation of the original linguistic model of the Functional Arabic Morphology includes both the declarative definition of the system in Haskell, and an extensive Arabic morphologic dictionary. Published under the GNU GPL license.
- first version of the online interface
- basic text interface and API
- basic analysis of word sequences</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku popisujeme náš systém, se kterým jsme se zúčastnili CoNLL 2009 Shared Task. Systém zahrnuje tři sřetězené komponenty: generativní závislostní syntaktický analyzátor (parser), klasifikátor syntaktických závislostí a sémantický klasifikátor. Výsledky pokusů ukazují, že náš systém dosahuje tzv. labeled macro F1 score (skóre makro-F1 se zohledněním značek) mezi 43,50 % (čínština) a 57,95 % (čeština), s průměrnou hodnotou 51,07 %.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe our CoNLL 2009 Shared Task system in the present paper. The system includes three cascaded components: a generative dependency parser, a classifier for syntactic dependency labels and a semantic classifier. The experimental results show that the labeled macro F1 scores of our system on the joint task range from 43.50% (Chinese) to 57.95% (Czech), with an average of 51.07%.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>DZ Interset je prostředek pro konverzi mezi různými sadami značek pro počítačové zpracování přirozených jazyků. Základní myšlenka je podobná strojovému překladu přes interlingvu. V DZ Intersetu je definována sada rysů, které lze zakódovat pomocí jednotlivých sad značek. Sada rysů je co možná nejuniverzálnější. Nemusí zakódovat vše, co je obsaženo v libovolných sadách značek, ale měla by kódovat veškerou informaci, ke které uživatelé mohou chtít přistupovat a/nebo je přenášet z jedné sady značek do druhé.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>DZ Interset is a means of converting among various tag sets in natural language processing. The core idea is similar to interlingua-based machine translation. DZ Interset defines a set of features that are encoded by the various tag sets. The set of features should be as universal as possible. It does not need to encode everything that is encoded by any tag set but it should encode all information that people may want to access and/or port from one tag set to another.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Morfologické značky jsou důležitou součástí anotace většiny korpusů. Bohužel se však v různých korpusech používají různé sady značek, a to i pokud jde o tentýž jazyk. Převody značek z jedné sady do druhé jsou obtížné a jejich implementace je obvykle ušitá na míru konkrétní dvojici sad značek. Zde naproti tomu navrhujeme univerzální přístup, který umožňuje jednou investované úsilí využít při pozdějších převodech do jiných formalismů. Prezentujeme také nepřímé vyhodnocení v kontextu syntaktické analýzy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Part-of-speech or morphological tags are important means of annotation in a vast number of corpora. However, different sets of tags are used in different corpora, even for the same language. Tagset conversion is difficult, and solutions tend to be tailored to a particular pair of tagsets. We propose a universal approach that makes the conversion tools reusable. We also provide an indirect evaluation in the context of a parsing task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>DZ Parser je program, který čte předzpracovanou větu v přirozeném jazyce a vrací závislostní strom, který popisuje syntaxi vstupní věty. Předpokládá, že jeho vstup byl tokenizován, morfologicky označkován a uložen ve formátu CSTS Pražského závislostního korpusu. Výstup je v tomtéž formátu. (Poznámka: Nyní jsou přiloženy i nástroje pro konverzi z a do formátu CoNLL shared task.)</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>DZ parser is a program that reads a pre-processed natural language sentence and returns a dependency tree describing the syntax of the input sentence. It assumes its input has been tokenized, morphologically annotated, morphologically disambiguated, and saved in the CSTS format (Prague Dependency Treebank). The output is in the same format. (Note: Tools for conversion from and to the CoNLL shared task format are now included.)</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme náš systém použitý v soutěži ICON 2009 NLP Tools: závislostní syntaktická analýza hindštiny, bengálštiny a telugštiny. Systém se skládá ze tří existujících, volně dostupných závislostních parserů, z nichž o dvou (MST a Malt) je známo, že dokáží generovat špičkové struktury na datech pro jiné jazyky. Zkoumáme různá nastavení jednotlivých parserů, abychom je přizpůsobili zmíněným třem indickým jazykům, a pomocí hlasování z nich vytváříme jeden superparser.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our system used for participation in the ICON 2009 NLP Tools Contest: dependency parsing of Hindi, Bangla and Telugu. The system consists of three existing, freely available dependency parsers, two of which (MST and Malt) have been known to produce state-of-the-art structures on data sets for other languages. Various settings of the parsers are explored in order to adjust them for the three Indian languages, and a voting approach is used to combine them into a superparser.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Morseus implementuje jednoduchou metodu neřízené morfematické segmentace slov v neznámém jazyce. Není k tomu potřeba nic kromě neanotovaného korpusu (nebo seznamu slov) v daném jazyce. Algoritmus identifikuje části slov, které se opakují v řadě slov, a interpretuje je jako kandidáty na morfémy (předpony, kmeny a přípony).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Morseus implements a simple method of unsupervised morpheme segmentation of words in an unknown language. All that is needed is a raw text corpus (or a list of words) in the given language. The algorithm identifies word parts occurring in many words and interprets them as morpheme candidates (prefixes, stems and suffixes).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme jednoduchou metodu neřízené morfematické segmentace slov v neznámém jazyku. Vše, co potřebujeme, je korpus prostého textu (nebo seznam slov) daného jazyka. Algoritmus identifikuje části slov, které se opakují v mnoha slovech, a interpretuje je jako kandidáty na morfémy (předpony, kmeny a přípony). Hlavní inovací ve srovnání s [1] je nové zpracování předpon. Po odfiltrování falešných hypotéz se seznam morfémů aplikuje na segmentaci vstupních slov. Prezentujeme oficiální výsledky Morpho Challenge 2008 spolu s některými doplňkovými pokusy. Zpracování předpon zlepšilo F-skóre o 5 až 11 bodů pro němčinu, finštinu a turečtinu, ale zhoršilo angličtinu a arabštinu. V závěru rozebíráme chyby s ohledem na zvolenou vyhodnocovací metodu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe a simple method of unsupervised morpheme segmentation of words in an unknown language. All that is needed is a raw text corpus
(or a list of words) in the given language. The algorithm identifies word parts occurring in many words and interprets them as morpheme candidates (prefixes, stems and suffixes). New treatment of prefixes is the main innovation in comparison
to [1]. After filtering out spurious hypotheses, the list of morphemes is applied to segment input words. Official Morpho Challenge 2008 evaluation is given together with some additional experiments. Processing of prefixes improved
the F-score by 5 to 11 points for German, Finnish and Turkish, while it failed to improve English and Arabic. We also analyze and discuss errors with respect to the evaluation method.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Podáme zprávu o experimentech se strojovým překladem z angličtiny do hindštiny pomocí překladače Moses. Vyhodnotíme vliv přídavných trénovacích dat z jiné domény, jak paralelních, tak jednojazyčných hindských, a experimenty se třemi metodami vylepšení slovosledu: standardní slovosledný model Mosese, pravidlové předzpracování a jazykově nezávislou identifikaci přípon.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present experiments with a Moses-based English-to-Hindi translation system.
We evaluate the impact of additional out-of-domain training data, both
parallel and Hindi-only, and experiment with three methods for improving word
order: standard Moses reordering model, rule-based pre-processing and
language-independent suffix identification.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>PlayCoref je návrh internetové jazykové hry zaměřené na získávání velkého objemu textu s anotací koreference. Detailně popisujeme rozličné aspekty návrhu hry a vlastnosti, které mají vliv na kvalitu získaných anotací.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>PlayCoref is a concept of an on-line language game designed to acquire a substantial amount of text data with the coreference annotation. We describe in detail various aspects of the game design and discuss features that affect the quality of the annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládáme návrh hry PlayCoref, jejímž cílem je získat velké množství textových dat opatřených anotací koreference.
Přinášíme popis návrhu hry, který sestává ze strategie, instrukcí pro hráče, výběru a přípravy vstupních dat a funkce pro výpočet skóre.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We propose the PlayCoref game, whose purpose is to obtain substantial amount of text data with the coreference annotation.
We provide a description of the game design that covers the strategy, the instructions for the players, the input texts selection and preparation, and the score evaluation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Překlady se budou používat výhradně v projektech tzv. automatického neboli strojového překladu mezi češtinou a angličtinou. Budou použity a zveřejněny pouze v elektronické formě, a to navíc způsobem, který umožní tzv. strojové učení. Přeložené texty tedy nebudou sloužit takovým účelům, kdy jde o informační obsah textu, a ani nebudou touto formou (na „papíře“, na internetu v textové podobě atd.) zveřejněny.
Metody strojového učení jsou do jisté míry tolerantní k nepřesnému překladu (koneckonců ani originální text není někdy úplně „hezky“ anglicky), avšak systematické chyby jsou již zavádějící a v textech bychom je měli neradi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>PCEDT is planned to be a corpus of syntactically annotated parallel texts (in English and Czech) intended chiefly for machine translation experiments. The texts for PCEDT were taken from Penn Treebank, which means there are mostly economical articles from the Wall Street Journal. 2312 documents were used in PCEDT (approximately 49,000 sentences) that are manually annotated with constituent trees in Penn Treebank. For the Czech part of PCEDT, the English texts have to be translated into Czech. This book contents the guidelines for translators.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje některé organizační aspektu budování rozsáhlého korpusu s bohatou lingvistickou anotací, jako příklad slouží PCEDT. Zdůrazňuje nevyhnutelnost rozdělení anotačního postupu do několika fází a představuje systém automatických kontrol korektnosti anotace. Popisuje také několik způsobů měření a evaluace anotace a anotátorů (mezianotátorská shoda, chybovost a výkon).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present some organizational aspects of building of a large corpus with rich linguistic annotation, while Prague Czech-English Dependency Treebank (PCEDT) serves as an example. We stress the necessity to divide the annotation process into several well planed phases. We present a system of automatic checking of the correctness of the annotation and describe several ways to measure and evaluate the annotation and annotators (inter-annotator accord, error rate and performance).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje system for annotation quality checking, jak byl navržen a jak je využíván při budování české části the Prague Czech-English Dependency Treebank. Nejprve je stručně představen projekt of the Prague Czech-English Dependency Treebank, jeho základní principy, a též anotační proces. V druhé části se pak článek podrobně věnuje jedné z důležitých fází anotačního procesu, totiž způsobu, jakým je průběžně již během anotace automaticky kontrolována správnost anotovaných dat. The system for annotation quality checking je názorně popsán na příkladu několika konkrétních automatických kontrol (které se týkají syntaktických jevů v anotaci). Zhodnocen je přínos tohoto systému nejen pro výslednou kvalitu anotovaných dat, ale také pro design celého korpusu, dopad na anotační pravidla a systém anotace jako takový.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article presents the system for annotation quality checking, proposed and used during the building of the Czech part of the Prague Czech-English Dependency Treebank. At first, the treebank project is introduced, as well as
its basic principles and annotation process. The second part of the article pursues in detail one of the important phases of the annotation process,
namely how the correctness of the annotated data is automatically and continuously checked during the process. The system of annotation quality
checking is demonstrated on several particular checking procedures concerning syntactical phenomena. We try to evaluate the contribution of the system not only to the quality of the data and annotation, but also to the corpus design,
impact on annotation rules and the annotation process as a whole.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Redukční závislostní analýza odkazuje na základní schopnost člověka zjednodušit větu. Taková schopnost je základem lidského porozumění přirozenému jazyku. Umožňuje nám získat (ne)závislosti na zýkladě správného zjednodušení věty, jakož i náležité zachycení slovosledných variant vět v jazyce s vysokým stupněm volného slovosledu.

Redukční závislostní analýza se stala důležitou motivací pro nový formální model pro Funkční generativní popis (FGD), původní rámec pro závislost-na popis české, založený na pojmu restartování automatů.

V tomto příspěvku představíme základní rysy FGD a použití závislostní redukční na několika základních jazykových jevech, především na zachycení jazykových závislostí a (ne-)projektivního slovosledu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Dependency analysis by reduction refers to a basic ability to simplify a sentence. Such ability underlies human understanding
of a natural language. It allows us to obtain (in)dependencies by the correct reductions of  sentences as well as to describe
properly the complex word-order variants of a language with a high degree of ‘free’ word order.

Dependency analysis by reduction became an important motivation for a new formal model for Functional Generative Description
(FGD), an original framework for dependency-based description of Czech, based on the notion of  restarting automata.

In this contribution we introduce the fundamental features of FGD and exemplify the application of analysis by reduction to some basic linguistic phenomena, mainly dependencies and (non-)projective word order.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Syntaktická analýza přirozeného jazyka je základním předpokladem mnoha aplikačních úkolů. Navrhujeme nový modul mezi morfologickou a syntaktickou analýzu, jehož cílem je stanovení
celkové struktury věty před její kompletní analýzu.

Pracujeme s konceptem segmentů, automaticky snadno zjistitelných a lingvisticky motivovaných jednotkek. Výstup modulu, takzvané "segmentační schéma", popisuje vztah mezi segmenty, zejména vztahy koordinace a apozice nebo
vztah podřízenosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Syntactic analysis of natural languages is the fundamental requirement of many applied tasks. We propose a new module between morphological and syntactic analysis that aims at determining the
overall structure of a sentence prior to its complete analysis.

We exploit a concept of segments, easily automatically detectable and linguistically motivated units. The output of the module, so-called `segmentation chart', describes the relationship among segments, especially relations of coordination and apposition or
relation of subordination.

In this text we present a framework that enables us to develop and test rules for automatic
identification of segmentation charts. We describe two basic experiments -- an experiment with segmentation patterns obtained
from the Prague Dependency Treebank and an experiment with the segmentation rules applied to plain text. Further, we discuss the
evaluation measures suitable for our task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem projektu je představit projekt, ve kterém je přiřazována struktura
českým větám z Pražského závislostní ho korpusu
(PDT) a tak je vytvářena nová rovina syntaktické anotace, rovina syntaktické struktury  věty. Anotace je založena na koncepci segmentů,
lingvisticky motivovaných a snadno automaticky zjistitelných jednotek. Úkolem anotátorů je určit vztahy mezi segmenty, zejména vztahy nadřazenosti a podřízenosti, koordinace, apozice a vsuvky. Potom se identifikují jednotlivé klauze, které tvoří souvětí.

V pilotní fázi anotace bylo zpracováno 2699 vět z PDT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The goal of the presented project is to assign a structure of clauses to Czech sentences from the Prague Dependency Treebank (PDT) as a new layer of syntactic annotation, a layer of clause
structure. The  annotation is based on the concept of segments, linguistically motivated and easily automatically detectable units. The task of the annotators is to identify relations among
segments, especially relations of super/subordination, coordination, apposition and parenthesis. Then they identify individual clauses forming complex sentences.

In the pilot phase of the annotation, 2,699 sentences from PDT were annotated with respect to their sentence structure.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek se zabývá restartovacími automaty,
které tvoří formální rámec pro Funkční generativní popis češtiny. Restartovací automaty pracující současně se čtyřmi rovinami jazykového popisu jsou určeny k tomu, aby prováděly redukční analýzu  českých vět a umožnily tak odvodit závislostní vztahy ve větě z možných pořadí
jednotlivých redukcí. Standardní model restartovacích automatů je zde obohacen o strukturovaný výstup, který umožňuje budovat
tektogramatickou závislostní strukturu odvozenou z redukční analýzy. Restartovací automaty se strukturovaným výstupem
představujeme v tomto příspěvku poprvé.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper deals with restarting automata, which are used as a formal framework for modeling the Functional Generative Description of Czech. The proposed model of restarting automaton works simultaneously with four layers of language description; it is intended to carry out reductive analysis of Czech sentences, thus allowing to derive the dependency relations in the sentence of the possible order of reduction. Standard model of restarting automaton is enriched with structured output, which allows us to build tectogrammatical dependency structure derived from the reduction analysis. Restarting automata with structured output are introduced for the first time.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek se zabývá restartovacími automaty,
které využíváme jako formální rámec pro Funkční generativní popis češtiny. Navrhovaný model restartovacího automatu pracující současně se
čtyřmi rovinami jazykového popisu je určeny k tomu, aby prováděl redukční analýzu  českých vět a umožnil tak odvodit závislostní vztahy ve větě z možných pořadí jednotlivých redukcí. Standardní model restartovacích automatů je zde obohacen o strukturovaný výstup, který umožňuje budovat
tektogramatickou závislostní strukturu odvozenou z redukční analýzy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper deals with restarting automata,
which we use as a formal framework for modeling the Functional Generative Description of Czech. The proposed model of restarting automaton works simultaneously with four layers of language description; it is intended to carry out reductive analysis of Czech sentences, thus allowing to derive the dependency relations in the sentence of the possible order of reduction. Standard model of restarting automaton is enriched with structured output, which allows to build tectogrammatical dependency structure derived from the reduction analysis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přesný morfologický popis slovních tvarů je prvním předpokladem pro úspěšné automatické zpracování jazykových dat.

Systém kategorií a jejich hodnot, které se k popisu používají, jsou náplní první části práce.

Základním principem je tzv. Zlaté pravidlo morfologie, které říká, že každý slovní tvar by měl být v systému popsán jednoznačně.
Existence variant na úrovni slovních tvarů i celých paradigmat však splnění tohoto pravidla komplikuje.
Koncept variant rozšiřujeme na tzv. mutace, mezi které řadíme i jiné množiny slovních tvarů se stejným popisem (např. víceré tvary osobních zájmen).
Mutace dělíme na globální pro popis na úrovni paradigmat a flektivní pro popis jednotlivých slovních tvarů.
Toto rozdělení nám umožňuje postihnout jejich časté kombinace.
Upouštíme od dělení variant (mutací) podle stylového příznaku jako neobjektivního kritéria.

V kapitole o lemmatizaci zavádíme vícenásobné lemma pro popis variantních lemmat.

Podrobně se zabýváme popisem tzv. složenin, tedy slovních tvarů typu zač, proň, koupilas, koliks.
Pro jejich lemmatizaci rovněž využíváme konceptu vícenásobného lemmatu.
Podle slovních druhů jejich složek je dělíme na několik typů.
Zabýváme se též problémem jejich vyhledávání v jazykových korpusech.

Druhá část práce popisuje systém vzorů pro popis slovních tvarů jednotlivých slovních druhů.
U každého vzoru uvádíme sadu parametrů, které umožní postihnout velkou variabilitu v tvoření konkrétních paradigmat.
Věnujeme se i pravidelnému odvozování příbuzných slov pomocí sufixů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Detailed morphological description of word forms represents one of the most important conditions of a successful automatic processing of linguistic data.

The system of categories and their values which are used for the description are the subject of the first part of the thesis.

The basic principle, so-called Golden rule of morphology, states that every word form has to be described by the system unambiguously.
The existence of variants of word forms and whole paradigms, however, complicates the accomplishment of this rule.
We introduce so called mutations as an extension of the variants to be able to include other sets of word forms with the same description (for instance multiple word forms of Czech personal pronouns).
We divide mutations into two parts - global ones describing all word forms of a paradigm, and inflectional ones for the description on the word form level.
This division enables us to express their various combinations.
We do not use features of style for the mutation division, for they are subjective.

With a consistent use of the categories called Inflectional Mutation and Global Mutation, the Golden rule of morphology will always be valid.

The concept of multiple lemma is introduced in a chapter dealing with lemmatization. It describes lemma variants.

We give a detailed description of so-called compounds, which incorporate word forms of the type zač, proň, koupilas, koliks.
The concept of multiple lemma is also used for their lemmatization.
According to the word class of their components we divide the compounds into several types.
We also deal with the problem of their searching in language corpora.

The second part of the thesis describes a system of patterns for word description. It is divided according to the part of speech.
Each pattern has a special set of parameters that allow to grasp a large variability in word formation.
We also deal with regular derivations of related words using suffixes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Mnoho nedokonavých (avšak ne iterativních) sloves má schopnost spojovat se s některými speciálními předponami a se zvratnou částicí se nebo si, a tím vytváří celé paradigma nových slovních tvarů s poměrně přesně definovaným významem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Many imperfective Czech verbs (but not iterative ones) is possible to concatenate with certain prefixes and the relative particle se or si, and create the whole paradigm of new wordforms with a relatively strict meaning.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek se zabývá lexikálním popisem frekventovaných, nicméně nepříliš kognitivně salientních (zde: sémanticky vyprázdněných) užití významových sloves jako jsou například jít, stát, držet, dát, položit z hlediska srovnání švédštiny s češtinou. Zvláštní pozornost je věnována užití těchto sloves v tzv. verbonominálních nebo analytických predikátech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper aims at a lexical description of frequent, but not enough cognitively salient uses of frequent lexical verbs in Swedish on the background of Czech, with some implications for the lexical description of basic verbs in general. It results in a draft of a production lexicon of Swedish basic verbs for advanced Czech learners of Swedish, with focus on their uses as light verbs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek se zabývá možnostmi tektogramatické anotace větných fragmentů v dialogu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Being confronted with spontaneous speech, our current annotation scheme requires alterations that would reflect the abundant use of non-sentential fragments with clausal meaning tightly connected to their context, which do not systematically occur in written texts. The purpose of this paper is to list the common patterns of non-sentential fragments and their con-texts and to find a smooth resolution of their semantic annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Abstrakt
Základní slovesa (basic verbs), tj. frekventovaná významová slovesa, jež zpravidla popisují fyzický pohyb, umístění, stav, nebo děj, procházejí řadou sémantických posunů, díky kterým se používají k vyjádření druhotných, přenesených významů. V
krajních případech se dané sloveso stává pomocným, způsobovým, nebo fázovým slovesem a přestávají pro ně platit kolokační omezení, jež se vztahují na sloveso užité v jeho primárním (tj. doslovném) významu. Tato užití sloves bývají většinou dobře dokumentována v gramatikách i učebnicích, stejně jako kvalitní slovníky podávají
podrobnou informaci o užití těchto sloves v ustálených frazeologických spojeních.
Mezi plně gramatikalizovaným užitím na jedné straně a idiomatickým, frazeologickým užitím na druhé straně však existuje celá škála užití základních sloves v přenesených významech, jejíž zvládnutí je pro nerodilého mluvčího značně obtížné: užití v přeneseném významu, jež mají omezenou kolokabilitu. To jsou především verbonominální konstrukce někdy nazývané analytické predikáty (light verb constructions), ale také užití, která za určitých omezených morfosyntaktických podmínek (např. pouze
v negaci) aktivují abstraktní sémantické rysy u jiných predikátů, např. zesilují význam,
nebo implikují, že daný děj již trvá dlouho, a podobně. Tato druhotná užití významových sloves (ve švédštině) většinou nepůsobí (českým pokročilým studentům švédštiny) potíže při porozumění textu, neboť bývají sémanticky transparentní, avšak tím, že jsou specifická pro konkrétní jazyk a v zásadě neprediktabilní na základě znalosti jiného jazyka, působí problémy při produkci textu. Rodilí mluvčí sami
je většinou nevnímají nebo jejich typické kontexty považují za frazeologismy, a proto
jim ani při výuce cizích studentů nevěnují dostatečnou pozornost.
Předkládaná práce se zaměřuje na švédská základní slovesa z kontrastivního pohledu českého pokročilého studenta švédštiny. Pozorování vybraných slovesných konstrukcí zobecňuje do návrhu struktury elektronického švédsko-českého slovníku zaměřeného na pochopení a osvojení méně zřejmých, ale přesto frekventovaných konstrukcí se
základními slovesy. Slovník je zakotven ve valenční teorii Funkčního Generativního
Popisu, spojené s kolokační analýzou podle sémanticky motivovaných principů Analýzy
Korpusových Vzorců (Corpus Pattern Analysis), jež umožňuje přesnější definici jednotlivých
slovesných užití pomocí jejich typických kolokátů.
Slovník sestává ze dvou vzájemně propojených částí: slovníku sloves SweVallex (vytvořeného na základě českého Vallexu) a slovníku nominálních částí analytických predikátů (Predicate Noun Lexicon). Slovesné kolokáty jednotlivých nominálních komponent jsou roztříděny podle Mel´čukových Lexikálních Funkcí. Kromě toho je u každého analytického predikátu, pokud je to možné, uvedena informace o jeho telicitě, o tom, zda děj je okamžitý, nebo spíše durativní (punctuality), a o tom, jestli jeho subjekt za normálních okolností jedná vědomě a ze své vůle (volitionality). Zvláštní pozornost je věnována morfosyntaktickému chování nominálních komponent ve spojení s jednotlivými slovesnými kolokáty (užití členů, možnosti rozvití).
Studie opouští slovník ve chvíli, kdy je jeho struktura navržena a prověřena na
příkladech. Pro umožnění kvalitní rutinní práce byly připraveny lingvistické zdroje.
Veřejně dostupný švédský korpus PAROLE byl lemmatizován pomocí zvlášť vyvinutého
pravidlového lemmatizátoru a vložen do korpusového rozhraní Bonito, v němž
je zabudován nástroj na automatickou kolokační analýzu – Word Sketch Engine.
Tento nástroj bylo zapotřebí adaptovat na švédštinu, což bylo také předmětem této
dizertační práce. V rámci jiného projektu vzniká paralelní švédsko-český korpus, který má v současné době přibližně 2 miliony tokenů. Ten byl v nejvyšší možné míře využit v kontrastivním pozorování vybraných slovesných konstrukcí. Doplňkově byl využíván také Český národní korpus.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Basic verbs, i.e. very common verbs that typically denote physical movements, locations,
states or actions, undergo various semantic shifts and acquire different secondary
uses. In extreme cases, the distribution of secondary uses grows so general that
they are regarded as auxiliary verbs (go and to be going to), phase verbs (turn, grow),
etc. These uses are usually well-documented by grammars and language textbooks,
and so are idiomatic expressions (phraseologisms) in dictionaries.
There is, however, a grey area in between, which is extremely difficult to learn for
non-native speakers. This consists of secondary uses with limited collocability, in particular
light verb constructions, and secondary meanings that only get activated under
particular morphosyntactic conditions. The basic-verb secondary uses and constructions
are usually semantically transparent, such that they do not pose understanding
problems, but they are generally unpredictable and language-specific, such that they
easily become an issue in non-native text production.
In this thesis, Swedish basic verbs are approached from the contrastive point of
view of an advanced Czech learner of Swedish. A selection of Swedish constructions
with basic verbs is explored. The observations result in a proposal for the structure
of a machine-readable Swedish-Czech lexicon, which focuses on basic verbs and their
constructions. The lexicon is anchored in the valency theory of the Functional Generative
description, coupled with analysis of collocations according to the semantically
motivated principles of Corpus Pattern Analysis, in order to achieve the necessary
level of delicacy to make meaning distinctions correctly.
The lexicon consists of two parts: SweVallex, which is a lexicon of verb frames, and
a Predicate Noun Lexicon, which captures predicate nouns (the nominal components
of light verb constructions). These two parts are interlinked. The verb collocates
of predicate nouns are sorted according to the Mel’čukian Lexical Functions. Features
such as telicity, punctuality, and volitionality are described for each light verb
construction, whenever possible. Special attention is paid to the morphosyntactic
behavior of the respective predicate nouns (determiner use, and modifier insertion).
In order to facilitate the routine of building such a lexicon, the 20-million morphosyntactically
annotated Swedish corpus PAROLE was lemmatized and loaded into the corpus GUI Bonito, which includes the Word Sketch Engine, a tool for automatic collocation analysis. Word Sketch Definitions for Swedish were created and loaded
into the Word Sketch Engine. In addition to the PAROLE corpus, a two-million parallel
Swedish-Czech corpus was used, which has been built within a different project.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje aktuální stav ruční tektogramatické anotace korpusu PEDT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper gives an overview of the current state of the Prague English Dependency Treebank project. It is an updated version of a draft text that was released along with a CD presenting the first 25\% of the PDT-like version of the Penn Treebank -- WSJ section (PEDT 1.0).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek pojednává o konstrukci a anotaci velkých sémantických sítí zachycujících obsah jazykových sdělení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a large-scale semantic-network annotation effort based on the MutliNet formalism.
Annotation is achieved via a process which incorporates several independent tools including a MultiNet graph editing tool, a semantic concept lexicon, a user-editable knowledge-base for semantic concepts, and a MultiNet parser. We present an evaluation metric for these semantic networks, allowing us to determine the quality of annotations in terms of inter-annotator agreement. We use this metric to report the agreement rates for a pilot annotation effort involving three annotators.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje novou metodu automatické detekce inkonzistentních anotací v korpusu obsahujícím komplexní ruční anotaci. Použitá metoda je založena na algoritmu Apriori. V článku předložíme vyhodnocení této techniky v ručně anotovaném korpusu PDT 2.0, analyzujeme chyby a ukážeme, že 20 z prvních 100 nalezených uzlů obsahovalo anotační chybu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a new method for automated discovery of inconsistencies in a complex manually annotated corpora. The proposed technique is based on Apriori algorithm for mining association rules from datasets. By setting appropriate parameters to the algorithm, we were able to automatically infer highly reliable rules of annotation and subsequently we searched for records for which the inferred rules were violated. We show that the violations found by this simple technique are often caused by an annotation error. We present an evaluation of this technique on a hand-annotated corpus PDT 2.0, present the error analysis and show that in the first 100 detected nodes 20 of them contained an annotation error.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Automatická identifikace předložkových a nepředložkových pádů v současné češtině na základě lingvistických pravidel a rozsáhlých korpusů současné češtiny</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Automatic rule-based identification of prepositional and non-prepositional cases in contemporary Czech based on large corpora of contemporary Czech</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládáme naše první experimenty s detekcí a automatickými opravami ručních anotací anglických textů převzatých z Penn Treebanku, na závislostní tektogramatické rovině, jak je definovaná v Pražském závislostním korpusu. Hlavní myšlenkou je, že anotační chyby jsou obvykle důsledkem nekonzistence, tj. stavu kdy jeden jev je na různých místech v korpusu anotován různě.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our first experiments with detecting and correcting errors in a manual annotation of English texts, taken from the Penn Treebank, at the dependency-based tectogrammatical layer, as it is defined in the Prague Dependency Treebank. The main idea is that errors in the annotation usually result in an inconsistency, i.e. the state when a phenomenon is annotated in different ways at several places in a corpus. We describe our algorithm for detecting inconsistencies (it got positive feedback from annotators) and we present some statistics on the manually corrected data and results of a tectogrammatical analyzer which uses these data for its operation. The corrections have improved the data just slightly so far, but we outline some ways to more significant improvement.(1)</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Projekt obsahuje morfologický analyzátor pro quechua, syntetizátor pro angličtinu a slovník quechua-angličtina.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This project provides a morphological analyzer for Quechua, a morphological synthesizer for English, as well as a bilingual Quechua-Spanish dictionary and a reference implementation of a Quechua-to-English MT system for OS X.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Experimentální překladový systém španělština-angličtina pro mobilní telefony.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This project is an experimental MT system (Spanish-to-English) optimized for smartphones.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kniha zkoumá roli syntaktické analýzy ve strojovém překladu mezi příbuznými jazyky a snaži se najít limity metod mělkého překladu. Zaměřujeme se na baltoslovanské jazyky a hybridní architekturu MT s převážně pravidlovými moduly.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This book explores the contribution of syntactic analysis to the machine translation (MT) between related languages and it also attempts to explore the limits of shallow MT methods. We focus on one group of languages, the Balto-Slavic language family, and one MT architecture, namely hybrid systems with prevalently rule-based modules.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Práce zkoumá roli syntaktické analýzy ve strojovém překladu mezi příbuznými jazyky a snaži se najít limity metod mělkého překladu. Zaměřujeme se na baltoslovanské jazyky a hybridní architekturu MT s převážně pravidlovými moduly.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This thesis explores the contribution of syntactic analysis to the machine translation (MT) between related languages and it also attempts to explore the limits of shallow MT methods. We focus on one group of languages, the Balto-Slavic language family, and one MT architecture, namely hybrid systems with prevalently rule-based modules.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje jednoduchou evaluační metriku pro strojový překlad, která se snaží vyřešit známé nedostatky standardně používané metriky BLEU.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes a simple evaluation
metric for MT which attempts to overcome
the well-known deficits of the standard
BLEU metric from a slightly different angle.
It employs Levenshtein’s edit distance
for establishing alignment between
the MT output and the reference translation
in order to reflect the morphological
properties of highly inflected languages. It
also incorporates a very simple measure
expressing the differences in the word order.
The paper also includes evaluation on
the data from the previous SMT workshop
for several language pairs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje architekturu systému strojového překladu pro slovanské jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes an architecture of a machine translation system designed
primarily for Slavic languages. The architecture is based upon a shallow transfer
module and a stochastic ranker. The shallow transfer module helps to resolve the
problems, which arise even in the translation of related languages, the stochastic
ranker then chooses the best translation out of a set provided by a shallow
transfer. The results of the evaluation support the claim that both modules newly
introduced into the system result in an improvement of the translation quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku popisujeme první etapu budování překladového
systému mezi čestinou a ruštinou, implementovaného v rámci
Česílka. Představujeme nástroje a data použitá v tomto projektu,
jmenovitě česko-ruský slovník a modul syntaktického transferu.
První výsledky Česílka, 1000 testovacích vět byly vyhodnoceny na
základě automaticke metriky BLEU a zároveň i lidského hodnocení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper is devoted to the ongoing research of the Machine Translation between Czech and Russian implemented in the system Česílko. We will describe the tools and data used in the project, namely the Czech-Russian dictionary and syntactic transfer module. As first results 1000 test sentences were evaluated by automatic metrics BLEU as well as by human evaluation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vystoupení shrnulo historii metod použitých při budování komerční aplikace kontroly gramatické správnosti českých textů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk provided a brief summary of the history of methods applied in the development of a grammar checker for Czech from initial ideas to the implementation as an indistrial application being used in Microsoft Office. The original idea of directly exploiting restarting automata for the error identification and localization in a language with a high degree of word order freedom (Czech) proposed in 1994 responded to a fact that the standard error checking methods using local error patterns were not applicable to such kind of a natural language. The concept of using a restarting automaton naturally allowed to simplify input sentences step by step by preserving an invariant of its correctness/incorrectness. Although the original automaton has transfgormed to a Robust Free-Order Dependency Grammar, it still retained this basic property.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek zdůvodňuje myšlenku, že ačkoli obor automatické analýzy přirozených jazyků byl prakticky ovládnut statistickými metodami, je stále důležité se zabývat syntaktickou analýzou pomocí ručně psaných pravidel. Tato metoda má stále co říci v oblasti teorie formálního popisu přirozených jazyků stejně jako v některých typech aplikací, např. kontrole gramaticky nesprávného vstupu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article tries to advocate the fact that although the field of rule-based syntactic analysis of natural languages has recently been practically taken over by data-driven (mostly stochastic) methods. it is still important both for the theory of formal description of natural languages as well as for certain types of applications, especially those dealing with an ill-formed input. The arguments are based upon the experience gained in the process of development a pilot implementation of a grammar checker of Czech. Although the methods described in the paper did not lead directly to a commercial application, they definitely increased the level of understanding of certain complicated linguistic phenomena, namely the phenomenon of grammaticality and non-projectivity of Czech sentences.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Informace o valenci sloves je podstatná pro mnoho odvětví NLP. Existuje proto již několik valenčních slovníků. V tomto článku představíme dva z nich (VALLEX a PDT-VALLEX), které jsou k disposici v elektronické podobě a které mají společné východisko. Oba mají své přednosti a naším cílem je spojit je v jeden slovník.

Máme k disposici data z korpusu PDT, kterým jsou ručně přiřazeny položky prvního ze slovníků. To nám pomůže provázat oba slovníky přes data využívajíce automatické identifikace (následované ruční prací s problémovými případy). Tímto poloautomatickým spojením dvou slovníků vznikne kvalitní lexikografický zdroj, který by jinak vyžadoval mnohem více lidské práce.

Průměrná úspěšnost namapování rámce z jednoho slovníku výběrem náhodného rámce z druhého je přibližně 60 %.

Článek se také zmiňuje o universálním formátu, ve kterém bude výhodné nová data ukládat odděleně od stávajících.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we present two valency lexicons (namely VALLEX and PDT-VALLEX). Our aim is to link them together. We can use data annotated by the first lexicon, which helps us to link some entries automatically.
Universal format for stand-off storage of this type of data is also mentioned here.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje anotaci víceslovných výrazů a víceslovných pojmenovaných entit v Pražském závislostním treebanku. Zahrnuje statistiky týkající se dat a mezianotátorskou shodu. Ukážeme také snadný způsob prohledávání a zobrazení anotací, ačkoli jsou úzce spjaty s hloubkově syntaktickým treebankem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe the annotation of multiword expressions and multiword named entities in the Prague Dependency Treebank. This paper includes some statistics of data and inter-annotator agreement. We also present an easy way to search and view the annotation, even if it is closely connected with deep syntactic treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dobře provedené rozlišení smyslu slova (Word Sense Disambiguation, WSD) je důležitým prvním krokem pro další úlohy NLP, jako je strojový překlad.
V tomto článku prezentujeme přístup k WSD v češtině pomocí Průměrovaného perceptronu.
Použili jsme data anotovaná (a do jisté míry opravená) synsety z českého WordNetu. Získali jsme 100 000 výskytů anotovaných slov v celých větách. Jako baseline jsme přiřadili ke každému výskytu jeho nejčastější synset. Zkusili jsme jak nejčastější synset pro dané slovo, tak pro lemma, které bylo výsledkem morfologické analýzy dat.
Pro naše experimenty jsme použili systém Morče, 
For our experiments we used the system Morče, který je založen na skrytých markovovských modelech a průměrovaném perceptronu.
Udělali jsme tři experimenty a překonali baseline ve všech z nich.
Nejprve jsme vzali data tak, jak jsou (s ruční morfologickou anotací) a dosáhli úspěšnosti 94,2% (s baseline 87,9 %). Poté jsme vzali holá vstupní data a přiřadili synsety slovním formám. Výsledek byl 90,7 %, oproti baseline 61,7 %. Nakonec jsme vstup doplnili morfologickým taggerem a dosáhli 94,2 % (při zachování baseline 61,7 %).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Well performed Word Sense Disambiguation is an important first step for other NLP tasks, such as machine translation or information retrieval. In our paper we present an Averaged Perceptron approach to WSD for Czech.
We used data annotated (and to some extend also corrected) by Czech WordNet synsets. We obtained 100,000 occurences of annotated words in whole sentences. As a baseline we assigned the most frequent synset to each occurence. We tried both the most frequent synset for a given word form and for a lemma, as the morphological analysis was done for input data.
For our experiments we used the system Morče, which is based on the Hidden Markov Model and the Averaged Perceptron.
We made three experiments and exceeded baselines in all of them. First we took data as it is (i.e. with manual morphological annotation); we achieved 94.2% (with 87.9% baseline). Then we used a bare input data and assigned synsets to word forms. Our result is 90.7%, compared with 61.7% baseline. Last we enriched the input by a morphological tagger and achieved 94.2% (the baseline remained 61.7%).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Disertační práce se zabývá významy kondicionálu v dnešní češtině s cílem začlenit tyto významy do popisu větné sémantiky, tak jak je postulován ve Funkčním generativním popisu. Z široké oblasti modality, kam významy kondicionálu spadají, Funkční generativní popis ovšem dosud věnoval pozornost především významům modálních sloves, při zkoumání funkcí kondicionálu proto budeme vycházet z koncepce Mluvnice češtiny. Na základě korpusového materiálu analyzujeme primární funkci kondicionálu a některé jeho funkce sekundární. Po této analýze navrhneme formální prostředek, díky kterému je možné primární význam kondicionálu zachytit v rámci sémantické anotace. Nastíníme rovněž problémy spojené s reprezentací sekundárních funkcí kondicionálu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The doctoral thesis deals with functions of the conditional mood in contemporary Czech texts. Two theoretical approaches to modality are described in more detail in Chapters 2 and 3, the approach of Grammar of Czech and that of Functional Generative Description (FGD), respectively. After a brief overview of terminology and corpus data used for the analysis of the conditional (cf. Chapter 4), we focus particularly on the primary function of this mood in Chapter 5. The conditional is primarily used to refer to events (also states etc.) which may be generally characterized as hypothetical. Besides this function, the conditional bears also other, secondary meanings in Czech (cf. Chapter 6). In Chapter 7, we propose a formal means for capturing the primary function of the conditional mood within a deep-syntactic annotation of a sentence. The problems connected with representation of the secondary functions of the conditional are also sketched. Conclutions are included in Chapter 8.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kniha se zabývá významy kondicionálu v dnešní češtině s cílem začlenit tyto významy do popisu větné sémantiky, tak jak je postulován ve Funkčním generativním popisu (FGP). Z široké oblasti modality, kam významy kondicionálu spadají, FGP ovšem dosud věnoval pozornost především významům modálních sloves, při zkoumání funkcí kondicionálu proto budeme vycházet z koncepce Mluvnice češtiny. Na základě korpusového materiálu analyzujeme primární funkci kondicionálu a některé jeho funkce sekundární. Po této analýze navrhneme formální prostředek, díky kterému je možné primární význam kondicionálu zachytit v rámci sémantické anotace. Přiblížíme rovněž problémy spojené s reprezentací sekundárních funkcí kondicionálu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present book deals with meanings of the conditional mood in contemporary Czech texts in order to include these meanings in the description of sentence semantics as proposed in Functional Generative Description (FGD). Since in FGD main attention has been paid to modal verbs, our analysis of meanings of the conditional is grounded on the detailed conception of modality as presented in Grammar of Czech. The primary and some secondary meanings of the conditional mood are analyzed on the basis of language data from two corpora, namely from the Prague Dependency Treebank 2.0, the annotation scenario of which was based on FGD, and from the SYN2005 corpus. We propose formal means for capturing the primary function of the conditional mood within a deep-syntactic annotation of a sentence. The problems connected with representation of the analyzed secondary functions of the conditional are also sketched.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kondicionál je jedním ze slovesných způsobů, v současné češtině plní celou řadu funkcí. V příspěvku se zaměřujeme na primární význam kondicionálu, který spočívá ve vyjadřování hypotetických dějů. Protože tento význam dosud nebyl – přes jeho sémantickou relevanci –zohledňován při popisu větného významu v Pražském závislostním korpusu 2.0 (PDT 2.0), pokusíme se navrhnout nový prostředek, který to umožní.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The conditional form is one of the moods of Czech verbs, and it renders several meanings in contemporary Czech texts (Sect. 2). The present paper focuses on the primary function of this mood, which is to express hypothetical events (Sect. 3). In Section 4, we briefly mention how modality has been treated up to now in PDT 2.0 and some other treebanks and finally in Section 5 we propose a new way how the primary meaning of the conditional mood should be captured in the annotation scheme of the tectogrammatical layer of PDT 2.0.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje proces efektivně využívající manuálních překladů pro konstrukci doménově specifických lexikálních databází.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Thesauri and controlled vocabularies facilitate access to digital collections by explicitly representing the underlying principles of organization. Translation of such resources into multiple languages is an important component for providing multilingual access. However, the specificity of vocabulary terms in most thesauri precludes fully-automatic translation using general-domain lexical resources. In this paper, we present an efficient process for leveraging human translations to construct domain-specific lexical resources. This process is illustrated on a thesaurus of 56,000 concepts used to catalog a large archive of oral histories. We elicited human translations on a small subset of concepts, induced a probabilistic phrase dictionary from these translations, and used the resulting resource to automatically translate the rest of the thesaurus. Two separate evaluations demonstrate the acceptability of the automatic translations and the cost-effectiveness of our approach.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozšířující modul pro editor TrEd, který umožní otevřít a zobrazit tmt soubory používané v systému TectoMT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Tree Editor TrEd extension provides ability to open and display *.tmt files used within the Natural Language Processing framework TectoMT.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá anotací vybraných nezávislostních vztahů v závislostním treebanku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The following paper has two aims. First, it introduces a procedure of a manual annotation of selected linguistic phenomena across a large-scale dependency treebank of English. The method was designed to provide higher consistency of annotated data, and so higher credibility of the treebank. Second, the first expert task completed by means of this method is being described – the annotation of rhematizers and discourse connectives and their modifiers, i.e. annotation of some non-dependency relations in a dependency approach.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek prezentuje dvě metody omezení komplexity prostoru víceznačných překladových hypotéz v systému strojového překladu s mělkým transferem bez taggeru.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article presents two automatic methods that
reduce the complexity of the ambiguous space introduced by the omission of the part of speech
tagger from the architecture of a shallow machine
translation system. The methods were implemented in a fully functional translation system
for related languages. The language pair chosen for the experiments was Slovenian-Serbian
as these languages are highly inflectional with
morphologically ambiguous forms. The empirical evaluations show an improvement over the
original system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku předkládáme některá pozorování týkající se sémantických charakteristik sloves vyžadujících směrové valenční doplnění. Slovesa byla komparačně studována na materiálu PDT a PCEDT. Více než 500 českých sloves se směrovým doplněním ve valenčním rámci bylo klasifikováno na základě sémantických rysů jejich doplnění typu DIR1, DIR2 nebo DIR3. Výsledky byly porovnány s chováním odpovídajících anglických sloves ve slovníku Engvallex.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present some of our observations on the semantic characteristics of verbs requiring some kind of directional valency complementation. The verbs have been studied on the Prague Dependency Treebank and Prague Czech-English Dependency Treebank material. More than 500 Czech verbs with a directional specification in the valency frame that appeared in the Prague Dependency Treebank and PDT-VALLEX valency lexicon have been classified according to the semantic features of their complementations represented as DIR1 („from“), DIR2 („through“) or DIR3 („to“) labels. The results have been compared to the behaviour of corresponding English verbs appearing in the EngValLex valency lexicon.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje současnou fázi projektu spojování dvou existujících elektronických valenčních slovníků v multilingvální valenční slovník. Základním cílem projektu je propojení PDT-Vallexu a Engvallexu, dvou slovníků užívaných při anotacích korpusu PCEDT. Výsledek projektu by měl poskytnout a) slovník překladových slovesných ekvivalentů, b) lingvistické poznatky o jejich valenčních charakteristikách, shodách a rozdílech mezi nimi a c) produkt vhodný k implementaci do experimentů se strojovým překladem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In my talk I will present an ongoing project focused on building a multilingual valency lexicon. The initial goal of the project is to bring together, refine and interlink two already existing electronic valency dictionaries, PDT-Vallex (valency characteristics of selected Czech verbs) and EngValLex (valency characteristics of selected English verbs). These dictionaries are embedded in a parallel corpus, the so called Prague Czech-English Dependency Treebank. The intended product should provide a) a dictionary of (rough) translational equivalents, b) important linguistic information about relations between valency characteristics of the equivalents (and possibly some more general information about the relation between verbal valency behaviour of the two language systems), and c) another device to be implemented into the proceeding experiments with automated machine translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje probíhající projekt budování dvoujazyčného valenčního slovníku v teoretickém rámci Funkčního generativního přístupu. Tento dvoujazyčný slovník vzniká jako výsledek propojení rámců a jednotlivých prvků rámců ve dvou již existujících valenčních slovnících.

Nejprve představíme oba spojované slovníky, poté vysvětlíme proces spojování rámců a v závěru uvedeme případovou studii, která dokládá, jakým způsobem dvoujazyčný valenční slovník ozřejmuje mezijazykové rozdíly obecně a jak jeho vznik přispívá k rozvíjení valenční teorie.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes an ongoing project of building a bilingual valency 
lexicon in the framework of Functional Generative Description. The bilingual 
lexicon is designed as a result of interlinking frames and frame elements of 
two already existing valency lexicons. 

First, we give an overall account of the character of the lexicons to be linked,
second, the process of frame linking is explained, and third, a case 
study is presented to exemplify what the information contained in frame 
links tells us about crosslinguistic differences in general and the linguistic 
theory applied.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá extrakcí skrytých informací o struktuře souvětí z Pražského závislostního korpusu. Jednotlivé klauze a jejich vzájemné vztahy totiž v tomto korpusu nejsou explicitně oznčkovány a proto bylo nutné vyvinout automatickou metodu, která je schopna transformovat původní anotaci, týkající se téměř výlučně syntaktických rolí jednotlivých slovních forem, do schématu popisujícího vztahy mezi jednotlivými klauzemi ve složených souvětích. Tento úkol byl komplikován jistým stupněm nekonzistencí v původní anotaci právě vzhledem ke klauzím a jejich struktuře. Článek popisuje metodu odvozování informací o klauzích z existující anotace a vyhodnocení úspěšnosti této metody.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper concentrates on deriving non-obvious information about clause structure of complex sentences from the Prague Dependency Treebank. Individual clauses and their mutual relationship are not explicitly annotated in the treebank, therefore it was necessary to develop an automatic method transforming the original annotation concentrating on the syntactic role of individual word forms into a scheme describing the relationship between individual clauses. The
task is complicated by a certain degree of inconsistency in original annotation with regard to clauses and their structure. The paper describes the method of deriving clause-related information from the existing annotation and its
evaluation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se soustředí na získávání skrytých vztahů ve složených větách Pražského závislostního korpusu. Tento korpus obsahuje informace o vzájemných vztazích mezi slovy a dalšími prvky ve větě (interpunkce apod.), ale explicitně nezachycuje vztahy mezi složitějšími jednotkami (klauzemi). Pro další experimenty s klauzemi a jejich částmi (segmenty), bylo nutné vyvinout automatickou metodu transformující původní anotaci s ohledem na klauze a jejich strukturu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper concentrates on obtaining hidden relationships among individual clauses of complex sentences from the Prague Dependency Treebank. The treebank contains only an information about mutual relationships among individual tokens (words, punctuation marks), not about more complex
units (clauses). For the experiments with clauses and their parts (segments) it was therefore necessary to develop an automatic method transforming the original annotation into a scheme describing the syntactic relationships between
clauses. The task was complicated by a certain degree of inconsistency in original annotation with regard to clauses and their structure. The paper describes the algorithm of deriving
clause-related information from the existing annotation and its evaluation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Postavení přísudkového slovesa je jedním z centrálních témat slovosledu ve starší češtině. V reprezentativním korpusu jazykového materiálu analyzuje autorka z kvantitativního hlediska postavení slovesa ve starší češtině obecně, ve specifických syntaktických konstrukcích a ve vybraných dílech. Tato analýza je nejen mimořádně významným přínosem k výzkumu starší češtiny a jeho metodologii, ale současně i exaktní metodologickou základnou pro další výzkum slovosledu v češtině i dalších jazycích. Monografie zároveň přináší podněty a podklady pro další rozvoj české historické stylistiky a slovosledné typologie.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Description of the placement of synthetic forms of verbal predicates in Czech during the period 1500-1620</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>srovnání paralelních ručních a automatických anotací aktuálního členění českých vět v souvislém textu jako podklad pro automatickou anotaci akutálního členění</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>comparison of parallel manual and automatic annotations of Topic-Focus Articulation in Czech sentences as a background for automatic annotation of Topic-Focus Articulation</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek předkládá a srovnává přístupy pro měření podobnosti slov založené na distributivních metodách a WordNet metodách. Článek obsahuje diskuzi o výhodách a nevýhodách obou přístupů při odhadování podobnosti a příbuznosti slov a prezentuje kombinaci obou metod. Každá z našich metod nezávisle dosahuje nejlepších výsledků ve své třídě na datech RG a WordSim353, přičemž jejich kombinace dosahuje nejlepších zatím publikovaných výsledků na obou množinách dat. Na závěr předkládáme metodu pro měření podobnosti napříč jazyky a ukazujeme, že naše metody lze snadno rozšířit na vícejazyčné úkoly pouze s malou ztrátou úspěšnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents and compares WordNet based
and distributional similarity approaches.
The strengths and weaknesses of each approach
regarding similarity and relatedness
tasks are discussed, and a combination is presented.
Each of our methods independently
provide the best results in their class on the
RG and WordSim353 datasets, and a supervised
combination of them yields the best published
results on all datasets. Finally, we pioneer
cross-lingual similarity, showing that our
methods are easily adapted for a cross-lingual
task with minor losses.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek studuje soulad automatických metrik strojového překladu s ručním hodnocením pro překlad do četštiny. Kromě vyhodnocení několika známých metrik zavádíme vlastní, která dosahuje výborných výsledků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the present work we study semi-automatic evaluation techniques of machine
translation (MT) systems. These techniques are based on a comparison of the MT
system's output to human translations of the same text. Various metrics were
proposed in the recent years, ranging from metrics using only a unigram
comparison
to metrics that try to take advantage of additional syntactic or semantic
information. The main goal of this article is to compare these metrics with
respect to their correlation with human judgments for Czech as the target
language and to propose the best ones that can be used for an evaluation of MT
systems translating into Czech language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato diplomová práce popisuje strojový překlad z angličtiny do češtiny implementovaný v systému TectoMT.
Překlad je založen na transferu přes tektogramatickou rovinu a využívá anotační schéma Pražského závislostního korpusu.

Prvotním cílem práce je zlepšení kvality překladu za pomoci pravidlového přístupu i statistických metod.
Nejprve je popsána ruční anotace překladových chyb ve vzorku 250 vět a následná analýza častých typů chyb a jejich příčin.
Hlavní část textu pak popisuje návrh a provedení úprav, které vedly k vylepšení tří fází překladu: analýzy, transferu a syntézy.
Nejvýraznější inovací je využití stromové modifikace skrytých Markovových řetězců (Hidden Markov Tree Models) ve fázi transferu.
Dosažené zlepšení je kvantitativně vyhodnoceno pomocí metrik BLEU a NIST.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This thesis describes English-Czech Machine Translation as it is implemented in TectoMT system.
The transfer uses deep-syntactic dependency (tectogrammatical) trees and exploits the annotation scheme of Prague Dependency Treebank.

The primary goal of the thesis is to improve the translation quality using both rule-base and statistical methods.
First, we present a manual annotation of translation errors in 250 sentences and subsequent identification of frequent errors, their types and sources.
The main part of the thesis describes the design and implementation of modifications in the three transfer phases: analysis, transfer and synthesis.
The most prominent modification is a novel approach to the transfer phase based on Hidden Markov Tree Models (a tree modification of Hidden Markov Models).
The improvements are evaluated in terms of BLEU and NIST scores.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek shrnuje nové výsledky v anglicko-českém strojovém překladu implementovaném v systému TectoMT. Obsahuje analýzu překladových chyb a návrh nového řešení transferu pomocí stromových HMM.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper summarizes our recent results concerning English-Czech
Machine Translation implemented in the TectoMT framework. The system
uses tectogrammatical trees as the transfer medium.
A detailed analysis of errors made by the previous version of the
system (considered as the baseline) is
presented first. Then we describe a number of improvements of the system that
led to better translation quality in terms of BLEU and NIST scores.
The biggest performance gain comes from applying Hidden
Tree Markov Model in the transfer phase, which is a novel technique
in the field of Machine Translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Compost Dutch je nástroj pro morfologické značkování holandštiny založený na taggeru Morče. Je trénován na holandské části korpusu CGN. Výsledný tagger svojí úspěšností (97.27 %) překonal dosavadní publikované taggery.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Compost Dutch is a tool for POS tagging of Dutch  based on Morče tagger. It is trained on Dutch part of CGN corpus. The resulting tagger obtained accuracy 97.27 % and thus overcome other previously published taggers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci prezentujeme holandský morfologický tagger, který překonal úspěšnost předchozích taggerů. Je založen na průměrovaném perceptronu a natrenován na korpusu CGN (pouze na holandské části). Úspěšnost na náhodně zvolených evaluačních datech dosáhla 97,2 %, což představuje více než 1% redukci chyb oproti předchozímu taggeru (Bosch a kol., 2006).
Přestože nelze určit signifikanci zlepšení úspěšnosti, COMPOST Dutch má několik dalších výhod. Algoritmus je implementován jako samostatný program, který je snadno spustitelný a rychlý (zpracuje okolo 100 tisíc slov za minutu).
COMPOST je volně stažitelný z našeho webu pro výzkumné účely. Funguje jako pod Linuxem, tak pod Windows.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this work we present a better than state-of-the-art POS tagger developed for Dutch. It is based on Averaged Perceptron algortithm  and trained on CGN corpus (Dutch part only so far). The accuracy on randomly selected eval-test data is 97.2%, which represents a 1% error reduction compared to previous work (Bosch et al., 2006).
Although the improvement of accuracy may not be significant compared, COMPOST Dutch has few more benefits. The algortithm is implemented as a stand-alone program, which is easy to use and quite fast - it can process about 100k words per minute. COMPOST is freely downloadable from our website for research purpose. It works both under Linux and Windows platform.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek je shrnutím PhD disertace Spoustova07 a rozšířením článku SpoustovaEtAl07. Popisuje několik metod značkování, které kombinují ručně psaná pravidla a stochastické taggery.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article is an extract of the PhD thesis Spoustova07 and it extends the article SpoustovaEtAl07. Several hybrid disambiguation methods are described which combine the strength of hand-written disambiguation rules and statistical taggers. Three different statistical taggers (HMM, Maximum-Entropy and Averaged Perceptron) and a large set of hand-written rules are used in a tagging experiment using Prague 
Dependency Treebank. The results of the hybrid system are better than any other method tried for Czech tagging so far.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Morphium je nástroj zmenšující tagset Penn treebanku na seznam tagů teoreticky použitelných pro dané slovo. Morphium je napsáno v Perlu a je dostupné pod GPL licencí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Morphium is a tool which reduces the full Penn Treebank POS tagset to a list of tags theretically plausible for each word. The analyzer still overgenerates a lot, but its precision is much higher than the precision of the full tagset, keeping the same recall.

Morphium is written in Perl and is available under the GPL license.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Compost English je nástroj kombinující morfologický analyzátor Morphium a tagger Morče za užití nové metody semi-supervised trénování. Výsledný tagger má nejlepší výsledky pro angličtinu (97.43 %).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Compost English is a tool which combines the Morphium morphological analyzer and the Morce tagger using an innovative semi-supervised training method. The resulting tagger gives the best accuracy achieved for English (on standard PTB data set) so far: 97.43 %

Compost English is written in Perl and C and is available for registered users only. However, the default package does not contain the Morce tagger source code, which you can dowload directly from the Morce website.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku prezentujeme metodiku použitou pro levnou kontrolu kvality morfologického značkování Pražského závislostního korpusu založenou na vícenasobném ručním přeznačkování s pomocí několika různých taggerů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In our paper we present a methodology used for low-cost validation of quality of Part-of-Speech annotation of the Prague Dependency Treebank based on multiple re-annotation of data samples carefully selected with the help of several different Part-of Speech taggers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem této práce je implementovat a zhodnotit softwarový nástroj pro automatické zarovnávání (alignment) českých a anglických tektogramatických stromů. Úkolem je najít odpovídající si uzly stromů, které reprezentují anglickou větu a její český překlad.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The goal of this thesis is to implement and evaluate a software tool for automatic alignment of Czech and English tectogrammatical trees. The task is to find correspondent nodes between two trees that represent an English sentence and its Czech translation. Great amount of aligned trees acquired from parallel corpora can be used for
training transfer models for machine translation systems. It is also useful for linguists in
studying translation equivalents in two languages. In this thesis there is also described
word alignment annotation process. The manual word alignment was necessary for evaluation of the aligner. The results of our experiments show that shifting the alignment task from the word layer to the tectogrammatical layer both (a) increases the inter-annotator agreement on the task and (b) allows to construct a feature-based algorithm which uses sentence structure and which outperforms the GIZA++ aligner in terms of f-measure on aligned tectogrammatical node pairs. This is probably caused by the fact that tectogrammatical representations of Czech and English sentences are much closer compared to the
distance of their surface shapes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek je zaměřen na zarovnávání paralelních českých a anglických tektogramatických stromů. Experiment ukazuje, že přesunutí ulohy zarovnávání na tektogramatickou rovinu umožní dosáhnout vyšší mezianotátorské shody.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we focus on alignment of Czech and English tectogrammatical dependency trees. The alignment of deep syntactic dependency trees can be used for training transfer models for machine
translation systems based on analysis-transfer-synthesis architecture. The results of our experiments show that shifting the alignment task from the word layer to the tectogrammatical layer both (a) increases the inter-annotator agreement on the task and (b) allows to construct a feature-
based algorithm which uses sentence structure and which outperforms
the GIZA++ aligner in terms of f-measure on aligned tectogrammatical
node pairs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek pojednává o částicích implikujících presupozici jako podstatnou složku větného významu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article discusses particles that imply pressupposition as the fundamental part of the meaning of the sentence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje vyhledávací experimenty týmu z Karlovy Univerzity v Praze na soutěži CLEF 2007 Ad-Hoc. Zaměřili jsme se na monolinguální úlohu a použili nástroj LEMUR pro náš vyhledávač. Naše výsledky ukazují, že pro češtinu lemmatizace signifikantně vylepšuje výsledky vyhledávání a manuální tvorba dotazů je pouze těsně lepší než automatickátvorba dotazů s popisu témet.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we describe retrieval experiments performed at Charles University in
Prague for participation in the CLEF 2007 Ad-Hoc track. We focused on the Czech
monolingual task and used the LEMUR toolkit as the retrieval system. Our results
demonstrate that for Czech as a highly inflectional language, lemmatization significantly
improves retrieval results and manually created queries are only slightly better
than queries automatically generated from topic specifications.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek přestavuje tři metody strojového učení k automatické analýze jmenných frází. První z nich ji zkoumá jako úlohu seshlukování. Druhá metoda na ni aplikuje rozhodovací stromy a poslední experimentuje s Bell stromem, jakožto vyhledávácí prostor pro vyřešení problému automatické analýzy koreferencí. Znalosti získané z těchto experimentů můžou být prospěšné pro vývoj systému automatické analýzy koreferencí na češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper introduces three machine learning approaches to noun phrase
coreference resolution. The first of them gives a view of coreference resolution as a
clustering task. The second one applies a noun phrase coreference system based on
decision tree induction and the last one experiments with using the Bell tree to
represent the search space of the coreference resolution problem. The knowledge
gained from these experiments can be conducive to development of a Czech
coreference resolution system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek přináší analýzu českých sloves mluvení. Popisuje tři sémantické participanty typické pro tato slovesa: mluvčího, příjemce a sdělení. Pozornost je zde věnována zejména participantu sdělení, který bývá často realizován závislou obsahovou klauzí. U sloves mluvení vymezujeme tři typy těchto klauzí: asertivní, interogativní a direktivní. Dále navrhujeme rozdělení skupiny sloves mluvení podle toho, který z typů závislé obsahové klauze k sobě dané sloveso  váže.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper provides an analysis of Czech verbs of communication. It describes their three semantic participants 'Speaker', 'Recipient' and 'Message'. Especially, dependent content clauses realizing the participant 'Message' are discussed in great detail. Three types of them are distinguished: assertive, interrogative and directive. A further subdivision of the class of verbs of communication is proposed in accordance with types of dependent content clauses which the verbs of communicatioon bind to themselves.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku představujeme projekt zaměřený na obohacení valenčního slovníku českých sloves o sémantické třídy. Využíváme k tomu FrameNet, sémantický lexikální zdroj. V této fázi byly sémantické informace z FrameNetu mapovány na
dvě skupiny sloves, slovesa komunikace a slovesa výměny. Možnosti tohoto přístupu ukázala mezianotátorská shoda - 85,9% u sloves komunikace a 78,5% u sloves výměny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a project aimed at enhancing a valency lexicon of Czech verbs with coherent semantic classes. For this purpose, we
make use of FrameNet, a semantically oriented lexical resource. At the present stage, semantic frames from FrameNet have been mapped to
two groups of verbs with divergent semantic and morphosyntactic properties, verbs of~communication and verbs of exchange. The
feasibility of this task has been proven by the achieved interannotator agreement -- 85.9% for the verbs of communication and 78.5% for the verbs of exchange. As a result of our experiment,
the verbs of communication have been grouped into~nine semantic classes and the verbs of exchange into ten classes, based on upper
level semantic frames from FrameNet.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku představujeme projekt obohacující valenční slovník českých sloves o sémantické role. Pro tento účel využíváme FrameNet. V současné fázi byly mapovány valenční doplnění sloves komunikace a výměny na FE ve Framenetu. Možnosti tohoto přístupu ukázala mezianotátorská shoda - 95,6% u sloves komunikace a 91,2% u sloves výměny. Tímto způsobem jsme získali 37 sémantických rolí pro slovesa komunikace a 34 pro slovesa výměny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a project to enhance a valency lexicon of Czech verbs with semantic roles. For this purpose, we make use of FrameNet. At the present stage, frame elements from FrameNet have
been mapped to valency complementations of verbs of communication and verbs of exchange. The feasibility of this task has been proven by the achieved interannotator agreement -- 95.6% for the verbs of communication and 91.2% for the verbs of exchange. As a result, we have obtained 37 semantic roles for the verbs of communication and 34 for the verbs of exchange, based on
frame elements of upper level semantic frames from FrameNet.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato příručka je výstupem z korpusového výzkumu věnovanému vzájemnému vztahu syntakticko-sémantické struktury věty a struktury diskurzu (textu) zpracovanému v diplomové práci Lucie Mladové (Ústav českého jazyka a teorie komunikace FF UK, 2008). Z rozsáhlé problematiky popisu diskurzu v jeho různých aspektech (např. koreferenční vztahy, tematicko-rematická výstavba diskurzu atd.) se zabývá především syntakticky motivovanými vztahy, tj. otázkou, do jaké míry lze ze syntaktického a sémantického popisu věty vyčíst informace o diskurzních vztazích a jakého charakteru tyto informace jsou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present manual is a contribution to the widely discussed issue of how the syntactic structure of a sentence and the structure of discourse (text) are related. The syntactic sentence structure along with other language phenomena participates in building a coherent, comprehensible discourse. The author calls the syntactically motivated relations in discourse connective relations. These relations include coordinating relations and some of the subordinating relations within a sentence and, secondly, adjoining of discourse units across the sentence boundary. The explicit means of expressing connective relations are called discourse connectives. It is a group of language expressions that connect or adjoin discourse units while indicating the type of semantic relation between them, i. e. conjunctions, some subjunctions, particles and adverbials, and marginally also some other parts-of-speech. The present thesis describes the semantic category of discourse connectives in Czech on the basis of language data and their syntactic annotation in the Prague Dependency Treebank, and thus aims to contribute to the design of a language corpus annotation scenario capturing the discourse relations in Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V oblasti zkoumání věty z hlediska jejího aktuálního členění (nebo funkční perspektivy) má česká lingvistika za sebou mnoho práce. Při bádáních zaměřených tímto směrem byla věnována značná pozornost výrazům, které signalizují kategorie aktuálního členění věty, tzv. rematizátorům (jsou to výrazy jako například: jen, zejména, také, hlavně, právě, alespoň atd.).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this article, we discuss the linguistic properties of the so-called rhematizers and discourse connectives.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Práce je věnována problematice vzájemného vztahu syntaktické struktury věty a struktury diskurzu (textu). Syntaktická struktura věty se spolu s dalšími jevy podílí na koherenci a tedy srozumitelnosti diskurzu. Syntakticky motivované vztahy v diskurzu nazývá autorka vztahy konektivními. Tyto vztahy zahrnují jednak koordinační a některé závislostní vztahy v rámci věty a jednak připojování či navazování textových jednotek přes hranice věty. Explicitní prostředky vyjádření konektivních vztahů tvoří skupina tzv. diskurzních konektorů, což jsou slova nebo slovní spojení, která spojují či připojují textové jednotky a zároveň vyjadřují druh sémantického vztahu mezi nimi, tj. spojky, některé částicové a adverbiální výrazy a okrajově také další slovní druhy. Práce si klade za cíl popsat skupinu diskurzních konektorů v češtině na základě jazykového materiálu a syntaktické anotace Pražského závislostního korpusu a přispět tak ke vzniku korpusu s anotací diskurzních vztahů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present thesis is a contribution to the widely discussed issue of how the syntactic structure of a sentence and the structure of discourse (text) are related. The syntactic sentence structure along with other language phenomena participates in building a coherent, comprehensible discourse. The author calls the syntactically motivated relations in discourse connective relations. These relations include coordinating relations and some of the subordinating relations within a sentence and, secondly, adjoining of discourse units across the sentence boundary. The explicit means of expressing connective relations are called discourse connectives. It is a group of language expressions that connect or adjoin discourse units while indicating the type of semantic relation between them, i. e. conjunctions, some subjunctions, particles and adverbials, and marginally also some other parts-of-speech. The present thesis describes the semantic category of discourse connectives in Czech on the basis of language data and their syntactic annotation in the Prague Dependency Treebank, and thus aims to contribute to the design of a language corpus annotation scenario capturing the discourse relations in Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Základní představení anotačního systému analýzy diskurzu v Pražském závislostním korpusu, vycházející z jeho tektogramatické roviny a z filadelfského projektu Penn Discourse Treebank</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Basic presentation of discourse annotation scenario in Prague Dependency Treebank, based on its tectogrammatical level of analysis and on the Philadelphian project Penn Discourse Treebank</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku stručně shrnujeme hlavní přístupy ke strojovému překladu. Popisujeme principy úspěšného frázového statistického přístupu a ukazujeme některá nedávná vylepšení, která využívají hybridizaci jak směrem k překladu založenému na příkladech, tak k překladu založenému na pravidlech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we shortly summarize the main paradigms of machine translation. We describe the principles of successful phrase-based statistical approach and show some recent improvements using hybridization towards both example-based and rule-based directions. We also describe the experiments in this topic English-to-Czech translation that carried out at our department, we comment the weak points and provide for improvement possibilities.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Implementace dolování stromových překladových ekvivalentů ze závislostního paralelního treebanku v jazyce Perl.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A Perl implementation of treelet pair extraction from a parallel dependency treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku podáváme komplexní obraz korpusu DIALOG, a to v časovém sledu: pojednáváme o historii korpusu, jeho současné podobě a výhledech na příští tři roky. V části věnované historii se zabýváme základní motivací, která vedla badatelky a badatele z oddělení stylistiky a lingvistiky textu Ústavu pro jazyk český AV ČR ke sběru a analýze televizních dialogických jazykových projevů; podáváme též přehled o nejdůležitějších publikacích z tohoto období, o hlavních výzkumných tématech a teoretických poznatcích. V části věnované současnosti korpusu se zabýváme obdobím zpracovávání sebraného materiálu do podoby elektronického jazykového korpusu. Podáváme základní charakteristiky korpusu z hlediska lingvistického i počítačově-technického a shrnujeme obsah hlavních publikací z tohoto období. Představujeme také první veřejně přístupnou verzi korpusu, nazvanou DIALOG 0.1, a webové stránky tohoto dílčího korpusu (viz http://ujc.dialogy.cz). V části věnované blízké budoucnosti korpusu DIALOG pojednáváme o cílech grantového projektu Mluvená čeština ve veřejných dialozích: dobudování, zpřístupnění a průzkum korpusu DIALOG (Grantová agentura AV ČR, doba řešení projektu: 2007–2009, identifikační číslo projektu: KJB900610701, řešitel projektu: Petr Kaderka). Představujme možnosti obsažené v technickém řešení projektu, jehož autorem je Nino Peterek, harmonogram zpřístupňování korpusu veřejnosti a lingvistické výzkumy, na nichž aktuálně pracujeme nebo které na nás v blízké budoucnosti čekají.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we show a comprehensive picture of DIALOG corpus, in chronological order: the corpus history, its current form and perspectives for the next three years. 
The DIALOG corpus is a prosodically annotated corpus of Czech television debates that has been recorded and annotated at the Czech Language Institute of the Academy of Sciences of the Czech Republic. This project has been carried out in cooperation with the Institute of Formal and Applied Linguistics of Faculty of Mathematics and Physics, Charles University, Prague.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek se zabývá procesem vývoje foneticky a prozodicky bohatého korpusu mluvené řeči pro jednotkovou syntézu mluvené řeči. Zvláštní pozornost je věnována nahrávání a verifikační fázi procesu. Pro zajištění nejvyšší možné kvality a konzistence nahrávek bylo použito speciální nahrávací zařízení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper deals with the process of designing a phonetically and prosodically rich speech corpus for unit selection speech synthesis. The attention is given mainly to the recording and verification stage of the process. In order to ensure as high quality and consistency of the recordings as possible, a special recording environment consisting of a recording session management and “pluggable” chain of checking modules was designed and utilised. Other stages, namely text collection (including) both phonetically and prosodically balanced sentence selection and a careful annotation on both orthographic and phonetic level are also mentioned.
Language</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Doktorská dizertační práce studuje vzájemný vztah mezi lingvistickými teoriemi, daty a aplikacemi. Zaměřujeme se na jednu konkrétní teorii, Funkční generativní popis, jeden konkrétní typ dat, totiž valenční slovníky, a jednu konkrétní aplikaci: strojový překlad z angličtiny do češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This thesis explores the mutual relationship between linguistic theories, data
and applications. We focus on one particular theory, Functional Generative
Description (FGD), one particular type of linguistic data, namely valency
dictionaries and one particular application: machine translation (MT) from
English to Czech.

First, we examine methods for automatic extraction of verb valency dictionaries
based on corpus data. We propose an automatic metric for estimating how much
lexicographers' labour was saved and evaluate various frame extraction
techniques using this metric.

Second, we design and implement an MT system with transfer at
various layers of language description, as defined in the framework of FGD. We
primarily focus on the tectogrammatical (deep syntactic) layer.

Third, we leave the framework of FGD and experiment with a rather direct,
"phrase-based" MT system. Comparing various setups of the system and
specifically treating target-side morphological coherence, we are
able to significantly improve MT quality and out-perform a commercial MT
system within a pre-defined text domain.

The concluding chapter provides a broader perspective on the utility of lexicons
in various applications, highlighting the successful features. Finally, we
summarize the contribution of the thesis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje naše dva systémy v soutěži WMT08: frázový překlad o více faktorech a pravděpodobnostní stromový transfer přes hloubkovou syntax.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes our two contributions to WMT08 shared task: factored
phrase-based model using Moses and a probabilistic tree-transfer model at a deep
syntactic
layer.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládáme experimenty se strojovým překladem z angličtiny do hindštiny založeným na systému Moses (Mojžíš). Vyhodnocujeme vliv dodatečných trénovacích dat z odlišné domény, a to jak paralelních, tak jednojazyčných hindských. Testujeme tři metody pro zlepšení práce se slovosledem: standardní slovosledný model zabudovaný v Mosesu, předzpracování založené na pravidlech a jazykově nezávislou identifikaci přípon.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present experiments with a Moses-based English-to-Hindi translation system. We evaluate the impact of additional out-of-domain training data, both parallel and Hindi-only, and experiment with three methods for improving word order: standard Moses reordering model, rule-based preprocessing and language-independent suffix identification.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme systém automatického překladu z angličtiny do češtiny založený na syntaktickém transferu na rovině hloubkové syntaxe (tzv. tektogramatické rovině). Podrobně je popsán anotační proces pro angličtinu a také vyhodnocena kvalita překladu prototypu celého systému.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We are presenting an overview of an English-to-Czech machine translation system. The system relies on transfer at the tectogrammatical (deep syntactic) layer of the language description. We report on the progress of linguistic annotation of English tectogrammatical layer and also on first end-to-end evaluation of our syntax-based MT system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek stručně popisuje rozdíly mezi závislostními a složkovými stromy včetně jejich vyjadřovací síly a dále uvádí odkazy na některé formalismy stromových gramatik.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The report briefly discusses the differences between constituency and dependency trees including their different expressivity. It also provides references to some tree grammar formalisms.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Experimentální systém strojového překladu převádějící strom na strom.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A release of an experimental tree-to-tree transfer system implemented in Mercury.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Technická zpráva pro projekt EuroMatrix popisující naši implementaci strojového překladu přes syntaktickou reprezentaci (synchronous tree-substituion grammars).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A technical report for the project EuroMatrix describing our implementation of machine translation via a syntactic representation (synchronous tree-substituion grammars).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje nový česko-anglický paralelní korpus CzEng 0.7. Korpus je volně dostupný pro výzkumné a výukové účely.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes CzEng 0.7, a new release of Czech-English parallel corpus freely available for research and educational purposes.
We provide basic statistics of the corpus and focus on data produced by a community of volunteers. Anonymous contributors manually
correct the output of a machine translation (MT) system, generating on average 2000 sentences a month, 70% of which are indeed correct
translations. We compare the utility of community-supplied and of professionally translated training data for a baseline English-to-Czech
MT system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Česko-rusko-anglický korpus textů, po dvojicích jazyků zarovnán po větách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Czech-Russian-English parallel corpus with automatic pairwise sentence alignments.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje vznik souboru korpusů v rámci rodiny Pražského závislostního korpusu, které obsahují mluvená data v češtině a v angličtině, a jejich ruční anotaci rekonstrukce mluvené řeši (speech reconstruction).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a description of a new resource (Prague Dependency
Treebank of Spoken Language) being created for English and Czech to be used for the task of speech understanding,broad natural language analysis for dialog systems and other speech-related tasks, including speech editing. The resources we have created so far contain audio and a standard transcription of spontaneous speech, but as a novel layer, we add an edited (“reconstructed”) version of the spoken tterances.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Syntax a sémantika konstrukcí českého akuzativu s infinitivem je předmětem studia této stati, a to v porovnání s jinými slovanskými jazyky. Obligatornost koreference v těchto konstrukcích oproti její fakultativnosti ve vedlejších větách významově blízkých a jistá omezení na pořadí slov jsou chápány jako argumenty pro jejich samostanou hloubkovou reprezentaci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The constructions „accusativus cum infinitivo“ with the verbs of perception are analyzed in Czech and in other Slavonic languages. The syntactic and semantic features of these Czech constructions and their relation to their respective paraphrases (expressed by subordinated clauses)are studied. The relation of grammatical coreference (obligatory with the AcI and only optional with the analyzed subordinated clauses) are used as arguments for the special position of the  AcI within the underlying structure of the sentence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve stati se zvažuje místo stupňování adjektiv v popisu jazyka. Zvláštní pozornost se věnuje tzv. "absolutnímu" užití komparativu, rozlišuje se lexikalizace, stylizace a porovnávání s průměrem. Podává se kontrastivní pohled na stupňování ve slovenštině a ruštině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The status of the gradation of adjectives in the description of language is considered. The cases of the "abolute" comparative constructions are studied on the corpus data (lexicalization, stylistic effects, comparison with an average). The Czech situation is compared with Slovac and Russian comparative constructions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přestože reflexivní konstrukce jsou ve slavistice často zpracovávány, jednotná metodologická báze popisu chybí. Potíže spočívají v mnohofunkčnosti reflexivního zájmena/částice se/si a časté víceznačnosti. Ve stati se navrhují kritéria pro řazení jednotlivých typů do lexikonu, slovotvorby, morfologie a syntaxe.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The difficulties of the analysis of Slavonic reflexive constructions are caused among other things also by the fact that se/si  constructions in Slavonic languages cross the boundaries between lexicon, word-formation, morphology, and syntax.
The criteria, how to distinguish the class of reflexives tantum, derived reflexives, reciprocals, derived reciprocals, true reflexives and deagentive constructions, are proposed, and the difficuilties with the analysis of real data  The Czech and Russian reflexives are compared only briefly.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Český akademický korpus 2.0 (ČAK 2.0) sestává z 650 tisíc slovních forem z rozličných novinových článků, časopisů a přepisů rozhlasových či televizních pořadů ze 70. a 80. let dvacátého století, ručně anotovaných na morfologické a analytické rovině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Czech Academic Corpus version 2.0 (CAC 2.0) consists of 650,000 words from various 1970s and 1980s newspapers, magazines and radio and television broadcast transcripts manually annotated for morphology and syntax.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Průvodce ČAK 2.0 je, podobně jako v případě ČAK 1.0, průvodce CD-ROM. Obsah průvodce je koncipován tak, že čtenář nemusí být předem seznámen s průvodcem ČAK 1.0, a přesto se  o projektu dozví vše potřebné. Pokud ho budou zajímat podrobnosti historie projektu Českého akademického korpusu a podrobnosti přípravy první verze, může si samozřejmě průvodce ČAK 1.0 otevřít. Čtenář, který je s průvodcem ČAK 1.0 seznámen, se bude v předkládaném průvodci orientovat velmi snadno, protože jsme se v něm přidrželi stejného členění kapitol do třech tematických celků.

První celek, kapitola 2, podává základní charakteristiku Českého akademického korpusu 2.0, popisuje strukturu anotací v něm obsažených a dokumentuje dílčí kroky spojené se syntaktickými anotacemi.

Druhý celek, kapitoly 3 až 6, se věnuje samotnému CD-ROM, tj. jeho datové komponentě, nástrojům, bonusům a tutoriálům. 

Kapitoly 8 a 9 jakožto třetí celek věnují pozornost personálnímu a finančnímu zabezpečení projektu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The CAC 2.0 Guide is a guide to the CD-ROM, just like the previous CAC 1.0 Guide. The contents of the Guide provide all the necessary information about the project; however the user does not need to be familiar with the CAC 1.0 Guide. The CAC 1.0 Guide can be referred to for the details of the CAC project’s history and its preparation details. Nevertheless, if you are already familiar with the CAC 1.0 Guide, navigating it will be easy, as we have maintained its chapters’ organisation into three main units.

The first unit, Chapter 2, describes the main characteristics of the Czech Academic Corpus 2.0, the structure of its annotations and the documentation of the partial steps of the syntactical annotations.

The second unit, Chapters 3 through 6, contain the CD-ROM information and the documentation of the data component, tools, bonus material and tutorials. 

Chapters 8 and 9 form the third unit of the Guide. They cover the personal and financial aspects of the project.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tree Aligner je experimentální systém pro získání slovníku převádějícího mezi sebou závislostní struktury částí vět. Toto je implementace v Javě.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Tree Aligner is an experimental implementation to extract treelet-to-treelet translation dictionary from a parallel treebank. This is the Java version.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>TectoMT je modulární softwarový systém pro vývoj aplikací pro zpracování přirozeného jazyka. Tento průvodce popisu architekturu systému, včetně aplikačního rozhraní pro práci se strukturovanými lingvistickými daty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>TectoMT is a highly modular software system for developing NLP applications. This guide gives a detailed overview of the system's architecture, including the API description.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nový systém strojového překladu z angličtiny do češtiny spojující lingvisticky motivované vrstvy jazykového popisu (jak jsou definovány v anotačním schématu Pražského závislostního korpusu) s prvky statistických metod počítačového zpracování jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a new English to Czech machine translation system combining linguistically motivated layers of language description (as deﬁned in the Prague Dependency Treebank annotation scenario) with statistical NLP approaches.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozbírají se hlavní metodologické zásady Pražské lingvistické školy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The main methodological insights of the Prague School are analyzed. It is shown that the ideas are still valid.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Probírají se pojmy jako spisovnost a standard a ukazuje se užitečnost toho druhého i nevhodnost prvního.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Concepts such as correctness, literary norm, or standard are examined; the last term is to be preferred.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Probírají se lexikální jednotky týkající se Velikonoc z hledsika jejich původu i slovotvorných aspektů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Lexical items concerning Easter are characterized as for different aspects of their origin and structure.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozbor některých lingvistických hypotéz, které je možné ověřovat nad anotovaným textovým korpusem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The building up and annotation of text corpora (both written and spoken) has become one of the urgent topics in present-day linguistics; the creation of the Czech National Corpus and the morphologically and syntactically annotated Prague Dependency Treebank documents that the Prague Linguistic School has not only kept contact with the recent trends of linguistic studies in the world, but in some aspects, it even sets an example. In the present contribution, several linguistic phenomena are selected to illustrate how a systematically designed and carefully implemented deep-level annotation of a large corpus of Czech texts may serve to verify linguistic theory. The theory underlying the annotation is that of Functional Generative Description (FGD) designed by Petr Sgall in the early 1960s as an original alternative to Chomskyan transformational grammar and developed since then by a group of Praguian theoretical and computational linguists at Charles University.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Úvodní přednáška na semináři k výročí PLK, v němž jsou shrnuty hlavní příspěvky členů této lingvistické školy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Introductory talk at a workshop marking the anniversary of the Prague Linguistics Circle, where the main contributions of the members of this linguistic school are summarized.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V pozvané přednášce byly analyzovány vztahy mezi teoretickou, počítačovou a korpusovou lingvistikou a vzájemného podílu každého z těchto odvětví; obecné závěry byly podloženy rozborem některých jazykových jevů spjatých především s komunikativní funkcí jazyka a bylo ukázáno, jak se v jazykové formě odráží dichotomie „o čem se mluví“ a „co se o tom říká“.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In view of the relationships between theoretical, computational and corpus linguistics, their mutual contributions are discussed and illustrated on the issue of the aspect of language related to the information structure of the sentence, distinguishing ”what we are talking about” and ”what we are saying about it”.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem příspěvku je vyhodnotit, které aspekty struktury diskurzu v textech mohou být odhaleny a zároveň objasněny. Analýza je provedena na větách z Pražského závislostního korpusu, které jsou anotovány na podkladové rovině, včetně anotace aktuálního členění a koreference.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of the contribution is to document, on the example of the annotation of sentences on an underlying syntactic layer including Topic-Focus Articulation and enriched by an establishment of basic coreference links in the Prague Dependency Treebank, which aspects of the discourse structure can be discovered and elucidated.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Při anotování Pražského závislostního korpusu se přechází od věty k textu a tedy i k anotaci koreferenčních vztahů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the annotation of text corpora it is essential to pass over from the annotation of sentence structure to an annotation of discourse relations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>PML-TQ je systém pro vyhledávání nad lingvisticky anotovanými korpusy závislostních či složkových stromů ve formátu PML. Systém definuje velmi silný dotazovací jazyk. Je implementován pomocí client-server architektury, s použitím SQL databáze  na straně serveru a zahrnuje webové a CLI rozhraní a grafické rozhraní zabudované do editoru TrEd. Jako rozsíření TrEdu je dále dostupný čistě klientský vyhledávací systém nad stejným dotazovacím jazykem, umožňující prohledávat lokální soubory.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>PML-TQ is a search system for linguistic TreeBanks in PML format. The system defines a powerfull query language, uses a client server architecture (with SQL database backend on the server side) and provides a command-line and a simple web-based search client. The system also includes a graphical client for PML-TQ and client-side PML-TQ search engine, allowing the users to use PML-TQ queries on their local data. The GUI and client-side search engine are distributed separately as extensions to the tree editor TrEd.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku představujeme aktuální vývoj již zavedeného pracovního prostředí pro syntakticky anotované korpusy, sestávajícího z abstraktního datového formátu založeného na XML, plně přizpůsobitelného editoru stromových struktur, nástroje automatizované zpracování anotovaných dat s podporou výpočtu na počítačovém clusteru a prototyp relačního vyhledávácího systému s grafickým uživatelským rozhraním zabudovaným do editoru stromů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents recent advances in
an established treebank annotation framework comprising of an abstract XML-based data format, fully customizable editor of tree-based annotations, a toolkit for all kinds of automated data processing with support for cluster computing, and
a work-in-progress database-driven search
engine with a graphical user interface built
into the tree editor.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje nasši účast v evaluační kampani MWE 2008 zaměřené na automatickou extrakci kolokací.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes our participation in the MWE 2008 evaluation campaign focused on ranking MWE candidates. Our ranking system employed 55 association measures combined by standard statistical-classification methods modified to provide scores for ranking. Our results were crossvalidated and compared by Mean Average Precision. In most of the experiments we observed significant performance improvement achieved by methods combining multiple association measures.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Frekvenční charakteristiky českých kolokačních kandidátů extrahovaných z Pražského závislostního korpusu jako závislostní bigramy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Frequency characteristics of Czech collocation candidates extracted as dependecy bigrams from the Prague Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Frekvenční charakteristiky českých kolokačních kandidátů extrahovaných z Českéno národního korpusu jako povrchové bigramy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Frequency characteristics of Czech collocation candidates extracted as surface bigrams from the Czech National Corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Frekvenční charakteristiky českých kolokačních kandidátů extrahovaných z Pražského závislostního korpusu jako povrchové bigramy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Frequency characteristics of Czech collocation candidates extracted as surface bigrams from the Prague Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Disertace se zabývá studiem lexikálních asociačních měr aplikovaných v úloze automatické extrakce kolokací.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A PhD dissertation studying lexical association measures and their application for collocation extraction.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Referenční množina kolokačních kandidátů extrahovaných z Pražského závislostního korpusu jako závislostní bigramy a anotovaných jako kolokace nebo nekolokace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A reference set of collocation candidates extracted as dependency bigrams from the Prague Dependency Treebank and annotated as collocational or non-collocational.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Referenční množina kolokačních kandidátů extrahovaných z Českého národního korpusu jako povrchové bigramy a anotovaných jako kolokace nebo nekolokace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A reference set of collocation candidates extracted as surface bigrams from the Czech National Corpus and annotated as collocational or non-collocational.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Referenční množina kolokačních kandidátů extrahovaných z Pražského závislostního korpusu jako povrchové bigramy a anotovaných jako kolokace nebo nekolokace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A reference set of collocation candidates extracted as surface bigrams from the Prague Depedency Treebank and annotated as collocational or non-collocational.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popis tří referenčních datových množin použitých v rámci evaluační kampaně MWE 2008 zaměřené na extrakci kolokací.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce three reference data sets provided for the MWE 2008 evaluation campaign focused on ranking MWE candidates. The data sets comprise bigrams extracted from the Prague Dependency Treebank and the Czech National Corpus. The extracted bigrams are
annotated as collocational and non-collocational and provided with corpus frequency information.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přehled výsledků Cross-Language Speech Retrieval Track organizované v rámci evaluační kampaně CLEF 2007.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The CLEF-2007 Cross-Language Speech Retrieval (CL-SR) track included two tasks: to identify topically coherent segments of English interviews in a known-boundary condition, and to identify time stamps marking the beginning of topically relevant passages in Czech interviews in an unknown-boundary condition. Six teams participated in the English evaluation, performing both monolingual and cross-language searches of ASR transcripts, automatically generated metadata, and manually generated metadata. Four teams participated in the Czech evaluation, performing monolingual searches of automatic speech recognition transcripts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Důležitou součástí Pražského závislostního korpusu je valenční slovník. Slovník obsahuje 5300 sloves s 8200 valenčními rámci, které jsou propojeny s korpusem. Reprezentace valenčního rámce je plně formalizována. Tento příspěvek je orientován na formální popis forem, kterých nabývají argumenty sloves v různých sekundárních diatezích, jako je například pasivizace (pasivum opisné a zvratné), rezultativ (sloveso mít, dostat), dispoziční modalita. Článek detailně popisuje fungování jednotlivých transformačních pravidel pro příslušné změny forem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>As an important part of the Prague Dependency Treebank project a valency lexicon is being distributed. In this lexicon, more than 5300 verb entries are fully formally represented, with more than 8200 valency frames for verb senses included. Moreover, the valency frames are interlinked with the Prague Dependency Treebank corpus, effectively providing a verb sense distinction and annotation for every occurrence of a verb in the corpus. More than 100,000 verb occurrences are annotated in this way. The valency frame representation is fully formalized. In this contribution, we will concentrate on the formal description of the form of the verb</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Webový portál umožňující editaci a vyhledávání v přepisech audio-visuálních nahrávek dialogů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A web portal allowing editing and searching in the transcripts of audio-visual recordings of dialogues. The dynamic web application provides access for registered users to the digitised archive. Playing selected parts is possible in the web browser.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Na řadě příkladů představujeme dotazovací jazyk Netgraphu - plně grafického 
nástroje pro vyhledávání v Pražském závislostním korpusu 2.0. Abychom 
ukázali, že dotazovací jazyk je pro tento korpus vhodný, studujeme anotační 
manuál nejsložitější - tektogramatické - roviny korpusu a dokládáme, že 
pomocí tohoto jazyka je možno vyhledávat lingvistické jevy anotované na této 
rovině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>On many examples we present a query language of Netgraph - a fully graphical tool for searching in the Prague Dependency Treebank 2.0. To demonstrate that the query language fits the treebank well, we study an annotation manual for the most complex layer of the treebank - the tectogrammatical layer - and show that linguistic phenomena annotated on the layer can be searched for using the query language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vyhledávání v lingvisticky anotovaných korpusech je důležitý úkol, vyžadující pokročilý nástroj. Netgraph byl navržen tak, aby vyhledávání bylo co nejpohodlnější, s minimem požadavků na uživatele. Ačkoliv byl vyvinut především pro Pražský závislostní korpus 2.0, může být použit také na jiné korpusy, a to jak závislostní, tak složkové, poté co je korpus převeden do vhodného formátu. V článku představujeme dotazovací jazyk Netgraphu a na mnoha příkladech ukazujeme, jak může být použit k vyhledávání běžných lingvistických jevů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Searching in a linguistically annotated treebank is a principal task that requires a sophisticated tool. Netgraph has been designed to perform the searching with maximum comfort and minimum requirements on its users. Although it has been developed primarily for the Prague Dependency Treebank 2.0, it can be used with other treebanks too, both dependency and constituent-structure types, as long as the treebank is transformed to a suitable format.
In this paper, we present Netgraph query language and on many examples show how it can be used to search for frequent linguistic phenomena.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Netgraph je aplikace typu klient-server pro vyhledávání v lingvisticky anotovaných korpusech, vyvinutá primárně pro vyhledávání v Pražském závislostním korpusu 2.0. Nicméně, Netgraph lze použít s dalšími korpusy, pokud jsou převedeny do formátu FS.
Tato zřejmě konečná verze Netgraphu je vyvrcholením dlouholetého vývoje a její pomocí lze vyhledávat a studovat všechny jevy anotované v PDT 2.0 s použitím jednoduchého a intuitivního graficky orientovaného dotazovacího jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Netgraph is a client-server application for searching in linguistically annotated treebanks, developed primarily for searching in the Prague Dependency Treebank 2.0. Nevertheless, Netgraph can work with other corpora too, as long as they are converted to FS format.
This supposedly last version of Netgraph concludes years of development and allows searching and studying all phenomena annotated in PDT 2.0 using a simple and intuitive, graphically oriented query language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku studujeme anotaci Pražského závislostního korpusu 2.0 a sestavujeme seznam požadavků na dotazovací jazyk, který by umožnil vyhledávat a studovat všechny lingvistické jevy anotované v tomto korpusu. Navrhujeme rozšíření dotazovacího jazyka existujícího nástroje Netgraph 1.0 a ukazujeme, že tento rozšířený dotazovací jazyk splňuje všechny jmenované požadavky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the article, we study the annotation of the Prague Dependency Treebank 2.0 and assemble a list of requirements on a query language that would allow searching for and studying all linguistic phenomena annotated in the treebank. We propose an extension to the query language of the existing search tool Netgraph 1.0 and show that the extended query language satisfies the list of requirements.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Lingvisticky anotované korpusy hrají zásadní roli v moderní počítačové lingvistice. Čím složitějšími se korpusy stávají, tím pokročilejší nástroje jsou potřeba pro jejich používání, zejména pro vyhledávání v těchto datech.
Zabýváme se lingvistickými jevy anotovanými v Pražském závislostním korpusu 2.0 a vytváříme seznam požadavků, které tyto jevy staví pro vyhledávací nástroj, obzvláště pro jeho dotazovací jazyk.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Linguistically annotated treebanks play an essential part in the modern computational linguistics. The more complex the treebanks become, the more sophisticated tools are required for using them, namely for searching in the data. We study linguistic phenomena annotated in the Prague Dependency Treebank 2.0 and create a list of requirements these phenomena set on a search tool, especially on its query language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dotazovací jazyk Netgraphu je dotazovací systém pro lingvisticky anotované korpusy, který se snaží být dostatečně silný pro lingvistické potřeby, ale zároveň chce být tak jednoduchý, aby nevyžadoval programátorské či matematické dovednosti od svých uživatelů. Poskytujeme úvod do tohoto systému a ukazujeme na příkladech, jak vyhledávat některé časté lingvistické jevy. Nabízíme rovněž srovníní dotazovací síly Netgraphu a TGrepu - tradičního a dobře známého dotazovacího systému pro strukturované korpusy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Netgraph query language is a query system for linguistically annotated treebanks that aims to be sufficiently powerful for linguistic needs and yet simple enough for not requiring any programming or mathematical skill from its users. We provide an introduction to the system along with a set of examples how to search for some frequent linguistic phenomena. We also offer a comparison to the querying power of TGrep – a traditional and well known treebank query system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme Netgraph - snadno použitelný nástroj pro vyhledávání v lingvisticky anotovaných korpusech. Na několika příkladech z Pražského závislostního korpusu ukazujeme vlastnosti vyhledávacího jazyka a ukazujeme rovněž, jak vyhledávat některé běžné lingvistické jevy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present Netgraph – an easy to use tool for searching in linguistically annotated treebanks. On several examples from the Prague Dependency Treebank we introduce the features of the searching language and show how to search for some frequent linguistic phenomena.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>ElixirFM je vysokoúrovňová implementace Funkční arabské morfologie zdokumentovaná na http://elixir-fm.wiki.sourceforge.net/. Jádro ElixirFM je napsáno v Haskellu, zatímco rozhraní v Perlu podporuje údržbu slovníku a další interakce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>ElixirFM is a high-level implementation of Functional Arabic Morphology documented at http://elixir-fm.wiki.sourceforge.net/. The core of ElixirFM is written in Haskell, while interfaces in Perl support lexicon editing and other interactions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pražský arabský závislostní korpus (PADT) sestává z rozpracovaných víceúrovňových lingvistických anotací moderní psané arabštiny. Druhem obsažených morfologických a syntaktických informací se PADT zřetelně odlišuje od Penn Arabic Treebanku (PATB).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Prague Arabic Dependency Treebank (PADT) consists of refined multi-level linguistic annotations over the language of Modern Written
Arabic. The kind of morphological and syntactic information comprised in PADT differs considerably from that of the Penn Arabic
Treebank (PATB). This paper overviews the character of PADT and its motivations, and reports on converting and enhancing the PATB
data in order to be included into PADT. The merged, rule-checked and revised annotations, which amount to over one million words, as
well as the open-source computational tools developed in the project are considered for publication this year.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Morfologické značky jsou důležitou součástí anotace většiny korpusů. Bohužel se však v různých korpusech používají různé sady značek, a to i pokud jde o tentýž jazyk. Převody značek z jedné sady do druhé jsou obtížné a jejich implementace je obvykle ušitá na míru konkrétní dvojici sad značek. Zde naproti tomu navrhujeme univerzální přístup, který umožňuje jednou investované úsilí využít při pozdějších převodech do jiných formalismů. Prezentujeme také nepřímé vyhodnocení v kontextu syntaktické analýzy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Part-of-speech or morphological tags are important means of annotation in a vast number of corpora. However, different sets of tags are used in different corpora, even for the same language. Tagset conversion is difficult, and solutions tend to be tailored to a particular pair of tagsets. We propose a universal approach that makes the conversion tools reusable. We also provide an indirect evaluation in the context of a parsing task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje jednoduchou metodu neřízené morfologické analýzy neznámého jazyka. Potřeba je pouze prostý textový korpus daného jazyka. Algoritmus se dívá na slova, rozpozná opakovaně se vyskytující kmeny a přípony a sestaví pravděpodobné morfologické vzory. Článek také popisuje způsob, jak byla tato metoda využita při řešení úlohy Morpho Challenge 2007, a prezentuje výsledky Morpho Challenge. Přestože tato práce byla původně studentským projektem bez návaznosti na obdobný výzkum ve světě, k našemu překvapení tento jednoduchý přístup překonal několik dalších algoritmů v podsoutěži segmentace slov. Věříme, že v metodě je dostatečný prostor pro zlepšení, který může výsledky dále zlepšit. V článku jsou rozebrány chyby a navržena budoucí rozšíření.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes a rather simplistic method of unsupervised morphological analysis of words in an unknown language. All what is needed is a raw text corpus in the given language. The algorithm looks at words, identifies repeatedly occurring stems and suffixes, and constructs probable morphological paradigms. The paper also describes how this method has been applied to solve the Morpho Challenge 2007 task, and gives the Morpho Challenge results. Although the present work was originally a student project without any connection or even knowledge of related work, its simple approach outperformed, to our surprise, several others in most morpheme segmentation subcompetitions. We believe that there is enough room for improvements that can put the results even higher. Errors are discussed in the paper; together with suggested adjustments in future research.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme jednoduchou metodu neřízené morfematické segmentace slov v neznámém jazyce. K tomu potřebujeme pouze korpus prostého textu (nebo seznam slov) v daném jazyce. Algoritmus rozpozná části slov, které se vyskytují v mnoha slovech, a interpretuje je jako kandidáty na morfémy (předpony, kmeny a přípony). Hlavní novinkou oproti Zeman (2007) je nové zpracování předpon. Po odfiltrování scestných hypotéz se seznam morfémů použije k segmentaci slov na vstupu. Uvádíme oficiální vyhodnocení Morpho Challenge 2008 a také dodatečné pokusy, které jsme vyhodnotili neoficiálně. Součástí práce je detailní rozbor chyb s ohledem na použitou vyhodnocovací metodu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe a simple method of unsupervised morpheme segmentation of words in an unknown language. All what is needed is a raw text corpus (or a list of words) in the given language. The algorithm identifies word parts occurring in many words and interprets them as morpheme candidates (prefixes, stems and suffixes). New treatment of prefixes is the main innovation over Zeman (2007). After filtering out spurious hypotheses, the list of morphemes is applied to segment input words. Official Morpho Challenge 2008 evaluation is given along with some additional experiments evaluated unofficially. We also analyze and discuss errors with respect to the evaluation method.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje metodu adaptace parseru na nový jazyk. Předpokládá se, že pro cílový jazyk je k dispozici mnohem méně lingvistických zdrojů než pro zdrojový jazyk. Postup byl kvůli dostupnosti testovacích dat testován na dvou evropských jazycích; je však snadno aplikovatelný na libovolné dva příbuzné jazyky, včetně některých indoárijských. V rámci našich pokusů jsme dokázali snížit chybovost o 34 %.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper describes an approach to adapting a parser to a new language. Presumably the target language is much poorer in linguistic resources than the source language. The technique has been tested on two European languages due to test data availability; however, it is easily applicable to any pair of related languages, including some of the Indic language group. Within our experimental setting, we were able to reduce the error rate by 34 %.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme systém STYX, který je navržen jako elektronická civčebnice českého tvarosloví a české syntaxe.
Téměř 12 000 cvičebnicových příkladů je vybráno z Pražského závislostního korpusu, nejrozsáhlejšího anotovaného korpusu češtiny. Cvičebnice nabízí komplexní zpracování věty z pohledu morfologických a syntaktických jevů, které pokrývají učební látku středních škol a vyšších tříd základních škol.
Využití anotovaných korpusů mimo jejich "domácí prostředí", např. v hodinách českého jazyka, je originální napříč bohatou škálou jazyků, pro které anotované korpusy existují.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the STYX system, which is designed as an electronic corpus-based exercise book of Czech morphology and syntax with sentences directly selected from the Prague Dependency Treebank, the largest annotated corpus of the Czech language. The exercise book offers complex sentence processing with respect to both morphologigal and syntactic phenomena, i. e. the exercises allow studens of basic and secondary schools to practice classifying parts of speech and particular morphological categories of words and in the parsing of sentences and classifying the syntactic functions of words. The corpus-based exercise book presents a novel usage of annotated corpora outside their original context.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Významy adverbiálních určení (času, místa, způsobu apod.) jsou v syntaktických příručkách propracovány do různé hloubky, s větší či menší přesností vymezení jejich jednotlivých významů. Určeny jsou intuitivně na základě omezeného množství příkladů. I pro tektogramatickou anotaci Pražského závislostního korpusu byl nejprve empiricky stanoven soubor významů adverbiálních určení – označovaných jako funktory. Podle navrženého souboru funktorů pak byla anotována data. V současné době probíhá vyhodnocování této anotace: oanotovaná data poskytují velké množství příkladů, na jejichž základě jsou významy funktorů precizněji definovány a jemněji tříděny na tzv. subfunktory. Výsledkem bude na korpusovém materiálu založený, ucelený popis významů a podvýznamů adverbiálních určení. V tomto příspěvku představíme funktory a subfunktory pro prostorová adverbiální určení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article will briefly introduce the Prague Dependency Treebank. In this treebank written Czech sentences are annotated on three layers: morphological layer (morphological categories), analytical layer (surface structure) and the tectogrammatical layer (deep structure). We focus on how sentences are represented on the tectogrammatical layer. Main aim of this article is to suggest a detailed specification of meanings of spatial modifications for annotation on this layer. This detailed specification of meanings of modifications represents necessary information for the translation from one language to another.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dokument obsahuje pravidla pro manuální anotaci, kterou je třeba provést při budování
závislostního korpusu mluveného jazyka. Tato anotace spočívá v tzv. rekonstrukci
standardizovaného textu z mluvené řeči, tj. původní segmenty mluvené řeči, mnohdy velmi
vzdálené gramaticky správným větám, se zde popsaným způsobem převádí do takové
„standardizované” podoby, na kterou již je možné uplatnit další anotační pravidla (přidávající
zejména informaci o syntaktické struktuře věty).
Anotační manuál je určen anotátorům Pražského závislostního korpusu mluvené češtiny, ale
lze jej chápat jako obecný návod pro podobně pojatou anotaci kteréhokoli jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This document is primarily meant to serve as a manual for human annotators of spoken data of The Prague Dependency Treebank of Spoken Czech (PDTSC).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek shrnuje dosavadní poznatky získané při budování Pražského závislostního
korpusu mluvené češtiny (Prague Dependency Treebank of Spoken Czech; dále PDTSC)
v Ústavu formální a aplikované lingvistiky MFF UK. PDTSC bude prvním korpusem
mluvené řeči, který nabídne i syntakticko-sémantickou anotaci promluv. Východiskem
projektu PDTSC je syntakticko-sémantická anotace korpusu psaných textů, která již byla
zpracována v projektu Pražského závislostního korpusu 2.0 (dále PDT).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>PDTSC aims to be the first spoken corpus including the syntactic annotation of speech. This corpus is based on the annotation of written language (PDT).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek v Kronice Slova a slovesnosti k jubileu prof. Jarmily Panevové.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Contribution in Chronicle of Slovo a slovesnost in honour of prof. Jarmila Panevova.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V~tomto textu shrnujeme výsledky dosažené při vytváření lexikální databáze českých sloves. Práce se soustřeďuje na tři základní okruhy. Prvním okruhem je formální zachycení valenčních vlastností českých sloves ve valenčním slovníku. Je zde představena logická stavba bohatě strukturovaných slovníkových dat. Druhým okruhem,
kterému se práce věnuje, jsou nové teoretické aspekty, které přináší zpracování rozsáhlého jazykového materiálu -- je to především koncept kvazivalenčních doplnění a adekvátní zpracování
slovesných alternací. Třetí okruh tvoří
problematika formálního modelování přirozeného jazyka. Je zde představen nový formální model závislostní syntaxe založený na originálním konceptu restartovacích automatů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present work summarizes the results of building a lexical database of Czech verbs. It concentrates on three essential topics. The first of them is the formal representation of valency properties of Czech verbs in the valency lexicon. The logical organization of richly structured lexicon data is presented here. The second topic concerns new theoretical aspects that result from the extensive processing of language material, namely the concept of quasi-valency complementation and adequate processing of verb alternations. The third topic addresses questions of formal modeling of a natural language. A new formal model of dependency syntax based on a novel concept of restarting automata is introduced here.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valence, tedy "počet a povaha míst (argumentů), které na sebe sloveso (-) váže" (Encyklopedický slovník češtiny, Karlík a kol., 2002), patří k jazykovým jevům, které vzbuzují zájem lingvistů, ale i odborníků zabývajících se automatickým zpracováním přirozených jazyků. Dobrá znalost valence je nezbytná pro každého, kdo pracuje s jazykem - pro učitele, redaktory, novináře, studenty. Valence zároveň hraje klíčovou roli při mnoha úlohách automatického zpracování přirozeného jazyka, jakými jsou např. strojový překlad či vyhledávání informací. Valenční slovník českých sloves VALLEX poskytuje informace o valenční struktuře českých sloves v jejich jednotlivých významech, které charakterizuje pomocí glos a příkladů. Pro jednotlivá valenční doplnění uvádí jejich možná morfematická vyjádření. Tyto základní údaje jsou doplněny o některé další charakteristiky, jako je možnost recipročního užití či syntakticko-sémantická třída slovesa.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Valency, that is "the number and character of places (arguments), which a verb (...) ties to itself" (Encyclopaedic Dictionary of Czech, Karlík et al., 2002), belongs among the linguistic phenomena, which interest linguists as well as experts concerned with automatic processing of natural languages. A good knowledge of valency is necessary for anybody who works with languages - teachers, editors, journalists, students. Valency also plays an important role in many tasks of automatic processing of natural languages, such as machine translation or information search. VALLEX, the valency dictionary of Czech verbs, provides information about the valency structure of Czech verbs with their individual meanings, which are characterised by notes and examples. It also presents possible morphematic forms for individual valency complements. These elementary data are complemented with some other characteristics, such as the possibility of their reciprocal use or the syntactic-semantic verb class.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Syntaktická analýza vět přirozeného jazyka, základní předpoklad mnoha aplikovaných úkolů, je složitá úloha, a to zejména pro jazyky s volným slovosledem. Přirozeným krokem, který snižuje
složitost vstupních vět, může být vytvoření modulu, ve kterém se určí struktura souvětí ještě před úplnou syntaktickou analýzou. Navrhujeme využít pojem segmentů, snadno automaticky rozpoznatelných úseků vět. Určujeme `segmentační
schémata' popisující vzájemné vztahy mezi segmenty -- zejména souřadná a apoziční spojení, podřadná spojení, případně vsuvky.

V tomto článku představujeme vývojový rámec, který umožňuje vyvíjet a testovat pravidla pro automatické určování segmentačních schémat. Popisujeme dva základní experimenty -- experiment se získáváním segmentačních schémat ze stromů Pražského závislostního korpusu a experiment se segmentačními pravidly aplikovanými na prostý text. Dále navrhujeme míry pro vyhodnocování úspěšnosti segmentačních pravidel.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Syntactic analysis of natural language sentences, the basic requirement of many applied tasks, is a complex task, especially for languages with free word order. Natural solution, which reduces
complexity of the input sentences, can be a module in which they determined the structure of sentences before full synyactic analysis. We propose to use the concept of segments, easily recognizable sections sentences automatically. We introduce `segmentation schemata' that describe the relationship between the segments - in particular, super/subordination, coordination and aposition, and parenthesis.

In this article we present framework that allows to develop and test rules for automatically determining the segmentation schemes. We describe two basic experiments - experiment to obtain the segmentation patterns from trees from Prague Dependency Treebank amd segmentation experiment with the rules applied to plain text. Furthermore, we propose measures for evaluating  segmentation rules.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek prezentuje základní formální pojmy, které umožňují formalizovat redukční analýzu pro Funkční generativní popis češtiny. Ilustruje metodu redukční analýzy a její aplikaci na zpracování závislostí a slovosledu v jazyce s volným slovosledem. Zavádí 4-úrovňový redukční systém založený na pojmu jednoduchých restartovacích automatů. Tento nový rámec umožňuje formálně definovat charakteristickou  relaci, a tedy zachytit synonymie a víceznačnost v studovaném jazyce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents the basic formal notions that allow for formalizing the notion of analysis by reduction for Functional Generative Description, FGD. We have outlined and exemplified the method of analysis by reduction and its application in processing dependencies and word order in a language with a high degree of free word order. Based on this experience, we have introduced the 4-level reduction system for FGD based on the notion of simple restarting automata. This new formal frame allows us to define formally the characteristic relation for FGD, which renders synonymy and ambiguity in the studied language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příssěvku se soustředíme na česká slovesa mluvení s imperativními rysy, která k sobě vážou valenční doplnění se sémantickou rolí informace. Toto může být morfematicky vyjádřeno jako závislá obsahová klauze. Dané klauze bývají prototypicky uvozeny spojkou aby. Spojka aby však může být za určitých podmínek nahrazena spojkou že. Tyto podmínky jsou pak předmětem naší analýzy založené na korpusovém materiálu. Korpusový materiál ukazuje, že daná slovesa vykazují tendenci vázat k sobě závislé obsahové klauze buď s modalitou nutnosti, nebo možnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper focuses on Czech verbs of communication with imperative features. One of their valency complementation corresponding to the participant 'Message' is morphematically realized as dependent content clause prototypically introduced by the conjunction 'aby' which may be replaced by the conjunction 'že' on certain conditions. We analyzed these conditions on the basis of corpus evidence. We conclude that this class of verbs of communication prefer either the modality of necessity, or possibility. On the other hand, the corpus material does not prove that these verbs are associated only with single modal meaning distinguished within the mentioned categories.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku popisujeme pokus o přiřazení sémantických informací z FrameNetu lexikálním jednotkám z VALLEXu, valenčního slovníku českých sloves. Popisujeme experiment s přiřazením sémantických rámců lexikálním jednotkám sloves mluvení. Další část experimentu spočívá v provázání valenčních doplnění daných sloves s elementy sémantických rámců Z FrameNetu. Mezianotátorská shoda týkající se přiřazení sémantických rámců dosáhla téměř 69%. Shody 84.6%  dosáhli anotátoři při přiřazení jednotlivých elementů sémantických rámců valenčním doplněním. Na závěr navrhujeme při obohacení VALLEXu o chybějící sémantické informace z FrameNetu využít sémantického vztahu dědičnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we report on our attempt at assigning semantic
information from FrameNet to lexical units in VALLEX, a valency
lexicon of Czech verbs. We focus on the class of communication
verbs. We experiment with assigning FrameNet semantic frames to
lexical units for communication verbs. The second task consists in linking
their valency complementations with FrameNet frame elements. The
exact pairwise inter-annotator agreement reaches almost 69% on
the semantic frames and 84.6% on the frame elements. We propose
enhancing VALLEX with missing semantic information from FrameNet
based on exploitation of the semantic relation 'Inheritance'.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zavádíme "Zlaté pravidlo morfologie", které vyžaduje odlišný morfologický popis každého slovního tvaru. V současných morfologických systémech češtiny toto pravidlo nebývá splněno, především kvůli nekonzistentnímu popisu různých typů morfologických a ortografických variant. Definujeme dvě hlavní typy variant - globální a flektivní.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents „Golden Rule of Morphology” that requires different morphological tags for different wordforms. This requirement is not fulfilled in recent morphological systems of Czech, mainly due to inconsistent treatment of different types of morphological and orthographic variants. We define two major types of variants, namely inflectional variants, that concern only some combinations of morphological categories, and global variants that affect all wordforms of a given lemma. We propose how to tag the variants in order that they belonged to the same lemma, but were still distinguishable.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme nástroj Affisix pro automatické rozpoznávání předpon. Na základě rozsáhlého seznamu slov určujeme řetězce - kandidáty na předpony. V nástroji jsou implementovány dvě metody - metoda entropie a metoda čtverců.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the paper, we present a software tool Affisix for automatic recognition of prefixes. On the basis of an extensive list of words in a language, it determines the segments - candidates for prefixes. There are two methods implemented for the recognition - the entropy method and the  squares method. We briefly describe the methods, propose their improvements and present
the results of experiments with Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nový systém morfologie, který zde prezentujeme, vychází ze současného pražského systému. Kromě nové implementace stávajícího slovníku pomocí konečného automatu přidává algoritmy na rozpoznání neznámých tvarů. V současné fázi je to především využití seznamu předpon, které lidé připojují bez větších omezení ke slovům a činí je tak pro automatické metody nerozpoznatelnými. Stačí předponu rozpoznat, odtrhnout a analyzovat zbylý řetězec. Výsledky této analýzy lze pak jednoduše aplikovat na původní tvar s předponou. K rozpoznání neznámých vlastních jmen používáme heuristiku. V příspěvku stručně popíšeme stávající pražský systém a použitý zárodek guessru.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>New system of automatic morphology of Czech is based on the existing Prague system. New features are mainly recognition of OOV prefixed words and a heuristics about foreign names.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zabýváme se dvěma typy asymetrie mezi slovními tvary a jejich (morfologickými) charakteristikami, totiž (morfologickými) variantami a homonymy. Zavádíme tzv. vícenásobné lemma.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We discuss two types of asymmetry between wordforms and their (morphological) characteristics, namely (morphological) variants
and homographs. We introduce a concept of multiple lemma that allows for unique identification of wordform variants as well as
`morphologically-based' identification of homographic lexemes. The deeper insight into these concepts allows further refining of morphological dictionaries and subsequently better performance of any NLP tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek se zabývá lexikografickým zachycením různých typů reflexívních sloves ve velkých německých jednojazyčných slovnících.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This contribution addresses various types of reflexivity in Germans verbs and their lexicographical description in current large monolingual dictionaries.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje manuální anotaci rekonstrukce mluvené řeči na dialogovém korpusu NAP. Korpus NAP obsahuje rozhovory anglických mluvčích nad alby fotografií. Anotace vzniká v rámci projektu Companions.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the ongoing manual speech reconstruction annotation of the NAP corpus, which is a corpus of recorded conversations between pairs of people above family photographs, relating it to a more complex annotation scheme of the Prague Dependency Treebank family. The result of this effort will be a resource that will contain, on top of the audio recording of the dialog and its usual transcription, an edited and fully grammatical “reconstructed” dialog. The format and alignment with the original audio and transcription on one side and a similar alignment (linking) to a deep analysis of the natural language sentences uttered in the dialog on the other side will be such that the resource can serve as a training and testing material for machine learning experiments in both intelligent editing as well as in dialog language understanding. The resource will be used in the Companions project, but it will be publicly available outside of the project as well.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek ilustruje, jak se tektogramatický popis dá použít jako měřítko porovnávání dvou nepříbuzných jazyků - zde češtiny a angličtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper is aimed to illustrate how the description of underlying structures carried out in annotating Czech texts may be used as a basis for comparison with a more or less parallel description of English. Specific attention is given to several points in which there are differences between the two languages that concern not only their surface or outer form, but (possibly) also their underlying structures.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek ilustruje, jak je možné využít tektogramatickou reprezentaci jako měřítko porovnávání dvou nepříbuzných jazyků - zde češtiny a angličtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This contribution illustrates how the description of underlying structures carried out in annotating Czech texts may be used as a basis for comparison with a more or less parallel description of English.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato publikace je návodem pro anotátory, jak anotovat rovinu rekonstrukce anglické mluvené řeči. Anotace zvaná Rekonstrukce mluvené řeči je prováděna na českém a anglickém korpusu a má dále sloužit pro uplatnění metod strojového učení na přirozený jazyk. Anotace obsahuje původní doslovný přepis nahrávek a také editovanou verzi, která vyhovuje standardům psaného textu. Obě verze představují oddělené roviny, které jsou navzájem propojeny na úrovni akustických segmentů, vět editovaného textu a jednotlivých slov.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This is an annotation manual of English speech reconstruction. Speech reconstruction has been carried out at the Prague Dependency Treebank of Spoken Language, which consists of a Czech and an English, to be used for the task of speech understanding, broad natural language analysis for dialog systems and other speech-related tasks, including speech editing. The resources we have created so far contain audio and a standard transcription of spontaneous speech, but as a novel layer, we add an edited (“reconstructed”) version of the spoken utterances.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje korpusové metody pro zjišťování změn v užívání slov. Metody byly použity na dvou stomilionových korpusech češtiny ze dvou po sobě jdoucích období.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents a corpus-based method for obtaining ranked wordlists that can
characterise lexical usage changes. The method is evaluated on two 100-million
representatively balanced corpora of contemporary written Czech that cover two consecutive
time periods. Despite similar overall design of the corpora, lexical frequencies have to be first
normalised in order to achieve comparability. Furthermore, dispersion information is used to
reduce the number of domain-specific items, as their frequencies highly depend on inclusion
of particular texts into the corpus. Statistical significance measures are finally used for
evaluation of frequency differences between individual items in both corpora.
It is demonstrated that the method ranks the resulting wordlists appropriately and several
limitations of the approach are also discussed. Influence of corpora composition cannot be
completely obliterated and comparability of the corpora is shown to play a key role.
Therefore, although highly-ranked items are often found to be related to changes of
language usage, their relevance should be cautiously interpreted. In addition to several
general language words, the real examples of lexical variation are found to be limited
mostly to temporary topics of public discourse or items reflecting recent technological
development, thus sketching an overall picture of lifestyle changes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pražský závislostní korpus (PDT) je cenným zdrojem lingvistických informací anotovaných na několika rovinách. Anotační roviny pokrývají škálu od mělkých po hloubkové a měly by obsahovat veškerou lingvistickou informaci o textu. Je tedy přirozené rozšířit je o sémantickou rovinu, která by sloužila jako báze znalostí pro úlohy jako je zodpovídání dotazů, extrakce informací atd.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Prague Dependency Treebank (PDT) is a valuable resource
of linguistic information annotated on several layers. These
layers range from shallow to deep and they should contain all
the linguistic information about the text. The natural extension
is to add a semantic layer suitable as a knowledge base
for tasks like question answering, information extraction etc.
In this thesis I set up criteria for this representation, explore
the possible formalisms for this task and discuss their properties.
One of them, Multilayered Extended Semantic Networks
(MultiNet), is chosen for further investigation. Its properties
are described and an annotation process set up. I discuss some
practical modifications of MultiNet for the purpose of manual
annotation. MultiNet elements are compared to the elements
of the deep linguistic layer of PDT. The tools and problems of
the annotation process are presented and initial annotation data
evaluated.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pražský závislostní korpus (PDT) je cenným zdrojem lingvistických informací anotovaných na několika rovinách. Anotační roviny pokrývají škálu od mělkých po hloubkové a měly by obsahovat veškerou lingvistickou informaci o textu. Je tedy přirozené rozšířit je o sémantickou rovinu, která by sloužila jako báze znalostí pro úlohy jako je zodpovídání dotazů, extrakce informací atd.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Prague Dependency Treebank (PDT) is a valuable resource
of linguistic information annotated on several layers. These
layers range from shallow to deep and they should contain all
the linguistic information about the text. The natural extension
is to add a semantic layer suitable as a knowledge base
for tasks like question answering, information extraction etc.
In this thesis I set up criteria for this representation, explore
the possible formalisms for this task and discuss their properties.
One of them, Multilayered Extended Semantic Networks
(MultiNet), is chosen for further investigation. Its properties
are described and an annotation process set up. I discuss some
practical modifications of MultiNet for the purpose of manual
annotation. MultiNet elements are compared to the elements
of the deep linguistic layer of PDT. The tools and problems of
the annotation process are presented and initial annotation data
evaluated.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme vyhodnocení anotace mezivětné koreference v kontextu ručně vytvořených sémantických sítí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an evaluation of inter-sentential coreference annotation in the context of manually created semantic networks. The semantic
networks are constructed independently be each annotator and require an entity mapping priori to evaluating the coreference. We
introduce a model used for mapping the semantic entities as well as an algorithm used for our evaluation task. Finally, we report the raw
statistics for inter-annotator agreement and describe the inherent difficulty in evaluating coreference in semantic networks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zcela přepracovaná verze systému STYX 2007:
- vylepšena pravidla filtrování vět
- vylepšeny transformace syntaktických stromů do školské podoby
- úpravy uživatelského rozhraní
- opravy chyb 

Systém STYX je elektronickou cvičebnicí češtiny postavenou nad daty Pražského závislostního korpusu. Skládá se z konzolové aplikace určené k filtrování vět (FilterSentences), administračního programu sloužícího k sestavování cvičení (Charon) a vlastní cvičebnice nazvané Styx.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Complete rewrite of the STYX 2008 system:
- improved rules for sentence filtering
- improved transformations of syntactic trees to the school form
- rewritten user interface
- bugfixes

STYX is an electronic exercise book of Czech based on data of the Prague Dependency Treebank. It consists of a console application used to filter unsuitable sentences (FilterSentences), an administrative application for creating exercises (Charon) and the exercise book itself, Styx.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem naší práce je představit elektronickou cvičebnici českého jazyka o celkovém objemu 12 tisíc vět k procvičování ve dvou oblastech: tvarosloví (určování slovních druhů a jejich morfologických kategorií) a větný rozbor (určování větných členů a vztahů mezi nimi). Uživatelům je k dispozici pro všechny věty i klíč k řešení. Cvičebnice je sestavena z výběru vět, které obsahuje akademický Pražský závislostní korpus.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce an electronic exercise book of Czech that consists of 12 thousand sentences to practice morphology and syntax. The correct answers are available to the users as well. The sentences in the exercise book represent a selection from the Prague Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Victoria je nástroj pro anotování textu na webových stránkách pomocí webového prohlížeče. Stránka je automaticky stažena ze zadaného URL, analyzována a anotátor může vybrat kategorii pro každý blok textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Victoria is a tool for web-page anotation using a web browser. Web page is downloaded and analzyed. Annotator can simply select a category for every text block.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku prezentujeme kompletní řešení pro automatické čištění HTML stránek, jehož cílem je použití webových dat pro vytvoření korpusu textů pro zpracování přirozeného jazyka nebo pro lingvistiku. Používáme algoritmus sekvenčního značkování Conditional Random Fields. Každému bloku textu analyzované webové stránky je přiřazena sada rysů extrahovaná z textu a HTML struktury stránky. Blokům jsou pak automaticky přiřazeny značky, které říkají, zda má být blok zachován, nebo odstraněn. Naše řešení je založeno na nástroji z CLEANEVAL 2007.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we present a complete solution for automatic cleaning of arbitrary HTML pages with a goal of using web data as a corpus in the area of natural language processing and computational linguistics. We employ a sequence-labeling approach based on Conditional Random Fields (CRF). Every block of text in analyzed web page is assigned a set of features extracted from the textual content and HTML structure of the page. The blocks are automatically labeled either as content segments containing main web page content, which
should be preserved, or as noisy segments not suitable for further linguistic processing, which should be eliminated. Our solution is based on the tool introduced at the CLEANEVAL 2007 shared task workshop. In this paper, we present new CRF features, a handy annotation tool, and new evaluation metrics. Evaluation itself is performed on a random sample of web pages automatically downloaded from the Czech web domain.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje velké textové korpusy Českého národního korpusu a jejich morfologické značkování a lemmatizaci. Popisuje metody značkování a jejich úspěšnost ve třech velkých korpusech ČNK.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents large textual corpora of the Czech National Corpus and their morphological tagging and lemmatization. It describes the methods used for morphological tagging and their precision in three large corpora of ČNK.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje experimentální systém strojového překladu pro mobilní zařízení a jeho hlavní komponentu - distribuovanou databázi pro pravidla lexikálního transferu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents an experimental machine translation system for mobile devices and its main component — a distributed database which is used in the module of lexical transfer. The database contains data shared among multiple devices and provides their automatic synchronization.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se věnuje popisu situace české (moravské) národnostní menšiny v pruském Slezsku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article describes the situation of the Czech (Moravian) minority in the Prussian part of Silesia.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se věnuje popisu situace slovinsé národnostní menšiny v rakouských Korutanech z historického hlediska.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article describes the situation of the Slovenian minority in Carianthia in Austria from the historical point of view.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Reimplementace interpretu Q-systémů v C++. Rozšíření o pojmenované proměnné a eliminaci duplicitních hran.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A reimplementation of the Q-systems interpreter in C++. Allows for named variables and elimination of multiple equal edges.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje formalismus pro mělký parsing pro překlad mezi příbuznými jazyky. Formalismus umožňuje psát pravidla pro částečnou disambiguaci vstupních vět. Stochastický ranker vybírá nejvhodnější překlad v závislosti na jazykovém modelu cílového jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes a shallow parsing formalism aiming at machine translation between closely related languages. The formalism allows to write grammar rules helping to (partially) disambiguate chunks in input sentences. The chunks are then translatred into the target language without any deep syntactic or semantic processing. A stochastic ranker then selects the best translation according to the target language model. The results obtained for Czech and Slovak are presented.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek zavádí hybridní přístup ke strojovému překladu mezi příbuznými jazyky. Zmiňuje předchozí experimenty pro skandinávské, slovanské, turkické a románské jazyky a popisuje novou metodu, kombinaci mělkého parseru a stochastického rankeru.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper introduces a hybrid approach to a very specific field in machine translation — the translation of closely related languages. It mentions previous experiments performed for closely related Scandinavian, Slavic, Turkic and Romanic languages and describes a novel method, a combination of a simple shallow parser of the source language (Czech) combined with a stochastic ranker of (parts of) sentences in the target language (Slovak, Russian). The ranker exploits a simple stochastic model of the target language and its main task is to choose the best translation among those provided by the system. The last section of the paper presents results indicating better translation quality compared to the existing MT system for Czech and Slovak and compares them top the results obtained by the translation from Czech to Russian using the same system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Reimplementace systému Česílko. Jedná se o verzi s mělkým parsingem a stochastickým rankerem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A reimplementation of the system Česílko. This version contains a shallow parser and a stochastic ranker.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje hybridní přístup k překladu blízce příbuzných jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents a hybrid approach to MT between related languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje vylepšení architektury systému Apertium pro pár portugalština-španělština.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper gives an overview of the shallow-transfer MT system Apertium, describes an experiment with the language pair Portuguese-Spanish and suggests a modification of the system architecture which leads to higher translation quality. Finally, consequences of the architecture improvement for the design of language resources for shallow-transfer based systems are discussed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje použití částečného parsingu v systému strojového překladu pro slovanské jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes the use of partial parsing in an MT system for Slavic languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje výsledky, které byly dosaženy v mezinárodním výzkumném projektu LT4eL z pohledu jednoho jazyka zpracovávaného v projektu - češtiny. Cílem projektu je využití jazykových technologií pro rozšíření funkcionality open source systém pro e-learning ILIAS. Nová funkcionalita je založena na stávajícíh a nově vytvořených nástrojích pro všechny zapojené jazyky. Článek podrobně popisuje problémy implementace extraktoru klíčových slov a slovníkových definic z textu pro češtinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the efforts undertaken in an international research project LT4eL from the perspective of one of the participating languages, Czech. The project aims at exploiting language technologies for adding new functionalities to an open source Learning Management System ILIAS. The new functionalities are based both on existing and newly developed tools for all languages involved. The paper describes in detail the issues of the implementation of a keyword extractor and a glossary candidate detector for Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku se zaměříme na využití technologií zpracování přirozeného jazyka k vylepšení současných systémů pro výuku (e-learning). Zaměříme se na vyhledávání definic a klíčových slov a technologii sémantického vyhledávání.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we present recent advances in using language technology to improve current learning management systems. We focus on definitions and keyword search as well as semantic search technology.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku pojednáme o probíhajícím projektu poloautomatické anotace víceslovných výrazů. V prvém plánu jde o doplňování dodatečné informace k datům PDT 2.0, v dlouhodobějším pohledu by mělo jít o součást přiblížení tektogramatické vrstvy PDT k tomu, jak byla tektogramatická analýza ve Funkčním generativním popisu (FGP) navržena. Vysvětlíme, jakou roli v našem pojetí hraje ruční anotace a co je možno provést automaticky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this article we want to demonstrate that
annotation of multiword expressions in the
Prague Dependency Treebank is a well deﬁned task and that it is useful as well as feasible.
We argue that some automatic pre-annotation is possible and it does not damage the results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku chceme ukázat, že anotace víceslovných výrazů Pražského závislostního korpusu je dobře definovaný úkol, že je potřebný a proveditelný, a že můžeme dosáhnout dobré mezianotátorské shody.
Ukážeme způsob, jak měřit shodu pro tento druh anotace. Dále tvrdíme, že určitá automatická předanotace je možná a nepoškozuje výsledky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this article we want to demonstrate that annotation of multiword expressions in the Prague Dependency Treebank is a well defined task, that it is useful as well as feasible, and that we can achieve good consistency of such annotations in terms of inter-annotator agreement. We show a way to measure agreement for this type of annotation. We also argue that some automatic pre-annotation is possible and it does not damage the results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá otázkou, zda zájmena uvozující obsahové věty klasifikovat jako tázací nebo vztažná. V české lingvistické literatuře lze najít oba přístupy, záleží přitom na tom, jak jsou v jednotlivých příručkách tyto dvě zájmenné podskupiny definovány. V příspěvku se přikláníme ke klasifikaci zájmen uvozujících obsahové věty jako zájmen tázacích. Výhody, které tento přístup podle našeho názoru přináší, se pokoušíme objasnit.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the contribution, the problem how pronouns introducing content clauses are classified in Czech linguistic literature is discussed. The classification of these pronouns depends on how interrogative and relative pronouns are defined in the given approaches. Basically, two different types of approaches are described. Firstly, relative pronouns are defined broadly as only attaching a dependent clause to its governing one; pronouns introducing content clauses belong then to this subgroup of pronouns. In this connection, interrogative pronouns occur only in direct questions. In approaches of the second type, which we subscribe to, the constitutive feature of relative pronouns is their relation to an element of the governing clause. The main characteristic of interrogative pronouns is the reference to an unknown person/object – pronouns introducing both questions and content clauses are interpreted as interrogatives; this classification is advantageous for several reasons, which we try to indicate.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ačkoli jsou vlastní jména nedílnou součástí textů, a tedy i textových korpusů, byla jim (přinejmenším) v české korpusové lingvistice dosud věnována jen okrajová pozornost. V příspěvku představíme, jak jsou vlastní jména popsána v dostupných korpusech češtiny. Ukážeme, že dosavadní způsob zachycení je nedostatečný, a pokusíme se proto navrhnout nový způsob jejich reprezentace. V Pražském závislostním korpusu 2.0 (PDT 2.0) by reprezentace vlastních jmen měla být součástí popisu větné sémantiky, pro tyto účely je nutné stávající anotační schéma adaptovat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Although proper nouns are an inseparable part of natural language texts and thus of text corpora, (at least) Czech corpus linguistics has been paying little attention to this area. We demonstrate that Czech corpora can be certainly used as a rich source for study of proper nouns. Firstly, we introduce how proper nouns are handled in present Czech corpora: e.g., in the Prague Dependency Treebank 2.0 a very simple annotation of proper nouns is involved in the morphological annotation whereas the syntactic annotation scheme of this corpus does not take proper nouns into consideration. We list several reasons why we consider such annotations to be insufficient and unsuitable. As for the position of the annotation of proper nouns within a multi-layered annotation scheme, we arrive at the conclusion that proper nouns could be annotated at a (deep) syntactic layer. Therefore, the annotation schemes have to be adapted for this purpose.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Recenze se zabývá sborníkem Language in its multifarious aspects, v němž jsou shromážděny články prof. Petra Sgalla týkající se obecných a teoretických témat, syntaxe, aktuálnho členění, otázkám významu a obsahu, typologie jazyků a rysům psané a mluvené češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The book "Language in its multifarious aspects" is reviewed, which involves Petr Sgall's articles on general and theoretical issues, syntax, topic-focus articulation, relation between meaning and content, typology of languages, and spoken and written Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valenční slovník zachycuje slovesa, jejich vazby, povinná a volná doplnění. Valenční slovník moderní spisovné arabštiny je budován s pomocí příkladů z Pražského arabského závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A valency dictionary presents verbs, their valency, obligatory and free modifiers. The valency dictionary of the Modern Standard Arabic language has been built using examples from the Prague Arabic Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek porovnává architekturu dvou morfo-syntaktických generátorů vět, generujících český a anglický výstup. Popisujeme vstupní struktury: závislostní stromy z Funkčního generativního popisu P.Sgalla a také vlastní systém generování vět v jazycích, které jsou typologicky velmí odlišné . Výkon obou generátorů je nakonec měřen pomocí BLEU skóre.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a work in progress on a pair of morpho-
syntactic realizers sharing the same architecture. We provide
description of input tree structures, describe our procedural
approach on two typologically different languages and finally
present preliminary evaluation results conducted on manually
annotated treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje projekt Pražského závislostního korpusu: jeho teoretické pozadí, rozdělení na roviny, anotační schémata a nástroje používané k anotaci. Udává také několik přikladů práce s treebankem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article presents the project of Prague Dependency Treebank 2.0: its theoretical background, division into layers, annotation schemata and tools used in annotation process. It gives several examples how the treebank could be used.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pražský závislostní korpus (PDT 2.0) obsahuje velké množství českých textů doplněných rozsáhlou a provázanou morfologickou (2 milióny slovních jednotek), syntaktickou (1,5 miliónu slovních jednotek) a sémantickou (0,8 miliónu slovních jednotek) anotací; na sémantické rovině jsou navíc anotovány aktuální členění věty a koreferenční vztahy.
PDT 2.0 vychází z dlouhodobé pražské lingvistické tradice, upravené pro současné potřeby výzkumu v oblasti komputační lingvistiky. Samotný korpus využívá nejnovější anotační technologie.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Prague Dependency Treebank 2.0 (PDT 2.0) contains a large amount of Czech texts with complex and interlinked morphological (2 million words), syntactic (1.5 MW) and complex semantic annotation (0.8 MW); in addition, certain properties of sentence information structure and coreference relations are annotated at the semantic level. PDT 2.0 is based on the long-standing Praguian linguistic tradition, adapted for the current Computational Linguistics research needs. The corpus itself uses the latest annotation technology. 
Besides the large corpus of Czech, a corpus of Czech-English parallel resources (The Prague Czech-English Dependency Treebank) is being developed. English sentences from the Wall Street Journal and their translations into Czech are being annotated in the same way as in PDT 2.0. This corpus is suitable for experiments in machine translation, with a special emphasis on dependency-based (structural) translation.
In the report, the basic annotation scheme is represented, with special reference to complex semantic (tectogrammatical) level. The system of syntactic functors and valency lexicon VALLEX are also discussed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek prověřuje tři metody intonační stylizace v češtině: sekvenci intonačních přízvuků, sekvenci hraničních tónů a sekvenci kontur. Účinnost těchto metod byla porovnána pomocí neuronové sítě, která předpovídala křivku F0 z každého ze tří uvedených vstupů, s následným percepčním vyhodnocením. Výsledky ukázaly, že českou intonaci se lze naučit přibližně se stejnou úspěšností ve všech třech případech. To hovoří ve prospěch rehabilitace kontur, jakožto tradičního popisu české intonace, stejně jako užití hraničních tónů, jakožto alternativního lokálního pohledu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper examines three methods of intonational
stylization in the Czech language: a sequence of pitch accents, a sequence of boundary tones, and a sequence of contours. The efficiency of these methods was compared by means of a neural network which predicted the f0 curve from each of the three types of input, with subsequent perceptual assessment. The results show that Czech intonation can be learned with about the same success rate in all three situations. This speaks in favour of a rehabilitation of contours as a traditional means of describing Czech intonation, as well as the use of boundary tones as another possible local approach.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Syntéza emotivního mluveného projevu je stále výzvou. Centrálním tématem výzkumu je jak začlenit do syntézy mluvené řeči emotivní projevy. \v tomto článku popisujeme dve konkatenativní syntézové systémy a navrhujeme nové přístupy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The synthesis of emotional speech is still an open
question. The principal issue is how to introduce expressivity without compromising the naturalness of the synthetic speech provided by the state-of-the-art technology. In this paper two
concatenative synthesis systems are described and some approaches to address this topic are proposed. For example,considering the intrinsic expressivity of certain speech acts, by
exploiting the correlation between affective states and
communicative functions, has proven an effective solution. This
implies a different approach in the design of the speech
databases as well as in the labelling and selection of the
“expressive” units. In fact, beyond phonetic and prosodic
criteria, linguistic and pragmatic aspects should also be
considered. The management of units of different type (neutral
vs expressive) is also an important issue.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Victor je nástroj pro čištění webových stránek.
Používá algoritmus sekvenčního značkování Conditional Random Fields. Každému bloku textu analyzované webové stránky je přiřazena sada rysů, která je extrahovaná z textu a HTML struktury stránky. Blokům jsou pak automaticky přiřazeny značky, které říkají, zda má být blok zachován, nebo odstraněn.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Victor is a tool for cleaning web pages. It employs a sequence-labeling approach based on Conditional Random Fields (CRF). Every block of text in the analyzed web page is assigned a set of features extracted from the textual content and HTML structure of the page. Text blocks are automatically labeled either as content segments containing main web page content, which should be preserved, or as noisy segments not suitable for further linguistic processing, which should be eliminated.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek představuje přípravný výzkum v oblasti slovesné valence a argumentové struktury. V souvislosti s právě budovaným vícejazyčným valenčním slovníkem si klademe otázku, zdali má být struktura hesel spíše jednoduchá a nestrukturovaná, či zda může být výhodné vybudovat ji hierarchicky a zachytit tak vztahy mezi jednotkami na více úrovních, včetně vztahů v rámci každého jazyka zvlášť.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents a preliminary research in the area of verbal valency and argument structure theory. With the perspective of building a multilingual archive of valency characteristics of verbs, the question is raised whether the structure of such a linguistic resource should be straight and simple, or to some extent hierarchical and capturing more relation types, including those among individual frames within a single language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>úvodní článek k časopiseckému číslu zaměřenému na aktuální členění a výstavbu diskursu</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>introductory article on journal number concerning the topic-focus articulation and discourse</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Role přísudkových sloves, rematizovaných skupin a příslovečných určení v aktuálním členění v českých větách</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The role of predicate verbs, focalized groups and adverbials in topic-focus articulation in Czech sentences - problematic points</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slovosledné postavení příklonky se/so v aktivních a pasivních konstrukcích</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Word order position of the clitic se/so in active and passive constructions</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek pojednává o nejednoznačnosti aktuálního členění výpovědi v básnické tvorbě Otokara Březiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article discusses ambiguity of the topic-focus articulation in the poetry of Otokar Březina.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popis česko-anglicko-ruského korpusu (po dvojicích paralelní) pro strojový překlad.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The description of a pairwise parallel English-Czech-Russian corpus for machine translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Compost Czech je program kombinující morfologickou analýzu z PDT 2.0 a tagger Morče s novým semi-supervised trénováním. Výsledný tagger má na češtině nejlepší úspěšnost: 96 %</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Compost Czech is a tool which combines the PDT 2.0 morphological analyzer and the Morce tagger using an innovative semi-supervised training method. The resulting tagger gives the best accuracy achieved for Czech (on standard PDT 2.0 data set) so far: 96 %</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tématem této práce jsou metody pro morfologické značkování (tagging) češtiny, zejména pak nejnovější experimenty s kombinováním pravidlových a statistických metod.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The thesis consists of three parts, which are all related to a rule-based morphological disambiguation project.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje několik disambiguačních metod, které kombinují ručně psaná pravidla a stochastické taggery (Feature-based, HMM, průměrovaný perceptron).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Several hybrid disambiguation methods are described which
combine the strength of hand-written disambiguation rules
and statistical taggers. Three different statistical (HMM,
Maximum-Entropy and Averaged Perceptron) taggers are used in
a tagging experiment using Prague Dependency Treebank.
The results of the hybrid systems are better than any other method
tried for Czech tagging so far.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje konverzi slovníku povrchové valence českých sloves do valenčního slovníku deverbativních adjektiv.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes conversion of a surface valency lexicon of Czech verbs
to a surface valency lexicon of adjectives that can be derived from these
verbs and that use their (possibly modified) valency frames.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje rozšíření pravidlového disambiguačního systému, které umožňuje využít disambiguační pravidla pro popvrchovou syntaktickou analýzu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper describes an extension to a rule-based morphological
disambiguation system, which makes the disambiguation rules
usable for shallow parsing. The whole system
(both the morphological disambiguation and the syntactic
extension) is under development, but it already gives some
promising results. Several methods of using the rules to
improve the performance of statistical parsers are discussed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje vyhledávací experimenty týmu z Karlovy Univerzity v Praze na soutěži CLEF 2007 Ad-Hoc. Zaměřili jsme se na monolinguální úlohu a použili nástroj LEMUR pro náš vyhledávač. Naše výsledky ukazují, že pro češtinu lemmatizace signifikantně vylepšuje výsledky vyhledávání a manuální tvorba dotazů je pouze těsně lepší než automatickátvorba dotazů s popisu témet.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we describe retrieval experiments performed at Charles University in
Prague for participation in the CLEF 2007 Ad-Hoc track. We focused on the Czech
monolingual task and used the LEMUR toolkit as the retrieval system. Our results
demonstrate that for Czech as a highly inflectional language, lemmatization significantly
improves retrieval results and manually created queries are only slightly better
than queries automatically generated from topic specifications.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje experimenty s vyhledáváním v mluvené řeči týmu z Karlovy Univerzity provedené v rámci evaluační kampaně CLEF 2007.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes a system built at Charles University in Prague for participation
in the CLEF 2007 Cross-Language Speech Retrieval track. We focused only on monolingual
searching the Czech collection and used the LEMUR toolkit as the retrieval
system. We employed own morphological tagger and lemmatized the collection before
indexing to deal with the rich morphology in Czech which significantly improved our
results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku je prezentován algoritmus pro rozpoznávání anaforických vztahů v českém textu. Vyhodnocení je provedeno s pomocí dat Pražského závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present a rule-based approach to resolution of anaphora links, as annotated in the Prague Dependency Treebank 2.0. The
created system consists of handwritten rules developed and tested using the Treebank data, which contain more than 45,000 coreference
links in almost 50,000 manually annotated Czech sentences. The F-measure of our system is 74.2%.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek opakuje specifické rysy předponových sloves v češtině a přístupy k jejich zachycení ve valenčním slovníku. Shrnujeme prostředky a funkce slovesné prefixace. Ukazujeme pravidelný vztah mezi základními a odvozenými slovesy s ohledem na jejich syntaktické a sémantické vlastnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper reviews the specific features of prefixed verbs in Czech and approaches to their capture in a valency lexicon. We summarise means and functions of verbal prefixation. We illustrate a regular relation between base verbs and derived verbs as to their syntactic and semantic properties.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku je prezentován nový algoritmus pro analýzu vět v přirozeném jazyce, který vede k vyšší úspěšnosti při zpracování koordinačních konstrukcí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an algorithm for parsing with detection of intra-clausal coordinations. The algorithm is based on machine learning techniques and helps to decompose a large parsing problem into several smaller ones. Its performance was tested on Slovene Dependency Treebank. Used together with the maximum spanning tree parsing algorithm it improved parsing accuracy.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popis experimentů s frázovým statistickým překladem z angličtiny do češtiny s cílem zlepšit tvaroslovnou koherenci výstupu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes experiments with English-to-Czech phrase-based machine
translation. Additional annotation of input and output tokens (multiple factors)
is used to explicitly model morphology.
We vary the translation scenario (the setup of multiple factors) and the amount
of information in the morphological tags.
Experimental
results demonstrate significant improvement of translation quality in terms of
BLEU.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Webová ukázka několika konfigurací systému strojového překladu mezi angličtinou a češtinou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An interactive demonstration of English-Czech machine translation in a simple web interface.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Experimentální systém strojového překladu s převodem stromu na strom.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A release of an experimental MT system with tree-to-tree transfer.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Návrh matematického modelu pro převod věty přirozeného jazyka reprezentované jako závislostní strom na závislostní strom v cílové řeči.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A proposed mathematical model mapping dependency analyses of source sentences in one language to dependency structures representing the sentence in another language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Česko-anglický paralelní korpus určený pro experimenty se strojovým překladem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Czech-English parallel corpus for Machine Translation experiments.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje systém automatického překladu z angličtiny do češtiny založený na syntaktickém transferu na rovině hloubkové syntaxe (tzv. tektogramatické rovině). Podrobně je popsán anotační proces pro angličtinu a také vyhodnocena kvalita překladu prototypu celého systému.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an overview of an English-to-Czech machine translation sys-
tem. The system relies on transfer at the tectogrammatical (deep syntactic)
layer of the language description. We report on the progress of linguistic
annotation of English tectogrammatical layer and also on first end-to-end
evaluation of our syntax-based MT system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Navrhujeme formát překladových slovníků vhodný pro strojový překlad. Formát je úsporný a zobecňuje položky slovníku zavedením pravidel pro morfologické generování.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We are proposing a format for translation dictionaries suitable for
machine translation. The dictionary format is concise and generalizes phrases by
introducing rules for morphological generation instead of using simple phrase to
phrase mapping.

We describe a simple way how to automatically construct our compact entries from
a machine-readable dictionary originally intended for human users using parallel
corpora. We further describe how to expand the compact dictionary entries to
phrase table dictionary that
can be used further on by machine translation systems (until the systems will
support morphological generation from a translation dictionary natively).
We performed manual annotation of a small set of entries to analyze problems of
this approach.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V posledních dvou nebo třech desetiletích se v počítačové lingvistice aplikují statistické metody, a to nejen v oblasti automatického rozpoznávání řeči, ale i v ostatních jazykových oblastech. V pozvané přednášce se autoři zabývají argumenty pro i proti uplatnění těchto metod a argumentují ve prospěch  hybridní metodologie</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the recent twenty or thirty years, a great interest in computational linguistics is paid to the application of statistical methods. Starting with the domain of speech recognition, this trend has become quite obvious also in the analysis of other linguistic domains. In the invited paper, the authors analyze the pros and cons for these methods, in relation to rule-based approaches and argue for a hybrid methodology.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>První část testovací části soutěžních dat pro CoNNL 2007 Shared Task pro Arabštinu, obsahující Arabskou anotaci na syntaktické a sémantické rovině. .</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Part one of the evaluation subset of the Arabic data set for the CoNLL 2007 Shared Task. The core of the task is to predict syntactic and semantic dependencies and their labeling.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Druhá část testovacích soutěžních dat pro CoNNL 2007 Shared Task pro Arabštinu, obsahující Arabskou anotaci na syntaktické a sémantické rovině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Part two of the evaluation subset of the Arabic data set for the CoNLL 2007 Shared Task. The core of the task is to predict syntactic and semantic dependencies and their labeling.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Soutěžní data pro CoNNL 2007 Shared Task pro Arabštinu, obsahující Arabskou anotaci na syntaktické a sémantické rovině. Toto je trénovací část.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An Arabic data set for the CoNLL 2007 Shared Task. The core of the task is to predict syntactic and semantic dependencies and their labeling. This is the training set.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku prezentujeme výsledky základních experimentů pro automatické extrahování definic (pro automatické generování glosářů) z nestrukturovaného (případně jen málo strukturovaného) textu v bulharštině, češtině a polštině. Extrakce je prováděna pomocí regulárních gramatik, které jsou použity na dokumenty v jednotném XML formátu. Výsledky nejsou uspokojivé a ukazujeme, že příčina je ve vnitřní složitosti tohoto úkolu, k čemuž nás opravňuje nízká mezianotátorská shoda. Dále navrhujeme zpracování pomocí hlubší lingvistické analýzy a klasifikačních metod strojového učení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the preliminary experiments in the automatic extraction of definitions (for semi-automatic glossary construction) from usually unstructured or only weakly structured e-learning texts in Bulgarian, Czech and Polish. The extraction is performed by regular grammars over XML-encoded morphosyntactically-annotated documents. The results are less than satisfying and we claim that the reason for that is the intrinsic difficulty of the task, as measured by the low interannotator agreement, which calls for more sophisticated deeper linguistic processing, as well as for the use of machine learning classification techniques.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Stupňování adjektiv je pomezní oblastí mezi morfologií a slovotvorbou. Komparativ je spojen s obligatorní valenční pozicí srovnání, které je na povrchu často vypuštěno. Zkoumají se podmínky jeho vypuštění a analyzují se konstrukce s "absolutním" komparativem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The position of gradation (degrees of comparison) of adjectives between morphology and word-formation is discussed as well as the determination of the semantic and syntactic characteristics of comparatives of adjectives (CMPR). The form of a comparative is connected with an obligatory valency slot filled by the modification of comparison (CPR); however, this valency member is very often missing on the surface. The reasons of the absence of this member are analyzed. The examples usually described as an “absolute” usage of CMPR are described and classified.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Stať je rozšířením autorčina přístupu k popisu recipročních konstrukcí z r. 1999. Je třeba rozlišovat inherentní lexikální reciprocitu a reciprocitu syntaktickou (obecněji přijatelnou). Možnou účast jednotlivých valenčních pozic na reciprokalizaci je třeba vyznačit ve valenčním rámci slovníkového hesla.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A continuation and modification of our approach (given in Panevová (1999)is presented. The necessity to distinguish an inherent lexical reciprocity, as a part of a lexical meaning, is and syntactical reciprocity is argued. The relation of symmetry between emancipated participants is, in general, present only with syntactical reciprocal constructions.The possible participation of the respective valency slot in the process of reciprocalization has to be marked in the valency frame of the verb.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku navázujeme na tři příspěvky ke zkoumání recipročních vztahů v češtině (Panevová, 1999, Panevová, v tisku, Panevová, Mikulová, v tisku) a rozšířit je jak o několik úvah, které vyplynuly z diskusí o těchto příspěvcích, tak o další doklady z elektronických korpusů ilustrujících tyto problémy (budeme používat ČNK ve verzi SYN2005, dále PDT ve verzi 2.0 a Internet).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We think that the topic of Czech reciprocals has not yet been exhausted. We have proposed several issues open for further studies, e. g. distribution of the optional lexical means, their position in word order, behavior of si-reflexives etc. Recalling our ontological considerations on vagueness in syntactical reciprocal relations (see Panevová, in press, Section 4, as well as Chrakovskij, op.c. ), our insight into the corpus material confirms for the whole domain of reciprocity, that there are many vague and ambiguous constructions, interpretation of which strongly depends on inferences provided by the speech participants with the knowledge of the broader context or situation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Stať rozvíjí přístup nastíněny v Panevová (1999). Rozlišuje se inherentní lexikální reciprocita od reciprocity syntaktické. Lexikální třídy sloves, na něž lze aplikovat operaci reciprokalizace, jsou uvedeny v odd. 3.1-3.3. Explicitní lexikální prostředky pro reciprocitu se analyzují v odd. 5 -8.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a follow-up and modification of the approach to the description of reciprocal constructions in Panevová (1999).Inherent lexical reciprocity is distinguished from reciprocal constructions which are syntactic. The syntactic operation of reciprocalization could be applied to the verbs from three classes proposed in Sections 3.1 – 3.3.The specific formal expressions used in Czech reciprocal constructions are analyzed in Sections 5-8.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Průvodce Českým akademickým korpusem verze 1.0 je průvodce CD-ROM. Objektem prohlídky je morfologicky ručně anotovaný korpus češtiny o celkovém objemu 660 tisíc slov a nástroje pro jeho prohlížení a úpravu spolu s nástroji pro zpracování textů z pohledu tvarosloví neboli morfologie. Průvodce dokumentuje historii korpusu, podněty k vytvoření jeho první verze a detaily jeho počítačové reprezentace. Zároveň poskytuje stručné návody k užívání nabízených nástrojů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The guide to the Czech Academic Corpus version 1.0 is a roadmap to the CD-ROM. Within the context of the CD-ROM you will find the manually morphologically annotated corpus of Czech consisting of nearly 660,000 words along with tools for viewing and modifying them. The tools treating texts from the standpoint of morphology are available as well. The guide provides the users with the fundamental characteristics of the academic corpus, presenting the evolution of the corpus together with the motivation of the current edition. In addition, the brief guidelines to the tools offered are included.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Český akademický korpus verze 1.0 je morfologicky ručně anotovaným korpusem češtiny o objemu 660 tisíc slov.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Czech Academic Corpus version 1.0 is a corpus with a manual annotation of morphology of the Czech language consisting of approximately 660,000 words.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přehled výsledků Cross-Language Speech Retrieval Track organizované v rámci evaluační kampaně CLEF 2006.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The CLEF-2006 Cross-Language Speech Retrieval (CL-SR) track included two tasks: to identify topically coherent segments of English interviews in a known-boundary condition, and to identify time stamps marking the beginning of topically relevant passages in Czech interviews in an unknown-boundary condition. Five teams participated in the English evaluation, performing both monolingual and cross-language searches of ASR transcripts, automatically generated metadata, and manually generated metadata. Results indicate that the 2006 evaluation topics are more challenging than those
used in 2005, but that cross-language searching continued to pose no unusual challenges when compared with collections of character-coded text. Three teams participated in the Czech evaluation, but no team achieved results comparable to those obtained with English interviews. The reasons for this outcome are not yet clear.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Závislostní analýza přirozeného jazyka získává na důležitosti díky své aplikovatelnosti v mnoha úlohách NLP. Mnohé ze vznikajících závislostních struktur jsou neprojektivní, vzniká tedy potřeba umět je podrobně popsat, zejména pro potřeby přístupů využívajících strojové učení, jako je např. parsing. Pomocí dat z dvanácti přirozených jazyků vyhodnocujeme několik omezení a měr na neprojektivních strukturách. Využíváme přitom přístupu založeného na vlastnostech jednotlivých hran oproti vlastnostem celých závislostních stromů. Ve svém vyhodnocení uvádíme dosud neprezentované míry neprojektivity, které v sobě zahrnují hladiny uzlů v závislostních stromech. Empirické výsledky podporují výsledky teoretické a prokazují, že přístup založený na vlastnostech hran využívající hladin uzlů poskytuje přesné a silné prostředky pro zachycení neprojektivních struktur v přirozených jazycích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Dependency analysis of natural language has gained importance for its applicability in tasks of NLP. Non-projective structures are common in dependency analysis, therefore we need fine-grained means of describing them, especially for the purposes of machine-learning oriented approaches like parsing. We present an evaluation on twelve languages which explores several constraints and measures on non-projective structures. We pursue an edge-based approach concentrating on properties of individual edges as opposed to properties of whole trees. In our evaluation, we include previously unreported measures taking into account levels of nodes in dependency trees. Our empirical results corroborate theoretical results and show that an edge-based approach using levels of nodes provides an accurate and at the same time expressive means for capturing non-projective structures in natural language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dizertace zkoumá projektivitu a neprojektivitu v závislostních stromech. Uvádíme původní výsledky, z nichž nejdůležitější jsou: nová definice projektivity, zavedení takzvaných úrovňových typů neprojektivních hran, pojem projektivizace, vztahy mezi úrovňovými typy neprojektivních hran a planaritou (slabou projektivitou) a dobrou zahnízděností (v obou případech se jedná o podmínky na uspořádání na závislostních stromech slabší než projektivita), a příslušné algoritmy včetně jejich složitosti. Ve druhé části zkoumáme neprojektivní struktury v téměř dvaceti závislostních korpusech různých přirozených jazyků s využitím prostředků představených v první části.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The thesis studies projectivity and non-projectivity in dependency trees. We present new mathematical results, the main ones being: a novel definition of projectivity, introduction of so-called level types of non-projective edges, the notion of projectivization, relationships between level types of non-projective edges and planarity (weak projectivity) and well-nestedness (which are both weaker conditions on the ordering of nodes in dependency trees than projectivity), and relevant algorithms together with their complexity analyses. In the second part of the thesis, we present an extensive evaluation of non-projective syntactic structures from about twenty natural language corpora using the formal means derived in the first part.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Závislostní analýza přirozeného jazyka se neobejde bez neprojektivních struktur. O podmínce dobré zahnízděnosti bylo nedávno prokázáno, že dobře zachycuje empirická jazyková data. Ukazujeme, jak ji lze přeformulovat pomocí vlastností neprojektivních hran, a dále jejich vztah k úrovňovým typům neprojektivních hran; odvozujeme také jednoduchý kvadratický algoritmus pro ověřování dobré zahnízděnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Dependency analysis of natural language gives rise to non-projective structures.  The constraint of well-nestedness on dependency trees has been recently shown to give a good fit with empirical linguistic data. We present a reformulation of this constraint using properties of non-projective edges and show its formal relationship to level types of non-projective edges; we also derive a simple $\BigO(n^2)$ algorithm for checking well-nestedness.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Sada nástrojů implementovaných v Perlu pro konverzi valenčních slovníků z anotačního formátu do HTML a PDF.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A set of perl scripts for converting valency lexicons from the annotation format into HTML and PDF presentation formats.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valenční slovník českých sloves (VALLEX 2.0) shromažďuje lingvisticky zpracovaná data a jejich dokumentaci. Poskytuje informace o valenční struktuře sloves v jejich jednotlivých významech, počtu a možných morfologické formy jejich doplnění a
další syntaktické informace, spolu s poznámkami a příklady. Primárním cílem textu je stručně popsat datovou strukturu slovníku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Valency Lexicon of Czech Verbs (VALLEX 2.0) is a collection of linguistically
annotated data and documentation. It provides information on the valency structure of verbs in
their particular meanings / senses, possible morphological forms of their complementations and
additional syntactic information, accompanied with glosses and examples. The primary goal of
the following text is to briefly describe the content of VALLEX~2.0 data from a structural
point of view.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Při anotování velkého korpusu je velmi užitečné všímat si i slovesné valence.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In annotating a large corpus, it is highly useful to present an analysis of verb valency.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vzhledem k různosti názorů v lingvistice je v ní vždycky nezbytné hledání konsenzu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Since there are different views in linguistics, it is useful to look for consense.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Anotace korpusu je chápána jako důležitý prostředek pro testování lingvistické teorie. Na základě systematického anotování korpusu je možné ověřovat teoretické lingvstické hypotézy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Corpus annotation is not a self-contained task but one of its important application is a testing of linguistic theories. A systematic annotation of corpus makes it possible to test linguistic hypotehses.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>The function-form viewpoint (means and ends, and the regard to the communicative function) is applied to the analysis of the information structure of the sentence, distinguishing between the semantically relevant topic-focus articulation and its means of expression (morphological, syntactic, prosodical).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Článek se věnuje analýze informační struktury věty z hlediska vztahu mezi funkcí a formou: rozlišujeme zde mezi sémanticky relevantním aktuálním členěním věty a prostředky jeho vyjádření (které mohou patřit do plánu morfologického, syntaktického a prozodického).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přehled životních osudů  vědeckých výsledků především z oblasti komputační a teoretické lingvistiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A critical survey of research achievements with special regard to computational and theoretical linguistics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozbor frunkčního přístupu Pražské lingvistické školy jako jednoho z významných myšlenkových proudů strukturalismu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Analysis of the approach of the Prague Linguistic School as one of the important trends of structuralist thinking.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Potřebnost anotace korpusu na podkladové rovině je dokumentována na rozboru několika jazykových jevů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Arguments are given for an underlying syntactic annotation of sentences of large text corpora.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Postavení aktuálního členění věty v teoretickém popisu jazyka na základě závislostních vztahů; relevance aktuálního členění pro význam věty</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Topic-Focus Articulation in a formal description of language based on dependency grammar; relevance of TFA for the meaning of the sentence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Porozumění přirozenému jazyku se chápe jako jeden z důležitých komponentlů systémů umělé inteligence. 
Ptáme se, kam až teoretická i komputační lingvistika ve svém příspěvku k umělé inteligenci došla a kudy podle našeho mínění i zkušeností vede k tomuto cíli cesta. 
Ilusrujeme na projektech a výsledcích pražské skupiny počítačové lingvistiky na MFF UK, a to ve třech oblastech: (i) příprava jazykových zdrojů, (ii) využití těchto zdrojů pro některé aplikace a (iii) na příkladu jednoho integrovaného projektu týkajícího se rozpoznávání mluveného jazyka a vyhledávání informací ve velmi velkých mluvených souborech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Natural language understanding is considered to be one of the urging issues of systems of artificial intelligence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V anotacich Pražského závislostního korpusu se při anotaci aktuálního členění bere v úvahu i rozlišení kontrastivního členu základu výpovědi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The claim of the paper is that the Topic/Focus structure of the sentence does not involve only a bipartioon of the sentence structure but that in the topic part of the sentence a special status should be given to a contrastive element.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Práce se zabývá studiem podmodelů modelů Peanovy aritmetiky a jejích fragmentů. Klíčovou úlohu v této práci hraje pojem diagonální nerozlišitelnosti, pomocí nějž se studují tři okruhům otázek: zobecnění nekonečné Ramseyovy věty na diagonální nerozlišitelnost, vlastnosti a rozložení specifických řezů modelů aritmetiky a zkoumání vlastností Stoneova prostoru algebry definovatelných množin spočetného modelu Peanovy aritmetiky užitím nestandardních metod (vnoření do velkého modelu).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this thesis, we study a range of questions concerning submodels of models of Peano arithmetic (PA) or its fragments. Our study focuses on three different areas that share a common central topic, namely diagonally  indiscernible elements.
    In the ﬁrst part, we explore a diagonal version of the Inﬁnite Ramsey Theorem provable in PA and partially provable in fragments of PA. We provide a detailed level-by-level analysis of the principle in terms of the arithmetical hierarchy and the corresponding fragments of the schemes of induction and collection. Then we derive a theorem characterizing Σn -elementary initial segments of a given model that satisfy (fragments of) PA as cuts on certain systems of diagonal indiscernibles.
    In the second part, we study initial segments with some speciﬁc properties, and especially their distribution in a given countable model M |= PA.
We provide a theorem that gathers general topological consequences of the method of indicators. We then extend the theorem with further results about some prominent families of Σn -elementary initial segments (among others,
those satisfying PA or IΣn+k , and those isomorphic to M). For example, by applying the results from the ﬁrst part, we prove that every interval that contains an Σn -elementary initial segment satisfying IΣn+k (or PA) contains
a closed subset of such initial segments that is order-isomorphic to the Cantor set. We conclude by proving some strict inclusions between the topological closures of the studied families.
    In the last part, we study the properties of the Stone space of the algebra of deﬁnable subsets of a given countable model M |= PA. We present
the topic from a non-standard viewpoint, situating the countable base model M into some ℵ1 -saturated elementary extension C, under which ultraﬁlters from the Stone space appear as sets of 1-indiscernible elements, called monads. Our main tool here is the Rudin-Keisler (RK) pre-order on monads. We investigate monads of diagonally indiscernible elements and diagonal partition properties on monads. Among other results, we prove that RK-minimal monads (which correspond to selective ultraﬁlters), p-monads (which corre-
spond to p-points), and regular monads, in this order, are properties of strictly
decreasing strengths. Furthermore, we show that the counter-examples (e.g. p-points that are not RK-minimal) form dense subsets in the the corresponding subspaces of the Stone space.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>MEd je anotační nástroj, v němž lze vytvářet a upravovat lineárně strukturované anotace textu nebo akustických dat. Tento nástroj podporuje několik vrstev anotací, které mohou být vzájemně propojeny odkazy. MEd lze použít také pro jiné účely, jako je zarovnání paralelních korpusů na bázi slov. MEd je hlavním nástrojem pro anotaci projektu Pražský závislostní korpus mluvené řeči (PDTSL). Nativním formátem tohoto nástroje je PML - Prague Markup Language.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>MEd is an annotation tool in which linearly-structured annotations of text or audio data can be created and edited. The tool supports multiple stacked layers of annotations that can be interconnected by links. MEd can also be used for other purposes, such as word-to-word alignment of parallel corpora. 
MEd is the main annotation tool for the project Prague Dependency Treebank of Spoken Language (PDTSL). The native format of the tool is PML - the Prague Markup Language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Testovací kolekce použita pro evaluaci systémů pro ad-hoc vyhledávní informací v rámci CLEF 2007.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A test collection employed for evaluation of the information retrieval systems at the Ad-Hoc track at CLEF 2007.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Testovací kolekce použita pro evaluaci systémů pro vyhledávní v mluvené řeči v rámci CLEF 2007</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The test collection employed for evaluation of the speech retrieval systems at Cross-language Speech Retrieval track at CLEF 2007.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přehled výsledků Cross-Language Speech Retrieval Track organizované v rámci evaluační kampaně CLEF 2007.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The CLEF-2007 Cross-Language Speech Retrieval (CL-SR) track included two tasks: to identify topically coherent segments of English interviews in a known-boundary condition, and to identify time stamps marking the beginning of topically relevant passages in Czech interviews in an unknown-boundary condition. Six teams participated in the English evaluation, performing both monolingual and cross-language searches of ASR transcripts, automatically generated metadata, and manually generated metadata. Four teams participated in the Czech evaluation, performing monolingual searches of automatic speech recognition transcripts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vyhodnocení jazykovědné soutěže, přehled vybraných úkolů z jazykovědy a slohu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Evaluation of the linguistic contest, overview of selected tasks in linguistics and essays.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje návrh první rozsáhlé české testovací kolekce pro vyhledávání informací v mluvené řeči.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the design of the first large-scale IR test collection built for the Czech language. This collection also happens to be very challenging, as it is based on a continuous text stream from automatic transcription of spontaneous speech and thus lacks clearly defined document
boundaries. All aspects of the collection building are presented, together with some initial experiments.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Obsahuje valenční slovník pro všechna slovesa a některá substantiva a adjektiva z PDT 2.0 v XML formátu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Contains valency lexicon for all verbs and some nouns and adjectives from PDT 2.0 in XML format.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje budování a automatické zpracování audio-visuálního korpusu DIALOG. Korpus DIALOG je prosodicky anotovaný korpus českých televizních diskusí nahrávaných a anotovaných v Ústavu pro jazyk český Akademie věd České republiky. 
V současnosti obsahuje více jak 400 VHS 240min kazet a 375 přepsaných pořadů. Popisovaný digitalisační proces a automatický alignment umožnily vznik uživatelsky přívětivého výzkumného prostředí podporujícího zkoumání prosodie češtiny, její analýzu a modelování. Tento projekt je řešen ve spolupráci s Ústavem formální a aplikované lingvistiky, MFF UK. První veřejně dostupná verze korpusu DIALOG obsahuje 10 revidovaných hodinových pořadů a je přístupná na adrese http://ujc.dialogy.cz.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article describes the development and automatic processing of the audio-visual DIALOG corpus. The DIALOG corpus is a prosodically annotated corpus of Czech television debates that has been recorded and annotated at the Czech Language Institute of the Academy of Sciences of the Czech Republic.
It has recently grown to more than 400 VHS 4-hour tapes and 375 transcribed TV debates. The described digitisation process and automatic alignment enable an easily accessible and user-friendly research environment, supporting the exploration of Czech prosody and its analysis and modelling. This project has been carried out in cooperation with the Institute of Formal and Applied Linguistics of Faculty of Mathematics and Physics, Charles University, Prague. Currently the first version of the DIALOG corpus is available to the public (version 0.1, http://ujc.dialogy.cz). It includes 10 selected and revised hour-long talk shows.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem navrhovaného dema by bylo předvést technologii stojící za Pražským arabským závislostním korpusem (Hajič et al., 2004), projektem jazykové anotace, který lze aplikovat v mnoha oblastech počítačového zpracování přirozeného jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The proposed demo would aim to present the technology behind the Prague Arabic Dependency Treebank (Hajič et al., 2004), a project of linguistic annotation having application in many areas of Natural Language Processing.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Implementace originálního lingvistického modelu Funkční arabské morfologie, která zahrnuje jak deklarativní definici systému v jazyce Haskell, tak i rozsáhlý arabský morfologický slovník. Publikováno pod licencí GNU GPL.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An implementation of the original linguistic model of the Functional Arabic Morphology includes both the declarative definition of the system in Haskell, and an extensive Arabic morphologic dictionary. Published under the GNU GPL license.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Implementace původního lingvistického modelu Funkční arabské morfologie zahrnuje jak deklarativní definici systému v Haskellu, tak rozsáhlý arabský morfologický slovník. Šířeno pod licencí GNU GPL.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An implementation of the original linguistic model of the Functional Arabic Morphology includes both the declarative definition of the system in Haskell, and an extensive Arabic morphologic dictionary. Published under the GNU GPL license.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Funkční arabská morfologie je formulace arabského flexivního systému, která se snaží najít fungující rozhraní mezi morfologií a syntaxí. ElixirFM je její vysokoúrovňová implementace, která využívá a rozšiřuje funkční morfologickou knihovnu pro Haskell.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Functional Arabic Morphology is a formulation of the Arabic inflectional system seeking
the working interface between morphology and syntax. ElixirFM is its high-level implementation
that reuses and extends the Functional Morphology library for Haskell.
Inflection and derivation are modeled in terms of paradigms, grammatical categories,
lexemes and word classes. The computation of analysis or generation is conceptually
distinguished from the general-purpose linguistic model.
The lexicon of ElixirFM is designed with respect to abstraction, yet is no more complicated
than printed dictionaries. It is derived from the open-source Buckwalter lexicon
and is enhanced with information sourcing from the syntactic annotations of the Prague
Arabic Dependency Treebank.
MorphoTrees is the idea of building effective and intuitive hierarchies over the information
provided by computational morphological systems. MorphoTrees are implemented
for Arabic as an extension to the TrEd annotation environment based on Perl.
Encode Arabic libraries for Haskell and Perl serve for processing the non-trivial and
multi-purpose ArabTEX notation that encodes Arabic orthographies and phonetic transcriptions
in parallel.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Toto je shrnutí autorovy doktorské disertační práce obhájené 17. září 2007 na Matematicko-fyzikální fakultě Univerzity Karlovy v Praze. Výsledků v disertaci prezentovaných bylo dosaženo během autorova doktorského studia matematické lingvistiky v letech 2001-2007. Celá disertace je dostupná přes http://sourceforge.net/projects/elixir-fm/.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This is a summary of the author's PhD dissertation defended on September 17, 2007 at the Faculty of Mathematics and Physics, Charles University in Prague. The results comprised in the thesis were obtained within the author's doctoral studies in Mathematical Linguistics during the years 2001-2007. The complete dissertation is available via http://sourceforge.net/projects/elixir-fm/.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pražský arabský závislostní korpus, nedávno vydaný ve své první verzi, je jednak kolekcí anotací arabských textů na více rovinách, jednak balíčkem softwarových nástrojů vytvořených pro použití v počítačovém zpracování přirozeného jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Prague Arabic Dependency Treebank, recently published in its first version, is both a collection of multi-level linguistic annotations of Arabic texts, and a suite of unique software implementations designed for general use in Natural Language Processing.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prostředí a Perlová knihovna pro konverzi mezi různými sadami značek.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A framework and Perl library for converting between various tag sets.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje jednoduchou metodu neřízené morfologické analýzy neznámého jazyka. Potřeba je pouze prostý textový korpus daného jazyka. Algoritmus se dívá na slova, rozpozná opakovaně se vyskytující kmeny a přípony a sestaví pravděpodobné morfologické vzory. Článek také popisuje způsob, jak byla tato metoda využita při řešení úlohy Morpho Challenge 2007, a prezentuje výsledky Morpho Challenge. Přestože tato práce byla původně studentským projektem bez návaznosti na obdobný výzkum ve světě, k našemu překvapení tento jednoduchý přístup překonal několik dalších algoritmů v podsoutěži segmentace slov. Věříme, že v metodě je dostatečný prostor pro zlepšení, který může výsledky dále zlepšit. V článku jsou rozebrány chyby a navržena budoucí rozšíření.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes a rather simplistic method of unsupervised morphological analysis of words in an unknown language. All what is needed is a raw text corpus in the given language. The algorithm looks at words, identifies repeatedly occurring stems and suffixes, and constructs probable morphological paradigms. The paper also describes how this method has been applied to solve the Morpho Challenge 2007 task, and gives the Morpho Challenge results. Although the present work was originally a student project without any connection or even knowledge of related work, its simple approach outperformed, to our surprise, several others in most morpheme segmentation subcompetitions. We believe that there is enough room for improvements that can put the results even higher. Errors are discussed in the paper; together with suggested adjustments in future research.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Lexical Annotation Workbench (LAW) je integrované prostředí pro morfologickou anotaci. Podporuje jednoduchou anotaci, porovnávání různých anotací téhož textu, hledání určitého slova, značky, atd.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Lexical Annotation Workbench (LAW) is an integrated environment for morphological annotation. It supports simple morphological annotation, comparison of different annotations of the same text, searching for particular word, tag etc.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Laudatio k sedmdesátýmpátým narozeninám profesora Fredericka Jelinka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Laudation for Professor Frederick Jelinek on the occasion of his 75th birthday.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme systém STYX, který je navržen jako elektronická civčebnice českého tvarosloví a české syntaxe. 

Téměř 12 000 cvičebnicových příkladů je vybráno z Pražského závislostního korpusu, nejrozsáhlejšího anotovaného korpusu češtiny. Cvičebnice nabízí komplexní zpracování věty z pohledu morfologických a syntaktických jevů, které pokrývají učební látku středních škol a vyšších tříd základních škol.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the STYX system, which is designed as an electronic corpus-based exercise book of Czech with exercises directly selected from the Prague Dependency Treebank, the largest annotated corpus of Czech. The exercise book offers a complex sentence processing with respect to morphological and syntactic phenomena, i. e. the exercises give practice in classifying parts of speech and particular morphological categories of words and in parsing a sentence and classifying syntactic functions of words. The exercise book covers a subject matter the students of secondary schools should master.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příručka je zkrácenou verzí rozsáhlého manuálu: Anotace na tektogramatické rovině Pražského závislostného
korpusu. Anotátorská příručka. (viz Prague Dependency Treebank 2.0, CDROM, doc/manuals/cz/t-layer/), který
obsahuje podrobný a úplný popis reprezentace věty na tektogramatické rovině (přesné, podrobné a úplné informace
je třeba vždy čerpat z „velkého“ manuálu).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This reference book is a shortened version of an extensive manual called Annotation on the tectogrammatical level
in the Prague Dependency Treebank. Annotation manual. (see Prague Dependency Treebank 2.0, CDROM,
doc/manuals/en/t-layer/), which contains the complete and detailed description of sentence representation at the
tectogrammatical level (it is necessary to consult the “big” manual for exact, detailed and complete information).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Poukazuje se na souvislost slovesné předpony a valenčních vlastností slovesa. Polysémické vlastnosti předpon souvisejí úzce s valencí slovesa.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The interdependance between verbal prefixes and valency frames with the verbs of motion is studied.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>VALLEX 2.5 poskytuje informaci o valenční struktuře sloves v jejich specifických významech. VALLEX je úzce spjat s Pražským závislostním korpusem, který také využívá Funkčního generativního popisu jako podkladovou teorii. Ve VALLEXu 2.5 je obsaženo zhruba 2.730 lexémů obsahujících dohromady kolem 6.460 lexikálních jednotek ("významů").</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Valency Lexicon of Czech Verbs, Version 2.5 (VALLEX 2.5), is a collection of linguistically annotated data and documentation, resulting from an attempt at formal description of valency frames of Czech verbs. VALLEX 2.5 has been developed at the Institute of Formal and Applied Linguistics, Faculty of Mathematics and Physics, Charles University, Prague. VALLEX 2.5 is a successor of VALLEX 1.0, extended in both theoretical and quantitative aspects. VALLEX 2.5 provides information on the valency structure (combinatorial potential) of verbs in their particular senses. VALLEX is closely related to the Prague Dependency Treebank project: both of them use Functional Generative Description (FGD), being developed by Petr Sgall and his collaborators since the 1960s, as the background theory. In VALLEX 2.5, there are roughly 2,730 lexeme entries containing together around 6,460 lexical units ("senses"). Note that VALLEX 2.5 - according to FGD, but unlike traditional dictionaries and also unlike VALLEX 1.0 - treats a pair of perfective and imperfective aspectual counterparts as a single lexeme (if perfective and imperfective verbs would be counted separately, the size of VALLEX 2.5 would virtually grow to 4,250 verb entries). To ensure high quality of the data, all VALLEX entries have been created manually, using several previously existing lexicons as well as corpus evidence from the Czech National Corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Funkční generativní popis je závislostní popis češtiny, který se vyvýjí od šedesátých let minulého století. Původně byl implementován jako generativní postup, ale později jsme se zabývali jeho deklarativní reprezentací. Tohoto článek se týká základů redukčního systému, který dovoluje zachycovat složitější vztahy než jsou povrchová syntaktické vztahy ve větě, protože poskytuje nejen možnost kontroly správnosti povrchové analýzy věty, ale i její hloubkové reprezentace. Takový redukční systém umožňuje formálně definovat analýzu a syntézu věty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Functional Generative Description is a dependency based descriptive system, which has been in
development since the 1960s. It was
originally implemented as a  generative procedure, but lately we have been interested in a declarative representation. The object
of the present paper concerns the foundations of a reduction system, which is more complex than a reduction system for a (shallow) syntactic analyzer, since it provides not only the
possibility of checking the well-formedness of the (surface) analysis of a sentence, but its underlying  representation as well. Such a reduction system makes it possible to define formally the analysis as well as the  synthesis of a sentence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku prezentujeme anotační schéma, které zachycuje obecné časové vztahy mezi událostmi vyjádřené v diskurzu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we present an annotation scheme that captures general temporal relations between events expressed in a discourse.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku prezentujeme funkční přístup k zachycení informace, kterou v diskurzu pokrývají různé časové výrazy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we present a functional approach to capture the information conveyed by various time expressions within a discourse. The
approach is motivated by annotation scheme that captures general temporal relations between events expressed in a discourse. A parser
for Czech as well as an inference engine that makes it possible to compare functional compositions is implemented.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Práce se zabývá automatickou desambiguací slovesných valenčních rámců na českých datech. Hlavním přínosem je stanovení nejužitečnějších rysů pro zjednoznačnění valenčních rámců.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This work deals with automatic disambiguation of verb valency frames on Czech data. Main contribution lies in determining of the most useful features for valency frame disambiguation. We experimented with diverse types of features, including morphological, syntax-based, idiomatic, animacy andWordNet-based. The considered features were classiffed using decision trees, rule-based learning and Naive Bayes classiffer. On a set of 7 778 sentences we achieved accuracy of 79.86% against baseline 68.27% obtained by assigning the most frequent frame.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku se zabýváme Funkčním generativním popisem češtiny (FGP) na pozadí formální teorie překladů a formální teorie jazyků. Klademe důraz na propojení formálních modelů s jejich jazykovou (lingvistickou) náplní. Jednou z formálních podob FGP byla kompozice překladů pomocí pěti překladových gramatik. Zavedeme zde rozlišovací sílu takových systémů. Dále představíme nový formální model založený na překladových restartovacích automatech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the Functional Generative Description Czech (FGP) upon the background of formal theory  of translation for formal languages. Emphasis is placed on linking formal models of language (linguistic) descriptions. One of the forms of formal model fo FGP was a composition of five translation grammars. Here we introduce the generative power of such systems. Furthermore, we introduce a new formal model based on the translation restarting automata.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje typologii chyb objevujících se v morfologicky anotovaných korpusech. Typy jsou demonstrovány na příkladech z českého korpusu SYN2000. Tři hlavní typy chyb jsou: původní chyby pocházející z originálních textů, kódovací chyby a anotační chyby.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article proposes a typology of errors that occur in morphologically annotated corpora, demonstrated on the example of the Czech National Corpus, its version SYN2000. It is morphologically annotated corpus with 3 attributes: word form, lemma and morphological tag. Word forms come from original texts acquired from various providers, the other two are added by corpus builders during the annotation. 
It explains the process of morphological annotation, its three phases – morphological analysis, guesser and disambiguation. It describes types of errors that can occur during the individual phases and why. And it discusses possibilities of their removal.
There are three main categories of errors: original errors coming from original texts, coding errors that come from possible recoding of various texts into one common format, and annotation errors due to faults in morphological dictionary and imperfections in the disambiguation – the statistical as well as rule-based. 
All types of corpus defects are documented by examples from the corpus SYN2000.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek pozoruje švédské sloveso "hålla på", běžně používané v pseudokoordinaci s významovým slovesem jako pomocné sloveso průběhového času u dějových sloves. V některých kontextech, zejména v kombinaci s negací, sloveso "hålla på" překvapivě dodává slovesu, s kterým je v koordinaci, významový rys konstantnosti daného děje. Korpusové konkordance prokazují tendenci rozšíření tohoto významového rysu na kontexty bez negace, což znamená, že se pomocná konstrukce "hålla på och" stává polysémním. Tento zajímavý významový posun je dáván do souvislosti s Hopperovou ideou gramatikalizace a s obecným mechanismem sémantického posunu, jenž popsali Heine, Claudi a Hünnemeyer (1991) a nazvali jej "context-induced reinterpretation".</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper observes the Swedish auxiliary verb "hålla på", commonly used as a progressivity marker in pseudocoordination with a process verb. In some contexts, especially in combination with negation, "hålla på" gives the process denoted by the lexical verb a flavour of constancy. The corpus evidence proves that this semantic feature is spreading onto non-negated contexts, too, which implies that the auxiliary construction "hålla på och" is becoming polysemous. This interesting semantic shift is related to Hopper's (1987) idea of grammaticalization regarded as "movement towards structure" as well as to the concept of "context-induced reinterpretation" as one type of semantic shifting described by Heine, Claudi and Hünnemeyer (1991).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>MMI_clustering je sada nástrojů příkazové řádky, kterým se provádí Mercer maximální vzájemnou výměnu informací na základě shlukování-technika. Hlavní program shlukování přichází s podpůrné nástroje pro třídu-založené text výsledek transformace a vizualizace. Dohromady tyto formuláře useful gadget pro jazykové modelování, studium sémantických tříd, nebo dokonce rozbor konkrétních autorů 'sdružení. Balíček obsahuje program počítačové zařazení stromu (roste ho dychtivě cestu z listů (to je slovo) na jeho root), program pro dělení tohoto stromu na dané úrovni, abychom mohli získat předem stanovený počet slovních druhů a nakonec je visualizer / transformátor kreslení stromů (ASCII art-based) a pomocí třídy transformovat vstupní text do textu, kde by každé slovo označí své třídě například (výstupní formát je velmi konfigurovatelný).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>MMI_clustering is a set of command line tools implementing Mercer's maximum mutual information-based clustering technique. Main clustering program comes with subsidiary tools for class-based text transformations and result visualization. Together these form useful gadget for language modeling, study of semantic classes, or even analysis of authors' specific associations. The package contains program computing classification tree (growing it in an eager way from leafs (that is words) to its root), program for cutting this tree at a given level so we could obtain predetermined number of word classes and finally there is a visualizer/transformer drawing trees (ASCII-art based) and using classes to transform input text into a text where each word would be annotated with its class for example (the output format is quite configurable).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme demonstraci anotačního nástroje pro anotování textů do formalismu sémantických sítí zvaného MultiNet. Nástroj je založen na grafickém uživatelském rozhraní Java Swing a umožňuje anotátorům editovat uzly a relace v síti a také vazby mezi uzly v síti a uzly předchozí anotační roviny. Data zpracovaná nástrojem v této prezentaci pocházejí z anglické verze Wall Street Journalu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a demonstration of an annotation tool designed to annotate texts into a semantic network formalism called MultiNet. The tool is based on a Java Swing GUI and allows the annotators to edit
nodes and relations in the network, as well as links between the nodes in the network and the nodes from the previous layer of annotation. The data processed by the tool in this presentation are from the English version of the Wall Street Journal.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento abstrakt popisuje projekt, jehož cílem je ruční anotace obsahu výpovědí v přirozeném jazyce v paralelních textových korpusech. Používáme formalismus zvaný MultiNet - Vícevrstvou rozšířenou sémantickou síť. Anotace by měla být zahrnuta jako nová anotační rovina do Pražského závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This abstract describes a project aiming at manual annotation of the content of natural language utterances in a parallel text corpora. The formalism used in this project is MultiNet – Multilayered Extended Semantic Network. The annotation should be incorporated into the Prague Dependency Treebank as a new annotation layer.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá možnostmi modifikace souboru rysů, který je použit v závislostní parseru založeném na hledání maximální kostry grafu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we present the results of our experiments with modifications of the feature set used in the Czech mutation of the Maximum Spanning Tree parser. First we show how new feature templates improve the parsing accuracy and second we decrease the dimensionality of the feature space to make the parsing process more effective without sacrificing accuracy.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Systém STYX je elektronickou cvičebnicí češtiny postavenou nad daty Pražského závislostního korpusu. Skládá se z konzolové aplikace určené k filtrování vět (FilterSentences), administračního programu sloužícího k sestavování cvičení (Charon) a vlastní cvičebnice nazvané Styx.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>STYX is an electronic exercise book of Czech based on data of the Prague Dependency Treebank. It consists of a console application used to filter unsuitable sentences (FilterSentences), an administrative application for creating exercises (Charon) and the exercise book itself, Styx.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku popisujeme námi vyvinutý systém, který jsme použili při veřejné úloze vícejazyčného parsingu CoNLL 2007. Systém tvoří tři komponenty: parser založený na hledání k nejlepších orientovaných koster, značkovač pro závislostní stromy a reranker, který přeuspořádává k nejlepších označkovaných stromů. Představujeme dva způsoby trénování parseru založeného na orientovaných kostrách: podmíněné trénování založené na stromové normalizaci a na grafové normalizaci. Přeuspořádávací model pro stromy dovoluje explicitně modelovat globální syntaktické jevy; rysy používané rerankerem zahrnují rysy popisující neprojektivní hrany. Analyzujeme chyby v parsingu našeho systému a navrhujeme možné změny použitých modelů, které by mohly přispět k jeho zlepšení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our system used in the CoNLL 2007 shared task on multilingual parsing. The system is composed of three components: a k-best maximum spanning tree (MST) parser, a tree labeler, and a reranker that orders the k-best labeled trees. We present two techniques for training the MST parser: tree-normalized and graph-normalized
conditional training. The tree-based
reranking model allows us to explicitly model global syntactic phenomena. We describe the reranker features which include non-projective edge attributes. We provide an analysis of the errors made by our system and suggest changes to the models and features that might rectify the current system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Klasifikace na základě Fisherovy lineární diskriminační analýzy (FLDA) je složitou úlohou v případě, že počet proměnných je o mnoho vyšší než počet daných instancí objektů. Původní FLDA je potřeba pečlivě modifikovat i s ohledem na fakt, že ve vysokých dimenzích hrají důležitou roli implementační otázky jako např. redukce pamětových nákladů. Článek probere různé metody pro high dimension/ small sample size problem a vybere metodu, která je v určitém smyslu nejblíže klasickému regulárnímu postupu. Článek se dále zabývá implementací vybrané metody a to rovněž s ohledem na její vylepšení vzhledem k výpočetním a paměťovým nákladům a vzhledem k numerické stabilitě. Vylepšení je dosaženo kombinací několika známých i zcela nových implementačních strategií. Provedené experimenty prokazují kvalitativně lepší hodnoty celkových numerických nákladů i chyby klasifikace u výsledného algoritmu než u ostatních metod.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Classification based on Fisher's linear discriminant analysis (FLDA) is challenging when the number of variables largely exceeds the number of given samples. The original FLDA needs to be carefully modified and with high dimensionality implementation issues like reduction of storage costs are of crucial importance. Methods are reviewed for the high dimension/small sample size problem and the one closest, in some sense, to the classical regular approach is chosen. The implementation of this method with regard to computational and storage costs and numerical stability is improved. This is achieved through combining a variety of known and new implementation strategies. Experiments demonstrate the superiority, with respect to both overall costs and classification rates, of the resulting algorithm compared with other methods.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje metody, jimiž byly označkovány tři velké textové korpusy (SYN2000, SYN2005 a SYN2006PUB). Postup značkování má několik fází: tokenizaci a segmentaci, morfologickou analýzu a dizambiguaci. Při značkování korpusů byly použity jak stochastické, tak pravidlové metody. V závěru článku je představena podrobná evaluace značkovacích metod a kvality značkování ve jmenovaných korpusech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the methods by which three large textual corpora (SYN2000, SYN2005 and SYN2006PUB) of the Czech National Corpus have been tagged and lemmatised. The process proceeded in several phases: tokenization and segmentation, morphological analysis and disambiguation. Statistical taggers as well as a rule-based method of disambiguation have been used in the process. SYN2000 has been tagged by a single feature-based tagger, SYN2005 and SYN2006PUB have been tagged by two different combinations of statistical and rule-based methods. In SYN2006PUB, the number of errors has been further reduced with some simple replacement algorithms. At the end of this paper, an evaluation of the different methods is presented: the method used for corpus SYN2006PUB shows approximatively twice less errors in tagging than in the older tagging of corpus SYN2000.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme pokusy s automatickým anotováním anglických textů z Penn Treebanku na závislostní tektogramatické rovině, jak ji definuje Pražský závislostní korpus. Navržený analyzátor je založen na metodách strojového učení a v nejdůležitějších atributech dosahuje vyšší úspěšnosti než nástroj založený na ručně psaných pravidlech, používaný pro částečnou tektogramatickou anotaci angličtiny dosud.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present experiments with automatic annotation of English texts, taken from the Penn Treebank, at the dependency-based tectogrammatical layer, as it is defined in the Prague Dependency Treebank. The proposed analyzer, which is based on machine-learning techniques, outperforms a tool based on hand-written rules, which is used for partial tectogrammatical annotation of English now, in the most important characteristics of tectogrammatical annotation. Moreover, both tools were combined and their combination gives the best results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje nejnovější vývojové tendence jablunkovského dialektu na pomezí Moravy a Slezska.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes current development tendencies in the dialect of the region of Jablunkov in the borderland between Moravia and Silesia.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje teorii podspecifikovanosti v kontextu slovanských jazyků. Speciálním zkoumaným případem je mizení klitik.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper sketches a theory of underspecification in the context of Slavic languages. A special case being investigated is the omission of clitics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pravidlový hloubkový transfer pro systém strojového překladu Česílko, založeného na zjednodušené analýze vstupního jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Rule-based deep transfer for the Machine Translation system Česílko, which is based on simplified source language analysis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Softwarový systém pro správu bibliografických údajů. Systém je implementován v jazyce Java a běží v servletovém kontejneru.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A software system for administration of bibliographic data. The system is implemented in Java and runs in a servlet container.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek se v první části věnuje experimentálnímu systému strojového překladu Česílko, zvláště funkčnímu popisu jeho jednotlivých modulů, jejich implementace a jimi používaných datových struktur, ve druhé části pak popisuje některé rozdíly mezi obecně velmi podobnými baltoslovanskými jazyky z lingvistického hlediska, konkrétně v systému příčestí a kategorii určitosti s možnostmi jejího vyjádření na povrchové rovině, a naznačuje možnosti formálního zachycení těchto rozdílů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes the MT system Česílko and linguistic differences between Balto-Slavic languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje metodu zpracování souvětí založenou na identifikaci segmentů, snadno zjistitelný a lingvisticky motivované jednotek, které mohou poskytnout základ pro další zpracování souvětí. Metoda byla vyvinuta pro český jazyk jako zastupupce jazyků
s vysokým stupněm volného slovosledu. V článku jsou zavedeny důležité pojmy, je zde popsáno
segmentační schéma - datová struktura používana pro popis vzájemného vztahu mezi
jednotlivými segmenty a oddělovači. Obsahuje základní soubor pravidel, která lze užít pro
segmentace souboru českých vět.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes a method of dividing complex sentences into segments, easily detectable and
linguistically motivated units, which may provide a basis for further processing of complex
sentences. The method has been developed for Czech as a language representing languages with
relatively high degree of word-order freedom. The paper introduces important terms, describes a
segmentation chart, the data structure used for the description of mutual relationship between
individual segments and separators. It contains a simple set of rules applied for the
segmentation of a small set of Czech sentences. The issues of segment annotation based on
existing corpus are also mentioned.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme tři poměrně odlišné přístupy k popisu slovního významu ve třech slovnících. Zabýváme se výhodami a nevýhodami zvolených přístupů. Se všemi slovníky jsme provedli experimenty (včetně strojového učení a ruční anotaci), které jsou stručně popsány. Na závěr tyto slovníky porovnáváme.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we present three quite different approaches to
word senses description in three particular lexicons.
The advantages and disadvantages of these approaches are mentioned.
We have done some practical experiments with all of them.
These experiments--including machine learning and manual annotation--are
briefly described. At the end, we conclude by comparing those three lexicons.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá zpracováním pojmenovaných entit v českých textech. Představíme dvouúrovňovou klasifikaci pojmenovaných entit, která byla použita při ruční anotaci 2000 vět. V tomto materiálu bylo identifikováno více než 11000 pojmenovaných entit. S použitím těchto dat jsme metodami strojového učení vyvinuli softwarový systém, který automaticky rozpoznává a klasifikuje pojmenované entity v českých textech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper deals with the treatment of Named Entities (NEs) in Czech. We introduce a two-level NE classification. We have used this
classification for manual annotation of two thousand sentences, gaining more than 11,000 NE instances. Employing the annotated data and
Machine-Learning techniques (namely the top-down induction of decision trees), we have developed and evaluated a software system aimed at automatic detection and classification of NEs in Czech texts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato technická zpráva shrnuje výsledky práce na tématu pojmenovaných entit v Ústavu formální a aplikované lingvistiky Matematicko-fyzikální fakulty Univerzity Karlovy v Praze v letech 2005 a 2006. Obsahuje rešerši zahraničních
přístupů k tomuto tématu, vlastní návrh klasifikace pojmenovaných entit v češtině, popis ruční anotace pojmenovaných entit na vzorcích z Českého národního korpusu, základní kvantitativní vlastnosti anotovaných dat a výsledky prvních experimentů s automatickým rozpoznáváním pojmenovaných entit v českých
textech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The technical report deals with classification and automatic regocnition of named entities in Czech texts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje volně šiřitelný soubor nástrojů pro strojový překlad, jehož nové rysy zahrnují podporu pro lingvisticky motivované informace, překlad tzv. confusion networks a úsporné datové formáty pro překladové a jazykové modely.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe an open-source toolkit for statistical
machine translation whose novel
contributions are (a) support for linguistically
motivated factors, (b) confusion network
decoding, and (c) efficient data formats
for translation models and language
models. In addition to the SMT decoder,
the toolkit also includes a wide variety of
tools for training, tuning and applying the
system to many translation tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje vývoj systému generujícího gramatické věty v českém jazyce ze vstupního syntakticko-sémantického závislostního stromu.

Zabýváme se dvěma lingvistickými teoriemi a implikacemi pro generování ze struktur, které popisují: (1) Funkční generativní popis P. Sgalla, který byl použit také pro anotaci Pražského závislostního treebanku a (2) Meaning-Text Theory.

Na závěr je výstup prototypu generátoru vyhodnocen.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We report work in progress on a complex system generating
Czech sentences expressing the meaning of input syntactic-semantic structures.
Such component is usually referred to as a realizer in the domain of Natural Language Generation.

Existing realizers usually take advantage of a background linguistic theory.
We introduce the Functional Generative Description, a framework of our choice conceived in 1960's by Petr Sgall.
This language theory lays out foundations of the formalism
in which our input syntactic-semantic structures are specified.
The structure definition was further elaborated and refined during the
annotation of the Prague Dependency Treebank,
now available in its second version.

A section of the paper is devoted to description of another theoretical
framework suitable for the task of
Natural Language Generation -- the Meaning-Text Theory.

We explore state-of-the-art realizers deployed in real life applications, describe common architecture of 
a generation system and highlight the strengths and weaknesses of our approach.
Finally, preliminary output of our surface realizer is compared against a
baseline solution.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku je navržen komplexní systém pro generování českých vět z jejich podkladové tektogramatické reprezentace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We propose a complex rule-based system for generating Czech sentences out of tectogrammatical trees, as introduced in Functional Generative Description (FGD) and implemented in the Prague Dependency Treebank 2.0 (PDT 2.0). Linguistically relevant phenomena including valency, diathesis, condensation, agreement, word order, punctuation and vocalization have been studied and implemented in Perl using software tools shipped with PDT 2.0. Parallels between generation from the tectogrammatical layer in FGD and deep syntactic representation in Meaning-Text Theory are also briefly sketched.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento dokument zkoumá vlastnosti populární varianta ROC - detekce chyb trade-off plot (DET).
Zejména čerpáme soubor podmínek pro základní rozdělení pravděpodobnosti
vyrábět lineární DET pozemků v zobecněné nastavení. Ukazujeme, že lineární DETs na normální odchýlit stupnice
nejsou vyráběná výhradně
normální distribuce se však, že běžné distribuce, hrají jedinečnou roli v prahu
chování, jak se pohybuje podél trati DET. Zajímavé spojení mezi lineárními a DETs
Kullback-Leibler divergence je také předmětem diskuse.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper investigates the properties of a popular ROC variant - the Detection Error Trade-Off plot (DET).
In particular, we derive a set of conditions on the underlying probability distributions
to produce linear DET plots in a generalized setting. We show that the linear DETs on a normal deviate scale
are not exclusively produced by
normal distributions, however, that normal distributions do play an unique role in the threshold
behavior as one moves along the DET line. An interesting connection between linear DETs and
the Kullback-Leibler divergence is also discussed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje datový přístup k modelování všech tří základních prozodických vlastností - základního hlasivkového tónu, intenzity a trvání.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes data-driven modelling of all three basic prosodic features - fundamental frequency, intensity and segmental duration - in the Czech text-to-speech system ARTIC. The fundamental frequency is generated by a model based on concatenation of automatically acquired intonational patterns. Intensity of synthesised speech is modelled by experimentally created rules which are in conformity with phonetics studies. Phoneme duration modelling has not been previously solved in ARTIC and this paper presents the first solution to this problem using a CART-based approach.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje účast Karlovy univerzity v soutěži Cleaneval 2007, společném úkolu a soutěži automatických systémů pro čištění webových stránek s cílem připravit data pro jazykový korpus z oplasti zpracování přirozeného jazyka. Tento úkol jsme pojali jako proces značkování sekvence jednotek, náš experimentální systém je založený na algoritmu Conditional Random Fields a používá rysy extrahované z textu a HTML struktury stránky. Nálepky přiřazené každému textovému bloku potom rozlišují, jestli se jedná o užitečnou část textu, která má být zachována, nebo o část, která se má z dalšího zpracování vyřadit.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the participation of the Charles University in Cleaneval 2007, the shared task and competitive evaluation of automatic systems for cleaning arbitrary web pages with the goal of preparing web data for use as a corpus in the area of computational linguistics and natural language processing. We try to solve this task as a sequence-labeling problem and our experimental system is based on Conditional Random Fields exploiting a set of features extracted from textual content and HTML structure of analyzed web pages for each block of text. Labels assigned to these blocks then discriminate between different types of content blocks containing useful material that should be preserved in the document and noisy blocks of no linguistics interest that should be eliminated.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci jsou zkoumány možnosti zachycování konstrukcí s koordinací tzv. nestejných kategorií. Za nestejné kategorie jsou ve zvoleném teoretickém rámci, funkčním generativním popisu, považovány nestejné sémantické hodnoty jednotlivých uzlů v syntaktickém stromě, tzv. funktory. Autorka navrhuje omezující pravidla, která podchycují podmínky gramatičnosti koordinace nestejných kategorií. Tato pravidla jsou založena na vztahu konjunktů k argumentové struktuře jejich řídícího slovesa. Podstatou těchto pravidel je fakt, že koordinace argumentové a adjunkční pozice generuje za všech okolností negramatické struktury. Na základě těchto pravidel se navrhuje, které případy je možno zachycovat jako koordinaci členskou a které jako koordinaci na úrovni slovesa. V souvislosti s cíli práce se autorka pokouší také o vlastní třídění koordinačních konstrukcí na základě valenčních charakteristik koordinovaných pozic. Práce si klade za cíl vyvrátit vžitý názor, že není možné definovat pravidla pro tvoření gramatických typů koordinace nestejných kategorií.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this work, theoretical aspects of a descriptive approach to the coordination of unlike categories are studied. According to the theory the author adopts, the so called Functional Generative Description, the different semantic values of coordinated nodes in a dependency tree (called functors) are referred to as unlike categories. The author offers constraints (of a kind) for generating coordinative structures with unlike categories. These constraints are based on the relation between a functor and its dominating verb with respect to the valency structure of the verb. The core of the theory is the fact that a coordination of an argument position and an adjunct position is forbidden as a rule. Considering the proposed constraints, a decision is made, which of the possible structures are to be represented as direct phrasal coordinations and which of them as coordinations on the level of the dominating verb. With respect to the aims of the work, the author submits her own proposal for categorisation of coordinative constructions according to the valency properties of the coordinated positions.  The work aspires to uproot the established opinion that basically there are no syntactic rules for the coordination of unlike categories.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Toto je první verze valenčního slovníku anglických sloves EngVallex, který vznikl částečně automatickou konverzí slovníku PropBank. Stejně jako PropBank, i EngVallex je propojen s anotací nad korpusem textu z Wall Street Journalu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This is the first version of EngVallex, a valency lexicon of English verbs, which arose partly by a semi-automatic conversion of the PropBank lexicon. Like PropBank, the lexicon is interlinked with the annotated data of the Wall Street Journal corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá rozdíly mezi hloubkovými syntaktickými strukturami češtiny a angličtiny. Upozorňujeme na některé jevy potenciálně problematické pro automatické zarovnávání uzlů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Our paper comments on the divergences of the deep syntactic layers of Czech and Eglish. We point out several phenomena potentially problematic for syntactic alignment.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Recenzovaná kniha je sborníkem konference Connectives as Discourse Landmarks 2005. Její studie se věnují jednak diskurzním konektorům z hlediska lexikologie a syntaxe, jednak také pragmatickým aspektům užití konektorů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The peer-reviewed book is the proceedings of the  conference "Connectives as Discourse Landmarks 2005". Its contributions focus on discourse connectives in terms of lexicology and syntax, as well as pragmatic aspects of the use of connectives.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předpokládáme, že rozdělení věty na téma a réma (o čem se vypovídá co) lze vyvodit automaticky z hodnot kontextové zapojenosti, připsaných každému uzlu v závislostním stromě reprezentujícím hloubkovou strukturu věty. Pro ověření této hypotézy byly provedeny kontrolní ruční paralelní anotace. Článek informuje o principech a předběžných výsledcích těchto kontrolních anotací.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We suppose that the bipartition of the sentence into its Topic and Focus ("aboutness") can be automatically derived from the values of contextual boundness assigned to each node of the dependency tree representing the underlying structure of the sentence. For the testing of this hypothesis, control manual parallel annotations have been carried out. The principles of the control annotations are described and preliminary results are reported on.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku jsou popsány některé nástroje a přístupy ke strojovému překladu z češtiny do ruštiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper deals with some aspects of MT between Czech and Russian.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá hledáním vhodného formalismu pro popis prozodie pro účely strojového učení. Cílovou aplikací je modul generující prozodii pro syntézu řeči z textu. Tento modul se naučí prozodické značky (parametry nebo symboly) z rozsáhlého korpusu. Formalismus, který hledáme, by měl být obecný, percepčně relevantní, obnovitelný, automaticky získatelný, objektivní a naučitelný. Popsali a porovnali jsme hlavní formalismy pro popis F0, jmenovitě Fujisakiho model, ToBI, Intsint, Tilt tzv. “Glissando threshold”. Nejvhodnější metoda popisu F0 pro úlohu strojového učení je “Glissando threshold” s přidaným zjednodušením.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We need to find the most suitable prosody formalism for the task of machine learning. The
target application is a prosody generative module for text-to-speech synthesis. This module will learn prosody marks (parameters or symbols) from large corpora. Formalism we are looking for should be general, perceptually relevant, restorable, automatically obtained, objective and learnable. Main formalisms for the pitch description are briefly described and compared, namely Fujisaki model, ToBI, Intsint, Tilt and “Glissando threshold” adaptation. The most suitable method of pitch description for the task of machine learning is “Glissando threshold” adaptation with an additional simplification.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Český morfologický tagger založený na Průměrovaném perceptronu. Obsahuje sadu nástrojů pro experimenty s různými sadami rysů a trénování na různých datech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Czech morphological tagger based on Averaged Perceptron. The package contains set of tools for experiments with feature sets and training on various data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Litomyšlská rodina Sgallů prošla holokaustem se ztrátou většiny členů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The family Sgall from Litomyšl went through Shoa loosing the majority of its members.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek navrhuje metodu přípravy a pořízení řečového korpusu pro úlohu syntézy řeči z textu s dynamickým výběrem jednotek řízenou pomocí symbolické prozodie. Soustředí se na algoritmus výběru foneticky a prozodicky bohatých vět. Foneticky přepsané věty jsou obohaceny o symbolický popis na hrubé prozodické úrovni s respektováním typu prozodému, ve kterém se fony objevují. Výsledný algoritmus pak vybírá věty s ohledem na fonetická i prozodická kritéria. Abychom též pokryli i supravětné prozodické jevy, náhodně jsme vybrali odstavce a nahráli je. Nový řečový korpus se může využít k syntéze řeči s dynamickým výběrem jednotek a také k trénování datově orientovaného prozodického parseru.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper proposes a way of preparing and recording a speech corpus for unit selection text-to-speech speech synthesis driven by symbolic prosody. The research is focused on a phonetically and prosodically rich sentence selection algorithm. Symbolic description on a deep prosody level is used to enrich the phonetic representation of sentences (by respecting the prosodeme types phones appear in). The resulting algorithm then selects sentences with respect to both phonetic and prosodic criteria. To cover supra-sentential prosody phenomena, paragraphs were selected at random and recorded as well. The new speech corpus can be utilised in unit selection speech synthesis and also for training a data-driven prosodic parser.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Experimenty s frázovým strojovým překladem a zamyšlení nad užitečností podrobných lingvistických analýz.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Experiments with phrase-based MT and a discussion on the utility of linguistic analyses.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku je popsán nový anglicko-český paralelní korpus CzEng 0.5, který obsahuje v obou jazycích přibližně 20 miliónů tokenů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce CzEng 0.5, a new Czech-English sentence-aligned parallel corpus consisting of around 20 million
tokens in either language. The corpus is available on the Internet and can be used under the terms of license
agreement for non-commercial educational and research purposes. Besides the description of the corpus, also
preliminary results concerning statistical machine translation experiments based on CzEng 0.5 are presented.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje experiment s česko-anglickým slovním zarovnáním. Na pěti stech párech ručně zarovnaných vět studujeme nejčetnější důvody neshody a srovnáváme ruční zarovnání s několika variantami automatického postupu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe an experiment with Czech-English word alignment. Half a thousand
sentences were manually annotated by two annotators in parallel and the most
frequent reasons for disagreement are described. We evaluate the accuracy of
GIZA++ alignment toolkit on the data and identify that lemmatization of the
Czech part can reduce alignment error to a half. Furthermore we document that
about 38% of tokens difficult for GIZA++ were difficult for humans already.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje experimenty s česko-anglickým strojovým frázovým překladem a vyhodnocuje několik technik zlepšujících kvalitu překladu (měřenou pomocí automatické metriky BLEU).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe experiments with Czech-to-English phrase-based machine translation. Several techniques for improving translation quality (in terms of well-established measure BLEU) are evaluated. In total, we are able to achieve BLEU of 0.36 to 0.41 on the examined corpus of Wall Street Journal texts, outperforming all other systems evaluated on this language pair.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek / kapitola popisuje jednotlivé úrovně anotace Pražského závislostního korpusu a vztahy mezi nimi, zejméne mazi sémantickou a syntaktickou rovinou anotace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Complex Corpus Annotation: The Prague Dependency Treebank. The article - chapter describes the layers of annotation in the Prague Dependency Treebank, and the relations among them.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kapitola pojednává o komplexní anotaci jazykových korpusů na úrovni morfologické, povrchově syntaktické a tektogramatické. Za příklad slouží Pražský závislostní korpus.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The chapter discusses complex annotation of language corpora on the layers of morphology, surface syntax and tectogrammatics. The Prague Dependency Treebank serves as an example.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kapitola pojednává o syntakticky anotovaných korpusech (treebancích) a principech návrhu sad morfologických a syntaktických značek, které se při anotaci takových korpusů využívají.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The chapter discusses syntactically annotated corpora (treebanks) and the design principles of sets of morphological and syntactic tags that are used for annotation of such corpora.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Závislostní korpus anotovaný na rovině morfologické (2 miliony slov), syntaktické (1,5 milionu slov) a sémantické (0,8 milionu slov), obsahující navíc doplňující informace o informační struktuře věty a koreference. Distribuováno organizací Linguistic Data Consortium.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Dependency corpus annotated at morphological, syntactic, and deep syntactic levels, with additional attributes describing information structure, coreferences and many other grammatical values. It is distributed by the Linguistic Data Consortium.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Technická zpráva shrnuje dosavadní poznatky získané při budování Pražského závislostního korpusu mluvené
češtiny (Prague Dependency Treebank of Spoken Czech; PDTSC) v Ústavu formální a aplikované lingvistiky MFF
UK Praha. PDTSC bude prvním korpusem mluvené řeči, který bude obsahovat anotace významu promluv. Výzkum
ukázal, že před vlastní hloubkovou analýzou mluvené řeči je nezbytné transkribované segmenty mluvené řeči
nejprve standardizovaným způsobem převést na gramaticky správné věty, tj. provést tzv. rekonstrukci
standardizovaného textu z mluvené řeči.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Unprepared spontaneous speech breaks many rules by which written texts are constituted. The speakers, for instance, often get the syntax wrong; they mispronounce or confuse lexical units, and their use of ellipsis as well as of deictic words and connectives is abundant, compared to standard written texts. The only way out seems to lead through machine learning: to process enough data for the machine to learn how to tell apart noise from relevant structures and how to restore the commonest types of ellipses to be able to analyze the spoken data with the tools already available. To build such data means smoothing the speech transcription to meet the usual written-text standards by re-chunking and re-building the original segments into grammatical sentences with acceptable word order, proper morphosyntactic relations between words, and appropriate wording, while preserving links to the original transcription.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>První nástroj na kontrolu gramatické správnosti českých vět, který je integrální součástí balíku kancelářských programů Office 2007. Nástroj pročítá text a zelenou barvou podtrhává konstrukce, které jsou gramaticky nesprávné.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The first tool checking the grammatical correctness of Czech sentences, which constitutes an integral part of the Microsoft Office 2007 software package. The tool reads the text and umderlines in green those strings, which are grammatically incorrect.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>První poznámka se vztahuje ke konstrukcím, kde jde o distributivitu, a k prostředkům, jak se na ně anaforicky odkazuje. Druhá se týká konstrukcí s předložkou mezi + Instr. a jejich významu v recipročních konstrukcích (plurálový participant reciproční konstrukce musí mít víc než 2 členy).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The first remark concerns the relation between distributive reading and anaforic expressions coreferred. The second remark is related to the meaning of the Czech preposition mezi in reciprocal constructions, where the plural of the noun after mezi means more than two.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve stati se posuzuje valence některých vyhraněných českých adjektiv. Zvažuje se na základě korpusového materiálu, zda lze oddělit jejich užití v absolutním významu (pyšný člověk, věrný přítel) od užití s obsazenou valenční pozicí (otec pyšný na dceru, své nevěstě věrný snoubenec).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The valency of some typical Czech adjectives is studied. The possibility to distinguish their "absolute" usage from the usage with filled valency position is considered.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Laudatio k sedmdesátým narozeninám profesorky Evy Hajičové. Laudatio bylo předneso v rámci setkání Pražského lingvistického kroužku, během kterého pronesla jubilantka přednášku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Laudation for Professor Eva Hajičová on the occasion of her 70th birthday. The laudation was presented at the meeting of the Prague Linguistic Circle during which the honored person gave a lacture.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Čas a šťastná náhoda si spolu velmi dobře rozumějí, většinou. Uvedou nás do situací, do kterých bychom se dostali za předpokladu opravdu „vychytralé“ intuice, většinou. Představíme jednu takovou situaci, která nese reálnou podobu a reálné jméno – Český akademický korpus. Zmínili jsme čas – proto jej představíme na pozadí této veličiny. 
Vrátíme se o dvacet let zpátky do doby, kdy vznikl. Připomeneme dobu před deseti lety, kdy otevřel nové možnosti aplikace stochastických metod v počítačovém zpracování češtiny. Popíšeme jeho současnou příbuznost s Pražským závislostním korpusem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Czech Academic Corpus was created during the 1970s and 1980s at the Czech Language Institute under the supervision of Marie Těšitelová. The main motivation to build it (a total of 540 thousand word tokens) was to obtain the quantitative characteristics of contemporary Czech. 
The corpus is structurally annotated on two levels - the morphological level and the syntactical-analytical level. The original stochastic experiment in morphological tagging of Czech were performed using the corpus at the beginning of the 1990s. Given this, the corpus-based processing of Czech was launched. At the end of 1990s, work on the Prague Dependency Treebank had started (independently from the corpus) and its first edition was published in 2001. In considering future released versions of the treebank, we have decided to convert the corpus into the treebank-like format. This article focuses on the twenty-year history of the Czech Academic Corpus. Special attention is devoted to thus far unpublished fats about the corpus annotation. The conversion steps resulting in the first version of the Czech Academic Corpus are described in detail.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přehled výsledků Cross-Language Speech Retrieval Track organizované v rámci evaluační kampaně CLEF 2006.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The CLEF-2006 Cross-Language Speech Retrieval (CL-SR) track included two tasks: to identify topically coherent segments of English interviews in a known-boundary condition, and to identify time stamps marking the beginning of topically relevant passages in Czech interviews in an unknown-boundary condition.  Five teams participated in the English evaluation, performing both monolingual and cross-language searches of speech recognition transcripts, automatically generated metadata, and manually generated metadata.  Results indicate that the 2006 English evaluation topics are more challenging than those used in 2005, but that cross-language searching continued to pose no unusual challenges when compared with monolingual searches of the same collection.  Three teams participated in the monolingual Czech evaluation using a new evaluation measure based on differences between system-suggested and ground truth replay start times, with results that were broadly comparable those observed for English.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Základní forma mluvené češtiny se užívá v Čechách, zatímco na většině Moravy je situace jiná.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Spoken Czech in its unmarked form is used in Bohemia, whereas the situation in most of Moravia is different.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Svým rodičům vděčím za první vhled do komplexnosti mluvené češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>My parents gave me a good start for seeing the intricaces of spoken Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vybrané spisy Petra Sgalla, Matematicko-fyzikállní fakulta Univerzity Karlovy Praha</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Selected linguistic writings of Petr Sgall, Faculty of Mathematics and Physics, Charles Univeristy, Prague.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Hyperkorekce podléhá jazykovému vývoji a je k ní třeba věnovat náležitou pozornost při popisu jazykového usu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Hypercorrection underlies language development and has to be taken into account in discussions on the current language situation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Místo jasné hranice je třeba vidět široké přechodné pásmo mezi „spisovnou“ a běžně mluvenou češtinou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Instead of such a boundary, it is necessary to see a large transition zone between standard (written) Czech and the everyday speech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Recenzovaná kniha usiluje o charakterizaci metod Pražské školy z různých aspektů pohledu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The reviewed book attempts at a characterization of the methods of the Prague School.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jádro jazykového systému lze vidět ve valenci jednotlivých slov, což je spojující prvek mezi slovní zásobou a mluvnicí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The core of language can be seen in the valency of individual words, which is the connecting link between lexicon and grammar.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zatímco standardní (ne vždy „spisovná“) čeština je typická pro psaný projev, je pro mluvenou češtinu charakteristická především obecná čeština.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>While written Czech typically corresponds to the standard norm, in spoken usage primarily Common Czech is used.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Stručný přehled přínosu Petra Sgall ke světové i české lingvistice při příležitosti jeho osmdesátin.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A brief article summarizing the scientific contribution of Profesor Petr
Sgall to the broad field of lingusitic.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kritický rozbor problémů, s nimiž se setkává závislostní popis jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A critical analysis of issues and problems of dependency description of language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozbor případů tzv. přerušených syntaktických vztahů mezi členy téhož rozvitého větného členu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A corpus based analysis of long distance dependencies in the representation of Czech sentences.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Podmínkou pro automatický překlad je úplné porozumění danému textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An adequate translation is possible only under the condition of  full understanding of the text.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tradice Pražské lingvstické školy a její pokračování v oblasti exaktního popisu jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Prague linguistic tradition and its reflection in the formal description of language. Functionalism, dependency syntax, syntactico-semantic structure of the sentence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Při příležitosti udělení odměny za celoživotní dílo v počítačové lingvistice autorka   vyčlenila a rozebrala některé výsledky klasické lingstiky, které podstatně přispěly k rozvoji moderní a počítačové lingvistiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>At the occasion of the Life Achievements Award presented to the author of the paper, the awardee singled out and analyzed several contributions of (classical) linguistics to modern  methods in linguistics and to computational lingusitics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Diskuse o některýh specifických otázkách vztahu překladu a různých stupňů porozuměni překládaému textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Specific issues concerning the relation of translation and the degree of understanding necessary for translation is discussed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Syntaktická anotace korpusů se nesmí zastavit na povrchové rovině, ale je třeba zachytit i podkladové syntaktické  vztahy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Arguments are given for an underlying syntactic annotation of sentences of large text corpora.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Anotování korpusu je  důležitým materiálem pro ověřování či modifikaci lingvistické teorie.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Linguistics is an empirical science and as such it works with hypotheses. Annotation of corpora offers an important way how to test these hypotheses.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Analýza nejdůležitější lingvistických studií Petra Sgalla při příležitosti uveřejnění jeho Vybraných spisů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A detailed analysis of the most important writings of Petr Sgall at the occasion of the publication opf his Selected Writings.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Stručný přehled a zhodnocení životního díla významného českého lingvisty Petra Sgalla.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A brief summary of the professional life and publications of Professor
Petr Sgall.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozbor životního přínosu profesora Petra Sgalla pro českou lingvistiku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Life-long contribution of Professor Petr Sgall to the Czech linguistic thinking.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Využití jazykových korpusů pro testování lingvistických teorií (na příkladu PZK).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The use of annotated corpora for testing linguistics hypotheses (on the example of PDT).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kritický rozbor vědeckého přínosu členů Pražské školy v průběhu její osmdesátileté existence.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A critical survey of the scientific contribution of the members of the Prague Linguistic Circle in the course of its eighty-years of existence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dynamický přístup ke struktuře diskurzu je charakterizován zapojením pojmu ´stupeň aktivovanosti´ objektů, které jsou přístupny v zásobníku sdílených znalostí, jenž mluvčí i posluchač sdílejí.

Představujeme první verzi algoritmu pro přiřazení stupně aktivovanosti objektům. Algoritmus vychází ze struktury pro reprezentaci větného ohniska a základu. Rovněž představíme graifckou vizualizaci výstupu algoritmu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A dynamic approach to discourse structure is characterized using
the notion of degrees of salience of items in the stock of
knowledge the speaker assumes s/he shares with the hearer. A
preliminary algorithm of salience assignment (based primarily on
the appearance of the nodes of the dependency tree representing
the underlying structure of the sentence to topic or focus) has
been implemented and a visualization of its results has been
produced in order to make the implications of proposed discourse
analysis more perspicuous.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek do diskuse o postaveni obecné a spisovné češtiny z hlediska korpusové lingvistiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A contribution to the ongoing discussion on standard and common Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku je představen obecný datový formát zaoložený na XML, tzv. PML (Prague Markup Language), který byl speciálně navržen pro potřeby bohaté vícevrstvé lingvistické anotace a vč. anotačních slovníků. PML je hlavní datový formát pro připravované vydání Pražského závislostního korpusu 2.0. Nejprve prezentujeme základní pojmy a myšlenky formátu, pak popisujeme, jak je PML aplikováno na PZK, a na závěr předkládáme návrhy pro další vylepšení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we introduce a generic XML-based data format called PML (Prague Markup Language), which was speciﬁcally designed for the needs of representing rich multi-layered linguistic annotation and other data sources such as annotation dictionaries. PML is the main data format for the upcoming release of version 2.0 of the PDT (Prague Dependency Treebank). We ﬁrst present the fundamental concepts and ideas behind the format, then describe how the annotation of PDT is represented in PML, and ﬁnally outline our plans for further improvement.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Testovací kolekce použita pro evaluaci systémů pro vyhledávní v mluvené řeči v rámci Cross-language Speech Retrieval track v rámci CLEF 2006.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A test collection employed for evaluation of the speech retrieval systems at Cross-language Speech Retrieval track at CLEF 2006.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme možnost kombinování lexikálních asociačních měr a přínášíme empirické výsledky metod z automatické extrakce kolokací. V první části se nalézá podrobný přehled asociačních měr a jejich výsledků na manuálně anotových datech, která byla evaluována pomocí grafů presision-recall a hodnot mean average precision. Ve druhé části popisujeme nekolik klasifikačních metod vhodných ke kombinování asociačních měr, poté následuje jejich evaluace a porovnání s jednotlivými individuálními měrami. Dále představujeme algoritmus pro výběr rysů, který podstatně snižuje počet kombinovaných měr, a to s minimálním snížením celkového výkonu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce the possibility of combining lexical association measures and present empirical results of several methods employed in automatic collocation extraction. First, we present a comprehensive summary overview of association measures and their performance on manually annotated data evaluated by precision-recall graphs and mean average precision. Second, we describe several classification methods for combining association measures, followed by their evaluation and comparison with individual measures. Finally, we propose a feature selection algorithm significantly reducing the number of combined measures with only a small performance degradation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme, jak přistupujeme k valenci v rámci anotování PZK. Zaměřujeme se zejména na valenci sloves a na specifické problémy spojené s touto problematikou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a description of how we dealt with the valency of verbs during the annotation of the PDT and the way the verbal part of the valency dictionary was built. We focus on some specific problems related to verbal valency (as well as some other verbal complementations) from the point of view of the PDT.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek představuje PZK a anotaci na třech lingvistických úrovních: morfologické, analytické a tektogramatické. Soustřeďuje se zejména na hloubkově syntaktickou rovinu a anotaci elips.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presentation will briefly introduce the PDT that aims at complex linguistic annotation of naturally occurring sentences. Written Czech sentences are annotated on three layers: morphological layer (lemmas, tags, morphological categories), analytical layer (surface structure, dependencies, analytical functions) and the tectogrammatical layer. In our contribution we focus mainly on how sentences are represented on the tectogrammatical layer. Demonstration of this layer will be part of the presentation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato stať je shrnutím disertační práce Valence deverbativních substantiv v češtině úspěšně obhájené v říjnu 2006. Obsahuje vlastní výsledky získané během autorčina doktorského studia na MFF UK v Praze v letech 1999–2006.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper is an overview of Doctoral Thesis Valence deverbativních substantiv v češtině successfully defended in October 2006, containing results obtained by the author during her doctoral study at the Faculty of Mathematics and Physics of the Charles University (MFF UK) in Prague from 1999 until 2006.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládaná práce popisuje kroky směřující k modelování prosodie češtiny. Po charakteristice a rozboru současných prosodických teorií a aplikací představuji ústřední bod této práce, popis vývoje snadno přístupného a uživatelsky přívětivého výzkumného prostředí Dialogy.Org, které podporuje zkoumání české prosodie, její analýzu i modelování.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This work describes steps towards prosody models of spoken Czech language.
After a characterisation and discussion of recent prosody definitions and of area of prosody applications, I present the central point of the work, development of an easy-accessible and user-friendly research environment Dialogy.Org, supporting exploration of Czech prosody and its automatic analysis and modelling.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Netgraph je snadno ovladatelný a intuitivní nástroj vyvinutý pro vyhledávání v Pražském závislostním korpusu, je ho ale možno použít i pro korpusy jiné. Je platformově nezávislý, založený na architektuře klient-server a pracuje v prostředí internetu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Netgraph is an easy to use and intuitive tool developed for searching in the Prague Dependency Treebank; it can be used for other corpora as well. It is platform-independent, based on the client-server architecture and works in the internet environment.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme Netgraph - nástroj pro vyhledávání v lingvisticky anotovaných treebancích. Pracuje v prostředí internetu, je víceuživatelský a založený na principu klient-server.
Netgraph má grafické hardwarově nezávislé uživatelsky přítulné rozhraní. Dotazovací systém je velice intuitivní, snadný k naučení, pochopení a použití, nicméně dokáže provádět pokročilá vyhledávání.
Ukážeme, jak může být Netgraph použit pro tak komplexní anotační schémata jako je Pražský závislostní korpus 2.0 se vztahem mezi uzly jednotlivých rovin jiného typu než 1:1. Zmíníme rovněž hlavní vlastnosti dvou dalších vyhledávacích nástrojů, které jsou pro tento treebank k dispozici.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present Netgraph - a tool for searching in linguistically annotated treebanks. It works in the Internet environment, on multi-user and client-server basis.
Netgraph has a graphically oriented hardware independent user-friendly interface. The query system is very intuitive, easy to learn, understand and use, nevertheless it can perform advanced searches.
We show how Netgraph can be used for such complex annotation schemes as the Prague Dependency Treebank 2.0 with its non-1:1 relation among nodes on different layers. We also mention main properties of two other searching tools available for this treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Implementace pro kódování arabštiny v jazycích Haskell a Perl. Podpora po ArabTeX, Buckwalter, UTF a další kódování. Interpretovatelné notace generující originální ortografii a/nebo fonetickou transkripci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Implementations for encodings of Arabic, in Haskell and Perl. Support for ArabTeX, Buckwalter, UTF and other encodings. Interpretable notations generating original orthography and/or phonetic transcriptions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek pojednává o anotačních detailech Pražského arabského závislostního korpusu, morfologicky a povrchově syntakticky anotovaného korpusu arabských novinových textů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The contribution discusses several annotation details of the Prague Arabic Dependency Treebank, a morphologically and surface-syntactically annotated corpus of Modern Standard Arabic newswire texts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Hlavním cílem příspěvku je ověřit možnost navýšení objemu Pražského závislostního korpusu o data obsažená v Českém akademickém korpusu.

Pražský závislostní korpus je komplexně anotován na třech rovinách, které zachycují moforlogické a syntaktické (povrchové i hloubkové) vlastnosti českých vět. Charakteristiky zachycené v anotacích Českého akademického korpusu odrážejí zejména vztahy mezi jednotkami vět. 

Integrace ČAK do PDT implikuje kompatibilitu obou datových bank -- kompatibilita na rovině morofologické již byla zajištěna.

Ve věci syntaktické kompatibility se v příspěvku ptáme, je-li automatická konverze syntaktických anotací Českého akademického korpusu efektivnější než přímá aplikace statistického parseru.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of the present paper is to investigate a possibility to enlarge the data in the Prague Dependency Treebank by the data included in the Czech Academic Corpus. The Prague Dependency Treebank annotation is based on a complex three-layer scenario capturing the morphemic and syntactic properties (both of the surface and of the underlying, tectogrammatical structures) of Czech sentences. The characteristics included in the Czech Academic Corpus reflect basic (mostly intra-clausal) relations between sentence elements. The integration of the Czech Academic Corpus material into the Prague Dependency Treebank implies, of course, the necessity to make the two sets of annotated data compatible. This has already been done as for the morphemic layer. The question the paper poses and attempts to answer is whether an automatic transition of the syntactic Czech Academic Corpus data into the Prague Dependency Treebank format is more effective than a direct annotation of the same texts by a statistical parser.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Technická zpráva je dokumentací k Pražskému závislostnímu korpusu verze 2.0 (PDT 2.0). Obsahuje podrobný komplexní popis dosavadních pravidel anotování českých vět na tektogramatické rovině, a to především po stránce lingvistické, ale i po stránce technické. Anotovaná data neodráží vždy přesně popisovaný stav pravidel anotace, proto je v technické zprávě zahrnut i co nejpřesnější popis anotovaných tektogramatických stromů v korpusu PDT 2.0.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This technical document provides detailed documentation of the Prague Dependency Treebank, version 2.0 (PDT 2.0). It includes a detailed complex description of the rules that have been used so far for the annotation of Czech sentences on the tectogrammatical layer both in linguistic and technical respect. The annotated data do not always reflect the described state of the rules precisely, therefore the technical document includes also a detailed description of the tectogrammatical trees that are annotated in PDT 2.0.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Technická zpráva je stručným popisem reprezentace věty na tektogramatické rovině Pražského závislostního korpusu verze 2.0 (PDT 2.0). Je určena uživatelům PDT, kteří se chtějí rychle zorientovat v námi použité reprezentaci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Technical report presents a short description of sentence representation at the tectogrammatical level in the Prague Dependecy Treebank 2.0 (PDT 2.0). It is aimed at those PDT users that look for a quick introduction into the used representation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Technická zpráva je stručným popisem reprezentace věty na tektogramatické rovině Pražského závislostního korpusu verze 2.0 (PDT 2.0). Je určena uživatelům PDT, kteří se chtějí rychle zorientovat v námi použité reprezentaci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Technical report presents a short description of sentence representation     at the tectogrammatical level in the Prague Dependecy Treebank 2.0(PDT 2.0). It is aimed at those PDT users that look for a quick introduction into the used representation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve stati jsou uvedeny základní rysy valenčního přístupu užívaného ve Funkčním generativním popisu a aplikovaného ve valenčním slovníku VALLEX. Z hlediska teoretického je rámec rozšířen o tzv. kvazivalenční doplnění. Podává se jejich charakteristika vzhledem k participantům a volným doplněním.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The main features of the approach to the description of valency in the FGD and applied in the valency dictionary VALLEX are described. The quasivalency modifications are introduced and characterized with regard to their position between inner participants and free modifications.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valenční slovník českých sloves VALLEX 2.0, založený na valenční teorii převzaté z funkčního generativního popisu. VALLEX zachycuje chování frekventovaných českých sloves (počet a typ doplnění a jejich merfematickou realizaci).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Valency Lexicon of Czech Verbs VALLEX 2.0, based on valency theory adopted from Functional Generative Description. VALLEX describes valency characteristics of frequent Czech verbs (number and type of complementations ant their morphemic realization).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valenční slovník českých sloves, obsahující přibližně 2730 záznamů o lexémech pokrývajících cca. 6460 lexikálních jednotek (významů). Slovník je volně k dispozici pro účely výzkumu.

V ramci ČR bylo nejvíce licencí vyžádáno z pracovišt Filozofické fakulty UK, Fakulty informatiky MU a Ústavu pro jazyk český AV, z desítek licencí pro zahraniční instituce pak lze uvést např. The Ohio State University, Saarland University, Zagreb University.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Valency Lexicon of Czech Verbs, Version 2.0 (VALLEX 2.0) is a collection of linguistically annotated data and documentation, resulting from an attempt at formal description of valency frames of Czech verbs. VALLEX 2.0 has been developed at the Institute of Formal and Applied Linguistics, Faculty of Mathematics and Physics, Charles University, Prague. VALLEX 2.0 is successor of VALLEX 1.0, extended in both theoretical and quantitative aspects.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku představujeme alternační model valenčního slovníku VALLEX. Alternace popisují pravidelné změny ve valenční struktuře sloves. Charakterizujeme a exemplifikujeme ‘syntaktické‘ a ‚sémantické‘ alternace a jejich dopad na valenční strukturu sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The main objective of this paper is to introduce an alternation-based model of valency lexicon of Czech verbs VALLEX. Alternations describe regular changes in valency structure of verbs – they are seen as transformations taking one lexical unit and return a modified lexical unit as a result. We characterize and exemplify ‘syntactically-based’ and ‘semantically-based’ alternations and their effects on verb argument structure. The alternation-based model allows to distinguish a minimal form of lexicon, which provides compact characterization of valency structure of Czech verbs, and an expanded form of lexicon useful for some applications.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku navrhujeme nový formální rámec pro FGD založený na restartovacích automatech. Tento přístup zrcadlí tzv. redukční analýzu, metodu implicitně využívanou lingvisty - tato analýza umožňuje odhalit závislostní vztahy ve větě na základě jejího postupného zjednodušování.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We propose a new formal frame for FGD based on restarting automata. This new approach mirrors straightforwardly the so-called analysis by reduction, an implicit method used for linguistic research - analysis by reduction allows to obtain dependencies from the correct reductions of Czech sentences as well as to describe properly
the complex word-order variants of free a word order language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zájmena a další zájmenná slova vytvářejí neproduktivní, uzavřené skupiny, v jejichž rámci lze (alespoň do jisté míry) identifikovat pravidelné slovotvorné a významové vztahy. V první části příspěvku je představena reprezentace českých zájmenných slov v rámci tektogramatické anotace Pražského závislostního korpusu (PDT 2.0). V druhé částí příspěvku se obdobným způsobem pokoušíme popsat anglická a německá zájmenná slova s cílem nastínit, že zájmenné systémy různých jazyků sdílejí celou řadu vlastností.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Pronouns and other pronominal words are unproductive, closed classes with (at least to a certain extent) transparent derivational relations not only in Czech, but also in other languages. In the first part of our contribution, we introduce the representation of Czech pronominal words at the underlying syntactic layer (so called tectogrammatical layer, t-layer) of the Prague Dependency Treebank version 2.0, the annotation scenario of which was built on the theoretical basis of the Praguian Functional Generative Description. In the second part, we try to apply this representation to English and German pronominals in order to illustrate some of the common (universal) properties of the pronominal systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku je popsána práce na systému gramatémů, což jsou nejčastěji sémantické protějšky morfologických kategorií (substantivního čísla, slovesného času apod.). Gramatémy byly navrženy v rámci Funkčního generativního popisu a dále rozpracovány při víceúrovňové anotaci Pražského závislostního korpusu (PDT 2.0). Představena je také typologie tektogramatických uzlů, která je využívána jako formální prostředek pro rozlišení, které gramatémy danému uzlu náležejí a které nikoli.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we report our work on the system of grammatemes (mostly semantically-oriented counterparts of morphological categories such as number, degree of comparison, or tense), the concept of which was introduced in Functional Generative Description, and has been recently further elaborated in the layered annotation scenario of the Prague Dependency Treebank 2.0. We present also a hierarchical typology of tectogrammatical nodes, which is used as a formal means for ensuring presence or absence of respective grammatemes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládáme anotační schéma, které zachycuje obecné časové vztahy mezi ději vyjádřenými v diskurzu. Jeho cílem je přirozeně rozšířit existující rovinu tektogramatické anotace Pražského závislostního korpusu. Představuje tak
krok k zachycení obsahu diskurzu. Existence korpusu anotovaného pomocí předkládaného schématu umožní trénování a testování algoritmů automatické extrakce časových vztahů, což přispěje k řešení mnoha úkolů ve zpracování přirozeného jazyka jako je získávání znalostí a strojový překlad. Celkem bylo zatím anotováno 233 vět českého překladu Wall Street Journalu (část Penn Treebanku). Překládáme informaci o statistickém rozdělení jednotlivých typů časových vztahů založenou na těchto prvních anotovaných datech a uvádíme též výsledek algoritmu pro automatické určování časových vztahů založeném na využití informace poskytované gramatikou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we present an annotation scheme that captures general temporal relations between events expressed in a discourse. The proposed scheme aims to naturally extend the existing tectogrammatic annotation of the Prague Dependency Treebank and represents a step
towards capturing the cognitive (ontological) content of a discourse. The existence of such an annotation will allow the training and testing of algorithms for automatic extraction of temporal relations which, in turn, contributes to various NLP tasks such as information retrieval and machine translation. 233 sentences of Czech translations of the Wall Street Journal (Penn Treebank) have been annotated so far. We also present statistics on the distribution of respective temporal relations based on this
preliminary annotation data as well as the performance of a grammar-based algorithm.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládáme formalismus určený k vyhledávání a případnému nahrazování podstromů v ohodnocených stromech. Zvláštní pozornost je věnována zpracování jazykových korpusů. Je-li interpret použit v nahrazovacím módu, zpracovává přepisovací pravidla obsahující levou a pravou stranu. Levá strana určuje vyhledávací formulí les podstromů, které mají být nalezeny. Pravá strana pak obsahuje odpovídající substituce. Ve vyhledávacím módu je přítomna pouze levá strana. Formalismus je plně implementován. Výkon  implementovaného nástroje umožňuje zpracovat i rozsáhlé korpusy v přijatelném čase. Hlavním přínosem předkládané práce je dotazovací síla vyhledávací formule, elegantní a intuitivní podoba
pravidel (a jejich reverzibilita) a výkon implementovaného nástroje.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>e presents a formalism capable of searching and
optionally replacing forests of subtrees within labelled trees. In particular, the formalism is developed to process linguistic treebanks. When used as a substitution tool, the interpreter
processes rewrite rules consisting of left and right side. The left side specifies a forest of subtrees to be searched for within a tree by imposing a set of constraints encoded as a query
formula. The right side contains the respective substitutions for these subtrees. In the search mode only the left side is present. The formalism is fully implemented. The performance of the
implemented tool allows to process even large linguistic corpora in acceptable time. The main contribution of the presented work consists of the expressiveness of the query formula, in the
elegant and intuitive way the rules are written (and their easy reversibility), and in the performance of the implemented tool.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek má za cíl představit návrh automatického generování valenčích rámců českých sloves, které dosud nejsou obsaženy ve valenčním slovníku VALLEX. V tomto návrhu využíváme syntakticko-sémantické klasifikace sloves. Popisujeme první krok v automatickém rozpoznávání valenčních rámců sloves mluvení a automatickém přiřazení valenčních rámců těmto slovesům. Představená metoda je vyhodnocena oproti dvěma verzím VALLEXu a FrameNetu 1.2. Pro účely generování valenčních rámců navrhujeme novou metriku založenou na konceptu vzdálenosti rámců.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We aim at a procedure of automatic generation of valency
frames for verbs not covered in VALLEX, a lexicon of Czech verbs.We ex-
ploit the classification of verbs into syntactico-semantic classes. This arti-
cle describes our first step to automatically identify verbs of communica-
tion and to assign the prototypical frame to them. The method of identi-
fication is evaluated against two versions of VALLEX and FrameNet 1.2.
For the purpose of frame generation, a new metric based on the notion
of frame edit distance is outlined.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá automatickou desambiguací slovesných valenčních rámců na českých datech. Hlavním přínosem je stanovení nejužitečnějších rysů pro zjednoznačnění valenčních rámců.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper deals with automatic disambiguation of verb valency frames on Czech data. Main contribution lies in determining of the most useful features for valency frame disambiguation. We experimented with diverse types of features, including morphological, syntax-based, idiomatic, animacy andWordNet-based. The considered features were classiffed using decision trees, rule-based learning and Naive Bayes classiffer.
On a set of 7 778 sentences we achieved accuracy of 79.86% against baseline 68.27% obtained by assigning the most frequent frame.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Mnoho současných systémů pro zpracování přirozeného jazyka potřebuje informaci o slovesné valenci. Tento článek popisuje převod již existujícího valenčního slovníku angličtiny do struktury FGP.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Many recent NLP applications, including machine translation and information retrieval, could benefit from semantic analysis of language data on the sentence level. This paper presents a method for automatic disambiguation of verb valency frames on Czech data. For each verb occurrence, we extracted features describing its local context. We experimented with diverse types of features, including
morphological, syntax-based, idiomatic, animacy and WordNet-based features. The main contribution of the paper lies in determining which ones are most useful for the disambiguation task. The considered features were classified using decision trees, rule-based learning and a Naive Bayes classifier. We evaluated the methods using 10-fold cross-validation on VALEVAL, a manually annotated corpus of frame annotations containing 7,778 sentences. Syntax-based features have shown to be the most effective. When we used the full set of features, we achieved an accuracy of 80.55% against the baseline 67.87% obtained by assigning the most frequent frame.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku srovnáváme automatické metody zjednoznačnění slovesných významů. Konkrétně zkoumáme naivní bayesovský klasifikátor, rozhodovací stromy a metodu založenou na pravidlech. Navrhujeme různé druhy rysů (morfologických, syntaktických, idiomatických a založených na WordNetu). Vyhodnocujeme metody i jednotlivé druhy rysů na dvou podstatně odlišných českých korpusech, VALEVALu a Pražském závislostním korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we compare automatic methods for disambiguation of verb senses, in particular we investigate Naive Bayes classifier, decision trees, and a rule-based method. Different types of features are proposed, including morphological, syntax-based, idiomatic, animacy, and WordNet-based features. We evaluate the methods together with individual feature types on two essentially different Czech corpora, VALEVAL and the Prague Dependency Treebank. The best performing methods and features are discussed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Systémy zpracování přirozených jazyků vyžadují informaci o slovesné valenci. Článek popisuje výrobu valenčního slovníku angličtiny EngVallex na základě již existujícího slovníku PropBank.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the English valency
lexicon EngValLex, built within the Functional
Generative Description framework.
The form of the lexicon, as well as the
process of its semi-automatic creation is
described. The lexicon describes valency
for verbs and also includes links to other
lexical sources, namely PropBank. Basic
statistics about the lexicon are given.
The lexicon will be later used for annotation
of the Wall Street Journal section
of the Penn Treebank in Praguian formalisms.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Manuální anotace paralelního esko-anglického korpusu na úrovni slov. Pouili jsme existující pokyny pro anglitinu a francouztinu (Melamed 1998) a rozíili jsme je, aby pokrývaly pípady systematicky se objevující v naem korpusu. Popisujeme hlavní pípady rozíení a uvádíme píklady. Vyhodnotili jsme jak intra-, tak inter-anotatorskou shodu s velmi dobrými hodnotami Kappa výrazn nad 0,9 a shodou 95% pro intra-anotatorskou shodu a 93% pro inter-anotatorskou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We report on our experience with manual alignment of Czech and English parallel corpus text. We applied existing guidelines for
English and French (Melamed 1998) and augmented them to cover systematically occurring cases in our corpus. We describe the main
extensions covered in our guidelines and provide examples. We evaluated both intra- and inter-annotator agreement and obtained very
good results of Kappa well above 0.9 and agreement of 95% and 93%, respectively.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Menší chyby v článku tří autorů lze opravit, aniž by se opouštělo přesvědčení, že pojem spisovnosti zastarává.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Several minor mistakes in the paper by the three authors may be revised without changing the conviction that the concept of "literary" language is getting obsolete.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku jsou popsány dvě řešení syntaktické analýzy českých vět. Vyhodnocení je provedeno s pomocí dat Pražského závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we describe in detail two
dependency parsing techniques developed
and evaluated using the Prague Dependency Treebank~2.0.  Then we
propose two approaches for combining various existing parsers in order
to obtain better accuracy. The highest parsing accuracy reported in
this paper is 85.84~\%, which represents 1.86~\% improvement compared
to the best single state-of-the-art parser. To our knowledge, no better result
 achieved on the same data has been published yet.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje první vydání Slovinského závislostního korpusu, který v současnosti obsahuje přibližně 2000 vět. Anotační postup je převzat z Pražského závislostní korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents the initial release of the Slovene Dependency Treebank, currently containing 2000 sentences or 30.000 words. Our approach to annotation is based on the Prague Dependency Treebank, which serves as an excellent model due to the similarity of the languages, the existence of a detailed annotation guide and an annotation editor. The initial treebank contains a portion of the MULTEXT-East parallel word-level annotated corpus, namely the first part of the Slovene twas first parsed automatically, to arrive at the initial analytic level dependency trees. These were then hand corrected using the treeranslation 
of Orwell’s “1984”. This corpus  editor TrEd; simultaneously, the Czech annotation manual was modified for Slovene. The current version is available in XML/TEI, as well as derived formats, and has been used in a comparative evaluation using the MALT parser, and as one of the languages present in  the CoNLL-X shared task on dependency parsing. The paper also discusses further work, in the first instance the composition of the corpus to be annotated next.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Na příkladu Frekvenčního slovníku češtiny ukážeme a vysvětlíme některé nové obecné principy, které poskytují lepší výsledky pro praktické použití frekvenčních slovníků. Jde především o použití průměrné redukované frekvence místo frekvence prosté.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>On the example of the recent edition of the Frequency Dictionary of Czech we describe and explain some new general principles that should be followed for getting better results for practical uses of frequency dictionaries. It is mainly adopting average reduced frequency instead of absolute frequency for ordering items. The formula for calculation of the average reduced frequencyis presented in the contribution together with a brief explanation, including examples clarifying the difference between the measures. Then, the Frequency Dictionary of Czech and its parts are described.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>EngValLex je valenční slovník anglických sloves používaných v korpusu Penn Treebank, který jsme získali poloautomatickou konverzí již existujícího valenčního slovníku PropBank-Lexicon do formátu použitelného pro tektogramatickou anotaci podle Funkčního generativního popisu. Tento článek se věnuje automatické konverzi dat a lingvistické problematice jejich následné manuální korektury.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>EngValLex is the name of an FGD-compliant valency lexicon of English verbs, built from the PropBank-Lexicon and following the structure of Vallex, the FGD-based lexicon of Czech verbs. EngValLex is interlinked with the PropBank-Lexicon, thus preserving the original links between the PropBank-Lexicon and the PropBank-Corpus. Therefore it is also supposed to be part of corpus annotation. This paper describes the automatic conversion of the PropBank-Lexicon into Pre-EngValLex, as well as the progress of its subsequent manual refinement (EngValLex). At the start, the Propbank-arguments were automatically re-labeled with functors (semantic labels of FGD) and the PropBank-rolesets were split into the respective example sentences, which became FGD-valency frames of Pre-EngValLex. Human annotators check and correct the labels and make the preliminary valency frames FGD-compliant. The most essential theoretical difference between the original and EngValLex is the syntactic alternations used by the PropBank-Lexicon, not yet employed within the Czech framework. The alternation-based approach substantially affects the conception of the frame, making in very different from the one applied within the FGD-framework. Preserving the valuable alternation information required special linguistic rules for keeping, altering and re-merging the automatically generated preliminary valency frames.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Technická zpráva je předběžným stručným popisem reprezentace anglické věty na tektogramatické rovině Pražského anglického závislostního korpusu verze. Je určena uživatelům PEDT, kteří se chtějí rychle zorientovat v námi použité reprezentaci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Technical report presents a preliminary short description of English sentence representation at the tectogrammatical level in the Prague English Dependecy Treebank. It is aimed at those PEDT users that look for a quick introduction into the used representation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Složenými predikáty (SP) rozumíme zejména konstrukce složené z významově vyprázdněného slovesa a nějakého abstraktního substantiva (často označujícího děj nebo stav). Toto substantivum může být jak deverbativní (např. učinit rozhodnutí, provést údržbu), tak nedeverbativní (např. dát pohlavek). Zmíněný typ predikátů je občas označován také za predikáty verbonominální, příp. analytické. V rámci anotací PDT volíme pro tyto predikáty název „složené predikáty“, a to z toho důvodu, že termín analytický je vyhrazen pro tzv. analytickou rovinu PDT a atributy s ní spojené, termín verbonominální pak zpravidla používáme pro označení jednoho z podtypů složených predikátů, a to predikátů tvořených sponovým slovesem být. V anglickém textu používáme pro označení všech typů složených predikátů termín complex predicates; pro složené predikáty, které nejsou tvořeny sponovým slovesem být, pak volíme obecně užívaný termín support verb constructions. V tomto příspěvku se věnujeme složeným predikátům bez sponového slovesa být.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Support Verb Constructions (SVCs) are combinations of a noun denoting an event or a state and a lexical verb. From the semantic point of view, the noun seems to be a part of a complex predicate rather than the object (or subject) of the verb, whatever the surface syntax may suggest. The meaning is concentrated in the noun component, whereas the semantic content of the verb is reduced or generalized.
	In this article we deal with the question of how to treat SVCs in the Prague Dependency Treebank (PDT subsequently). In the second section we briefly describe what PDT is, what linguistic theory it is based on and what questions regarding the SVCs arose during the annotation. In the third section we explain how SVCs have been identified and inventoried in PDT. We also give a brief survey of how SVCs have been treated within other linguistic frameworks and, based on this knowledge, what conclusions were drawn for PDT. Of course, this survey does not claim to be exhaustive. The fourth section focuses on the semantic aspects of SVCs. The last section describes how the FGD-based valency theory has been implemented in the case of SVCs to provide both a consistent and a linguistically justified annotation in PDT.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje extrakci verbonominálních kolokací do švédského valenčního slovníku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article presents a method of collocation extraction for a proposed Swedish valency lexicon.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>LEMPAS je pravidlový lematizátor pro švédský morfosyntakticky tagovaný korpus PAROLE. Vznikl jako jako provizorium při přípravě PAROLE pro kolokační analýzu prováděnou nástrojem Sketch Engine. Přesto dává poměrně uspokojivé výsledky v lematizaci substantiv, sloves a částečně i adjektiv, jak jsme zjistili jeho testováním na ručně lematizovaném korpusu SUC.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>LEMPAS, the lemmatizer for the Swedish corpus PAROLE, came into existence as a by-product of running the Sketch Engine (Kilgarriff et al., 2004) on Swedish, since many of the desirable features of the Sketch Engine, such as building word sketches, are only available for lemmatized corpora. We did not have access to any Swedish lexical sources and the time allowed for the lemmatization was very limited. Consequently, the lemmatizer had no great design ambitions.  Initially, we were only attempting to bring related forms together under a prelemma, using general rules, and avoiding explicit lists where possible. When the initial rules gave surprisingly good lemmatizations of nouns, verbs and adjectives, we decided to transform the pre-lemmas into real lemmas. The improved lemmatizer made a very good impression. We have tested the program on the manually lemmatized Stockholm-Umeå Corpus (SUC), and have analyzed the results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zde, já
předložit některé poznámky týkající se provádění nyní klasické metody
Údaje shlukování, tzv. maximální vzájemné výměny informací clusteru. Bylo to
zaveden v [\ markcite ((\ it Mercer et al.)) 1992] v souvislosti s jazykem
modelování. Původní článek obsahuje některé podněty k jeho provádění.
Ty jsou prováděny v detailu tady, spolu s některými novými triky. Výsledky
z test na 110M slov dlouho Český národní korpus je stručně
pak popsány. Také jsou identifikovány některé problémy z původní přístup
a jejich možné příčiny se navrhuje.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Herein, I
present some notes concerning implementation of now classical method of
data clustering, called Maximum Mutual Information Clustering. It was
introduced in [\markcite{{\it Mercer et al.}, 1992}] in context of language
modeling. The original article contained some cues concerning its implementation.
These are carried out in detail here, together with some new tricks. Results
of the test run on 110M words long Czech National Corpus are briefly
described then. Also, some problems of the original approach are identified
and their possible cause is suggested.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme srovnání dvou formalismů pro reprezentaci výpovědí v přirozeném jazyce, totiž hloubkově syntaktickou tektogramatickou rovinu Funkčního generativního popisu (FGD) a sémantický formalismus MultiNet. Probíráme možnou pozici MultiNetu v rámci FGD a prezentujeme předběžné zobrazení reprezentačních prostředků těchto dvou formalismů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a comparison of two formalisms
for representing natural language
utterances, namely deep syntactical Tectogrammatical
Layer of Functional Generative
Description (FGD) and a semantic
formalism, MultiNet. We discuss the
possible position of MultiNet in the FGD
framework and present a preliminary mapping
of representational means of these
two formalisms.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nedávno vydaný Pražský závislostní korpus 2.0 (PDT 2.0) je největším korpusem anotovaným na tektogramatické rovině ("rovině jazykového významu") popsané ve Sgall et al. (2004) a obsahuje asi 0,8 miliónu slov (viz Hajič (2004)). Doufáme, že tato rovina anotace je již natolik blízko významu výpovědí v korpusu obsažených, že umožní automatickou transformaci textů do báze znalostí použitelné pro extrakci informací, zodpovídání dotazů, sumarizaci atd.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Recently, the Prague Dependency Treebank 2.0 (PDT 2.0) has emerged as the largest text corpora annotated on the level of tectogrammatical
representation ("linguistic meaning") described in Sgall et al. (2004) and containing about 0.8 milion words (see Hajic (2004)).
We hope that this level of annotation is so close to the meaning of the utterances contained in the corpora that it should enable us to automatically
transform texts contained in the corpora to the form of knowledge base, usable for information extraction, question answering,
summarization, etc. We can use Multilayered Extended Semantic Networks (MultiNet) described in Helbig (2006) as the target formalism.
In this paper we discuss the suitability of such approach and some of the main issues that will arise in the process.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme systém elektronické cvičebnice české morfologie a syntaxe. Jednotlivá cvičení jsou
vybrána přímo z Pražského závislostního korpusu. Chceme takto seznámit žáky s důležitým akademickým
produktem. Pochopitelně neočekáváme, že studenti si budou procvičovat gramatiku s takovým nadšením,
s jakým surfují po webu, povídají si s kamarády nebo si píší blog. Na druhou stranu věříme, že
elektronická cvičebnice může přinést zábavnější formu procvičování.

Poskytujeme dva typy cvičení: morfologická a syntaktická, tedy cvičení v určování slovních druhů a
jejich morfologických kategorií a v rozboru věty a určování větných členů. Vzhledem k rozdílům mezi
akademickým a školským přístupem nemohou být data Pražského závislostního korpusu použita přímo.
Některé věty musejí být zcela vynechány, na jiné je potřeba aplikovat řadu transformací a tím je
převést z původní reprezentace do podoby, na niž jsou žáci zvyklí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a system designed as an electronic corpus-based exercise book of Czech morphology and
syntax with the exercises directly selected from the Prague Dependency Treebank. In this way we want
to make schoolchildren familiar with such an important academic product. Obviously, we do not
expect that the schoolchildren (will) do grammar practicing so enthusiastically as they surf the
web, chat with friends or write a web log. However, we believe that an electronic exercise book can
make practicing more fun.

Two kinds of exercises are provided, morphological and syntactic, i. e. the exercises give practice
in classifying parts of speech and morphological categories of words and in parsing a sentence and
classifying syntactic functions of words. The Prague Dependency Treebank data cannot be used
directly though, because of the differences between the academic approach and the approach taught in
schools. Some of the sentences have to be completely discarded and several transformations have to
be applied to the others in order to convert the original representation to the form the
schoolchildren are familiar with.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pražský závislostní korpus (PDT) patří mezi nejvýznamnější
jazykové korpusy na světě. Cílem této práce je představit softwarový
systém, který nad daty PDT tvoří cvičebnici českého jazyka. Procvičování
probíhá ve dvou oblastech: tvarosloví (určování slovních druhů a jejich
morfologických kategorií) a větný rozbor (určování větných členů a
závislostí mezi nimi). Vzhledem k odlišnostem mezi akademickými rozbory
vět a rozbory tak, jak jsou vyučovány ve školách, však nelze data
PDT použít zcela přímočaře. Mnoho vět je potřeba z dat úplně vyřadit,
na ostatních je nutné provést množství transformací, které převedou
původní reprezentaci do tvaru, na který jsou žáci zvyklí ze školy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Prague Dependency Treebank (PDT) is one of the top
language corpora in the world. The aim of this work is to introduce
a software system that builds an exercise book of Czech
using the data of PDT. Two kinds of exercises are provided: morphology
(selecting correct parts of speech and their morphological categories)
and sentence parsing (selecting analytical functions and dependencies
between them). The PDT data cannot be used directly though, because
of the differences between the academic approach in sentence parsing
and the approach that is used in schools. Some of the sentences have
to be discarded completely, several transformations have to be applied
to the others in order to convert the original representation to the
form to which the students are used to from school.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pražský závislostní korpus (PDT) patří mezi nejvýznamnější jazykové korpusy na světě. Cílem naší
práce je představit softwarový systém, který nad daty PDT vytvoří elektronickou cvičebnici českého
jazyka. Procvičování probíhá ve dvou oblastech: tvarosloví (určování slovních druhů a jejich
morfologických kategorií) a větný rozbor (určování větných členů a závislostí mezi nimi). Vzhledem k
odlišnostem mezi akademickými rozbory vět a rozbory tak, jak jsou vyučovány ve školách, však nelze
data PDT použít zcela přímočaře. Mnoho vět je potřeba z dat úplně vyřadit, na ostatních je nutné
provést množství transformací, které převedou původní reprezentaci do tvaru, na nějž jsou žáci
zvyklí ze školy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Prague Dependency Treebank (PDT) is one of the most important language corpora in the world. The aim
of our work is to introduce a software system which builds an exercise book of Czech language upon
the PDT data. Two kinds of exercises are provided, morphological (classifying parts of speech and
morphological categories of words) and syntactic (parsing a sentence and classifying syntactic
functions of words). Due to the differences between the academic approach and the school approach
to the parsing of sentences, the PDT data cannot be used directly. Many sentences have to be
discarded completely, several transformations need to be applied to the others in order to convert
them to a form the students are familiar with.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Webové stránky přináší unikátní příležitost vytvořit velkou a kvalitní kolekci textů s malým úsilím. V tomto článku popisujeme nutné podmínky pro vytvoření dobrého korpusu textů a přinášíme program program pro jeho vytvoření z webových stránek. Jako pilotní studii jsme vytvořili slovinský korpus o velikosti přesahující miliardu slov.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The World Wide Web offers a unique possibility to create very large and high quality text collections with low manual work necessary. In this paper, we describe requirements for usable linguistic corpus and we present a routine for building such corpus from the web pages. Several important issues that must be resolved for successful processing are discussed. As a pilot study, we create a billion-token Slovenian corpus based solely on the contents of web pages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve zpracování přirozeného jazyka se při studiu kolokačního jevu používají tzv. asociační míry, které jsou často původně definovány v oborech jako statistika, teorie informace nebo komputační lingvistiky. Zde přestavujeme procesy výroby, evaluace a vizualizace asociačních měr, které se dají prakticky využít. Zároveň navrhujeme způsob, jak míry kombinovat za účelem zlepšení výsledků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In natural language processing task  you can use so called association measures while studying collocation event. Those measures are often defined in branches as statistics, information theory and computational linguistics. Here we are presenting the processes of preparation, evaluation and visualisation of association measures that can be used in practical experiments. Moreover, we are proposing the way how to combine individual measures in order to result improvements.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku je naším cílem optimalizovat implementování Fisherovy lineární diskriminační analýzy (FLDA), která je základní klasifikační metodou pro úlohu s malým vzorkem pozorovaných objektů. Mezi metodami řešícími tento problém představujeme metodu, která je v jistém smyslu nejblíž původnímu klasickému regulárnímu přístupu a jejíž implementace zlepšuje komputační a pamětové nároky a numerickou stabilitu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Here we aim at optimizing the implementation of Fisher’s Linear Discriminant Analysis (FLDA) - based classification for the small sample size problem. Among methods for this problem it considers the one closest, in some sense, to the classical regular approach and improves its implementation with regards to computational and storage costs and numerical stability.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Čeština (jako ostatní slovanské jazyky) je dobře známa svou morfologickou bohatostí. Zpracování textu (např. strojový překlad, syntaktická analýza) obvykle vyžaduje jednoznačný výběr gramatických kategorií (tzv. morfologické značky) pro každé slovo v textu. Morfologické značkování sestává ze dvou částí - přiřazení všech možných značek každému slovu v textu a volbu správné značky v daném kontextu. Projekt Morče řeší druhou část, obvykle zvanou disambiguace.
Byla použita statistická metoda založená na kombinaci skrytého Markovova modelu a průměrovaného perceptronu. Autor provedl mnoho experimentů porovnávajících různá nastavení parametrů algoritmu, aby dosáhl nejlepší možné úspěšnosti.
Výsledná úspěšnost Morčete na datech z PDT 2.0 byla 95,431 % (březen 2006), což je na češtině zatím nejlepší dosažený výsledek samostatného taggeru.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Czech (like other Slavic languages) is well known for its complex morphology. Text processing (e.g., automatic translation, syntactic analysis...) usually requires unambiguous selection of grammatical categories (so called morphological tag) for every word in a text. Morphological tagging consists of two parts – assigning all possible tags to every word in a text and selecting the right tag in a given context. Project Morče attempts to solve the second part, usually called disambiguation. Using a statistical method based on the combination of a Hidden Markov Model and the AveragedAveraged Perceptron algorithm, a number of experiments have been made exploring different parameter settings of the algorithm in order to obtain the best success rate possible. Final accuracy of Morče on data from PDT 2.0 was 95.431% (results of March 2006). So far, it is the best result for a standalone tagger.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce předkládá nástroje pro analýzu na analytické a tektogramatické rovině, které jsou základem Pražského závislostního korpusu.

Nástroje pro analytickou anotaci sestávají ze dvou parserů a nástroje přiřazujícího tzv. analytické funkce. Ačkoli úspěšnost parserů je daleko za úspěšností nejlepších parserů, oba mohou být chápány jako určitý přínos k parsingu, neboť jsou založeny na nových metodách. Nástroj přiřazující analytické funkce dělá o 15 % chyb méně než nástroj, který se k tomuto účelu používal dosud.

Nástroj vyvinutý pro tektogramatickou anotaci je jediný, který tuto úlohu nyní zvládá v takové šíři. Ačkoli jiné, specializované nástroje možná řeší některé její podúlohy lépe, pro češtinu dělá můj nástroj o 29 %, resp. 47 % méně chyb než kombinace existujících nástrojů určujících tektogramatickou strukturu, resp. hloubkové funktory, což je obojí jádrem tektogramatické roviny.

Předkládané nástroje jsou navrženy tak, aby je bylo možno použít i pro jiné jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The thesis presents tools for analysis at analytical and tectogrammatical layers that the Prague Dependency Treebank is based on.

The tools for analytical annotation consist of two parsers and a tool for assigning syntactic tags. Although the performance of the parsers is far below that of the state-of-the-art parsers, they both can be considered a certain contribution to parsing, since the methods they are based on are novel. The tool for assigning syntactic tags makes 15% less errors than a tool used for this purpose previously.

The tool developed for tectogrammatical annotation is the only one that can currently perform this task in such a breadth. Although other, specialized tools may have a better performance of some of its particular subtasks, my tool makes 29% and 47% less errors for the Czech language than the combination of existing tools for annotating the tectogrammatical structure and deep functors, respectively, which are the core of the tectogrammatical layer.

The proposed tools are designed the way they can be used for other languages as well.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nástroj používající trénovací data pro vytvoření svého jazykového modelu přidává anotaci na tektogramatické rovině (tak, jak je definována v Pražském závislostním korpusu) k textům v češtině anotovaným na morfologické a analytické rovině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The tool, using training data for creation of its language model, adds annotation at the tectogrammatical layer (as it is defined in the Prague Dependency Treebank) to Czech text annotated at morphological and analytical layers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje dvě metody závislostní syntaktické analýzy pomocí pravidel automaticky odvozených z trénovacích dat. Úspěšnost takto získaných analyzátorů na češtině je zřetelně pozadu za nejlepšími známými parsery, avšak článek přináší novou metodu odvozování pravidel a novátorským způsobem aplikuje transformační učení na syntaktickou analýzu. Navíc parsery využívají vlastnosti češtiny jen ve velmi omezené míře, takže mohou být snadno nasazeny na jiný jazyk. Článek také představuje metodu přiřazení takzvaných analytických funkcí, která vykazuje lepší výsledky než metody používané dosud.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper introduces two methods of dependency parsing by means of rules automatically inferred from training data. For Czech the accuracy of the proposed parsers is considerably behind that of the state-of-the-art parsers, but the paper brings a new method of inference of rules and applies transformation-based learning to parsing in a novel way. Moreover, the parsers use properties of Czech language in a very limited way and thus can easily be used for parsing another language. The paper also introduces a method of assigning so called analytical functions
that outperforms that currently used.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Existuje několik nástrojů, které podporují ruční anotaci dat na tektogramatické rovině, jak ji definuje Pražský závislostní korpus. S pomocí transformačního strojového učení jsme vyvinuli nový nástroj, který zlepšuje dosavadní výsledky preanotace tektogramatických struktur o 29 % (měřeno jako relativní snížení chybovosti) a hloubkových funktorů (tj. sémantických funkcí) o 47 %.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>There are several tools that support manual annotation of data at the Tectogrammatical Layer as it is defined in the Prague Dependency Treebank. Using transformation-based learning, we have developed a tool which outperforms the combination of existing tools for pre-annotation of the tectogrammatical structure by 29%
(measured as a relative error reduction) and for the deep functor (i.e., the semantic function) by 47%. Moreover, using machine-learning technique makes our tool almost independent of the language being processed. This paper gives details
of the algorithm and the tool.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje situaci polské národnostní menšiny na Těšínsku, historický vývoj dialektu a jeho současný stav.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes the language situation of the Polish minority in Teshen Silesia, its history and development up to the present day.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek navrhuje míru pro měření podobnosti jazyků založenou na strukturní podobnosti povrchových závislostních stromů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper outlines a measure of language similarity based on structural similarity of surface syntactic dependency trees. Unlike the more tradi- tional string-based measures, this measure tries to reflect deeper correspondences among languages. The development of this measure has been inspired by the experience from MT of syntactically similar languages. This experience shows that the lexical similarity is less important than syntactic similarity. This claim is supported by a number of examples illustrating the problems which may arise when a measure of language similarity relies too much on a simple similarity of texts in different languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje architekturu systému strojového překladu pro slovanské jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes an architecture of a machine translation system designed primarily for Slavic languages. The architecture is based upon a shallow transfer module and a stochastic ranker. The shallow transfer module helps to resolve the problems, which arise even in the translation of related languages, the stochastic ranker then chooses the best translation out of a set provided by a shallow transfer. The results of the evaluation support the claim that both modules newly introduced into the system result in an improvement of the translation quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek zavádí pojem (větného) segmentu, jednotky, která je lingvisticky motivovaná a přitom snadno automaticky rozpoznatelná.
Rozpoznání segmentů umožňuje určovat segmentační strukturu věty (reprezentovanou segmentačním schématem), na jejímž základě lze
vymezit jednotlivé klauze souvětí a jejich vzájemný vztah, a tím i syntaktickou strukturu souvětí. Metoda segmentace je navržena
pro automatické zpracování češtiny, jazyka s relativně velmi volným slovosledem.

V příspěvku je dále popsána sada jednoduchých pravidel, která je využita pro budování segmentačních schémat. Výsledky segmentace
jsou  vyhodnoceny  vzhledem k malému ručně anotovanému korpusu českých vět.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>On segmentation of Czech sentences. The paper introduces a concept of segments, linguistically motivated and easily detectable language units. These segments may be subsequently combined into clauses and thus provide a structure of a complex sentence with regard to the mutual relationship of individual clauses. The method has been developed for Czech as a language representing languages with relatively high degree of word-order freedom. 

The paper introduces important terms and describes a segmentation chart. It also contains a simple set of rules applied for the
segmentation of a small set of Czech sentences. The segmentation results are evaluated against
a small hand-annotated corpus of Czech complex sentences.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje metodu zpracování souvětí založenou na identifikaci segmentů.
Metoda byla vyvinuta pro český jazyk, který je charakteristický volností slovosledu. Příspěvek zavádí důležité pojmy a popisuje
datovou struktury používanou pro popis vzájemného vztahu mezi jednotlivými segmenty. Obsahuje také základní soubor pravidel pro
segmentaci a vyhodnocení na malém souboru českých vět.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes a method of dividing complex sentences into segments, easily detectable and
linguistically motivated units that may be subsequently combined into clauses and thus provide
a structure of a complex sentence with regard to the mutual relationship of individual clauses.
The method has been developed for Czech as a language representing languages with relatively
high degree of word-order freedom. The paper introduces important terms, describes a
segmentation chart, the data structure used for the description of mutual relationship between
individual segments and separators. It also contains a simple set of rules applied for the
segmentation of a small set of Czech sentences. The segmentation results are evaluated against
a small hand-annotated corpus of Czech complex sentences.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce se zabývá desambiguací významu substantiv a adjektiv.
 Významy rozlišuje dvojím způsobem, na základě synsetů z Českého WordNetu
 a podle valenčních rámců zachycených v
PDT-VALLEXU.
 Tím vznikají dvě oddělené úlohy, spočívající v přiřazování synsetů/rámců
 víceznačným lemmatům.
 Řešíme je metodou strojového učení, konkrétně využíváme rozhodovací stromy.
 K disposici máme korpus PDT s bohatou anotací na více jazykových rovinách
 obsahující i valenční rámce a dále část PDT s ruční anotací pomocí ČWN.
 V první části práce se zabýváme spojením obou korpusů, neboť jsou v různých
 formátech.
 Ve druhé části s použitím syntaktických závislostí věty a morfologických
 informací přiřazujeme synsety z ČWN.
 Dosahujeme úspěšnosti 91,4% (proti 90,0% baseline) pro substantiva a 93,9%
 (proti 93,2%) pro adjektiva.
 To představuje opravení 13,7% (resp.
 10,0%) chyb vůči baseline.
 Krátce tuto metodu aplikujeme také na t-rovinu, kde je však patrný nedostatek
 dat.
 Ve třetí části přiřazujeme valenční rámce substantivům na základě informací
 z hloubkové roviny významu.
 Dosahujeme úspěšnosti 86,9% proti 84,9% baseline.
 Na závěr porovnáváme oba použité slovníky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This thesis deals with word sense disambiguation of nouns and
 adjectives.
 We make use of the two approaches to distinguish word senses: the first
 method is derived from synsets from the Czech WordNet, the second one uses
 valency frames from the PDT-VALLEX dictionary.
 This means we have the two separate tasks, the goal of which is to assign
 appropriate senses (synsets/frames) to ambiguous lemmas.
 We are employing machine learning methods, the most notable of which are
 the decision trees.
 For this purpose we are using two types of data, namely the PDT corpus
 with rich annotations on several language layers, and also the part of
 the PDT manually annotated using the CWN.
 In the first part we are trying to merge both corpora, since they are stored
 in the different formats.
 In the second part we are using syntactic and morphologic features to assign
 CWN synsets.
 We reach 91,4% accuracy (compared to 90,0% baseline) for nouns and 93,9%
 (compared to 93,2%) for adjectives.
 In other words this means that we can correct 13,7% (10,0% respectively)
 of errors made by the baseline.
 This method is applied on the t-layer, where lack of data is obvious.
 In the third part we are assigning valency frames to nouns using information
 from deep syntactic layer.
 The accuracy gained by us has grown to 86,9% compared to 84,9% baseline.
 At the very end, we compare the both dictionaries obtained.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje zkušenost s lexikálně-sémantickou anotací Pražského závislostního korpusu (Prague Dependency Treebank, PDT). Jako slovník jsme používali Český WordNet (ČWN) a anotovali jsme každé slovo vyskytující se v ČWN. Na základě chybové analýzy jsme anotační slovník upravili a následně přeanotovali vybraná lemmata. Předkládáme výsledky anotací a zlepšení dosažená díky opravám.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents our experience with the lexico-semantic annotation of
the Prague Dependency Treebank (PDT). We have used the Czech WordNet (CWN)
as an annotation lexicon (repository of lexical meanings) and we annotate each word
which is included in the CWN. Based on the error analysis we have performed some
experiments with modiﬁcation of the annotation lexicon (CWN) and consequent re-
annotation of occurrences of selected lemmas. We present the results of the annotations
and improvements achieved by our corrections.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přípěvek představuje formální lingvistický systém pro anotaci zájmenných slov, který byl vyvinut a implementován v rámci tektogramatické roviny Pražského závislostního korpusu (PDT 2.0). Z hlediska vytváření anotovaných korpusů a jejich užívání se ukazuje jako užitečné, pokud jsou pravidelnosti, které lze uvnitř určité podskupiny zájmenných slov sledovat, explicitně popsány. Při takovém postupu jsou významové rysy, které jednotlivá zájmenná slova sdílejí, z těchto slov extrahovány a zachyceny hodnotou atributů. V příspěvku je sada atributů navržených pro tyto účely podrobně popsána.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present a formal linguistic system for the annotation of proforms which has been developed and implemented in the framework of the tectogrammatical layer of the Prague Dependency Treebank 2.0. The main motivation of our approach is the following: if there is a semantically relevant regularity within a certain subset of pro-forms, then it is more useful – at least from the viewpoint of treebank users interested in natural language semantics, in conversions into logical forms etc. – if such information is available in the treebank in an explicit, machine-tractable form. In this case, the semantic features originally present in the word form (given its context) are extracted and stored as values of inner parameters of tectogrammatical nodes corresponding to the given word form.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Publikace Miroslava Červenky mají základní význam pro teorii verše a pro literarni vědu obecně.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Miroslav Červenka’s publications are of great importance for the theory of verse and for Czech literary science in general.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje efektivní postup využívající strukturální podobnosti a znovuvyužitelných manuálních překladů pro překlad rozsáhlých ontologií.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a process for leveraging structural relationships and reusable
phrases when translating large-scale ontologies. Digital libraries are becoming more and
more prevalent. An important step in providing universal access to such material is to provide
multi-lingual access to the underlying principles of organization via ontologies,
thesauri, and controlled vocabularies. Machine translation of these resources requires high
accuracy and a deep vocabulary. Human input is often required, but full manual translation
can be slow and expensive. We report on a cost-effective approach to ontology translation.
We describe our technique of prioritization, our process of collecting aligned translations
and generating a new lexicon, and the resulting improvement to translation system output.
Our preliminary evaluation indicates that this technique provides significant cost savings
for human-assisted translation. The process we developed can be applied to ontologies in
other domains and is easily incorporated into other translation systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje efektivní postup využívající manuálních překladů pro konstrukci doménově specifických lexikálních zdrojů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Thesauri and ontologies provide important
value in facilitating access to digital
archives by representing underlying principles
of organization. Translation of
such resources into multiple languages is
an important component for providing
multilingual access. However, the specificity
of vocabulary terms in most ontologies
precludes fully-automated machine
translation using general-domain
lexical resources. In this paper, we present
an efficient process for leveraging
human translations when constructing
domain-specific lexical resources. We
evaluate the effectiveness of this process
by producing a probabilistic phrase dictionary
and translating a thesaurus of
56,000 concepts used to catalogue a large
archive of oral histories. Our experiments
demonstrate a cost-effective technique
for accurate machine translation of
large ontologies.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Závěrečná zpráva z workshopu JHU. Popisuje implementaci a experimenty s frázovým systémem Moses, který podporuje dodatečnou anotaci vstupu i výstupu pomocí tzv. faktorů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The final report of JHU workshop. The report describes the implementation and experiments with a phrase-based MT system Moses.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku je popsán nový pravidlový přístup k úloze generování přirozeného textu ze vstupní reprezentace v podobě tektogramatických stromů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we deal with a new rule-based approach to the~Natural
Language Generation problem. The presented system synthesizes Czech
sentences from Czech tectogrammatical trees supplied by the Prague
Dependency Treebank~2.0 (PDT~2.0). Linguistically relevant phenomena including
valency, diathesis, condensation, agreement, word order, punctuation
and vocalization have been studied and implemented in Perl using
software tools shipped with PDT~2.0. BLEU score metric is used for the
evaluation of the generated sentences.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje různé metody a nástroje používané k poanotačním opravám dat PDT 2.0. Anotační proces treebanku byl komplikován několika faktory, například rozdělením dat na několik rovin, které si odpovídají. Pravidla pro anotaci se navíc během anotačního procesu měnila a vyvíjela. Některé části dat byly navíc anotovány odděleně a paralelně, takže se později musely s daty slévat. Dalším zdrojem chyb byl převod ze starého anotačního formátu do nového, kromě všudypřítomné lidské chybovosti. Kontrolní procedury zajišťující integritu a korektnost dat jsou dále klasifikovány z různých hledisek, např. podle jejich lingvistické relevance a role v kontrolním procesu. V poslední části článku jsou metody porovnány a ohodnoceny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Various methods and tools used for the post-annotation checking of Prague Dependency Treebank 2.0 data are being described in this article. The annotation process of the treebank was complicated by several factors: for example, the corpus was divided into several layers that must reflect each other. Moreover, the annotation rules changed and evolved during the annotation. In addition, some parts of the data were annotated separately and in parallel and had to be merged with the data later. The conversion of the data from an old format to a new one was another source of possible problems besides  omnipresent human inadvertence. The checking procedures used to ensure data integrity and correctness are classified according to several aspects, e.g. their linguistic relevance and their role in the checking process, and prominent examples are given. In the last part of the article, the methods are compared and scored.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje metody a nástroje používané k poanotačním opravám dat PDT 2.0. Anotační proces byl komplikován několika faktory, například rozdělením treebanku na několik rovin, které si musejí odpovídat. Anotační pravidla se v průběhu anotace měnila a vyvíjela, části dat byly anotovány odděleně a paralelně, takže se musely slévat dohromady. Dalším zdrojem problémů byl převod ze starého datového formátu do nového. Kontrolní procedury jsou klasifikovány podle různých faktorů, např. jejich lingvistické relevance či role v kontrolním procesu, a jsou představeny některé příklady. V závěru článku jsou metody číselně porovnány.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes methods and tools used for the post-annotation
checking of Prague Dependency Treebank~2.0 data. The annotation
process was complicated by many factors: for example, the corpus is
divided into several layers that must reflect each other; the
annotation rules changed and evolved during the annotation process;
some parts of the data were annotated separately and in parallel and
had to be merged with the data later. The conversion of the data
from the old format to a new one was another source of possible
problems besides omnipresent human inadvertence. The checking
procedures are classified according to several aspects, e.g. their
linguistic relevance and their role in the checking process, and
prominent examples are given. In the last part of the paper, the
methods are compared and scored.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Práce popisuje nejrůznější metody a nástroje používané při anotaci a poanotačních opravách dat Pražského závislostního korpusu 2.0. Anotační proces byl komplikován několika faktory: například rozdělením treebanku na několik rovin, které si odpovídají. Anotační pravidla se navíc měnila a vyvíjela v průběhu anotačního procesu. Některé části dat byly navíc anotovány odděleně a paralelně, takže se s daty později slévaly. Vedle nevyhnutelné lidské chybovosti byl dalším potenciálním zdrojem problémů převod dat ze starého formátu do nového

V první kapitole je představeno teoretické pozadí PDT. Jsou popsány rozdíly mezi teoretickými předpoklady a jejich realizací v treebanku a různé vlastnosti anotačních schémat a datových formátů.

Druhá kapitola popisuje nástroje používané k anotaci a poanotačním opravám. Zvláštní pozornost je věnována koordinačním a apozičním konstrukcím, porovnává se několik možných přístupů a uvádějí se funkce, které byly k jejich zachycení použity.

Třetí kapitola popisuje poanotační kontroly, které zaručovaly integritu a korektnost dat. Procedury jsou klasifikovány podle různých kritérií a uvedeno je mnoho lingvisticky motivovaných příkladů. V poslední části kapitoly se studuje dopad kontrolních procedur na počet změn v datech, procedury se porovnávají a zkoumá se jejich přínos.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This work describes various methods and tools used during the
annotation and post-annotation checking of Prague Dependency
Treebank 2.0 data. The annotation process of the treebank was
complicated by several factors: for example, the corpus was divided
into several layers that must reflect each other. Moreover, the
annotation rules changed and evolved during the annotation. In
addition, some parts of the data were being annotated separately and
in parallel and had to be merged with the data later. Conversion of
the data from an old format to a new one was another source of
possible problems besides omnipresent human inadvertence.

In the first chapter, the theoretical background of the Prague
Dependency Treebank is given. Differences between theoretical
assumptions and their realization in the treebank are described
together with various aspects of annotation schemata and data format.

The second chapter describes the tools used for annotation and during
the post-annotation checking. Special attention is paid to
coordination- and apposition-like constructions: several possible
approaches are compared and functions for resolving complex structures
(encoded according to the used approach) are presented.

The third chapter expounds the post-annotation checking procedures
used to ensure data integrity and correctness. The procedures are
classified by several criteria and many linguistically inspired
examples are given. In the last part of the chapter, the impact of the
checking procedures is measured by counting the number of changes in
the data, the procedures are compared and their contribution and
usefulness is discussed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V nedávné době jsme představili náš nový datový model prozodie pro systémy převodu textu na řeč. Tento model je založen na parametrizaci textu používající generativní prozodickou gramatiku a na metodách automatické analýzy řečových korpusů. Model byl úspěšně testován a kladně hodnocen při syntéze češtiny. V předkládaném článku jsou prezentovány první výsledky jeho aplikace na syntézu němčiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We have recently introduced our new data-driven prosody model for text-to-speech synthesis. The model is based on text parametrisation using a generative prosodic grammar and on automatic speech corpora analysis methods. The model has been successfully tested for Czech language with highly assessed naturalness of resulting speech. The current stage of the model development involves the first experiments with its application to German synthesis. The results of these experiments are presented in the paper.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Analýza umístění přísudkového slovesa v různých jazykových žánrech, a to v závislosti na aktuálním členění a dalších jazykových jevech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Analysis of the location of the predicate verb in different language genres, depending on the information structure and other linguistic phenomena.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Netypické slovosledné uspořádání kontextově nezapojeného aktora, adresáta a patientu v češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Non-typical word order of contextually non-bound actor, addresee and patient in Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje probíhající práce na česko-anglickém slovníku pro strojový
překlad. Slovník vychází z dostupných strojově čitelných slovníků, kombinací
ručních a automatických metod jsou doplňovány informace nutné pro nasazení
slovníku v automatickém systému. Jedná se zejména o morfologická omezení nutná
pro uplatnění hesel, popis syntaktické struktury hesel a překladové ekvivalenty
slovesných rámců.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes ongoing work on our Czech-English translation dictionary aimed at machine translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme metodu extrakce překladových slovesných rámců (paralelních valenčních rámců) z paralelního závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe a method for extracting translation verb frames (parallel
subcategorization frames) from a parallel dependency treebank. The extracted
frames constitute an important part of machine translation dictionary for a
structural machine translation system. We evaluate our method independently,
using
a manually annotated test dataset, and conclude that the bottleneck of the method
lies in quality of automatic word alignment of the training data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Projekt Enti má za cíl vytvořit nástroj pro navrhování umělých agentů podobných lidem. Hlavní komponenty tohoto nástroje jsou virtuální prostředí pro agenty a programovací jazyk E pro popis chování agentů. Příspěvek popisuje první verzi tohoto nástroje.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Project ENTs aims at providing a universal high-level tool for
prototyping human-like artificial agents. The main features of this tool are a
virtual test-bed environment and E language, which enables description of
human-like agents’ behaviours using various techniques, in particular reactiveThe Project ENTs aims at providing a universal high-level tool for
prototyping human-like artificial agents. The main features of this tool are a
virtual test-bed environment and E language, which enables description of
human-like agents’ behaviours using various techniques, in particular reactive
planning and BDI-architecture. In this paper, we present first generation of this
tool together with an example agent—an artificial gardener that acts in a virtual
family house. We then briefly discuss applicability of this tool and introduce
some requirements for its second generation.
planning and BDI-architecture. In this paper, we present first generation of this
tool together with an example agent—an artificial gardener that acts in a virtual
family house. We then briefly discuss applicability of this tool and introduce
some requirements for its second generation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Souhrn experimentu s ruční anotací příkladů sloves hesly slovníku VALLEX pro změření mezianotátorské shody a kontrolu kvality slovníkových hesel.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An experiment with manual annotation of verb examples using entries in the lexicon VALLEX aimed at an estimate of inter-annotator agreement and the quality of VALLEX entries.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popis pokusu o recyklaci částí systému strojového překladu z češtiny do ruštiny v nového systému do angličtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes an attempt to recycle parts of the Czech-to-Russian machine translation system (MT) in the new Czech-to-English MT system. The paper describes the overall architecture of the new system and the details of the modules which have been added. A special attention is paid to the problem of named entity recognition and to the method of automatic acquisition of lexico-syntactic information for the bilingual dictionary of the system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje pokus o recyklaci systému strojového překladu z češtiny do ruštiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes an attempt to recycle parts of the Czech-to-Russian machine translation system (MT) in the new Czech-to-English MT system. The paper describes the overall architecture of the new system and the details of the modules which have been added. A special attention is paid to the problem of named entity recognition and to the method of automatic acquisition of lexico-syntactic information for the bilingual dictionary of the system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popis pokusů o revitalizaci staršího systému strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A summary of attempts at revitalizing an older implementation of an MT system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Oblast morfologického zjednoznačňování arabštiny zaznamenala v poslední době výrazné úspěchy. Díky nim se Penn Arabic Treebank etabluje jako standard pro vývoj a vyhodnocování systémů pro automatické zpracování arabské morfologie a Buckwalterův arabský morfologický analyzátor se stává nejrespektovanějším lexikálním zdrojem svého druhu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The field of morphological disambiguation of Arabic has recently witnessed significant achievements. Through them, the Penn Arabic Treebank is being confirmed as a standard for development and evaluation of systems for automatic morphological processing of Arabic, and the Buckwalter Arabic Morphological Analyzer is becoming the most respected lexical resource of its kind.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento software ke stažení poskytuje první dostupnou kontrolu české gramatiky pro sadu Microsoft Office 2003. Kontrola české gramatiky analyzuje text a zeleně podtrhává úseky, které nemusejí být gramaticky správně.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This downloadable software provides the first publicly available grammar checker for Czech for the  Microsoft Office 2003. The grammar checker analyzes the text and underlines in green the strings, which may not be grammatically correct.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje metodu klasifikace a rozpoznávání morfologických kolokací. Problémy jsou popsány pomocí příkladů v litevštině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes a method of classification and recognition of morphological collocations. The problems are explained by means of Lithuanian examples.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve stati se porovnávají dva přístupy k popisu valence: Přístup autorek Valenčního slovníku slovenských sloves je charakterizován jako více lexikologicky a sémanticky orientovaný, zatímco přístup Funkčního generativního popisu je pokládán za syntakticky založený. Hledání hranic a operačních kritérií je nezbytné pro oba přístupy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Two approaches to the description of verbal valency are compared. The former (the approach of the authors of the Valency dictionary of Slovac verbs) is considered as more lexicologically and semantically based, the latter (valency theory in the Functional Generative Description)  is considered as more syntactically based. The necessity of providing  clear boundaries between the phenomena constituting the sentence nucleus and the other phenomena belonging to the sentence periphery s is stressed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Děti používají  počítač ke psaní, kreslení, hraní her, listování v encyklopediích, skládání hudby - proč by jej nemohli používat k procvičování větných rozborů a určování rodu, čísla, pádu, ...?

Představujeme systém STYX, který je navržen jako elektronická cvičebnice českého tvarosloví a české syntaxe. Příklady jsou vybrány z Pražského závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The schoolchildren can use a computer to write, to draw, to play games, to page encyclopedia, to 
compose music - why they could not use it to parse a sentence, to determine gender, number, case, …? 

We are presenting a system "Styx" that is designed 
to be an exercise book of Czech morphology and syntax with the exercises directly selected from 
PDT.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje Prague Czech-English Dependency Treebank (PCEDT), nový zdroj dat pro experimenty se strojovým překladem využívajícím větnou strukturu. Popisujeme popis tvorby základních částí -- paralelního syntakticky anotovaného korpusu a překladových slovníků. Část Penn Treebanku byla přeložena do češtiny, anotace české části byla vytvořena utomatickými nástroji, anotace anglické části byla automaticky převedena z formalismu Penn Treebanku do formalismu Pražského závislostního korpusu. Podmnožina paralelních dat byla anotována ručně. První experimenty s česko-anglickým strojovým  překladem proběhly. Tento datový zdoj byl vytvořen na Karlově Universitě v Praze a jeho vydání v Linguistic Data Consortium je plánováno na rok 2004.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper introduces the Prague Czech-English Dependency Treebank
(PCEDT), a new Czech-English parallel resource suitable for
experiments in structural machine translation. We describe the process
of building the core parts of the resources -- a bilingual
syntactically annotated corpus and translation dictionaries. A part of
the Penn Treebank has been translated into Czech, the dependency
annotation of the Czech translation has been done automatically from
plain text. The annotation of Penn Treebank has been tranformed into
dependency annotation scheme. A subset of corresponding Czech and
English sentences has been annotated by humans. First experiments in
Czech-English machine translation using these data have already been
carried out. The resources being created at Charles University in
Prague are scheduled for release as Linguistic Data Consortium data
collection in 2004.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V práci se zabýváme projektivitou a algoritmy pro projektivizaci a nalezení neprojektivních hran v úplně uspořádaných kořenových stromech (takové stromy se používají v syntaktické analýze přirozeného jazyka, kde se pro ně užívá název závislostní stromy). Definujeme kanonickou projektivizaci úplně uspořádaného kořenového stromu (zachovávající stromovou strukturu a pro všechny vnitřní vrcholy relativní uspořádání těchto vrcholů a jejich dětí) a dokazujeme její jednoznačnost; uvádíme také zobecnění tohoto výsledku. Dále se zabýváme vlastnostmi neprojektivních hran, které jsou podstatné pro nově představené algoritmy: První algoritmus pro vstupní strom vrací jeho projektivizaci, druhý algoritmus ve vstupním stromu nalezne neprojektivní hrany dvou ze tří zavedených typů (ukazujeme také, jak je možné výstup algoritmu využít k nalezení všech neprojektivních hran). Dokazujeme, že algoritmy jsou optimální: jejich časová složitost je O(n).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper discusses the notion of projectivity and algorithms for projectivizing and detecting non-projective edges in totally ordered rooted trees (such trees are used in dependency syntax analysis of natural language, where they are called dependency trees). We define the canonical projectivization of a totally ordered rooted tree (preserving the tree structure and the relative ordering for all inner nodes and their immediate dependents) and show its uniqueness; we also give a generalization of this result. We then discuss some properties of non-projective edges relevant for the newly presented algorithms:The first algorithm computes the projectivization of the input tree, the second algorithm detects non-projective edges of certain types in the input tree (we also give a hint on finding all non-projective edges using its output). Both algorithms can be used for checking projectivity. We prove that the algorithms are optimal: they have time complexities O(n).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V práci se zabýváme projektivitou a algoritmy pro projektivizaci a nalezení neprojektivních hran v úplně uspořádaných kořenových stromech (takové stromy se používají v syntaktické analýze přirozeného jazyka, kde se pro ně užívá název závislostní stromy). Definujeme kanonickou projektivizaci úplně uspořádaného kořenového stromu (zachovávající stromovou strukturu a pro všechny vnitřní vrcholy relativní uspořádání těchto vrcholů a jejich dětí) a dokazujeme její jednoznačnost; uvádíme také zobecnění tohoto výsledku. Dále se zabýváme vlastnostmi neprojektivních hran, které jsou podstatné pro nově představené algoritmy: První algoritmus pro vstupní strom vrací jeho projektivizaci, druhý algoritmus ve vstupním stromu nalezne neprojektivní hrany dvou ze tří zavedených typů (ukazujeme také, jak je možné výstup algoritmu využít k nalezení všech neprojektivních hran). Dokazujeme, že algoritmy jsou optimální: jejich časová složitost je O(n).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper discusses the notion of projectivity and algorithms for projectivizing and detecting non-projective edges in totally ordered rooted trees (such trees are used in dependency syntax analysis of natural language, where they are called dependency trees). We define the canonical projectivization of a totally ordered rooted tree (preserving the tree structure and the relative ordering for all inner nodes and their immediate dependents) and show its uniqueness; we also give a generalization of this result. We then discuss some properties of non-projective edges relevant for the newly presented algorithms:The first algorithm computes the projectivization of the input tree, the second algorithm detects non-projective edges of certain types in the input tree (we also give a hint on finding all non-projective edges using its output). Both algorithms can be used for checking projectivity. We prove that the algorithms are optimal: they have time complexities O(n).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku se hodnotí vývoj strojového překladu od jeho začátků přes novější systémy až po současný stav, v němž převažují metody statistické založené na paralelních korpusech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The development of machine translation systems is discussed. The rule-based systems of machine translation used since 60-ies last century and the statisticaly based systems used nowadays are compared. The existence of paralel corpora is a useful tool for the developmnet of such systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pražský česko-anglický závislostní korpus (Prague Czech-English Dependency Treebank, PCEDT) je syntakticky anotovaný česko-anglický paralelní korpus. Jedná se o Penn Treebank, který byl přeložen do češtiny a jeho anotace automaticky transformována do závislostní anotace. Závislostní anotace češtiny byla provedena plně automaticky. Malá množina paralelních vět byla anotována ručně. Na tomto korpusu byly provedeny první pokusy se strojovým překladem z češtiny do angličtiny. Jazykové zdroje byly vytvořeny na Univerzitě Karlově v Praze a publikovány prostřednictvím  Linguistic Data Consortium v roce 2004.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Prague Czech-English Dependency Treebank (PCEDT) is a syntactically annotated Czech-English parallel corpus. The Penn Treebank has been translated to Czech, and its annotation automatically transformed into dependency annotation scheme. The dependency annotation of Czech is done from plain text by automatic procedures. A small subset of corresponding Czech and English sentences has been annotated by humans. First experiments in Czech-English machine translation using these data have already been carried out. The resources have been created at Charles University in Prague and released by Linguistic Data Consortium in 2004.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Funční generativní popis (FGP) je stratifikační závislostní přístup k popisu přirozeného jazyka. V tomto článku uvádíme některé jeho vlastnosti společné s teorií Smysl-Text.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Functional Generative Description (FGD) is a stratificational dependency-based approach to natural language description, which has been developed by Petr Sgall and his collaborators in Prague since 1960's. Although FGD bears surprisingly many resemblances with the Meaning-Text Theory, to our knowledge there is no reasonably detailed comparative study available so far. In this paper, we try to point out at least the basic obvious parallels (and also differences),
which - in our opinion - remain generally unknown.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V práci je představen valenční slovník českých sloves VALLEX. V první části je uveden přehled literatury související s valenčními slovníky. V druhé části je podrobně popsána struktura navrženého slovníku, softwarové nástroje pro jeho vytváření a jeho základní kvantitativní vlastnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Valency is a property of language units reflecting their combinatorial potential in language utterances. The availability of the information about valency is supposed to be crucial in various Natural Language Processing tasks. In general, valency of language units cannot be automatically predicted, and therefore it has to be stored in a lexicon. The primary goal of the presented work is to create a both human- and machine-readable lexicon capturing valency of the most frequent Czech verbs. For this purpose, valency theory developed within Functional Generative Description (FGD) is used as the theoretical framework. The thesis consists of three major parts. The first part contains a survey of literature and language resources related to valency in Czech and other languages. Basic properties of as many as eighteen different language resources are mentioned in this part. In the second part, we gather the dispersed linguistic knowledge necessary for building valency lexicons. We demonstrate that if manifestations of valency are to be studied in detail, it is necessary to distinguish two levels of valency. We introduce a new terminology for describing such manifestations in dependency trees; special attention is paid to coordination structures. We also preliminarily propose the alternation-based lexicon model, which is novel in the context of FGD and the main goal of which is to reduce the lexicon redundancy. The third part of the thesis deals with the newly created valency lexicon of the most frequent Czech verbs. The lexicon is called VALLEX and its latest version contains around 1600 verb lexemes (corresponding to roughly 1800 morphological
lemmas); valency frames of around 4400 lexical units (corresponding to the individual senses of the lexemes) are stored in the lexicon. The main software components of the dictionary production system developed for VALLEX are outlined, and selected quantitative properties of the current version of the lexicon are discussed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme, jak přistupujeme k valenci v rámci anotování PZK. Zaměřujeme se zejména na valenci sloves a na specifické problémy spojené s touto problematikou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a description of how we dealt with the valency of verbs during the annotation of the PDT and the way the verbal part of the valency dictionary was built. We focus on some specific problems related to verbal valency (as well as some other verbal complementations) from the point of view of the PDT.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Činnost Pražského lingvistického kroužku byla zastavena v komunistickém období, ale v r. 1989-90 byla plně obnovena.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The activity of the Prague Linguistic Circle was stopped in the Communist years, but it was possible to resume it in 1989-90.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jubilea se dožívá Eva Hajičová, vynikající lingvistka, které česká věda vděčí za podstatný přínos.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Eva Hajičová has achieved an anniversary after bringing a major contribution to the Czech linguistics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Autoři popisují jazykovou situaci ve velké části západní Afriky a její vývoj.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The authors describe the language situation of and development in a large part of West Africa.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dizertace je zaměřena na problematiku modelování dokumentů, podobnosti dokumentů, témat dokumentů a souvisejících věcí. Nejprve analyzujeme, jaký druh informací může být z dokumentů extrahován s použitím automatických method lingvistické analýzy. Dále je dizertace zaměřena na explicitní identifikaci témat textů v modelech dokumentů. Specifická témata se hledají jako podgrafy v grafy celé kolekce textových dokumentů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the thesis we focus mainly on the area of document models, document
similarity and topics, and related things.
We first analyze what information can be extracted from natural
language texts using automatic linguistic processing. Then we try to
directly and explicitly capture the topics of the texts in the
document models for this information seems to be of critical
importance. We view a text collection as a graph and try to discover
topics in the text collection as a specific subgraphs.
We also developed some model data that capture the human understanding
about document similarity and topics. The data is used for tests and
for learning parameters.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kniha pokrývá nové technologie z oblasti zpracování přirozeného jazyka. Zaměřuje se na vyhledánání informací, automatickou extrakci informací z textu a klasifikaci textů podle tématu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This book covers the emerging technologies of document retrieval, information extraction, and text categorization in a way which highlights commonalities in terms of both general principles and practical issues.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Recenze monografie, v níž autor prezentuje svůj návrh na reprezentaci
informatické struktury věty na základě triadických struktur.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A review of a monograph the author of which presents its proposalw to
represent the information structure of the sentence based on triadic
structures.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přínos anotovaných jazykových korpusů pro lingvistické studium aktuálního členění.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Annotated corpora are discussed from the viewpoint of their contribution to linguistic theories.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Proces překladu je nahlížen z hlediska porozumění; je navržen možný přístup k reprezentaci významu textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The process of translation is viewed from the point of view of understanding and a representation of meaning is argued for.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozbor jazykových problémů, které je třeba řešit pro automatické porozumění textů v přirozeném jazyce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An analysis of several linguistic issues that must be taken into account in any automatic natural language understanding system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vztah mezi mluveným a psaných jazykem z hlediska jazykové víceznačnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Language is characterized by several types of asymmetries, one of the most important is the case of homonymy, which can be found both in spoken and in written languge.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Počítačové ověření algoritmu pro určení základu a ohniska výpovědi na
základě primárniho příznaku kontextoveho zapojeni.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A computational implementation of the algorithm for determination of topic
and focus on the basis of the primary marker of contextual boundness.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Postavení aktuálního členění věty v popisu hloubkové (podkladové)struktury jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The status of the representation of topic/focu articulation in the overall
underlying representation of the structure of the sentence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Technická zpráva popisuje design nového datového formátu založeného na XML, který má zajistit lepší  interoperabilitu a nahradit rozličné starší datové formáty používané v různých oblastech            korpusové lingvistiky se strukturovanou anotací.
Zpráva představuje první verzi formátu pod názvem Prague Markup Language (PML) a to včetně aktuální verze technické specifikace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the first part of this technical report we describe our approach to design
        a new data format, based on XML (Extensible Markup Language)
            and aimed to provide a better and unifying alternative to
            various legacy data formats used in various areas of
            corpus linguistics and specifically in the field of
            structured annotation.
            We introduce the first version of the format, called Prague
            Markup Language (PML). This version has already been employed as
            the main data format for the upcoming Prague Dependency
            Treebank 2.0 (PDT).
Finally we outline our ideas and proposals for further improvement of
            PML, based on our current experience with using and
            processing data in PML format in the PDT 2.0 project.
       
            The second part of the technical report contains the state-of-the-art
            specification of PML.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Leonard Talmy shrnuje jeho práci v oblasti sémantiky (kterou ztotožňuje s kognitivní lingvistika) v posledních 30 let. Ačkoli se to může zdát zbytečné, když používá atribut 'kognitivní' v souvislosti s 'sémantiky' zdůraznit zájem o pojmové struktury, neboť se tvoří v lidském myšlení a také jeho fenomenologický přístup. Ke knize Kognitivní sémantika se skládá ze dvou svazků. Objemu v rámci přezkumu myšlenku obecného pojmu-struktury systémů v jazyce a jejich vztahy k jiným systémům vnímání je předložit, načež analyzuje tři takové systémy jsou poskytovány. Objem je rozdělen do čtyř částí: Základy Koncepční Strukturování v jazyce, Configurational struktura, pozornost, síly a jejich příčin.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Leonard Talmy sums up his work in the field of semantics (which he identifies with cognitive linguistics) in the last 30 years. Though it may seem redundant, he uses the attribute 'cognitive' in connection with 'semantics' to emphasize the concern for the conceptual structures as they are formed in a person's mind and also his phenomenological approach. The book Toward a Cognitive Semantics consists of two volumes. In the volume under review the idea of general concept-structuring systems in language and of their relationships to other perception systems is put forward, whereupon the analyzes of three such systems are provided. The volume is divided into four parts: Foundations of Conceptual Structuring in Language, Configurational Structure, Attention, Force and Causation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Empirická studie popisující současný stav výzkumu metod pro extrakci kolokací.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a status quo of an
ongoing research study of collocations –
an essential linguistic phenomenon having
a wide spectrum of applications in
the field of natural language processing.
The core of the work is an empirical evaluation
of a comprehensive list of automatic
collocation extraction methods using
precision-recall measures and a proposal
of a new approach integrating multiple
basic methods and statistical classification.
We demonstrate that combining
multiple independent techniques leads to
a significant performance improvement in
comparisonwith individual basic methods.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Hlavní problém jazykového modelování v češtině je enormní velikost slovníku způsobená velkým množstvím různých slovních tvarů odvozených z jednoho slova (lemmatu).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Main problem of language modeling encountered in Czech is enormous vocabulary growth caused by great number of different word forms derived from one word (lemma).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje vytváření LVCSR systému pro rozpoznávání spontánní promluvy českých, ruských a slovenských svědků holokaustu v projektu MALACH. Ruční transkripce byly použity jako hlavní zdroj pro jazykové modelování a normalizovány s použitím standardního slovníku, což přineslo 2-3% snížení chybovosti. Následná interpolace s tématicky podobnými větami automaticky vybranými z obecného korpusu přinesla další zlepšení o 3%.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the building of LVCSR systems for recognition of spontaneous speech of Czech, Russian, and Slovak witnesses of the Holocaust in the MALACH project. Manual transcripts as one of the main sources for language modeling were automatically „normalized” using standardized lexicon, which brought about 2 to 3% reduction of the word error rate. The subsequent interpolation of such LMs with models built from an additional collection (consisting of topically selected sentences from general text corpora) resulted into an additional improvement of performance of up to 3%.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Disertace se zabývá problematikou valence deverbativních substantiv v češtině. Úvodní oddíly jsou věnovány přehledu zahraničních i českých přístupů k otázkám valence substantiv a shrnutí dosavadních poznatků týkajících se některých vybraných problémů v oblasti valence českých deverbativních substantiv (zejména vztahu mezi povrchovým vyjádřením valenčních doplnění deverbativních substantiv a formou odpovídajících valenčních doplnění u příslušného základového slovesa). Vlastní výsledky zkoumání jsou představeny jednak v podobě detailních studií věnovaných syntakticky nebo sémanticky uceleným skupinám deverbativních substantiv (zejména substantiv s dativní valencí), jednak v podobě teoretických závěrů vyplývajících z prozkoumaného jazykového materiálu. Jazykový materiál byl čerpán ze dvou českých elektronických korpusů, Českého národního korpusu a Pražského závislostního korpusu. Jako teoretický rámec pro popis valenčních vlastností zkoumaných substantiv byl zvolen funkční generativní popis.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present Doctoral Thesis deals with valency properties of deverbal nouns in Czech. After an overview of Czech and foreign approaches to the valency of nouns and a summary of current knowledge concerning some special issues of valency of Czech deverbal nouns, we present results consisting of our analysis of syntactically and semantically compact groups of deverbal nouns as well as theoretical conclusions following from the examined language material. We have focused our attention on Czech deverbal nouns that can be modified by a participant expressed by prepositionless dative. Such nouns were searched for in two Czech electronic corpora, Czech National Corpus (CNC) and Prague Dependency Treebank (PDT). The obtained occurrences were manually sorted and analysed and their valency behaviour is described within the theoretical framework of the Functional Generative Description (FGD).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Sborník anotací vyšel už při konferenci v roce 2005. Kniha s úplnými články vyšla až o 2 roky později: GRAMATIKA A KORPUS, GRAMAR &amp; CORPORA 2005, editoři František Štícha a Josef Šimandl, ISBN 80-86496-32-5, 304 stran, 31 textů, vydavatel Ústav pro jazyk český.

Tento příspěvek se zaměřuje na porovnání valenčního chování dvou skupin deverbativních substantiv odvozených od sloves s dativní valencí, a to substantiv dávání a substantiv mluvení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The book of abstracts was published at the conference in 2005. A book with full papers was published 2 years later: GRAMATIKA A KORPUS, GRAMAR &amp; CORPORA 2005, editors František Štícha and Josef Šimandl, ISBN 80-86496-32-5, 304 pages, 31 texts, publisher Ústav pro jazyk český (Institute of the Czech Language).

Studying valency properties of nouns derived from verbs that can also be modified by a valency complementation expressed by prepositionless dative, we focused on two distinct semantic classes, i.e. nouns of giving and nouns of saying.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Učení založené na paměti (memory-based learning) patří k tzv. líným (lazy) metodám generalizace. Trénovací data jsou pouze zapamatována a během testování se mezi nimi hledají co nejpodobnější příklady. Tuto metodu používáme k disambiguaci morfologického značkování češtiny a ukazujeme první výsledky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The memory-based learning belongs to so-called lazy methods of generalization. The training data are only stored in the memory and during the testing, the most similar examples are retrieved. We use this method for the morphological disambiguation of Czech and show the first results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pražský závislostní korpus patří k nejvyspělejším jazykově anotovaným korpusům světa. Je anotován na třech rovinách jazykové abstrakce - morfologické, analytické a tektogramatické. K jeho prohledávání vytváříme uživatelsky přítulný nástroj Netgraph, který nabízí intuitivní ovládání a především intuitivní dotazovací jazyk, spolu s přehledným zobrazením výsledku dotazů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Prague Dependency Treebank belongs to the most advanced linguistically annotated corpora in the world. It is annotated on three layers of abstraction: morphological, analytical and tectogrammatical. For searching in the corpus, we develop a user-friendly tool called Netgraph. It offers an intuitive interface and most of all, an intuitive query language, along with a comprehensive depicting of the results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Syntaktický analyzátor morfologicky označkovaného českého textu v CSTS formátu Pražského závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Syntactic parser of morphologically tagged Czech text in the CSTS format of the Prague Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento dokument zkoumá možnosti zlepšení výsledků analýzy kombinací výstupů několika parsery. Pro některé ex-stanu, jsme přenášeli myšlenky Hender-syn a Brill (1999), do světa závislosti struktur. Lišíme se od nich v kontextu zkoumání vlastností hlouběji. Všechny naše experimenty byly kon-odváděny na českém, ale metoda je lan-guage-nezávislé. Byli jsme dokázali výrazně zlepšit po pars-ing nejlepší výsledek pro dané nastavení, známé doposud. Navíc naše experimenty ukazují, že i parsery hluboko pod stavem může přispět k celkovému zlepšení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper explores the possibilities of improving parsing results by combining outputs of several parsers. To some ex-tent, we are porting the ideas of Hender-son and Brill (1999) to the world of dependency structures. We differ from them in exploring context features more deeply. All our experiments were con-ducted on Czech but the method is lan-guage-independent. We were able to significantly improve over the best pars-ing result for the given setting, known so far. Moreover, our experiments show that even parsers far below the state of the art can contribute to the total improvement.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dokument popisuje anotace Pražského závislostního korpusu na morfologické rovině. Vedle přehledu morfologických značek přináší také informace o značkách připojených k lemmatům. Na řadě příkladů rozebírá sporné situace, kde není na první pohled jasné, jakou značku zvolit.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The document describes annotation of the Prague Dependency Treebank on the morphological layer. Besides an overview of the morphological tags it also brings information about the tags attached to lemmas. It gives many examples to analyze difficult situations where it is not obvious what tag to use.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozpoznávání souvislé řeči s rozsáhlým slovníkem flexivních jazyků jako čeština, ruština nebo srbochorvatšitna je silně negativně ovlivněno vysokým výskytem neznámých slov. V tomto článku řešíme problém výběru slovníku, jazykového modelování a prořezávání pro flexivní jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Large vocabulary continuous speech
recognition of inflective languages, such
as Czech, Russian or Serbo-Croatian, is
heavily deteriorated by excessive out of
vocabulary rate. In this paper, we tackle
the problem of vocabulary selection, language
modeling and pruning for inflective
languages. We show that by explicit
reduction of out of vocabulary rate we
can achieve significant improvements
in recognition accuracy while almost
preserving the model size. Reported
results are on Czech speech corpora.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Závislostní syntaktickou analýzu (parsing) formalizujeme jako hledání maximální kostry v orientovaném grafu. Při této reprezentaci lze použít Eisnerův (1996) algoritmus pro prohledání všech projektivních stromů v čase O(n3). Překvapivě lze tuto reprezentaci přirozeně rozšířit na neprojektivní analýzu pomocí algoritmu Chu-Liu-Edmonds (1965, 1967). Výsledný algoritmus má složitost O(n2).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n3) time. More surprisingly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (1965, 1967) MST algorithm, yielding an O(n2) parsing algorithm.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek prezentuje metodu kořenových stromů a její využití pro závislostní parsing češtiny včetně souhrnu experimentů, které přibližují chybovost, popřípadě úspěšnost takto získaných závislostních stromů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents rooted trees and their usage for dependency parsing of Czech, including a summary of experiments that demonstrate the error/success rate of the resulting dependency trees.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato technická zpráva je dokumentací k Pražskému závislostnímu korpusu verze 2.0 (PDT 2.0). Obsahuje
podrobný komplexní popis dosavadních pravidel anotování českých vět na tektogramatické rovině,
a to především po stránce lingvistické, ale i po stránce technické. Anotovaná data neodráží vždy přesně
popisovaný stav pravidel anotace, proto je v technické zprávě zahrnut i co nejpřesnější popis anotovaných
tektogramatických stromů v korpusu PDT 2.0.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This technical document provides detailed documentation of the Prague Dependency Treebank, version
2.0 (PDT 2.0). It includes a detailed complex description of the rules that have been used so far for
the annotation of Czech sentences on the tectogrammatical layer both in linguistic and technical respect.
The annotated data do not always reflect the described state of the rules precisely, therefore the technical
document includes also a detailed description of the tectogrammatical trees that are annotated in
PDT 2.0.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto textu chceme čtenáře seznámit s možností formálního zavedení podkladové reprezentace
přirozeného jazyka, tedy s reprezentací, která zachycuje jazykový význam. Vycházíme přitom
z Funkčního generativního popisu, což je stratifikační, závislostně orientovaný popis češtiny. Přibližujeme zde přístupnou formou hlavní myšlenky článku Vl. Petkeviče A New Formal Specification of Underlying
Structures, který vyšel v časopise Theoretical Linguistics, vol. 21, No.1 v roce 1995.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this article a new formal model of underlying representation of natural language is provided. As a background, Functional Generative Description of Czech is used. We aim at clear and comprehensive introduction of main ideas published by Vl. Petkevič in the article A New Formal Specification of Underlying Structures, Theoretical Linguistics, vol. 21, No.1, 1995.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>VALLEX je slovník sloves zaměřený na popis syntaktické informace užitečné pro NLP. Slovník obsahuje asi 2500 ručně anotovaných českých sloves s více než 6000 valenční rámce (léto 2005). V tomto příspěvku je představen VALLEX a popsán experiment, kde byl pomocí jednotlivých rámcům VALLEXu anotován korpus 10.000 instancí 100 českých sloves - mezianotátorská shody činila 75%. Část dat byla použita pro WSD, tedy pro automatické přiřazování významu, ve kterém jsme dosáhli úspěšnosti 78,5%.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>VALLEX is a linguistically annotated lexicon aiming at a~description of syntactic
information which is supposed to be useful for NLP. The lexicon contains roughly 2500 manually annotated Czech verbs with over 6000 valency frames (summer 2005). In this paper we introduce VALLEX and describe an experiment where VALLEX frames were assigned to 10,000 corpus
instances of 100 Czech verbs -- the pairwise inter-annotator agreement reaches 75%.
The part of the data where three human annotators agreed were used for an automatic word sense disambiguation task, in which we achieved the precision of 78.5%.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek vysvětluje principy závislostní redukční analýzy a její vztah závislostem a závislostním stromům. Závislostní redukční analýzu dokládá českými příklady charakteristickými slovoslednou volností. příspěvek shrnuje základní prvky metody závislostní syntaxe. Tato metoda slouží jako základ pro ověřování (a vysvětlení) přiměřenosti formální a výpočetních modelů z těchto metod.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper explains the principles of dependency analysis by reduction and its correspondence to the notions of dependency and dependency tree. The explanation is illustrated by examples from
Czech, a language with a relatively high degree of word-order freedom. The paper sums up the basic features of methods of dependency syntax. The method serves as a basis for the verification (and explanation) of the adequacy of formal and
computational models of those methods.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku je popsáno anotační schéma koreference v Pražském závislostním korpusu. Dále je popsán a vyhodnocen nový systém na vyhledávání antecedentů osobních zájmen.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of this paper is two-fold. First, we want to present a part of the annotation scheme of the Prague Dependency Treebank 2.0 related to the annotation of coreference on the tectogrammatical layer of sentence representation (more than 45,000 textual and grammatical coreference links in almost 50,000 manually annotated Czech sentences). Second, we report a new pronoun resolution system developed and tested using the treebank data, the success rate of which is 60.4 %.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pražský závislostní korpus je využit k testování hypotéz o diskurzních vztazích, především v oblastio anafor.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A dependency based annotated corpus of Czech is used for testing several hypotheses
concerning topic/focus of the sentence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku představujeme systém gramatémů, který byl koncipován ve Funkčním generativním popisu a dále rozpracován při anotaci Pražského závislostního korpusu 2.0 (PDT 2.0).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the contribution, the work on the system of grammatemes (mostly semantically-oriented counterparts of morphological categories such as number, degree of comparison, or tense) is presented, The concept of grammatemes was introduced in Functional Generative Description, and is now further elaborated in the context of Prague Dependency Treebank 2.0.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku je představen systém gramatémů, který byl koncipován ve Funkčním generativním popisu a rozpracován při anotaci Pražského závislostního korpusu 2.0 (PDT 2.0). Gramatémy jsou nejčastěji významové protějšky morfologických kategorií, např. kategorie čísla nebo času.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we report our work on the system of grammatemes (mostly semantically-oriented counterparts of morphological categories such as number, degree of comparison, or tense), the concept of which was introduced in Functional Generative Description, and is now further elaborated in the context of Prague Dependency Treebank 2.0. We present also a new hierarchical typology of tectogrammatical nodes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Morfologické značkování je důležitý problém v oblasti počítačové lingvistiky, protože na něm staví další významné úlohy jako syntaktická analýza a strojový překlad. V současnosti se tento problém nejčastěji řeší statisticky. Další slibnou metodou jsou umělé neuronové sítě (ANN). V článku prezentujeme výsledky aplikace známé zpětné propagace (BP) na několik druhů experimentů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Morphological tagging is an important problem in the area of computational linguistics as it underlies other crucial tasks such as syntactic parsing and machine translation. Nowadays, the problem is being most commonly solved by a statistical approach. Artificial neural networks (ANN) represent another promising approach to this kind of problems for which the exact algorithmic solution is unknown or not efficient enough. In this paper we present the results obtained by application of the well-known backpropagation (BP) neural network in several types of experiments. We have focused on the Czech morphology, because its morphological system is very rich and no experiments concerning application of artificial neural networks have been carried out for this language. First, we have verified on a set of preliminary experiments that the neural network is capable of capturing the Czech morphology, which, secondly, served also for determination of appropriate network and context parameters. Thirdly, we have used neural networks for a voting experiment. The aim of the voting experiment was to select the correct tag (if present) from the outputs of two statistical taggers, the Markov model tagger and the Featurebased tagger. In this experiment, BP showed higher tagging precision (93,56%) than any of the input statistical methods (92,74%, 92,58%) and exceeded even the currently best available statistical result (93,47%). BP has proved to be a worthy post-processing tool that is able to perform implicit evaluation on complementary aspects of different statistical approaches.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Morfologické značkování je důležitý problém v oblasti počítačové lingvistiky, protože je předstupněm pro další zásadní úlohy jako syntaktickou analýzu a strojový překlad. V současné době je nejčastěji řešen statistickými metodami. Jiný slibný přístup k tomuto druhu problémů, pro které není známo nebo není dostatečně efektivní exaktní algoritmické řešení, představují umělé neuronové sítě (ANN). V tomto článku prezentujeme výsledky aplikace neuronových sítí s "backpropagation" (BP) na pokusy několika druhů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Morphological tagging is an important problem in the area of computational linguistics as it underlies other crucial tasks such as syntactic parsing and machine translation. Nowadays, the problem is being most commonly solved by a statistical approach. Artificial neural networks (ANN) represent another promising approach to this kind of problems for which the exact algorithmic solution is unknown or not efficient enough. In this paper we present the results obtained by application of the well-known backpropagation (BP) neural network in several types of experiments. We have focused on the Czech morphology, because its morphological system is very rich and no experiments concerning application of artificial neural networks have been carried out for this language. First, we have verified on a set of preliminary experiments that the neural network is capable of capturing the Czech morphology, which, secondly, served also for determination of appropriate network and context parameters. Thirdly, we have used neural networks for a voting experiment. The aim of the voting experiment was to select the correct tag (if present) from the outputs of two statistical taggers, the Markov model tagger and the Feature-based tagger. In this experiment, BP showed higher tagging precision (93.56%) than any of the input statistical methods (92.74%, 92.58%) and exceeded even the currently best available statistical result (93.47%). BP has proved to be a worthy post-processing tool that is able to perform implicit evaluation on complementary aspects of different statistical approaches.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek představuje různé přístupy k sémantické klasifikaci sloves. Vychází z Funkčního generativního popisu, který představuje teoretický rámec valenčního slovníku VALLEX, a pokouší se formulovat základní kritéria pro vymezení syntakticko-sémantických tříd sloves. Tento přístup je demonstrován na příkladech některých slovesných tříd, jako jsou slovesa mluvení, myšlení, vnímání nebo výměny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the contribution, different approaches to verb classification are characterized. We briefly describe valency theory of Functional Generative Description that provides the theoretical background for valency lexicon VALLEX. The basic criteria for classifying verbs from the valency lexicon VALLEX are formulated. Some illustrative instances of delimitation of classes such as communication, mental action, perception, exchange and providing are introduced in this paper.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek pojednává o metodách, kterými lze automaticky přiřazovat sémantické rámce pomocí syntakticko-sémantického rozhraní v lexikálních funkčních gramatikách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article discusses methods that can be used to automatically assign semantic frames using the Syntax-Semantics Interface in Lexical Functional Grammars.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pojem spisovnosti zastarává, je třeba nahradit ho ne tak ostře ohraničeným chápáním češtiny standardní.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The concept of "literary" language is getting obsolete, it should be changed into that of standard language, lacking clear boundary lines.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zavádíme nový typ restartovacích automatů, abychom získali prostředek umožňující klasifikaci jevů
souvisejících s valencí a slovosledem u přirozených jazyků. Studujeme dva základní typy omezení výpočtů:
j-roztržitost interpretujeme jako míru nesouvislosti valencí (redukcí) a j-volnost jako míru volnosti slovosledu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a new type of restarting automaton as a means for classification phenomena related to valency and word order in natural languages. Two basic types of constraints are studied: j-abstractedness as a measure for discontinuity of valences and j-freedom as a measure for free word order.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Průměrná redukovaná frekvence je číslo, které slouží k seřazení slov podle běžnosti. Na rozdíl od prosté frekvence bere v úvahu rozložení slov v korpusu, a tím se vypořádává s problémem výskytu slov ve shlucích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Average reduced frequency is a value that should be used instead of pure frequency for ranking words according to their commonness. It takes into account not only number of occurrences but also the word distribution within the whole corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku je popsán experiment s automatickým překladačem Česílko, který překládá mezi blízkými jazyky, konkrétně češtinou a slovenštinou. Experiment byl proveden na české a slovenské verzi Orwellova románu 1984.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The contribution will describe an experiment with the automatic translational tool Česílko that was designed for translation between very close languages, namely Czech and Slovak. We had at our disposal the Czech version of the Orwell's novel 1984, morphologically annotated, and the Slovak version without the annotation. We automatically translated the Czech version into Slovak and compared the result with the automatic morphological annotation of the Slovak version. We evaluated the experiment using manually annotated part of the Slovak version.

During the experiment we had to deal with different non-consistent morphological tagsets used for the annotation of different input data.
Conversions among them made the hardest problems of the whole work. The contribution will concentrate on overcoming inconsistent tagsets, as the problem is quite common.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje návrh švédského valenčního slovníku, jehož mikrostruktura (struktura slovníkového hesla) by byla obohacena o informaci o telicitě a způsobu slovesného děje.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article presents a proposed Swedish valency lexicon. The lexicon microstructure contains additional event-structure information.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek se zabývá podstatnými jmény, které figurují v jmenných částech tzv. analytických predikátů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This contribution addresses nouns in light verb constructions and their lexicographical description for NLP purposes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Druhý základní problém s touto knihou je, že pro většinu z ka -
ters Cachia draws on texts which appeared before 1970. vat Cachia čerpá z textů, které se objevily před rokem 1970. Modern Arab literature Moderní arabské literatuře
develops fast, and this is particularly true of the last twenty-five years. rychle vyvíjí, a to platí především za posledních dvacet pět roků. Some char- Některé char -
acteristics which he apparently sees as still valid have now been modified. acteristics kterou zřejmě považuje za stále platné již byly upraveny. As far as Co se týče
'Islamic inspiration' is concerned, Sufism has formed a point of reference for two 'Islámský inspirace' se obává, Súfismus vytvořila referenční bod pro dva
leading Egyptian writers of the post-Mahfuz generation. přední egyptský spisovatelé post-Mahfuz generace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The second fundamental difficulty with this book is that for most of the chapters
Cachia draws on texts which appeared before 1970. Modern Arab literature
develops fast, and this is particularly true of the last twenty-five years. Some characteristics
which he apparently sees as still valid have now been modified. As far as
'Islamic inspiration' is concerned, Sufism has formed a point of reference for two
leading Egyptian writers of the post-Mahfuz generation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předvádíme korektivní model pro získávání neprojektivních závislostních struktur ze stromů generovaných nejlepšími dostupnými parsery založenými na složkách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a corrective model for recovering non-projective dependency structures from trees generated by state-of-the-art constituency-based parsers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku popisujeme automatické rozpoznání derivačních předpon českých slov. Metoda spočívá v použití dvou statistických měr - Entropie a Ekonomického principu. Experimenty byly prováděny se seznamem téměř 170 tisíc lemmat z Českého národního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the application of a method for the automatic, unsupervised recognition of derivational prefixes of Czech words. The technique combines two statistical measures - Entropy and the Economy Principle. The data were taken from the
list of almost 170 000 lemmas of the Czech National Corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje konverzi slovníku povrchové valence českých sloves na slovník povrchové valence deverbativních adjektiv. Po manuální přípravě dat je celá konverze plně automatická a veškeré změny ve zdrojovém slovníku se automaticky odrazí ve slovníku cílovém.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes conversion of a surface valency lexicon of Czech verbs
to a surface valency lexicon of adjectives that can be derived from these
verbs and that use their (possibly modified) valency frames. After
preparing the necessary data by hand, the conversion can be fully automatic
and every change of the source lexicon can be automatically reflected in
the destination lexicon. We have successfully converted the verb valency
lexicon Brief with about 15,000 verbs to a valency lexicon of about
27,000 deverbal adjectives. The paper also describes some interesting
peculiarities in the process of creating passive adjectives and their
valency frames.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje spolupráci čtyř evropských univerzit zaměřenou na popularizaci evropských magisterských studií v oblasti jazykových a komunikačních technologií. Tato spolupráce byla formálně posvěcena v rámci programu Erasmus Mundus jako Zvláštní podpůrná akce v roce 2004. Konsorcium se také zaměřuje na vytvoření pevného základu púro společný magosterský program v oblasti jazykových technologií a informatiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the cooperation of four European Universities aiming at attracting more students to European mas-ter studies in Language and Communica-tion Technologies. The cooperation has been formally approved within the frame-work of the new European program “Erasmus Mundus” as a Specific Support Action in 2004. The consortium also aims at creating a sound basis for a joint master program in the field of language technol-ogy and computer science.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem předložené práce je vytvořit metody pro automatické přiřazování morfologických vzorů českým slovům. Nejprve je provedena analýza problému, ve které jsou zdůrazněny některé podproblémy, se kterými se musíme vypořádat. Poté jsou navrženy čtyři různé algoritmy pro výběr z možných vzorů, pracující na základě analýzy slova a jeho kontextu. Dále jsme navrhli algoritmus pro rozdělení množiny slov na třídy ekvivalence podle společného lemmatu. Pro odhad optimálních parametrů jednotlivých metod jsme použili různé zdroje dat, na kterých jsme provedli přes 250 testů s různými hodnotami parametrů. Součástí
práce je popis použitých algoritmů a jejich implementace v programovacích jazycích Perl a C++.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Aim of the presented work is to explore possibility of automatic morphological paradigms assignment for the Czech words. Theoretical part of our work consists of the problem analysis with emphasized issues we have to deal with. We present four different algorithms for morphological paradigm assignment, using both word form analysis and contextual information processing. Word forms are partitioned into equivalence classes according to their lemma, using another algorithm. We performed more than 250 tests on the various corpus data with the purpose of estimating best method parameters. Presented algorithms are thoroughly described and implemented.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Statistické rozhodovací pravidla mohou být využita v genetice, přesněji v odvětví microarray, za účelem klasifikace proteinů do několika několika tříd, a to podle informací shrnutých v několika naměřených proměnných.

Příspěvěk pojednává o použití metody Fisherovy lineární diskriminační analýzy na reálná data o 268 proteinech klasifikovaných použitím 400 proměnných do 42 tříd. Takovýto klasifikační problém je typický v microarray, kde počet proměnných překračuje počet pozorovaných objektů (tzv. p>>n problém). Data byla již dříve studována; souhrn všech výsledků lze nalézt na http://www.dkfz.de/biostatistics/protein/DEF.html. Byly použity lineární i neliární metody, nejnižší chybovost bylo dosaženo pomocí Support Vector Machines.

Bude přestavena implementace Fisherovy (lineární) discriminační analýzy, která může v chybovosti konkurovat i nelineárním metodám. Bude přiloženo i porovnání této nové implementace s klasickou implementací funkcí lda() ve výpočetním prostředí R (http://www.r-project.org).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Statistical decision rules can be used in genetics, more accurately in microarray, to classify proteins into several classes according information taken from other measured variables.

The contribution concerns on using Fisher linear discriminant analysis on a real data consisting of 268 proteins classified according 400 variables into 42 classes. The classification task of such a case is typical for microarray where number of variables is higher than number of observed objects (also known as  p>>n problem). The data was investigated several times earlier; the project web page of these studies as well as with other results is http://www.dkfz.de/biostatistics/protein/DEF.html. A lot of linear and nonlinear methods was used, the lowest error was reached with nonlinear method called Support Vector Machines.

It will be shown that with a good implementation of Fisher discriminants the linear method can beat nonlinear ones. A small comparison with classical implementation by lda() function in R (http://www.r-project.org) will be shown as well.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Statistická metada Fisherovy lineární diskrimianční analýzy slouží je jedna z nejoblíbenějších a nejstarších klasifikačních metod. Byla původně definována ve 30. letech 20. století pomocí rovnic. Později numericky přeformulována jako úloha řešení vlastních čísel a vlastních vektorů symetrické pozitivně semi-definitní matice.
Zde přestavujeme některé méně známé aspekty impelementace, které mohou výrazně urychlit výpočty a zlepšit výsledné hodnoty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Fisher linear discriminat analysis is one of the most used and the oldest of the classification methods. Originally, it was defined at the thirties of the twentieth century by the equations. Later, it was numerically refolmulated as the eigen-decomposition task of the symmetric positive semi-definitive matrix. We are presenting some less known aspects of the implementation that can significantly speed-up calculations and improve output values.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku zkoumáme numerické aspekty klasifikace při použití různých implementací Fisherovy lineární diskriminační analýzy (FLDA). FLDA je založena na maximalizačním kritériu poměru mezitřídního a vnitrotřídního rozptylu daných proměnných. Maximalizační problém je často řešen jeho redukcí na obecný symetrický problém vlastních čísel a vektorů definovaný pomocí mezitřídní a vnitrotřídní kovarianční matice. Tyto matice jsou ovšem singulární, pokud počet proměnných překročí počet objektů pozorovaných při trénování. Následkem čehož se efektivní numerické řešení problému vlastních čísel a vektorů stává významným [1], v historii bylo použito již několik technik, jak se singularitou matic v FLDA zacházet [2]. V těchto kamparativních studiích jsou ovšem na úkor klasifikačního výkonu zanedbávány numerické aspekty jako výpočetní a pamětové nároky. Zaměříme se na vztah mezi implementací a výkonem metody FLDA. Přidáme porovnání několika populárních implementací, včetně detailů o numerické stabilitě, pamětových a komputačních nákladech a odhadů komputačních chyb. Dále předkládáme numerické výsledky vzniklé aplikací vybraných metod na data z klasifikace proteinů [3], rozdíly v klasifikačním výkonu je udivující. Implementace založena na Moore-Penrosově pseudoinverzi svým výkonem převyšuje všechny ostatní strategie, včetně metody Support Vector Machines, která je obecně považováná za nejúčinnější pro tato data. Závěrem zmiňujeme důsledky tohoto pozorování, možná rozšíření na další úlohy a přehled relevantního softwaru.
Reference:
[1] Z. Bai, J. Demmel, J. Dongarra, A. Ruhe and H. van der Vorst (eds.) (2000): Templates for the solution of Algebraic Eigenvalue Problems: A Practical Guide. Philadelphia, SIAM
[2] Y.-Q. Cheng,  Y.-M. Zhuang and  J.-Y. Yang (1992): Optimal Fisher discriminant analysis using the rank decomposition. Pattern recognition, vol. 25, 101--111
[3] L. Edler  and J. Grassmann  (1999): Protein fold prediction is a new field for statistical classification and regression. In Seillier-Moiseiwitsch F(Ed): Statistics in Molecular Biology and Genetics. IMS Lecture Notes Monograph Series 33, 288--313</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this contribution we investigate numerical aspects of classification using various implementations of Fisher's linear discriminant analysis (FLDA). FLDA is based on maximizing the ratio of between-group variance to within-group variance of given variables. The maximization problem is frequently solved by reducing it to the symmetric generalized eigenproblem defined by the between-group and the within-group covariance matrices. Unfortunately, these matrices are singular in classification tasks where the number of given variables exceeds the number of objects for training. Consequently, efficient numerical solution of the eigenproblem becomes a challenging problem (see, e.g. [1]) and several techniques to handle the singularity have been proposed in the literature about FLDA (see, e.g. [2]). In comparative studies of these techniques, however, assessment of classification performance more or less discards numerical aspects such as computational or storage costs.
We focus on the relation between implementation and performance of FLDA. We give a comparison of a number of popular FLDA implementations including detailed information about their numerical stability, storage costs, computational costs and estimation of computational error. Moreover, we provide numerical examples by applying the discussed methods to a particular protein classification problem (see, e.g. [3]). The differences of classification performance are striking. An implementation based on reduction to a classical eigenproblem via Moore-Penrose pseudeoinverses outperforms all other strategies, including the Support Vector Machines approach that is generally considered the most powerful for the given data. We discuss some consequences of this observation and possible extension to other types of problems and conclude with a brief overview of relevant software.
References:
[1] Z. Bai, J. Demmel, J. Dongarra, A. Ruhe and H. van der Vorst (eds.) (2000): Templates for the solution of Algebraic Eigenvalue Problems: A Practical Guide. Philadelphia, SIAM
[2] Y.-Q. Cheng,  Y.-M. Zhuang and  J.-Y. Yang (1992): Optimal Fisher discriminant analysis using the rank decomposition. Pattern recognition, vol. 25, 101--111
[3] L. Edler  and J. Grassmann  (1999): Protein fold prediction is a new field for statistical classification and regression. In Seillier-Moiseiwitsch F(Ed): Statistics in Molecular Biology and Genetics. IMS Lecture Notes Monograph Series 33, 288--313</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce navazuje na implementačně-výzkumný projekt Morče, jehož cílem bylo vytvoření co nejlepšího morfologického taggeru češtiny, založeného na skrytém Markovově modelu s průměrovaným perceptronem. Úspěšnost algoritmu závisí především na zvolené sadě rysů popisujících kontext, na jehož základě se značky vybírají. Práce stručně popisuje zvolený algoritmus a jeho implementaci. Její stěžejní část spočívá ve velké řadě provedených experimentů, které v rámci daných možností důkladně mapují možné sady rysů, jejich úspěšnosti a vztahy mezi nimi. Pro tento účel jsou definována pravidla, podle kterých se verze porovnávají. Využívá se pětinásobná crossvalidace a pro zjištění statistické významnosti výsledků je aplikován t-test. Při zahájení práce byla dána k dispozici nová data pro češtinu, takže veškeré experimenty se již prováděly nad daty z PDT 2.0. Vedlejším výsledkem práce je i statisticky významné zvýšení úspěšnosti taggeru, nicméně nejlepší tagger zřejmě překonán nebyl. Kromě ručního vývoje verzí byl projekt také upraven pro automatický vývoj, který byl v menším rozsahu proveden a popsán.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This work continues in implementational and experimental project Morče, which aimed to create the best possible morphological Czech tagger based on hidden Markov model with averaged perceptron. Successfulness  of algorithm depends mainly on a selected set of features describing a context, which determines choice of a tag. The work describes briefly the algorithm and its implementation. Main part of the work consists of a lot of experiments which explore possible feature sets, their successfulness and their relationships. Few clear rules are defined for comparison of versions. Fivefold crossvalidation with t-test is used for verification of statistical significance. After the work was started, new Czech data became available, so all experiments used data from PDT 2.0. Side effect of this work was statistical significant improvement of successfulness. However, the best Czech tagger was obviously not overwhelmed. Some modifications were made in order to perform automatic version development. It was executed in small extent and also described.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek prezentuje systém strojového překladu z češtiny do dolnolužické srbštiny, menšinového jazyka používaného v Německu v okolí Chotěbuze. Popisujeme architekturu systému a zaměřujeme se na morfologickou disambiguaci, částečný syntaktický parser a transfer.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents a machine translation system from Czech to Lower Sorbian, a minority language spoken in a region around Cottbus in Germany. This West Slavonic language, which is spoken by less than 20,000 people, is very archaic, it, has supine, dual and some other grammatical forms, which disappeared in most Slavonic languages. The paper describes the architecture of the system and focuses on morphological disambiguation, partial syntactic parser and lexical and structural transfer. First evaluation results on a small set of sentences are also presented.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přehled zejména syntaktických jevů, jejichž užívání v humanistické češtině bylo podpořeno vzorem klasické latiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Overview of syntactic phenomena in Humanistic Czech usage of which was supported by the ideal and language example of Classic Latin.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slovosledné postavení jednoslovného slovesného přísudku ve vybraných typických syntaktických konstrukcích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Word order position of one-word verbal predicate in specific syntactic constructions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předsuny kvalifikačních, kvantifikačních a negačních výrazů na počátek věty a jejich vztah k aktuálnímu členění (kontrast, réma).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The initial position of qualifying, quantifying and negative expressions in a sentence and their relation to the functional perspective of the sentence (contrast, rheme).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem tohoto článku je prezentovat počáteční výsledky adaptace SProUTu, vícejazyčné platformy pro počítačové zpracování přirozeného jazyka, pro polštinu. Článek popisuje některé problémy způsobené integrací Morfeusze, externího morfologického analyzátoru polštiny, a různá řešení problému neexistujících rozsáhlých polských zeměpisných slovníků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of this article is to present the initial results of adapting SProUT, a multi-lingual Natural Language Processing platform developed at DFKI, Germany, to the processing of Polish. The article describes some of the problems posed by the integration of Morfeusz, an external morphological analyzer for Polish, and various solutions to the problem of the lack of extensive gazetteers for Polish. The main sections of the article report on some initial experiments in applying this adapted system to the Information Extraction task of identifying various classes of Named Entities in financial and medical texts, perhaps the first such Information Extraction effort for Polish.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje pokus o implementaci závislostní gramatiky pro četšinu ve formalismu Extensible Dependency Grammar (XDG) založeném na omezujících podmínkách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article describes an attempt to implement a constraint-based dependency
grammar for Czech, a language with rich morphology and free word order, in the
formalism Extensible Dependency Grammar (XDG). The grammar rules are
automatically inferred from the Prague Dependency Treebank (PDT) and constrain
dependency relations, modification frames and word order, including
non-projectivity. Although these simple constraints are adequate from the
linguistic point of view, their combination is still too weak and allows an
exponential number of solutions for a sentence of n words.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg># Obsahuje 37 původních článků napsaných vůdčími osobnostmi oboru.
# Zabývá se ústředními otázkami sdílenými zájemci o obor.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg># Contains 37 original articles written by leaders in the field.
# Addresses the central concerns shared by those interested in the subject.
# Major sections focus on the experience of particular disciplines in applying computational methods to research problems; the basic principles of humanities computing; specific applications and methods; and production, dissemination and archiving.
# Accompanied by a website featuring supplementary materials, standard readings in the field and essays to be included in future editions of the Companion.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje anotaci korpusu mluvené češtiny - nahrávek z archívu Shoa VHF a problémy s tím spojené, např. kvalitu těchto nahrávek z akustckého hlediska.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes the problems adn solutions to the annotation of the Czech recordings in the Shoa VHF collection, e.g. the problems related to the acoustic quality etc.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku probíráme některé problémy spojené s podmínkou projektivity v závislostním popisu jazyka (viz Sgall, Hajičová a Panevová (1986), Hajičová, Partee a Sgall (1998)) se zvláštním zřetelem na anotační schéma Pražského závislostního korpusu (PDT, viz Hajič (1998)).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the present paper we discuss some issues connected with the condition of projectivity in a dependency based
description of language (see Sgall, Hajičová, and Panevová (1986), Hajičová, Partee, and Sgall (1998)), with a
special regard to the annotation scheme of the Prague Dependency Treebank (PDT, see Hajič (1998)). After a short
Introduction (Section 1), the condition of projectivity is discussed in more detail in Section 2, presenting its formal
definition and formulating an algorithm for testing this condition on a subtree (Section 2.1); the introduction of
the condition of projectivity in a formal description of language is briefly substantiated in Section 2.2. and some
problematic cases are discussed in Section 2.3. In Section 3, a preliminary classification into three main groups
and several subgroups of Czech non-projective constructions on the analytical level is presented (Section 3.1),
with illustrations of each subgroup in Section 3.2. A discussion of (surface) non-projectivities viewed from the
perspectives of the underlying (tectogrammatical) structures is given in Section 4; the classification outlined in
Section 4.1 reflects the types of deviations from projectivity caused by topic-focus articulation (TFA). In Section
4.2 we examine the motivation and factors of non-projective constructions. The treatment of non-projective
constructions in the annotation scenario of PDT is presented in Section 5. In the Conclusion (Section 6) we summarize
the results and outline some directions for further research in this domain. The present contribution is an
enlarged and slightly modified version of the paper Veselá, Havelka, and Hajičová (2004).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Finite State Morphology. Vydal CSLI Publications, Stanford, Kalifornie, 2003 (CSLI Studies in Computational Linguistics, xviii+510 pp a CD-ROM, ISBN 1-57586-434-7)</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Finite State Morphology. CSLI Publications, Stanford, California, 2003 (CSLI Studies in Computational Linguistics, xviii+510 pp and CD-ROM, ISBN 1-57586-434-7)</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Datově-orientovaná syntaktická analýza je název pro originální statistický přístup k syntaktické analýze, který se učí výskyty jednotlivých podstromů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Data-oriented parsing is a name for a novel statistical approach to syntactic parsing that learns occurrences of subtrees.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem této technické zprávy je popsat neprojektivní konstrukce v Pražském závislostním korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The goal of this technical report is to describe non-projective constructions in the Prague Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato disertační práce popisuje statistický parser (syntaktický analyzátor) češtiny. Krok za krokem v ní analyzujeme vývoj parseru a přínos jednotlivých jeho částí. Parser je postaven na metodě přímého statistického modelování závislostí mezi slovy, která je originální i při srovnání se zahraničními pracemi. Přestože se nepodařilo tímto způsobem získat nejúčinnější možný nástroj pro syntaktickou analýzu češtiny a existují české adaptace původně anglických parserů, které si vedou lépe, ukážeme, že díky odlišnému přístupu dělá náš parser chyby jiného druhu a lze tedy jeho kombinací s lepšími parsery dosáhnout výsledku, kterého žádný z nich není sám o sobě schopen.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This thesis presents a statistical parser of Czech. We analyze step by step its evolution and the merit of its various parts. The parser is based on the original approach of dependency bigram modeling. Although there are other parsers that have been originally developed for English and their adaptations for Czech perform better than ours, we demonstrate that our parser makes different errors and thus it can help the better parsers to become even better.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje počítačovou aplikaci vyvinutou týmem autorů na Univerzitě Karlově v Praze, jejímž cílem je podpořit víceaspektové zpracování příkladů kulturního dědictví v inteligentním prostředí informačních technologií.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article presents a description of a computer application, developed by a team of authors at the Charles University in Prague, meant to support a multiaspect processing of examples of the cultural heritage in the intelligent environment of the information technologies. Provided is a possibility for the application to be used when describing and analyzing medieval manuscripts, experimented in the project of the Faculty of Mathematics and Physics Annotation corpora of Text (ACT, see http://prometheus.ms.mff.cuni.cz/act). The system is a linguistically-independent product for lexical and corpora processing of written texts. It has been created with the purpose to process large corpora with linguistic annotation, which mostly includes lexicographic and morphological analysis, and the syntactic and semantic information is marked on basic level. The system allows the users when working at different places, to compare the data between one another, and to keep the results regardless of the number of specialists working at the same time. A product which completely solves the problems of the linguistic analysis of medieval records with the help of computers is offered.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku vyložíme podstatu  závislostní redukční analýzy
(DAR, dependency analysis by reduction) a její souvislost s pojmy
závislost a závislostní strom. Výklad budeme ilustrovat příklady z
češtiny, což je jazyk s (výrazně) volným slovosledem.
Tento výklad shrnuje základní rysy vývojových postupů závislostní syntaxe.
 Bude využit jako podklad pro ověřování (a vysvětlování) adekvátnosti
 formálních a počítačových modelů těchto postupů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The syntactic structure of English sentences or sentences of other fixed word-order language is commonly described by phrase structure grammars. The description of syntactic structure of Latin, Italian, German, Arabic, Slavic languages and some other languages is more often based on approaches generally called dependency approaches. Both approaches are based
on stepwise simplification of individual sentences, on the so-called analysis by reduction (AR). The basic principles of the phrase-structure and dependency based analysis by reduction are substantially different. The phrase-structure based analysis (of fixed word-order languages) can be naturally modeled by the
bottom-up analysis using phrase structure (Chomskian) grammars. However, this type of grammars cannot model naturally the dependency analysis by reduction (of free word-order languages). In this paper we explain the basic principles of dependency analysis by reduction
(DAR) and its correspondence to notions used in the traditional dependency oriented linguistics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato recenze se zabývá slovníkem švédských pomístních jmen editovaný Matsem Wahlbergem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This review focuses the dictionary of Swedish location names edited by Mats Wahlberg.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Analýza obecných funkcí predikátu a jejich uplatnění v humanistické češtině</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Analysis of general functions of predicate and their implementation in Czech in the period of humanism</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje valenční slovník PDT-Vallex, který je  propojený s reálnými texty v Pražském závislostním korpusu. Pojetí valence vychází z FGP jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes the PDT-Vallex valency dictionary, which is linked to real texts in the  Prague Dependency Treebank The concept of valency is based on the FGD.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Abstrakt tohoto článku není k dispozici, protože autoři žádný nedodali.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>No abstract of this paper is available because the authors have not provided any.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Na moderní valenční slovník klademe řadu požadavků. Kromě strojové čitelnosti a dostatečné explicitnosti použitého popisu jde zejména
o kvalitu dat ve slovníku obsaľených. V článku přibližujeme nástroje navržené pro testování konzistence a úplnosti slovníku VALLEX. Rozebíráme metody využívané pro zvýšení jeho kvality od odstraňování technických chyb přes porovnání s existujícími lexikografickými zdroji po testování vnitřní konzistence
budovaného slovníku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>There are number of requirements put on vaůency dictionaries. We focus on testing the consistency and completeness of a valency lexicon of Czech verbs VALLEX.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Short Czech abstract must be provided for all publications regardless their original language. Minimum is 64 characters, there is no upper bound but long abstracts will be truncated when reporting the publication.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Short English abstract must be provided for all publications regardless their original language. Minimum is 64 characters, there is no upper bound but long abstracts will be truncated when reporting the publication.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>K tomuto článku není abstrakt k dispozici, protože autoři žádný nedodali.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>No abstract of this paper is available because the authors have not provided any.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V současné době existuje poměrně velká základna publikací o automatickém získávání lexikálně-syntaktických preferencí (valence) z korpusů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Today there is a relatively large body of work on automatic acquisition of lexico-syntactical preferences (subcategorization) from corpora. Various techniques have been developed that not only produce machine-readable subcategorization dictionaries but also they are capable of weighing the various subcategorization frames probabilistically. Clearly there should be a potential to use such weighted lexical information to improve statistical parsing, though published experiments proving (or disproving) such hypothesis are comparatively rare. One experiment is described in (Carroll et al., 1998) — they use subcategorization probabilities for ranking trees generated by unification-based phrasal grammar. The present paper, on the other hand, involves a statistical dependency parser. Although dependency and constituency parsing are of quite a different nature, we show that a subcategorization model is of much use here as well.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Detris je šablona obsahující makro, s jehož pomocí můžete vytvářet jednotně vypadající závislostní stromečky do svých článků ve Wordu. Stromečky jsou reprezentované jako vektorová grafika, což znamená, že při změně jejich velikosti, popř. grafického rozlišení obrazovky nebo tiskárny bude minimalizován negativní dopad na vzhled stromu („schodovité čáry“ apod.) Vstupem makra je popis stromečku ve formátu CSTS, který je použit i v Pražském závislostním korpusu (PDT) verze 1. Stačí tedy do vašeho dokumentu okopírovat příslušnou část CSTS souboru, kde je popsána vaše věta. Pokud vytváříte umělý příklad, který není odkud zkopírovat, můžete použít značně zjednodušenou podmnožinu CSTS.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Detris is a template with a macro that helps you to create uniformly looking dependency trees for your documents in Word. The trees are represented as vector graphics which means that on changing their size, screen or printer resolution the deterioration will be minimal. Input is a tree description in the CSTS format, used in Prague Dependency Treebank (PDT) version 1. You can directly copy the corresponding part of the CSTS file to your document. If you are creating an artificial example that cannot be copied from anywhere, you can use a simplified subset of CSTS.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek je studií vnějších faktorů, které mohou mít negativní vliv na úspěšnost parserů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper is a study of outside factors that may have a negative impact on parsers’ accuracy. The discussed phenomena can be divided into two classes, those related to treebank design, and those related to morphological tagging. Although the scope of the paper is limited to the Prague Dependency Treebank, two particular taggers and one particular parser, we believe that our observations may be of interest to future treebank designers.

Anyway, since training parsers is usually not the only purpose of building treebanks, other good reasons may prevail and push aside the parser’s preferences. For that case, we suggest how the parser may work around the nasty input. More fine-grained elaboration of these workarounds and their evaluation is a matter of future work.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>K tomuto článku není k dispozici abstrakt, protože ho autoři nedodali.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>No abstract is available for this paper because the authors have not provided any.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>De Cock M., Žabokrtský Z. and Kerre E. E. (2001): Representing
linguistic hedges by L-fuzzy modifiers. — Proc.
CIMCA’01 (Int. Conf. Computational Intelligence for
Modelling Control and Automation), Las Vegas (USA),
pp. 64–72 (CD-ROM).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>L-fuzzy modifiers based on L-fuzzy relations prove to be
powerful tools for the representation of linguistic hedges.
They provide a general framework that is far more applicable
than the traditional approaches, and even in the
cases where traditional fuzzy hedges can be used, they
are still clearly outperformed by the fuzzy relation based
modifiers on the semantic level.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje pokusy s kombinováním statistického parseru a preprocesoru založeného na regulárních výrazech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper describes experiments on combining a statistical parser with a preprocessor built of regular expres­sions. While the syntactic structure of a sentence may be very complex and will never be fully covered by such simple machine, there are phenomena such as noun phrases and simple coordinations that do quite well. Even these “chunks” may theoretically be unlimitedly complex but we show that in reality they rarely do. So a shallow parser based on regular expressions can be built to preprocess the input text and solve the simple chunks without introducing too many errors. We discuss one implementation of such preprocessor that is very easy to write and covers roughly 20% of input words with an accuracy of over 90%. Then we describe two ways of combining the preprocessor with a parser and show that the performance of the parser improves both in speed and accuracy.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek podává zprávu o probíhajícím projektu, který se snaží kombinovat statistické modelování s ručně psanými regulárními výrazy pro syntaktickou analýzu češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper reports on a work in progress, which tries to combine statistical modeling and manually written regular rules for parsing Czech. While the syntactic structure of a sentence may be very complex and will never be fully covered by a finite-state machine, there are phenomena such as noun phrases and simple coordinations that do quite well. Even these “chunks” may theoretically be unlimitedly complex but I show that in real world they rarely do. So a shallow finite-state parser can be built to preprocess the input text and solve the simple chunks without introducing too many errors. With a minimum of programming effort, such preprocessor currently covers almost 20% of input words with an accuracy of more than 90%. Further increasing of the coverage without loss of accuracy would require much more effort, so the rest is left for the statistical parser.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vývoj počítačových technologií a počítačového zpracování jazykových dat spolu
s převratnými změnami v obou složkách jsou spolu zcela neodmyslitelně spjaty. Jejich
vzájemnou souvislost i podmíněnost nových možností automatického zpracování přirozeného
jazyka v kontextu nových informačních technologií se tu pokusíme ukázat
z hlediska dneška</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The development of computer technology and computational processing of language data are related to each other.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nové metody strojového učení pro identifikaci slovesných rámců v češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present some novel machine learning techniques for the identification of subcategorization information for verbs in Czech. We compare three different statistical techniques applied to this problem. We show how the learning algorithm can be used to discover previously unknown subcategorization frames from the Czech Prague Dependency Treebank. The algorithm can then be used to label dependents of a verb in the Czech treebank as either arguments or adjuncts. Using our techniques, we are able to achieve 88 % accuracy on unseen parsed text.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Recenze na knihu Stochastically-Based Semantic Analysis, vydanou nakladatelstvím Kluwer v roce 1999</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A review of the book Stochastically-Based Semantic Analysis, published by the Kluwer publishing house in 1999</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nové metody strojového učení pro identifikaci slovesných valenčních rámců v češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present some novel machine learning techniques
for the identification of subcategorization information
for verbs in Czech. We compare three different
statistical techniques applied to this problem. We
show how the learning algorithm can be used to discover
previously unknown subcategorization frames
from the Czech Prague Dependency Treebank. The
algorithm can then be used to label dependents of
a verb in the Czech treebank as either arguments
or adjuncts. Using our techniques, we are able to
achieve 88% precision on unseen parsed text.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Syntaktická analýza přirozených jazyků je úloha, kterou se pokoušela řešit řada lidí v minulosti, v současnosti a nepochybně také v budoucnosti. Všeobecně se předpokládá, že pokud dokážeme automaticky získat dobrý rozbor dané vstupní věty (např. funkce slov a vztahy mezi nimi), budou z něj mít užitek všechny úlohy související s "porozuměním jazyku".</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Parsing natural languages is a task approached by many people in the past and present and undoubtedly also in the future. It is widely believed that if one can obtain automatically a good parse for a given input sentence (i.e. functions of the words and relations between them), then all the "language understanding" tasks can profit from it.

English has been studied most for this purpose and many good results have been obtained, especially using statistically-based techniques coupled with automatic learning from large annotated corpora. However, these techniques have not been applied to other languages than English very much, especially to languages which display substantially different behavior (both morphologically and syntactically) than English (such as Slavic languages, spoken mostly in Europe by approx. 350 million people).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Syntaktická analýza přirozených jazyků je úloha, kterou se pokoušela řešit řada lidí v minulosti, v současnosti a nepochybně také v budoucnosti. Všeobecně se předpokládá, že pokud dokážeme automaticky získat dobrý rozbor dané vstupní věty (např. funkce slov a vztahy mezi nimi), budou z něj mít užitek všechny úlohy související s "porozuměním jazyku".</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Parsing natural languages is a task approached by many people in the past and present and undoubtedly also in the future. It is widely believed that if one can obtain automatically a good parse for a given input sentence (i.e. functions of the words and relations between them), then all the "language understanding" tasks can profit from it.
English has been studied most for this purpose and many good results have been obtained, especially using statistically-based techniques coupled with automatic learning from large annotated corpora. However, these techniques have not been applied to other languages than English very much, especially to languages which display substantially different behavior (both morphologically and syntactically) than English (such as Slavic languages, spoken mostly in Europe by approx. 350 million people).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek je založen na mé diplomové práci, která byla obhájena na Univerzitě Karlově v květnu 1997.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper is based on my master thesis that was defended at Charles University in May 1997. I introduce here a simple probabilistic model of Czech syntax, based on training data extracted from a text corpus. Then I try to employ this model in building syntactic trees of Czech sentences. In this paper, a brief description of the model is given, as well as a summary of the test results. The work is one of the first attempts to use statistical learning methods for parsing of Czech. It is to be said here that the procedures used to parse English cannot be easily ported to Czech because some specific characteristics of Czech (free word order, and rich flexion implying a huge number of word forms) cause a different behaviour of a parser. The work presented is neither final, nor satisfying solution of the problem. It rather demonstrates the primary results and proposes some basic ideas of further research.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vypracovat matematický model významových zápisů vět
    Vypracovat program (C/C++) pro
        zjištění pravděpodobnosti daného zápisu na základě modelu
        generování zápisů na základě vedlejší informace
        zjištění hodnot entropie, křížové entropie a perplexity</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Create a mathematical model of sentence meaning. Create a program (C/C++) to estimate probability of a given structure based on the model; to generate structures based on additional information; to compute entropy, cross entropy and perplexity.</seg>
            </tuv>
        </tu>
        
  </body>
</tmx>
