<?xml version="1.0" encoding="UTF-8"?>
<tmx version="1.4">
  <header creationtool="Python Script" creationtoolversion="1.0" datatype="PlainText" segtype="sentence" o-tmf="XML" adminlang="en" srclang="cs"/>
  <body>

        <tu>
            <tuv xml:lang="cs">
                <seg>Studie zkoumá kvalitu překladů, zejména styl včetně přítomnosti rušivých vlivů zdrojového jazyka, a to na základě analýzy kvality českých překladů dvou anglických novinových článků. Překlady vyhotovili tři překladatelé najatí překladatelskou agenturou. Další oblastí, kterou studie zkoumá, je tvorba nadlidských překladů, tj. překladů považovaných za nejlepší možné za daných podmínek. Hodnocení kvality se zaměřuje především na styl stávajících překladů a zjištěné problémy spadají do široké škály jazykových kategorií, jako je pravopis, morfologie, gramatika, slovní zásoba a tvorba slov. Zvláštní důraz je kladen na interference a návrh typologie sestavený autory lze rozšířit o několik dalších typů interference, které se v analyzovaných překladech opakují. Nadlidské překlady, vyplývající z týmové práce dvou překladatelů a teoretiků, kteří syntetizovali to nejlepší ze tří existujících překladů a přitom využívali značnou část vlastní kreativity, mohou být v budoucnu použity při hodnocení vynikajících strojových překladů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The study explores translation quality, particularly style including the presence of source-
language interference, by analysing the quality of Czech translations of two English newspaper
articles, done by three translators hired by a translation company; the other area explored by the study
is the creation of superhuman translations, i.e. translations thought to be the best possible under the
given conditions. The primary focus of the quality assessment is on the style of the existing
translations, and the problems identified fall into a wide range of linguistic categories such as spelling,
morphosyntax, grammar, lexicon and word formation. Special emphasis is placed on interference, and
the draft typology compiled by the authors can be expanded to include several other types of
interference recurrent in the translations analysed. The superhuman translations, resulting from the
teamwork of two translators-cum-theoreticians who synthesised the best of the three existing
translations while employing a considerable amount of their own creativity, may be used in future
assessments of excellent machine translations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem systémů účastnících se v morfosegmentační úloze při workshopu SIGMORPHON 2022 bylo rozdělit slovo na sekvenci morfémů a pokrýt přitom většinu morfologických procesů (inflexi, derivaci a skládání). Podúloha 1 byla zaměřená na rozdělování 5 miliónů slov v 9 jazycích (čeština, angličtina, španělština, maďarština, francouzština, italština, ruština, latina, mongolština). Zúčastnilo se 13 systémů od 7 týmů, přičemž nejlepší systém dosáhl průměrné úspěšnosti 97.29% F1, s rozsahem hodnot od 93.84% pro angličtinu po 99.38% pro latinu.
Druhý podúkol byl zaměřený na segmentaci slov ve větném kontextu, celkam 18.735 vět pro tři jazyky (čeština, angličtina, mongolština). Zúčastnilo se 10 systémů od 3 týmů. Nejlepší systémy překročily dosavadní nejlepší metody od 30.71 % absolutně. Pro zjednodušení chybové analýzy a přípravu budoucích studií jsme všechny predikce zveřejnili.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The SIGMORPHON 2022 shared task on morpheme segmentation challenged systems to decompose a word into a sequence of morphemes and covered most types of morphology: compounds, derivations, and inflections. Subtask 1, word-level morpheme segmentation, covered 5 million words in 9 languages (Czech, English, Spanish, Hungarian, French, Italian, Russian, Latin, Mongolian) and received 13 system submissions from 7 teams and the best system averaged 97.29% F1 score across all languages, ranging English (93.84%) to Latin (99.38%). Subtask 2, sentence-level morpheme segmentation, covered 18,735 sentences in 3 languages (Czech, English, Mongolian), received 10 system submissions from 3 teams, and the best systems outperformed all three state-of-the-art subword tokenization methods (BPE, ULM, Morfessor2) by 30.71% absolute. To facilitate error analysis and support any type of future studies, we released all system predictions, the evaluation script, and all gold standard datasets.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace o projektech na ÚFALu a PhD projektu Multi-Source Speech Translation pro mgr. studenty na LCT Summer School 30.8.2022, délka 15 minut.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Prezentation about ÚFAL projects and PhD project.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Statické a kontextové vícejazyčné embeddingy mají komplementární přednosti. Statické embeddingy, i když jsou méně expresivní než kontextové jazykové modely, se dají lépe párovat mezi jazyky. V tomto článku kombinujeme přednosti statických a kontextových, čímž docílíme vyšší kvality vícejazyčných kontextových embedingů. Z modelu XLM-R extrahujeme statické embeddingy pro 40 jazyků, validujeme jejich kvalitu pomocí indukce dvojjazyčných slovníků a pak je zarovnáváme pomocí nástroje VecMap. Výsledkem jsou vysoce kvalitní, vysoce vícejazyčné statické embeddingy. Poté aplikujeme nový přístup pokračujícího pre-training modelu XLM-R, kde využíváme tyto statické embeddingy pro lepší zarovnání reprezentačního prostoru XLM-R. Náš postup dosazuje pozitivních výsledků pro sémanticky náročných úloh. Statické embeddingy a kód pokračujícího pre-training jsou veřejně dostupné. Na rozdíl od většiny předchozí práce náš přístup pokračujícího pre-training nevyžaduje paralelní text.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Static and contextual multilingual embeddings have complementary strengths. Static embeddings, while less expressive than contextual language models, can be more straightforwardly aligned across multiple languages. We combine the strengths of static and contextual models to improve multilingual representations. We extract static embeddings for 40 languages from XLM-R, validate those embeddings with cross-lingual word retrieval, and then align them using VecMap. This results in high-quality, highly multilingual static embeddings. Then we apply a novel continued pre-training approach to XLM-R, leveraging the high quality alignment of our static embeddings to better align the representation space of XLM-R. We show positive results for multiple complex semantic tasks. We release the static embeddings and the continued pre-training code. Unlike most previous work, our continued pre-training approach does not require parallel text.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Hodnoticí kampaň 19. Mezinárodní konference o překladu z mluveného jazyka (IWSLT 2021) letos představila osm sdílených úkolů: (i) Simultánní překlad mluvené řeči, (ii) Offline překlad mluvené řeči, (iii) Překlad řeči na řeč, (iv) Překlad řeči s malým množstvím zdrojů, (v) Vícejazyčný překlad řeči, (vi) Překlad řeči v dialektu, (vii) Kontrola formality pro překlad řeči, (viii) Izometrický překlad řeči. Celkem se zúčastnilo 27 týmů alespoň na jedné z úloh. Tento dokument u každého sdíleného úkolu podrobně popisuje účel úkolu, zveřejněná data, použité metriky hodnocení, obdržené podání a dosažené výsledky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The evaluation campaign of the 19th International Conference on Spoken Language Translation featured eight shared tasks: (i) Simultaneous speech translation, (ii) Offline speech translation, (iii) Speech to speech translation, (iv) Low-resource speech translation, (v) Multilingual speech translation, (vi) Dialect speech translation, (vii) Formality control for speech translation, (viii) Isometric speech translation. A total of 27 teams participated in at least one of the shared tasks. This paper details, for each shared task, the purpose of the task, the data that were released, the evaluation metrics that were applied, the submissions that were received and the results that were achieved.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Technické termíny mohou vyžadovat zvláštní zacházení, pokud je cílová skupina dvojjazyčná, v závislosti na kulturních a vzdělávacích
normách dané společnosti. Zejména některé překladové situace mohou vyžadovat „zachování termínů“, tj. ponechání technických termínů zdrojového jazyka v cílovém jazyce výstupu, aby vznikla plynulá a srozumitelná věta.
Ukazujeme, že standardní model strojového překladu založený na Transformeru lze snadno přizpůsobit k dosažení tohoto cíle, aniž by přitom trpěla kvalita jeho výstupu obecně.
Představujeme anglicko-hindský model, který je natrénovat k uposlechnutí signálu „zachování“, tj. za běhu zajistí, že vybrané termíny nebudou přeloženy. Navrženou metodu vyhodnocujeme automatickými metrikami (BLEU pro překlad obecně, F1 pro zachování termínů), i ručně (celková kvalita výstupních vět).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Technical terms may require special handling
when the target audience is bilingual,
depending on the cultural and educational
norms of the society in question. In particular,
certain translation scenarios may require
“term retention” i.e.
preserving of the
source language technical terms in the target
language output to produce a fluent and
comprehensible code­switched sentence.
We show that a standard transformer­based
machine translation model can be adapted
easily to perform this task with little or no
damage to the general quality of its output.
We present an English­to­Hindi model that is
trained to obey a “retain” signal, i.e. it can
perform the required code­switching on a list
of terms, possibly unseen, provided at runtime.
We perform automatic evaluation using BLEU
as well as F1 metrics on the list of retained
terms; we also collect manual judgments on
the quality of the output sentences.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slovní embeddingy jsou ve zpracování přirozeného jazyka čím dál podstatnější komponentou. Tento článek představuje novou metodu pro přenos statických podslovných embeddings z jazyka s relativním dostatkem zdrojů do jazyka s nedostatkem zdrojů. Primárně pracujeme s jazykovým párem hindština-márátština, přičemž pro márátštinu je nedostatek zdrojů pouze simulovaný a výsledky dále potvrzujme na nepálštině. Náš přístup výrazně překonává baseline fastText pro oba jazyky na úlohách podobnost slov a testování synonymie. Na první úloze je úspěšnost na máráštině dokonce srovnatelná s úspěšností standardní metody při využití o tři řády většího množství dat.


 We primarily work with Hindi-Marathi, simulating a low-resource scenario for Marathi, and confirm observed trends on Nepali. We demonstrate the consistent benefits of unsupervised morphemic segmentation on both source and target sides over the treatment performed by fastText. Our best-performing approach uses an EM-style approach to learning bilingual subword embeddings; we also show, for the first time, that a trivial “copy-and-paste” embeddings transfer based on even
perfect bilingual lexicons is inadequate in cap-
turing language-specific relationships. We find that our approach substantially outperforms the fastText baselines for both Marathi and Nepali on the Word Similarity task as well as WordNet-Based Synonymy Tests; on the former task, its  performance for Marathi is close to that of pretrained fastText embeddings that use three orders of magnitude more Marathi data.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Word embeddings are growing to be a crucial resource in the field of NLP for any language. This work introduces a novel technique for static subword embeddings transfer for Indic languages from a relatively higher resource language to a genealogically related low resource language. We primarily work with Hindi-Marathi, simulating a low-resource scenario for Marathi, and confirm observed trends on Nepali. We demonstrate the consistent benefits of unsupervised morphemic segmentation on both source and target sides over the treatment performed by fastText. Our best-performing approach uses an EM-style approach to learning bilingual subword embeddings; we also
show, for the first time, that a trivial “copy-
and-paste” embeddings transfer based on even perfect bilingual lexicons is inadequate in capturing language-specific relationships. We find that our approach substantially outperforms the fastText baselines for both Marathi and Nepali on the Word Similarity task as well as WordNet-Based Synonymy Tests; on the former task, its performance for Marathi is close to that of pretrained fastText embeddings that use three orders of magnitude more Marathi data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nástroj vytvořený na bázi hlubokého učení nazvaný Word Formation Analyzer for Czech, který na základě vstupního lexému vrátí lemma nebo lemmata, jež tvoří jeho základ. Této úloze říkáme rozpoznávání základových slov. Na základě počtu slov ve výstupní sekvenci a jejím srovnání se vstupem lze pak vstup klasifikovat do jedné ze tří kategorií: kompozitum, derivát, nebo nemotivované. Této úloze říkáme slovotvorná klasifikace. V rozpoznávání základových slov dosáhl Word Formation Analyzer for Czech accuracy 71%, v klasifikaci slovotvorných procesů pak 87%.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a deep-learning tool called Word Formation Analyzer for Czech, which, given an
input lexeme, automatically retrieves the lemma or lemmas from which the input lexeme was
formed. We call this task parent retrieval. Furthermore, based on the number of words in the
output sequence and its comparison to the input, the input word is classified into one of three
categories: compound, derivative or unmotivated. We call this task word formation classification.
In the task of parent retrieval, Word Formation Analyzer for Czech achieved an accuracy of 71%.
In word formation classification, the tool achieved an accuracy of 87%.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nedávný pokrok ve standardizaci anotovaných jazykových zdrojů vedl k úspěšným velkým projektům jako Universal Dependencies (UD), kde se syntakticky anotují data pro mnoho jazyků. Anotace koreference, která spojuje opakované zmínky téže entity v textu a je pro porozumění jazyku velmi důležitá, je zatím standardizačním úsilím relativně nepoznamenaná. V tomto článku prezentujeme CorefUD, mnohojazyčnou sbírku korpusů a standardizovaný formát pro anotaci koreference, kompatibilní s morfosyntaktickou anotací v UD a rozšiřitelný na příbuzné úlohy, jako je rozpoznávání pojmenovaných entit. Jde o první krok směrem ke konvergenci koreferenčních zdrojů napříč jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Recent advances in standardization for annotated language resources have led to successful large scale efforts, such as the Universal Dependencies (UD) project for multilingual syntactically annotated data. By comparison, the important task of coreference resolution, which clusters multiple mentions of entities in a text, has yet to be standardized in terms of data formats or annotation guidelines. In this paper we present CorefUD, a multilingual collection of corpora and a standardized format for coreference resolution, compatible with morphosyntactic annotations in the UD framework and including facilities for related tasks such as named entity recognition, which forms a first step in the direction of convergence for coreference resolution across languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CorefUD je kolekce existujících korpusů s anotací koreference, které jsme převedli na jednotné anotační schéma. Ve své současné verzi 1.0 CorefUD celkem obsahuje 17 korpusů pro 11 jazyků a v porovnání s verzí 0.2 obsahuje přepracovaný formát souborů a opravy řady chyb.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CorefUD is a collection of previously existing datasets annotated with coreference, which we converted into a common annotation scheme. In total, CorefUD in its current version 0.2 consists of 17 datasets for 11 languages, and compared to the version 0.2, the file format has been reworked and a number of annotation errors have been fixed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládáme přehled o stavu výzkumu v oblasti strojového překladu s nízkými zdroji. V současnosti se na světě mluví asi 7 000 jazyky a téměř všem jazykovým párům chybí významné zdroje pro trénování modelů strojového překladu. Vzrůstá zájem o výzkum zabývající se problémem vytváření užitečných překladatelských modelů, když je k dispozici velmi málo přeložených trénovacích dat. Předkládáme shrnutí této aktuální oblasti na vysoké úrovni a poskytujeme přehled osvědčených postupů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a survey covering the state of the art in low-resource machine translation. There
are currently around 7000 languages spoken in the world and almost all language pairs lack
significant resources for training machine translation models. There has been increasing interest
in research addressing the challenge of producing useful translation models when very little
translated training data is available. We present a high level summary of this topical field and
provide an overview of best practices.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Presentation of language tools for Česká spořitelna.
Speech Recognition, Immediate Response Machine Translation, Cross-Lingual Information Retrieval, Question Answering, Automatic Summarization of Meetings.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Prezentace jazykových nástrojů pro Českou spořitelnu.
Rozpoznávání mluvené řeči, strojový překlad v rámci okamžité odpovědi, mezijazyčné vyhledávání informací, odpovídání na otázky, automatické shrnování schůzek.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této kapitole uvádíme hlavní úspěchy české velké výzkumné infrastruktury LINDAT/CLARIAH-CZ. Poskytujeme krátký popis infrastruktury a její historii a stručný popis jejího vědeckého, technologického a infrastrukturního rozsahu. Zaměřujeme se na technologické inovace již implementované v úložišti a v nabídkách služeb a nastiňujeme některé budoucí plány.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this chapter we present the main achievements of the Czech large research infrastructure LINDAT/CLARIAH-CZ. We provide a short description of the infrastructure and its history, and a brief account of its scientific, technological and infrastructural scope. We  focus on the technological innovations already implemented in the repository and in the service offerings, and outline some future plans.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku jsou uvedeny příklady synonymie a homonymie v češtině. Zdrojem homonymie jsou morfologické, syntaktické a lexikální prostředky a jejich kombinace ve větě vedoucí k jejich povrchové identitě. Synonymii demonstrují věty s různými syntaktickými a morfemickými formami vyjadřujícími stejný význam. Místní příslovce s významem Kde? (LOC) jsou zde analyzovány podrobněji. Pozornost je zaměřena na analýzu předložkových konstrukcí s předložkou v(e) + Loc [in] a na + Loc [on], u + Gen [at/by]. Jsou navržena kritéria pro určování synonymie a gramatických požadavků na formu lokálních určení. Analýza se omezuje na úzkou část lokálních významů s cílem představit obě strany jazykového znaku z hlediska jeho asymetrie.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper examples of synonymy and homonymy in Czech are presented. The sources of   homonymy are exemplified by the morphological, syntactic and lexical means and their combinations in the sentence leading to their surface identity. Synonymy is demonstrated by sentences with different syntactic and morphemic forms expressing the same meaning. Local adverbials with the meaning Where? (LOC) are analyzed here in a more detail. Attention is focused on the analysis of the prepositional constructions with the preposition v(e) + Loc [in] and na + Loc [on] and the differences between the construction including them as well as examples of their interchangeability  are demonstrated. Prepositional constructions with the preposition u + Gen [at/by] are added to the central domain of the local adverbials connected with the answer to the question Where? [LOC]. Criteria for the description of grammatical requirements for their form as well as for their lexicalized forms applied in this domain are proposed as the sources of the relation of homonymy and synonymy. The analysis is, however, limited to the narrow part of local meanings with the aim to present both sides of the language signs from the point of view of their asymmetry demonstrated by means of the transparent part of grammatical description.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Náš článek se zaměřuje na vývoj vícejazyčného zdroje dat pro morfologickou segmentaci. Představujeme přehled 17 existujících
datových zdrojů relevantních pro segmentaci ve 32 jazycích a analyzujeme rozmanitost způsobů, jakými jsou v nich jednotlivé jazykové jevy zachyceny. Nechali jsme se inspirovat úspěchem Universal Dependencies a navrhujeme harmonizované schéma pro reprezentaci segmentaci a převádíme data z těchto zdrojů do jednotného schématu. Harmonizované verze zdrojů dostupné pod bezplatnými licencemi jsou publikovány jako kolekce s názvem UniSegments 1.0.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Our work aims at developing a multilingual data resource for morphological segmentation. We present a survey of 17 existing
data resources relevant for segmentation in 32 languages, and analyze diversity of how individual linguistic phenomena are
captured across them. Inspired by the success of Universal Dependencies, we propose a harmonized scheme for segmentation
representation, and convert the data from the studied resources into this common scheme. Harmonized versions of resources
available under free licenses are published as a collection called UniSegments 1.0.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Segmentations (UniSegments) je kolekce lexikálních zdrojů zachycujících morfologické segmentace mnoha jazyků harmonizované do lingvisticky konzistentního anotačního schématu. Anotační schéma je uloženo jednoduchém sloupcovém formátu, přičemž jednotlivé sloupce jsou odděleny tabulátory. K jednotlivým slovům se ukládá jejich morfologická segmentace, včetně různých informací o slovech a segmentovaných jednotkách, např. slovní druhy, typy morfů/morfémů atd. Současná veřejná verze kolekce obsahuje 38 harmonizovaných datových souborů pokrývajících 30 různých jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Segmentations (UniSegments) is a collection of lexical resources capturing morphological segmentations harmonised into a cross-linguistically consistent annotation scheme for many languages. The annotation scheme consists of simple tab-separated columns that stores a word and its morphological segmentations, including pieces of information about the word and the segmented units, e.g., part-of-speech categories, type of morphs/morphemes etc. The current public version of the collection contains 38 harmonised segmentation datasets covering 30 different languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje přehled výsledků soutěže (společné úlohy) ve vícejazyčné automatické analýze koreference, která byla přidružená k workshopu CRAC 2022. Účastníci soutěže měli vyvinout trénovatelné systémy schopné identifikovat zmínky o entitách a shlukovat tyto zmínky na základě identické koreference. Jako zdroj trénovacích a vyhodnocovacích dat byla použita veřejná část CorefUD 1.0, která obsahuje 13 korpusů pro 10 jazyků. Jako hlavní vyhodnocovací metrika bylo použito skóre CoNLL, které se používalo v dřívějších soutěžích zaměřených na koreferenci. 5 účastnických týmů vyvinulo celkem 8 systémů na predikci koreference; kromě toho jsou k dispozici výsledky baseline systému, který je založen na transformerech a byl poskytnut organizátory na začátku soutěže. Vítězný systém překonal baseline o 12 procentních bodů (průměr CoNLL skóre přes korpusy jednotlivých jazyků).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents an overview of the shared task on multilingual coreference resolution associated with the CRAC 2022 workshop. Shared task participants were supposed to develop trainable systems capable of identifying mentions and of clustering the mentions according to identity coreference. The public edition of CorefUD 1.0, which contains 13 datasets for 10 languages was used as the source of training and evaluation data. The CoNLL score used in previous coreference-oriented shared tasks was used as the main evaluation metric. There were 8 coreference prediction systems submitted by 5 participating teams; in addition, there was a competitive transformer-based baseline system provided by the organizers at the beginning of the shared task. The winner system outperformed the baseline by 12 percent points (in terms of the CoNLL scores averaged across all datasets for individual languages).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dobře teoreticky podložená všestranná anotace  je velmi důležitou součástí jazykových korpusů. Jako příklad ve stati uvádíme Pražský závislostní korpus, který obsahuje anotace na několika jazykových rovinách, a tento přístup ilustrujeme na několika případkových studiích z oblasti aktuálního členění. Zároveň na těchto příkladech dokládáme, že práce s anotovaným korpusem může vést k upřesnění výchozí teorie.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A theoretically well-founded annotation of language corpora is a very importatnt component part of such corpora. In our paper, we support this point by the example of the Prague Dependency Treebank  and its annotation scheme. We bring some case studies concerning the information structure of the sentence. At the same time, we also demonstrate that the work with a consistently annotated corpus may lead to some modifications of the original theory.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zamyšlení nad snahou vysvětlit některé pravidelnosti a zdánlivé nepravidelnosti v češtině a jiných slovanských jazycích (především v oblasti deklinační i slovotvorné morfologie) pomocí pravidel, tedy tyto jevy "vypočítat".</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A glimpse at an effort to explain certain regularities as well as (seemingly) irregularities in Czech and other Slavonic languages (first of all in the domain of inflectional and derivational morphology) by means of rules, i.e. to "calculate" these phenomena.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem této kapitoly je ukázat, že dobře navržená a teoreticky podložená korpusová anotace významně přispívá k využití korpusu pro testování lingvistické teorie a pro její další rozvoj. Data
našich analýz pocházejí z korpusové rodiny Prague Dependency Treebank; jde o  jednojazyčná česká data a o paralelní anglicko-česká data, která 
se týkají základní syntaktické úrovně popisu jazyka a anotace struktury diskurzu. Konkrétní případové studie se týkají tří výzkumných otázek, a to (i) sémantické relevance informační struktury věty, (ii) vztahu mezi fokalizátory a diskurzními konektory vzhledem k sémantice diskurzních vztahů a (iii) vztahu mezi primárními a sekundárními spojovacími výrazy. V příloze jsou uvedeny a rozebrány údaje o měření mezianotátorské shody.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of the present chapter is to demonstrate that a well-designed
and theoretically founded corpus annotation contributes significantly to the use of
the corpus for testing a linguistic theory and its further development. The data for
our analyses come from the Prague Dependency Treebank family both monolingual
Czech and parallel English-Czech and concern the underlying syntactic level of
language description and the annotation of discourse structure. In particular, the
case studies concern three research questions, namely (i) the semantic relevance
of information structure of the sentence, (ii) the relation between focus sensitive
particles and discourse connectives with respect to the semantics of discourse
relations, and (iii) the relation between primary and secondary connectives. In the
Appendix, some data on measuring inter-annotator agreement are presented and
discussed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek je zaměřen na komplexní popis zacházení s tzv. funkčními slovy v rámci závislostního funkčního generativního popisu a jeho odraz v Pražském závislostním korpusu. Teoretický rámec i treebank jsou založeny na stratifikačním modelu jazyka, jehož součástí jsou dvě úrovně syntaktické struktury založené na závislosti, jedna orientovaná na povrchovou syntaktickou strukturu věty (analytická rovina) a druhá orientovaná na hloubkovou větnou strukturu (tektogramatická). Závislostní stromová struktura věty na analytické úrovni obsahuje všechna slova přítomná ve větě jako samostatné uzly, zatímco závislostní reprezentace věty na tektogramatické úrovni koncipované jako lingvisticky strukturovaný význam věty obsahuje pouze obsahová slova jako její uzly. Na analytické úrovni se rozlišují různé třídy funkčních slov, přičemž hlavní hranice je mezi funkčními slovy fungujícími v rámci verbálních komplexů a obsahujícími informace o morfosyntaktických vlastnostech sloves a těch, které jsou součástí nominálních skupin (předložky) nebo spojují věty (případně části vět) v jeden celek (spojky). V příspěvku se tvrdí, že pomocná slovesa se považují za závislé na slovesu, ke kterému „patří“, a předložky a spojky se naopak považují za hlavy podstatných jmen, jejichž formu „řídí“ nebo „ovládají“. Na tektogramatické rovině se sémantický přínos funkčních slov k významu věty odráží v informacích připojených k uzlům tektogramatického stromu ve formě komplexních atributů. Pomocná slovesa, předložky a spojky jsou nejzřetelnějšími třídami funkčních slov, existují skupiny také slov, jako jsou částice, které stojí na pomezí mezi funkčními slovy a obsahovými slovy, kterým je ve studii také věnována pozornost.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present contribution is aimed at a complex description of the treatment of the so-called function words within the framework of a dependency-based Functional Generative Description as proposed in Prague by Petr Sgall and his team and its reflection in the Prague Dependency Treebank, an original annotated corpus of Czech. Both the framework and the treebank are based on a stratificational model of language, a part of which are two levels of dependency-based syntactic structure, one oriented towards the syntactic structure of the sentence on the surface layer called analytical and the other oriented towards the underlying, deep sentence structure called tectogrammatical. The dependency tree structure of the sentence on the analytical level contains all the words present in the sentence as separate nodes, while the dependency representation of a sentence on the tectogrammatical level conceived of as a linguistically structured meaning of the sentence contains only content words as its nodes. On the analytical level, a distinction is made between different classes of function words, the main boundary being between the function words functioning within verbal complexes and containing information about the morpho-syntactic properties of verbs (i.e. auxiliaries) and those being parts of nominal groups (prepositions) or connecting clauses (or, as the case may be, parts of clauses) into one whole (conjunctions). It is argued in the paper that auxiliaries should be considered to be dependents on the verb that is their governor and to which they „belong“, and the prepositions and conjunctions, on the contrary, should be considered to be the heads of the nouns or clauses the form of which they “govern” or “control”. On the tectogrammatical level, the semantic contribution of the function words to the meaning of the sentence is reflected by information attached to the nodes of the tectogrammatical tree in the form of complex labels.  Auxiliaries, prepositions and conjunctions are the most evident classes of function words, though there are some groups of words such as particles that stand on the borderline between function words and content words, to which we also pay attention in our study.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V poslední době bylo vyvinuto mnoho korpusů, které obsahují více anotací různých jazykových jevů, od morfologických kategorií slov přes syntaktickou stavbu vět až po diskurz a koreferenční vztahy v textech. Probíhají diskuse o vhodném anotačním schématu pro velké množství různorodých informací. V našem příspěvku vyjadřujeme přesvědčení, že vícevrstvé anotační schéma nabízí pohled na jazykový systém v jeho komplexnosti a v interakci jednotlivých jevů a že existují minimálně dva aspekty, které vícevrstvé anotační schéma podporují: (i) Vícevrstvé anotační schéma umožňuje použít anotaci jedné vrstvy k návrhu anotace další vrstvy (vrstev) jak koncepčně, tak ve formě předanotačního postupu nebo pravidel kontroly anotace. (ii) Vícevrstvé anotační schéma představuje spolehlivý základ pro korpusové studie založené na vlastnostech napříč vrstvami. Tyto aspekty jsou demonstrovány na případu Pražského závislostního korpusu. Jeho více-rovinné anotační schéma obstálo ve zkoušce času a dobře poslouží i pro složité textové anotace, ve kterých se s výhodou používají dřívější morfosyntaktické anotace. Kromě odkazu na předchozí projekty, které využívají toto anotační schéma, uvádíme několik aktuálních výzkumů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Recently, many corpora have been developed that contain multiple annotations of various linguistic phenomena, from morphological categories of words through the syntactic structure of sentences to discourse and coreference relations in texts. Discussions are ongoing on an appropriate annotation scheme for a large amount of diverse information. In our contribution we express our conviction that a multilayer annotation scheme offers to view the language system in its complexity and in the interaction of individual phenomena and there are at least two aspects that support a multilayer annotation scheme:
(i) A multilayer annotation scheme makes it possible to use the annotation of one layer to design the annotation of another layer(s) both conceptually and in a form of a pre-annotation procedure or annotation checking rules.
(ii) A multilayer annotation scheme presents a reliable ground for corpus studies based on features across the layers.
These aspects are demonstrated on the case of the Prague Dependency Treebank. Its multilayer annotation scheme withstood the test of time and serves well also for complex textual annotations, in which earlier morpho-syntactic annotations are advantageously used. In addition to a reference to the previous projects that utilise its annotation scheme, we present several current investigations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>SynSemClass lexicon 4.0 zkoumá s ohledem na kontextově založenou slovesnou synonymii, sémantickou „ekvivalenci“ českých, anglických a německých sloves a jejich valenční chování v paralelních česko-anglických a německo-anglických jazykových zdrojích. SynSemClass 4.0 je vícejazyčná ontologie typu události založená na třídách synonymních významů sloves, doplněná sémantickými rolemi a odkazy na existující sémantické lexikony. Slovník je nejen obohacen o další množství tříd, ale v rámci hierarchizace obsahu byly některé třídy sloučeny. V porovnání se starou verzí obsahuje slovník nově  definice tříd a definice rolí. Kromě již použitých odkazů na položky PDT-Vallex, EngVallex, CzEngVallex, FrameNet, VerbNet, PropBank, Ontonotes a English WordNet pro české a anglické záznamy jsou nové odkazy na německé jazykové lexikální zdroje, jako je Woxikon, E-VALBU a GUP, využívány pro německé slovesné záznamy. Německá část lexikonu byla vytvořena v rámci projektu Multilingual Event-Type-Anchored Ontology for Natural Language Understanding (META-O-NLU) dvěma spolupracujícími týmy - týmem Univerzity Karlovy, Matematicko-fyzikální fakulty, Ústavu formální a aplikované lingvistiky, Praha (ÚFAL), Česká republika a týmem Německého výzkumného centra pro umělou inteligenci (DFKI) Speech and Language Technology, Berlín, Německo.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The SynSemClass synonym verb lexicon version 3.5 investigates, with respect to contextually-based verb synonymy, semantic ‘equivalence’ of Czech, English, and German verb senses and their valency behavior in parallel Czech-English and German-English language resources. SynSemClass4.0 is a multilingual event-type ontology based on classes of synonymous verb senses, complemented with semantic roles and links to existing semantic lexicons. The SynSemClass is not only enriched by an additional number of classes but in the context of content hierarchy, some classes have been merged. Compared to the older version of the lexicon, the novelty is the definitions of classes and the definitions of roles.
Apart from the already used links to PDT-Vallex, EngVallex, CzEngVallex, FrameNet, VerbNet, PropBank, Ontonotes, and English WordNet for Czech and English entries the new links to German language lexical resources are exploited for German verb entries, such as Woxikon, E-VALBU, and GUP. The German part of the lexicon has been created within the project Multilingual Event-Type-Anchored Ontology for Natural Language Understanding (META-O-NLU) by two cooperating teams - the team of the Charles University, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics, Prague (ÚFAL), Czech Republic and the team of the German Research Center for Artificial Intelligence (DFKI) Speech and Language Technology, Berlin, Germany.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme rozšíření ontologie typu události SynSemClass, původně koncipované jako dvojjazyčný česko-anglický zdroj. Do tříd představujících koncepty ontologie jsme přidali německé záznamy. Vzhledem k tomu, že jsme měli jiný výchozí bod než
původní práce (neoznačený paralelní korpus bez odkazů na valenční lexikon a samozřejmě různé existující lexikální zdroje), bylo náročné přizpůsobit pokyny pro anotaci, datový model a nástroje použité pro původní verzi. Popisujeme proces a výsledky práce v takovém nastavení. Dále ukazujeme další kroky k úpravě procesu anotace,
datové struktury a formáty a nástroje nezbytné k tomu, aby přidání nového jazyka v budoucnu bylo plynulejší a efektivnější a případně aby různé týmy mohly pracovat na rozšíření SynSemClass do mnoha jazyků současně.
představujeme nejnovější verzi, která obsahuje výsledky přidání němčiny, volně dostupné ke stažení i pro online přístup.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an extension of the SynSemClass event-type ontology, originally conceived as a bilingual Czech-English resource. We added German entries to the classes representing the concepts of the ontology. Having a different starting point than the original work (unannotated parallel corpus without links to a valency lexicon and, of course, different existing lexical resources), it was a challenge to adapt the annotation guidelines, the data model and the tools used for the original version. We describe the process and results of working in such a setup. We also show the next steps to adapt the annotation process, data structures and formats and tools necessary to make the addition of a new language in the future more smooth and efficient, and possibly to allow for various teams to work on SynSemClass extensions to many languages concurrently. We also present the latest release which contains the results of adding German, freely available for download as well as for online access.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme NomVallex, ručně anotovaný valenční slovník českých substantiv a adjektiv. Slovník je vytvořený v teoretickém rámci Funkčního generativního popisu a je založený na korpusových datech. Obsahuje celkem 1027 lexikálních jednotek v celkovém počtu 570 lexémů, přičemž zahrnuje následující slovnědruhové a derivační kategorie: deverbální a deadjektivní substantiva, a deverbální, desubstantivní, deadjektivní a primární adjektiva. Valenční vlastnosti lexikálních jednotek zachycuje v podobě valenčního rámce, který pro každé valenční doplnění uvádí funktor a množinu možných morfematických vyjádření, a dokládá je pomocí příkladů, které se vyskytly v použitých korpusech. NomVallex je koncipován jako lexikografický zdroj umožňující výzkum valence derivačně příbuzných lexikálních jednotek, proto v relevantních případech poskytuje odkaz od určité lexikální jednotky k odpovídající lexikální jednotce jejího základového slova, obsaženého buď v NomVallexu, nebo – v případě sloves – ve slovníku VALLEX, čímž propojuje až tři slovní druhy, konkrétně substantivum – sloveso, adjektivum – sloveso, substantivum – adjektivum nebo substantivum – adjektivum – sloveso. NomVallex umožňuje srovnání valenčního chování velkého počtu českých substantiv a adjektiv s valencí jejich základových slov, což je jeden
z předpokladů ke zkoumání teoretické otázky dědičnosti valence a k popisu systémového a nesystémového valenčního chování.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present NomVallex, a manually annotated valency lexicon of Czech nouns and adjectives. The lexicon is created in the theoretical
framework of the Functional Generative Description and based on corpus data. In total, NomVallex 2.0 is comprised of 1027 lexical
units contained in 570 lexemes, covering the following part-of-speech and derivational categories: deverbal and deadjectival nouns,
and deverbal, denominal, deadjectival and primary adjectives. Valency properties of a lexical unit are captured in a valency frame
which is modeled as a sequence of valency slots, supplemented with a list of morphemic forms. In order to make it possible to study
the relationship between valency behavior of base words and their derivatives, lexical units of nouns and adjectives in NomVallex are
linked to their respective base words, contained either in NomVallex itself or, in case of verbs, in a valency lexicon of Czech verbs
called VALLEX. NomVallex enables a comparison of valency properties of a significant number of Czech nominals with their base
words, both manually and in an automatic way; as such, we can address the theoretical question of argument inheritance, concentrating
on systemic and non-systemic valency behavior.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>NomVallex 2.0 je ručně anotovaný valenční slovník českých substantiv a adjektiv, vytvořený v teoretickém rámci Funkčního generativního popisu a založený na korpusových datech (korpusy řady SYN Českého národního korpusu a korpus Araneum Bohemicum Maximum). Slovník obsahuje celkem 1027 lexikálních jednotek v celkovém počtu 570 lexémů, přičemž zahrnuje následující slovnědruhové a derivační kategorie: (i) deverbální a deadjektivní substantiva, (ii) deverbální, desubstantivní, deadjektivní a primární adjektiva. Valenční vlastnosti lexikálních jednotek zachycuje v podobě valenčního rámce, který pro každé valenční doplnění uvádí funktor a množinu možných morfematických vyjádření, a dokládá je pomocí příkladů, které se vyskytly v použitých korpusech. NomVallex je koncipován jako lexikografický zdroj umožňující výzkum valence derivačně příbuzných lexikálních jednotek, proto v relevantních případech poskytuje odkaz od určité lexikální jednotky k odpovídající lexikální jednotce jejího základového slova, obsaženého buď v NomVallexu, nebo – v případě sloves – ve slovníku VALLEX, čímž propojuje až tři slovní druhy, konkrétně substantivum – sloveso, adjektivum – sloveso, substantivum – adjektivum nebo substantivum – adjektivum – sloveso. Za účelem umožnit srovnání obsahuje toto vydání rovněž zkrácená hesla základových sloves daných substantiv a adjektiv ze slovníku VALLEX a zjednodušená hesla příslušných substantiv a adjektiv ze slovníku PDT-Vallex.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>NomVallex 2.0 is a manually annotated valency lexicon of Czech nouns and adjectives, created in the theoretical framework of the Functional Generative Description and based on corpus data (the SYN series of corpora from the Czech National Corpus and the Araneum Bohemicum Maximum corpus). In total, NomVallex is comprised of 1027 lexical units contained in 570 lexemes, covering the following parts-of-speech and derivational categories: deverbal or deadjectival nouns, and deverbal, denominal, deadjectival or primary adjectives. Valency properties of a lexical unit are captured in a valency frame (modeled as a sequence of valency slots, each supplemented with a list of morphemic forms) and documented by corpus examples. In order to make it possible to study the relationship between valency behavior of base words and their derivatives, lexical units of nouns and adjectives in NomVallex are linked to their respective base lexical units (contained either in NomVallex itself or, in case of verbs, in the VALLEX lexicon), linking up to three parts-of-speech (i.e., noun – verb, adjective – verb, noun – adjective, and noun – adjective – verb).
In order to facilitate comparison, this submission also contains abbreviated entries of the base verbs of these nouns and adjectives from the VALLEX lexicon and simplified entries of the covered nouns and adjectives from the PDT-Vallex lexicon.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V první části přednášky představíme projekt Universal Dependencies. Ve druhé se zaměříme na parsery natrénované na českých korpusech v UD a na jejich využití při budování staročeského treebanku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the first part of the talk we will present the Universal Dependencies project. In the second part we will focus on parsers trained on the Czech corpora in UD and their usage in building an Old Czech treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je mezinárodní komunitní projekt a sbírka morfosyntakticky anotovaných dat (treebanků) pro více než 100 jazyků. Tato sbírka je neocenitelným zdrojem pro různé jazykovědné studie od gramatických konstrukcí v jednom jazyku po jazykovou typologii, dokumentaci ohrožených jazyků a historického vývoje jazyka.
V tomto tutoriálu nejdříve rychle představím hlavní principy UD, potom ukážu vlastní data a různé nástroje, které jsou k dispozici pro práci s nimi: parsery, dávkové procesory, vyhledávače a prohlížeče.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is an international community project and a collection of morphosyntactically annotated data sets (“treebanks”) for more than 100 languages. The collection is an invaluable resource for various linguistic studies, ranging from grammatical constructions within one language to language typology, documentation of endangered languages, and historical evolution of language.
In the tutorial, I will first quickly show the main principles of UD, then I will present the actual data and various tools that are available to work with it: parsers, batch processors, search engines and viewers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je šestnácté vydání treebanků UD, verze 2.10.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the sixteenth release of UD Treebanks, Version 2.10.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce se zaměřuje na detekci zdrojů v českých článcích publikovaných na zpravodajském serveru Českého veřejnoprávního rozhlasu. Hledáme zejména přiřazení ve větách a rozeznáváme přiřazené zdroje a jejich větný kontext(signály). Zorganizovali jsme crowdsourcingovou anotační úlohu, jejímž výsledkem byl datový soubor 2 167 článků s ručně rozpoznanými signály a zdroji. Zdroje byly navíc zařazeny do kategorií jmenovaných a nejmenovaných zdrojů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper focuses on detection of sources in the Czech articles published on a news server of Czech public radio. In particular, we search for attribution in sentences and we recognize attributed sources and their sentence context (signals). We organized a crowdsourcing annotation task that resulted in a data set of 2,167 stories with manually recognized signals and sources. In addition, the sources were classified into the classes of named and unnamed sources.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vstupní data, jednotlivé experimentální anotace a úplný a podrobný přehled naměřených výsledků souvisejících s experimentem zabývajícím se vlivem automatické před-anotace na kvalitu a efektivitu anotačního úsilí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Input data, individual experimental annotations, and a complete and detailed overview of the measured results related to the experiment dealing with the influence of automatic pre-annotation on the quality and efficiency of manual annotation efforts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje analýzu anotace pomocí automatické před-anotace pro úkol anotace závislostní syntaxe. Porovnává úsilí anotátorů, kteří používají před-anotovanou verzi (s vysoce přesným parserem), a úsilí vytvořené plně ruční anotací. Cílem experimentu je posoudit výslednou kvalitu anotace při použití před-anotace. Kromě toho hodnotí vliv automatických lingvisticky založených (pravidlově formulovaných) kontrol a jiné anotace na stejných datech, kterou mají anotátoři k dispozici, a jejich vliv na kvalitu a efektivitu anotace. Experiment potvrdil, že pre-anotace je účinným nástrojem pro rychlejší manuální syntaktickou anotaci, která zvyšuje konzistenci výsledné anotace bez snížení její kvality.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents an analysis of annotation using an automatic pre-annotation for a mid-level annotation complexity task - dependency syntax annotation. It compares the annotation efforts made by annotators using a pre-annotated version (with a high-accuracy parser) and those made by fully manual annotation. The aim of the experiment is to judge the final annotation quality when pre-annotation is used. In addition, it evaluates the effect of automatic linguistically-based (rule-formulated) checks and another annotation on the same data available to the annotators, and their influence on annotation quality and efficiency. The experiment confirmed that the pre-annotation is an efficient tool for faster manual syntactic annotation which increases the consistency of the resulting annotation without reducing its quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Naše studie vznikla jako první pokus zmapovat podhoubí české pedagogiky digitálních humanitních věd v roce 2018. Máme za to, že české paměťové instituce jsou už dobře integrovány do evropských infrastrukturních projektů (například Knihovna Akademie věd ČR, Národní knihovna a Národní muzeum v projektu Europeana ) a čeští badatelé se již také úspěšně připojili k mezinárodní DH komunitě (o čemž svědčí také jejich příspěvky v tomto svazku). Zatím však málo víme o DH paradigmatu ve výuce humanitních oborů na českých vysokých školách.Proto jsme během roku 2018 ručně prohledali aktuální studijní katalogy většiny českých veřejných vysokých škol s cílem shromáždit kontakty na vyučující, kteří by mohli stát u zrodu digitálně humanitní pedagogické komunity v Česku, a udělat si představu o jejich specializacích. K agregaci informací jsme použili Latentně sémantickou analýzu na názvech kurzů a institucí, kde se vyučují.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Our study was created as the first attempt to map the Czech pedagogy of Digital Humanities in 2018. We believe that Czech memory institutions are already well integrated into European infrastructure projects (eg Library of the Academy of Sciences, National Library and National Museum in Europeana) and Czech researchers have also successfully joined the international DH community (as evidenced by their contributions in this volume). So far, however, we know little about the DH paradigm in the teaching of humanities at Czech universities. To aggregate the information, we used Latent semantic analysis on the names of the courses and institutions where they are taught.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Studie se zabývá českými konverzními dvojicemi, v nichž podstatné jméno i sloveso mají dějový význam (test 'zkoušet.n' - testovat 'zkoušet'). V návaznosti na předchozí výzkum prototypických případů deverbální a denominální konverze v češtině je směr v těchto dvojicích určován na základě toho, zda sloveso tvoří svůj vidový protějšek změnou kmenotvorné přípony (což je charakteristické pro deverbální směr), nebo zda sufixální protějšek není k dispozici (typické pro denominální slovesa). Analýza provedená na korpusovém vzorku 1 300 podstatných jmen s dějovým významem a přímo souvisejících sloves ukazuje, že dvojice založené na domácích kořenech mají většinou rysy deverbálního tvoření, zatímco denominální směr se uplatňuje v menší skupině domácích dvojic a v datech s cizími kořeny jasně převažuje. Denominální směr přisuzovaný dvojicím s cizími kořeny je v souladu s typologickou hypotézou, že slovesa jsou přejímána do češtiny spíše jako podstatná jména a následně se v cílovém jazyce mění na slovesa.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The study deals with Czech conversion pairs of a noun and a verb, both of which denote actions (test ‘test.n’ – testovat ‘to test’). Elaborating on previous research on prototypical verb-to-noun and noun-to-verb conversion in Czech, the direction in these pairs is determined based on whether the verb forms its aspectual counterpart by changing the theme (which is characteristic of the deverbal direction), or whether the suffixed counterpart is not available (typical of denominal verbs). The analysis, carried out on a corpus sample of 1,300 action nouns and directly related verbs, demonstrates that pairs with native roots mostly conform to the deverbal pattern, whereas the denominal direction applies to a smaller subset of the native sample but clearly prevails in the data with foreign roots. The denominal direction ascribed to foreign pairs is consistent with the typological hypothesis that verbs are borrowed rather as nouns and subsequently turned into verbs in the target language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přestože dějový význam je prototypicky přisuzován slovesu, existují slovesa, u nichž se předpokládá, že jsou odvozena nebo konvertována od podstatných jmen s dějovým významem. Předkládaný příspěvek se zabývá českými konverzními dvojicemi, kde jak podstatné jméno, tak sloveso vyjadřují děj (např. řez - řezat, test - testovat). V návaznosti na náš předchozí výzkum tvoření slov bez derivačních afixů v češtině jsou pro určení směru konverze v jednotlivých dvojicích využity fonologické a morfologické vlastnosti podstatných jmen a sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Although actions are prototypical verbal concepts, there are verbs that are assumed to be derived or converted from nouns with action meanings, cf. apology > to apologize or attack.n > to attack. Conversion of action nouns to verbs, discussed as the PERFORMATIVE category by Plag (1999) or Bauer et al. (2013), has been attested since the earliest period documented by the Oxford English Dictionary and, remarkably, have become predominant among verbs converted from nouns in English during the 20th century (Gottfurcht 2008).
The present paper deals with Czech conversion pairs, where both the noun and the verb denote an action (e.g. řez ‘cut.n’ – řezat ‘to cut’, test ‘test.n’ – testovat ‘to test’). Elaborating on our previous research on word formation without derivational affixes in Czech, phonological and morphological features of nouns and verbs are employed to determine the direction of conversion in individual pairs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Studie se zabývá anglickými konverzními dvojicemi podstatných a sloves, které mají formálně i významově blízké protějšky v češtině. Cílem studie je prozkoumat, jak se tato substantiva a slovesa, která jsou v angličtině a češtině spojena podobnými sémantickými vztahy, chovají v jazycích, které mají odlišnou morfologickou strukturu a v jejichž slovotvorných systémech hraje konverze odlišnou roli. Dvojice substantiv a sloves, extrahované z Britského národního korpusu a z korpusu SYN2000, jsou popisována jako dvoučlenná paradigmata a zkoumány spolu s vybranými deriváty. Data ukazují, že v českých dvojicích jsou pro vyjadřování konkrétních významů preferována substantiva před slovesy a většina sloves je používána jako denominální formace, často odlišně od anglických protějšků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The study deals with English noun/verb conversion pairs that have both formally and semantically close counterpart pairs in Czech. The study’s aim is to examine how these nouns and verbs, linked with similar semantic relations in English and Czech, are accommodated in the two languages with different morphological structures and conversion playing a different role. The noun/verb pairs, extracted from the British National Corpus and from the SYN2000 corpus, are analysed as two-cell paradigms and examined along with selected derivatives. The data suggest that in the Czech sample, nominals are preferred over verbs in expressing the particular meanings and most verbs appear as denominal formations, often differently from their English counterparts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce představuje korpus ParlaMint obsahující přepisy ze zasedání 17 evropských národních parlamentů s půl miliardou slov. Korpusy jsou jednotně kódovány, obsahují bohatá metadata o 11 tisících mluvčích a jsou lingvisticky anotovány podle Universal Dependencies a s pojmenovanými entitami. Vzorky korpusů a konverzních skriptů jsou k dispozici v úložišti projektu na GitHub a kompletní korpusy jsou otevřeně k dispozici prostřednictvím úložiště CLARIN.SI ke stažení, stejně jako prostřednictvím NoSketch Engine a KonText a rozhraní Parlameter pro on-line průzkum a analýzu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the ParlaMint corpora containing transcriptions of the sessions of the 17 European national parliaments with half a billion words. The corpora are uniformly encoded, contain rich meta-data about 11 thousand speakers, and are linguistically annotated following the Universal Dependencies formalism and with named entities. Samples of the corpora and conversion scripts are available from the project’s GitHub repository, and the complete corpora are openly available via the CLARIN.SI repository for download, as well as through the NoSketch Engine and KonText concordancers and the Parlameter interface for on-line exploration and analysis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek je zasazen do kontextu projektu SSHOC a jeho cílem je prozkoumat, jak mohou jazykové technologie pomoci při podpoře a usnadnění vícejazyčnosti v sociálních a humanitních vědách (SSH). Přestože většina výzkumných pracovníků v oblasti SSH vytváří kulturně a společensky relevantní práce ve svých místních jazycích, metadata a slovníky používané v oblasti SSH k popisu a indexování výzkumných dat jsou v současné době většinou v angličtině. Zkoumáme proto přístupy zpracování přirozeného jazyka a strojového překladu s cílem poskytnout zdroje a nástroje na podporu vícejazyčného přístupu k obsahu SSH a jeho vyhledávání v různých jazycích. Jako případové studie vytváříme a poskytujeme jako volně dostupná data soubor vícejazyčných metadatových konceptů a automaticky extrahovanou vícejazyčnou terminologii Data Stewardship. Tyto dvě případové studie umožňují také vyhodnotit výkonnost nejmodernějších nástrojů a odvodit soubor doporučení, jak je nejlépe použít. Přestože nejsou přizpůsobeny konkrétní doméně, použité nástroje se ukázaly být platným přínosem pro překladatelské úlohy. Nicméně ověření výsledků experty na danou doménu, kteří ovládají daný jazyk, je nevyhnutelnou fází celého pracovního postupu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper is framed in the context of the SSHOC project and aims at exploring how Language Technologies can help in promoting and facilitating multilingualism in the Social Sciences and Humanities (SSH). Although most SSH researchers produce culturally and societally relevant work in their local languages, metadata and vocabularies used in the SSH domain to describe and index research data are currently mostly in English. We thus investigate Natural Language Processing and Machine Translation approaches in view of providing resources and tools to foster multilingual access and discovery to SSH content across different languages. As case studies, we create and deliver as freely, openly available data a set of multilingual metadata concepts and an automatically extracted multilingual Data Stewardship terminology. The two case studies allow as well to evaluate performances of state-of-the-art tools and to derive a set of recommendations as to how best apply them. Although not adapted to the specific domain, the employed tools prove to be a valid asset to translation tasks. Nonetheless, validation of results by domain experts proficient in the language is an unavoidable phase of the whole workflow.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace jazykových nástrojů systému Elitr a generování scénářů THEaiTRE pro účastníky EUROSAI kongresu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presentation of Elitr language tools and THEaiTRE script generation for EUROSAI congress attendees.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Hackování jazykového modelu GPT-2
Sestavení webové aplikace
Vytváří se scénář divadelní hry
Představení hry na jevišti
Demo nástroje</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Hacking the GPT-2 language model
Building a web application
Generating a theatre play script
Performing the play on stage
Demo of the tool</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace interaktivního generování scnářů divadelních her pro zaměstnance společnosti Novartis.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presentation of interactive theatre script generation for employees of the Novartis company.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Experimentujeme s adaptací generativních jazykových modelů pro generování dlouhých souvislých vyprávění v podobě divadelních her. Protože plně automatické generování celých her není v současné době proveditelné, vytvořili jsme interaktivní nástroj, který umožňuje lidskému uživateli poněkud nasměrovat generování a zároveň minimalizovat zásahy. Pro generování dlouhých textů sledujeme dva přístupy: ploché generování se sumarizací kontextu a hierarchický dvoufázový přístup text-to-text, kdy je nejprve vygenerována synopse a poté použita k podmiňování generování konečného scénáře. Naše předběžné výsledky a diskuse s divadelními profesionály ukazují zlepšení oproti základnímu generování, ale také identifikují důležitá omezení našeho přístupu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We experiment with adapting generative language models for the generation of long coherent narratives in the form of theatre plays. Since fully automatic generation of whole plays is not currently feasible, we created an interactive tool that allows a human user to steer the generation somewhat while minimizing intervention. We pursue two approaches to long-text generation: a flat generation with summarization of context, and a hierarchical text-to-text two-stage approach, where a synopsis is generated first and then used to condition generation of the final script. Our preliminary results and discussions with theatre professionals show improvements over vanilla language model generation, but also identify important limitations of our approach.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme volně dostupné online demo THEaiTRobot, dvojjazyčný nástroj s otevřeným zdrojovým kódem pro interaktivní generování scénářů divadelních her, ve dvou verzích. THEaiTRobot 1.0 používá jazykový model GPT-2 s minimálními úpravami. THEaiTRobot 2.0 používá dva modely vytvořené doladěním GPT-2 na cíleně shromážděných a zpracovaných datových souborech a několika dalších komponentách, které hierarchicky generují scénáře divadelních her (název → synopse → skript). Podkladový nástroj se používá v projektu THEaiTRE pro generování scénářů divadelních her, které pak na jevišti hraje profesionální divadlo.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a free online demo of THEaiTRobot, an open-source bilingual tool for interactively generating theatre play scripts, in two versions. THEaiTRobot 1.0 uses the GPT-2 language model with minimal adjustments. THEaiTRobot 2.0 uses two models created by fine-tuning GPT-2 on purposefully collected and processed datasets and several other components, generating play scripts in a hierarchical fashion (title → synopsis → script). The underlying tool is used in the THEaiTRE project to generate scripts for plays, which are then performed on stage by a professional theatre.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme velký a různorodý český korpus pro opravu gramatických chyb s cílem přispět ke stále nedostatkovým datovým zdrojům v této doméně pro jiné jazyky než angličtinu. Korpus pro gramatickou opravu chyb pro češtinu (GECCC) nabízí čtyři domény, které pokrývají distribuci chyb od esejů s vysokou hustotou chyb napsaných nerodilými mluvčími až po texty webových stránek, kde jsou chyby mnohem méně časté. Porovnáváme několik českých GEC systémů, včetně několika na bázi architektury Transformer, a nastavujeme tak silnou baseline pro budoucí výzkum. V neposlední řadě také provádíme meta-evaluaci běžných GEC metrik pomocí ručního hodnocení na našich datech. Nový český GEC korpus zveřejňujeme pod licencí CC BY-SA 4.0 na adrese http://hdl.handle.net/11234/1-4639.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a large and diverse Czech corpus annotated for grammatical error correction (GEC) with the aim to contribute to the still scarce data resources in this domain for languages other than English. The Grammar Error Correction Corpus for Czech (GECCC) offers a variety of four domains, covering error distributions ranging from high error density essays written by non-native speakers, to website texts, where errors are expected to be much less common. We compare several Czech GEC systems, including several Transformer-based ones, setting a strong baseline to future research. Finally, we meta-evaluate common GEC metrics against human judgements on our data. We make the new Czech GEC corpus publicly available under the CC BY-SA 4.0 license at http://hdl.handle.net/11234/1-4639.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V ParlaMint I, projektu podporovaném CLARIN-ERIC v době pandemie, byla v roce 2021 vyvinuta a vydána sada srovnatelných a jednotně anotovaných vícejazyčných korpusů pro 17 národních parlamentů. Pro roky 2022 a 2023 byl projekt rozšířen na ParlaMint II, opět s finanční podporou CLARIN-ERIC, s cílem rozšířit stávající korpusy o nová data a metadata; aktualizovat schéma XML; přidat korpusy pro 10 nových parlamentů; poskytnout více scénářů aplikace a provést další experimenty. Dokument informuje o těchto plánovaných krocích, včetně některých, které již byly podniknuty, a nastiňuje budoucí plány.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In ParlaMint I, a CLARIN-ERIC supported project in pandemic times, a set of comparable and uniformly annotated multilingual corpora for 17 national parliaments were developed and released in 2021. For 2022 and 2023, the project has been extended to ParlaMint II, again with the CLARIN ERIC financial support, in order to enhance the existing corpora with new data and metadata; upgrade the XML schema; add corpora for 10 new parliaments; provide more application scenarios and carry out additional experiments. The paper reports on these planned steps, including some that have already been taken, and outlines future plans.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve většině modelů kombinujících jazyk a vizuální informaci (Vision-Language, VL) je porozumění struktuře obrazu umožněno přidáním informací o poloze objektů v obraze. V naší případové studii se věmnujeme modelu VL modelu LXMERT a zkoumáme použití jakým způsobem poziční informaci používá a studujeme její vliv na úspěšnost v úloze odpovídání otázek o obrázcích. Ukazujeme, že model není schopen poziční informaci využít pro přiřazování textů k obrázkům, pokud se texty liší polohou objektů. A to i přesto, že další experimenty ale ukazují, že PI je v modelech skutečně přítomna. Představujeme dvě strategie, jak se s tímto problémem vypořádat: (i) předtrénování s přidanou informací o poloze a (ii) kontrastní učení s porovnáváním napříč modalitami. Tímto způsobem může model správně klasifikovat, zda se obrázky s podrobnými výroky PI shodují. Kromě 2D informací o objektech na obrázku, přidáváme hloubku objektu pro lepší lokalizaci v prostoru. Přestože se nám podařilo zlepšit vlastnosti modelu, na kvalitu odpovídání otázek to má jen zanedbatelný vliv. Naše výsledky tak poukazují na důležitý problém multimodálního modelování: pouhá přítomnost informace detekovatelné klasifikátorem není zárukou, že tato informace je k dispozici napříč modalitami.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In most Vision-Language models (VL), the understanding of the image structure is enabled by injecting the position information (PI) about objects in the image. In our case study of LXMERT, a state-of-the-art VL model, we probe the use of the PI in the representation and study its effect on Visual Question Answering. We show that the model is not capable of leveraging the PI for the image-text matching task on a challenge set where only position differs. Yet, our experiments with probing confirm that the PI is indeed present in the representation. We introduce two strategies to tackle this: (i) Positional Information Pre-training and (ii) Contrastive Learning on PI using Cross-Modality Matching. Doing so, the model can correctly classify if images with detailed PI statements match. Additionally to the 2D information from bounding boxes, we introduce the object’s depth as new feature for a better object localization in the space. Even though we were able to improve the model properties as defined by our probes, it only has a negligible effect on the downstream performance. Our results thus highlight an important issue of multimodal modeling: the mere presence of information detectable by a probing classifier is not a guarantee that the information is available in a cross-modal setup.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku popisujeme naše předložení k Simultaneous Speech Translation na
IWSLT 2022. Zkoumáme strategie využití offline modelu v simultánním prostředí bez nutnosti upravovat původní model. V našich experimentech ukazujeme, že náš online algoritmus je téměř na stejné úrovni jako offline nastavení, přičemž je 3× rychlejší než offline, pokud jde o latenci na testovací sadě. Náš systém zpřístupňujeme veřejnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we describe our submission to the Simultaneous Speech Translation at IWSLT 2022. We explore strategies to utilize an offline model in a simultaneous setting without the need to modify the  riginal model. In our experiments, we show that our onlinization algorithm is almost on par with the offline setting while being 3×  faster than offline in terms of latency on the test set. We make our system publicly available.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Sumarizace schůzek se primárně zaměřuje spíše na aktuální téma než na plynulost nebo koherenci. Je to náročný a zdlouhavý úkol, i když se shrnutí schůzek vytváří ručně. Výsledná shrnutí se liší v cílech, stylu a jsou nevyhnutelně velmi subjektivní kvůli člověku, který je ve smyčce. Pro vytvoření adekvátních a informativních shrnutí je rovněž nezbytné uvědomění si kontextu schůzky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Meeting summarization is primarily focused on topi cal coverage rather than on fluency or coherence. It is a challenging and tedious task, even when meeting summaries are created manually. The resulting sum maries vary in the goals, style, and they are inevitably very subjective due to the human in the loop. Also, the awareness of the context of the meeting is essential to create adequate and informative summaries.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Práce se zabývá paradigmatickým systémem české flexe z perspektivy distribuční sémantiky. Využíváme rozsáhlé morfologické zdroje a korpusy k získání modelů distribuční sémantiky češtiny a zkoumáme chování vybraných morfosémantických vlastností českých podstatných a přídavných jmen.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The work deals with the paradigmatic system of Czech inflexion from the perspective of distributional semantics. We use extensive morphological and corpus resources available for Czech to obtain models of the Czech distributional vector space and examine the behaviour of selected morphosyntactic features of Czech nouns and adjectives.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace shrnuje dosavadní práci našeho grantového týmu START na tématu kompetice ve slovotvorbě. Prezentují se jak jazykové zdroje, tak metodologie používané v rámci dané výzkumné oblasti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presentation summarises the work of our START grant team on the topic of competition in word formation. It presents language resources as well as the methodology used in the mentioned research area.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Práce se zabývá hranicí mezi flektivní a derivační morfologií v češtině, zvláště pak z perspektivy distribuční sémantiky. Využíváme rozsáhlé morfologické zdroje a korpusy k získání modelů distribuční sémantiky češtiny a zkoumáme kolekci 24 typů morfologických kontrastů reprezentujících jak kanonickou flexi a kanonickou derivaci, tak různé typy mezních případů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The work deals with the borderline between inflexion and derivation in Czech, especially from the perspective of distributional semantics. We use extensive morphological and corpus resources available for Czech to obtain models of the Czech distributional vector space and examine a collection of 24 types of morphological contrasts exemplifying canonical inflexion, canonical derivation, and different types of intermediate cases.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slova jsou v jazycích provázána slovotvornými vztahy, např. sloveso examplify a substantivum examples obě souvisí s example, přičemž uvedené sloveso z něj vzniklo odvozením a substantivní tvar inflexí. Mezi jazykovými zdroji pro ruštinu je inflexe pokrytá dostatečně, nicméně derivace je pokryta datovými zdroji daleko omezeněji. Tento článek je věnován vylepšení metody konstrukce derivačních sítí a aplikaci tohoto postupu na ruštinu, vedoucí k vytvoření dosud největšího datového zdroje ruských derivačních relací. Výsledná databáze DeriNet.RU obsahuje víc než 300 tisíc lemmat spojených s více než 164 tisíci slovotvornými relacemi. Pro vytvoření takových dat jsme použili metody strojového učení. Databáze je zveřejněna pod otevřenou licencí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Words of any language are to some extent related thought the ways they are formed. For instance, the verb exempl-ify and the noun example-s are both based on the word example, but the verb is derived from it, while the noun is inflected. In Natural Language Processing of Russian, the inflection is satisfactorily processed; however, there are only a few machine-tractable resources that capture derivations even though Russian has both of these morphological processes very rich. Therefore, we
devote this paper to improving one of the methods of constructing such resources and to the application of the method to a Russian lexicon, which results in the creation of the largest lexical resource of Russian derivational relations. The resulting database dubbed DeriNet.RU includes more than 300 thousand lexemes connected with more than 164 thousand binary
derivational relations. To create such data, we combined the existing machine-learning methods that we improved to manage this goal. The whole approach is evaluated on our newly created data set of manual, parallel annotation. The resulting
DeriNet.RU is freely available under an open license agreement.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme soubor dat Eyetracked Multi-Modal Translation (EMMT), který obsahuje záznamy monokulárních očních pohybů, zvuková data a data 4elektrodového nositelného elektroencefalogramu (EEG) 43 účastníků, kteří se věnovali překladu z angličtiny do češtiny, a to na základě psaného textu a doprovodných obrázků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present Eyetracked Multi-Modal Translation (EMMT), a dataset containing monocular eye movement recordings, audio data and 4-electrode wearable electroencephalogram (EEG) data of 43 participants while engaged in sight translation task supported by an image.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem této experimentální studie je prozkoumat proces překladu z angličtiny do češtiny v multimodálním scénáři s využitím eye trackeru. Zkoumáme specifické aspekty překladu nejednoznačných a jednoznačných vět a současně se zaměřujeme na možný vliv vizuálních informací na proces překladu. Ukazujeme tak, jak lze na základě různých údajů o pohybu očí zkoumat mechanismy vizuálního vyhledávání, jakož i mechanismy přítomnosti a pozornosti zapojené do těchto překladatelských procesů, tj. kognitivní mechanismy zapojené do čtení originálních vět a vytváření odpovídajícího překladu studujeme pomocí několika metrik specifických pro sledování očí. Článek mimo jiné ukazuje, jak se v experimentálním uspořádání projevuje Stroopův efekt.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This experimental study aims to investigate the translation process from English to Czech in a multimodal scenario by using an eye tracker.
We investigate specific aspects of translating ambiguous and unambiguous sentences, and simultaneously, we focus on the possible impact of visual information on the translation process. Thus, we show how mechanisms of visual search, as well as the presence and attention mechanisms involved in such translation processes, can be explored based on various eye-movement data, i.e., cognitive mechanisms involved in reading original sentences and producing the corresponding translation are studied using a plethora of eye-tracking-specific metrics.
Among other things, the paper demonstrates how the Stroop effect is visible in the experimental setup.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Data sledování očí jsou velmi užitečným zdrojem informací pro studium kognice a zejména porozumění jazyku u lidí. V tomto článku popisujeme naše systémy pro sdílenou úlohu CMCL 2022 o předpovídání informací o sledování očí. Popisujeme naše experimenty s předem připravenými modely, jako jsou BERT a XLM, a různé způsoby, jak jsme tyto reprezentace použili k predikci čtyř funkcí sledování očí. Spolu s analýzou účinku použití dvou různých druhů předtrénovaných vícejazyčných jazykových modelů a různých způsobů sdružování reprezentací na úrovni tokenů také zkoumáme, jak kontextové informace ovlivňují výkon systémů. Nakonec také zkoumáme, zda faktory, jako je rozšíření jazykových informací, ovlivňují předpovědi. Naše příspěvky dosáhly průměrného MAE 5,72 a umístily se na 5. místě ve sdíleném úkolu. Průměrný MAE ukázal další snížení na 5,25 při hodnocení po úkolu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Eye-Tracking data is a very useful source of information to study cognition and especially language comprehension in humans. In this paper, we describe our systems for the CMCL 2022 shared task on predicting eye-tracking information. We describe our experiments with pretrained models like BERT and XLM and the different ways in which we used those representations to predict four eye-tracking features. Along with analysing the effect of using two different kinds of pretrained multilingual language models and different ways of pooling the tokenlevel representations, we also explore how contextual information affects the performance of the systems. Finally, we also explore if factors like augmenting linguistic information affect the predictions. Our submissions achieved an average MAE of 5.72 and ranked 5th in the shared task. The average MAE showed further reduction to 5.25 in post task evaluation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve hře Shannon je cílem uhodnout další písmeno ve větě na základě předchozího kontextu.
Od té doby se stal široce známým myšlenkovým experimentem, na kterém jsou založeny koncepty v psycholingvistice, počítačové lingvistice a zpracování přirozeného jazyka.
Tuto hru rozšiřujeme o volitelnou extra modalitu ve formě obrázků a provádíme experiment na lidských účastnících.

Zjistili jsme, že přítomnost obrázku výrazně zvyšuje důvěru a přesnost uživatelů ve všech POS.
To zahrnuje determinanty (a, an, the), které by jinak měly být predikovány výhradně z předchozího (levého) kontextu věty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the Shannon game, the goal is to guess the next letter in a sentence based on the previous context.
It has since become a widely known thought experiment on which concepts in psycholinguistics, computational linguistics and natural language processing are based.
We extend this game by including an optional extra modality in the form of images and run an experiment on human participants.

We find that the presence of an image greatly improves users' confidence and accuracy accross all POS.
This includes determiners (a, an, the), which should otherwise be predicted solely from the previous (left) context of the sentence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Není jasné, zda, jak a kde velké předem trénované jazykové modely zachycují jemné lingvistické rysy, jako je nejednoznačnost, gramatika a složitost vět.
Prezentujeme výsledky automatické klasifikace těchto znaků a porovnáváme jejich životaschopnost a vzorce napříč typy reprezentace.
Ukazujeme, že datové sady založené na šablonách s artefakty na úrovni povrchu by neměly být používány pro sondování,
měla by být provedena pečlivá srovnání se základními hodnotami
a že grafy t-SNE by se neměly používat k určení přítomnosti rysu mezi reprezentacemi hustých vektorů. Také ukazujeme, jak mohou být prvky vysoce lokalizovány ve vrstvách těchto modelů a ztratit se v horních vrstvách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>It is unclear whether, how and where large pre-trained language models capture subtle linguistic traits like ambiguity, grammaticality and sentence complexity.
We present results of automatic classification of these traits and compare their viability and patterns across representation types.
We demonstrate that template-based datasets with surface-level artifacts should not be used for probing,
careful comparisons with baselines should be done
and that t-SNE plots should not be used to determine the presence of a feature among dense vectors representations. We also demonstrate how features might be highly localized in the layers for these models and get lost in the upper layers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Neurální reprezentace v multimodálním a vícejazyčném modelování (NEUREM3) je projekt financovaný Grantovou agenturou ČR (GAČR) program Výzkum, experimentální vývoj a inovace pro podporu grantových projektů základního výzkumu EXPRO 2019 od ledna 2019 do prosince 2023. Tato zpráva se týká prvních tří let 2019--2021. Vědecká práce se soustředila na 5 širokých oblastí: Základy, Interpretovatelnost a závislost na úkolu, Těsná integrace, Robustnost a Vztah neurálních reprezentací k vícejazyčným konceptům. Jeho popis je seskupený podle 5 úkolů definovaných v návrhu projektu a několika technických témat. V několika z nich jsme dosáhli nad rámec nejmodernějších výsledků. Zahraniční spolupráce na projektu byla intenzivní od najímání zahraničních specialistů, přes studentské stáže v respektovaných zahraničních laboratořích až po synergie s projekty EU a USA. Tým projektu je konsolidovaný a má dobrou rovnováhu mezi špičkovými PI a co-PI, výzkumníky/postdoktorandy a českými i zahraničními doktorandy. Projekt je konkurenceschopný na mezinárodní úrovni, hodnocení probíhá prostřednictvím mezinárodních technologických hodnocení (výzev), bibliografických metrik a pořádání špičkových mezinárodních vědeckých akcí. Tým NEUREM3 efektivně spolupracuje a je jádrem budování české i mezinárodní řečové/NLP/MT komunity. Dosud vedl projekt k celkem 96 publikacím, z nichž 12 bylo v recenzovaných časopisech, 49 na špičkových konferencích a 35 na místních workshopech, challenge a evaluačních workshopech atd. Pravidelně také zveřejňujeme výzkumná data a kód v otevřená úložiště.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Neural Representations in Multi-modal and Multi-lingual Modeling (NEUREM3) is a
project funded by the Czech Science Foundation (GAČR) program “Research, Experimental Development and Innovation for the Support of Basic Research Grant Projects”
– EXPRO 2019 from January 2019 till December 2023. This report covers its first three
years, 2019–2021. The scientific work was articulated around 5 broad areas: Foundations, Interpretability and task-dependence, Tight integration, Robustness, and Relation
of neural representations to multi-lingual concepts. Its description is clustered according
to 5 tasks defined in the project proposal and several technical topics. In several of these,
we have reached beyond state-of-the-art results. Foreign cooperation in the project was
intensive ranging from hiring foreign specialists, through student interns in respected foreign laboratories, to synergies with EU and US projects. The team of the project is consolidated and has a good balance of top-class PI and co-PI, researchers/post-docs, and
both Czech and international PhD students. The project is competitive on the international level, the assessment is done via international technology evaluations (challenges),
bibliographic metrics, and organization of top international scientific events. NEUREM3
team efficiently cooperates and is at the core of building both Czech and international
speech/NLP/MT communities.
So far, the project led to a total of 96 publications of which 12 were in peer-reviewed
journals, 49 at top conferences, and 35 at local workshops, challenge and evaluation workshops, etc. We are also regularly releasing research data and code in open repositories.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se věnuje textovým konektorům, jejichž pozice v textu se liší od obvyklých pozic spojek a textově-propojovacích částic a adverbií. Jsou to takové konektory, které se nevyskytují ani v jednom z textových segmentů, které propojují - a jako takové představují výzvu pro automatickou analýzu diskurzu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper investigates discourse connectives whose position in a text deviates from the usual setting - namely connectives that occur in neither of the two arguments - and as such present a challenge for discourse parsers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Při trénování systémů pro generování textu z dat na konkrétní doméně dochází k nadměrnému přizpůsobování modelů reprezentaci dat a opakování chyb v trénovacích datech na výstupu. Zkoumáme, jak se obejít bez dotrénovávání jazykových modelů na datasetech pro tuto úlohu a zároveň přitom využít schopností těchto modelů pro povrchovou realizaci. Inspirováni sekvenčními přístupy navrhujeme generovat text transformací krátkých textů pro jednotlivé položky pomocí posloupnosti modulů natrénovaných na obecných textových operacích: řazení, agregaci a kompresi odstavců. Modely pro provádění těchto operací trénujeme na syntetickém korpusu WikiFluent, který pro tento účel vytváříme z anglické Wikipedie. Naše experimenty na dvou významných datasetech pro převod RDF trojic na text — WebNLG a E2E — ukazují, že náš přístup umožňuje generování textu z RDF trojic i při absenci trénovacích dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In data-to-text (D2T) generation, training on in-domain data leads to overfitting to the data representation and repeating training data noise. We examine how to avoid finetuning pretrained language models (PLMs) on D2T generation datasets while still taking advantage of surface realization capabilities of PLMs. Inspired by pipeline approaches, we propose to generate text by transforming single-item descriptions with a sequence of modules trained on general-domain text-based operations: ordering, aggregation, and paragraph compression. We train PLMs for performing these operations on a synthetic corpus WikiFluent which we build from English Wikipedia. Our experiments on two major triple-to-text datasets — WebNLG and E2E — show that our approach enables D2T generation from RDF triples in zero-shot settings.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Efektivní modely strojového překladu jsou komerčně důležité, protože mohou zvýšit rychlost překladu a snížit náklady a emise uhlíku. V poslední době je velký zájem o neautoregresivní (NAR) modely, které slibují rychlejší překlad. Paralelně s výzkumem modelů NAR proběhly úspěšné pokusy o vytvoření optimalizovaných autoregresních modelů v rámci sdíleného úkolu WMT o efektivním překladu. V tomto článku poukazujeme na nedostatky v metodice hodnocení v literatuře o modelech NAR a poskytujeme spravedlivé srovnání mezi nejmodernějším modelem NAR a autoregresivními příspěvky ke sdílenému úkolu. Zastáváme důsledné hodnocení modelů NAR a také důležitost porovnávání modelů NAR s jinými široce používanými metodami pro zlepšení efektivity. Provádíme experimenty s modelem NAR založeným na konekcionisticko-temporální klasifikaci (CTC) implementovaným v C++ a porovnáváme jejich čas s autoregresivními modely AR. Naše výsledky ukazují, že ačkoli jsou modely NAR rychlejší na GPU, s malými velikostmi dávek, jsou téměř vždy pomalejší za reálnějších podmínek použití. V budoucí práci požadujeme realističtější a rozsáhlejší hodnocení modelů NAR.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Efficient machine translation models are commercially important as they can increase inference speeds, and reduce costs and carbon emissions. Recently, there has been much interest in non-autoregressive (NAR) models, which promise faster translation. In parallel to the research on NAR models, there have been successful attempts to create optimized autoregressive models as part of the WMT shared task on efficient translation. In this paper, we point out flaws in the evaluation methodology present in the literature on NAR models and we provide a fair comparison between a state-of-the-art NAR model and the autoregressive submissions to the shared task. We make the case for consistent evaluation of NAR models, and also for the importance of comparing NAR models with other widely used methods for improving efficiency. We run experiments with a connectionist-temporal-classification-based (CTC) NAR model implemented in C++ and compare it with AR models using wall clock times. Our results show that, although NAR models are faster on  GPUs, with small batch sizes, they are almost always slower under more realistic usage conditions. We call for more realistic and extensive evaluation of NAR models in future work.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Úspěch hlubokého učení NLP je často popisován tak, že o jazyku nic nepředpokládáme a necháváme data mluvit sama za sebe. Ačkoli je to na mnoha úrovních diskutabilní, jedna věc je neobyčejně podezřelá: většina nejmodernějších NLP modelů předpokládá existenci diskrétních tokenů a používá segmentaci na podslova, která kombinuje pravidla s jednoduchými statistickými heuristikami. Vyhnout se explicitní segmentaci vstupi je obtížnější, než se zdá.

První část přednášky představí neurální neuronovou edistační vzdálenost, novou interpretovatelnou architekturu založenou na dobře známé Levenshteinově vzdálenosti, kterou lze použít pro čistě znakové úlohy, jako je transliterace nebo detekce kognátů. Ve druhé části přednášky si přiblížíme neuronový strojový překlad na úrovni znaků. Představíme, jak inovace v oblasti trénování a návrhu architektur mohou zlepšit kvalitu překladu. I přes tento pokrok se ukazuje, že metody na úrovni znaků ve strojovém překladu stále zaostávají za modely na bázi podslov téměř ve všech ohledech, které lze měřit.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The success of deep learning NLP is often narrated as not assuming anything about the language and letting the data speak for itself. Although this is debatable on many levels, one thing is outstandingly suspicious: most state-of-the-art NLP models assume the existence of discrete tokens and use subword segmentation which combines rules with simple statistical heuristics. Avoiding explicit input segmentations is more difficult than it seems.

The first part of the talk will present neural edit distance, a novel interpretable architecture based on well-known Levenshtein distance that can be used for purely character-level tasks such as transliteration or cognate detection. In the second part of the talk, we will zoom out and have a look at character-level methods for neural machine translation. We will present how innovations in training and architectures design can improve translation quality. Despite this progress, we will show that character-level methods in machine translation still lack behind the subword-based models nearly in all respect that can be measured.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Navrhujeme model neuronová editační vzdálenost pro párování řetězců a převod řetězců na základě naučené editační vzdálenosti. Upravili jsme původní MT algoritmus tak, aby využíval diferencovalnou ztrátovou funkci, což nám umožňuje integrovat ji do neuronové sítě poskytující kontextovou reprezentaci vstupu. Hodnotíme detekci kognatů, transliteraci a konverzi grafémů na fonémy a ukazujeme, že v jednom teoretickém rámci připravovat modely, kde jde proti sobě intepretovatelnost a přesnost. Pomocí kontextových reprezentací, které jsou ale hůře interpretovatelné, dosahuje stejné přesnosti jako nejlepší metody pro párování řetězců. Pomocí statických embedingů a mírně odlišné ztrátové funkce dokážeme vynutit interpretabilitu na úkor poklesu přesnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We propose the neural string edit distance model for string-pair matching and string transduction based on learnable string edit distance. We modify the original expectation-maximization learned edit distance algorithm into a differentiable loss function, allowing us to integrate it into a neural network providing a contextual representation of the input. We evaluate on cognate detection, transliteration, and grapheme-to-phoneme conversion, and show that we can trade off between performance and interpretability in a single framework. Using contextual representations, which are difficult to interpret, we match the performance of state-of-the-art string-pair matching models. Using static embeddings and a slightly different loss function, we force interpretability, at the expense of an accuracy drop.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku prezentuje přehled literatury a empirický průzkum, který kriticky hodnotí předchozí práci v oblasti strojového překladu na úrovni znaků. Navzdory tvrzením v literatuře, že systémy na úrovni znaků jsou srovnatelné se systémy, které pracují na úrovni podslov, prakticky nikdy se nepoužívají v soutěžních systémech WMT. Empiricky ukazujeme, že i s nedávnými inovacemi v modelování zpracování přirozeného jazyka na úrovni znaků se systémy strojového překladu na úrovni znaků stále obtížně vyrovnávají svým protějškům na bázi podslov. Strojový překlad na úrovni znaků nevykazuje ani lepší doménovou robustnost, ani lepší morfologické zobecnění, přestože to bývá často hlavní motivace pro jejich vývoj. Systémy zpracovávající vstup po znacích naopak vykazují velkou robustnost vůči šumu a že kvalita překladu neklesá ani s klesající mírou ořezávání během dekódování.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a literature and empirical survey that critically assesses the state of the art in character-level modeling for machine translation (MT). Despite evidence in the literature that character-level systems are comparable with subword systems, they are virtually never used in competitive setups in WMT competitions. We empirically show that even with recent modeling innovations in character-level natural language processing, character-level MT systems still struggle to match their subword-based counterparts. Character-level MT systems show neither better domain robustness, nor better morphological generalization, despite being often so motivated. However, we are able to show robustness towards source side noise and that translation quality does not degrade with increasing beam size at decoding time.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>jen anglický abstrakt k dispozici</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Semantics is a central concept to natural language understanding.
A qualitative measure to perceive language semantics is to measure the similarity between texts. Semantic Textual Similarity finds applications in almost
all major areas of Natural Language Processing (NLP) including textual entailment, summarization, machine translation, etc. Despite major progress in
the recent years, especially with the intrusion of deep learning in NLP, mea-
suring semantic textual similarity, especially for longer texts, is still a chal-
lenging and open research problem. The ambiguous nature of language makes
semantic similarity more challenging and difficult to model. It is trivial to
produce examples of sentence pairs which are superficially similar but bear a
very different meaning and vice-versa. To address this problem, several type
of methods have been proposed over the years which span from traditional
knowledge-based methods to recent ones relying on deep neural networks and
large volumes of that data. Here in this work, we present a comprehensive sys-
tematic review of the various semantic similarity techniques. We emphasize the
various paradigms of semantic similarity measurement and resources available
to aid further research on this crucial topic. Our systematic literature review
would aid the reader to know about the evolution of semantic similarity techniques, the latest state-of-the-art methods, datasets to work-upon, challenges
and hence identify the research gaps.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nalezení rodokmenu výzkumného tématu je klíčové pro pochopení předchozího stavu umění a postupujícího vědeckého posunu. Záplava odborných článků ztěžuje nalezení nejvhodnější předchozí práce. Výzkumníci tak tráví značné množství času sestavováním seznamu literatury. Citace hrají zásadní roli při objevování relevantní literatury. Nicméně ne všechny citace jsou vytvořeny stejně. Většina citací, které noviny obdrží, poskytuje podkladové a kontextové informace citujícím dokumentům. V těchto případech není citovaný dokument ústředním tématem citujících listin. Některé dokumenty však vycházejí z daného papíru, čímž se dále posouvá hranice výzkumu. V těchto případech hraje dotčený citovaný dokument v citujícím dokumentu stěžejní roli. Podstata citace, kterou prvně jmenovaný obdrží od druhého, je tedy významná. V této práci probíráme naše výzkumy směřující k objevení významných citací daného dokumentu. Dále ukazujeme, jak můžeme využít významné citace k sestavení výzkumné linie prostřednictvím významného citačního grafu. Účinnost naší myšlenky demonstrujeme dvěma případovými studiemi v reálném životě. Naše experimenty přinášejí slibné výsledky, pokud jde o současný stav klasifikace významných citací, předčí ty předchozí s relativním náskokem 20 bodů, pokud jde o přesnost. Předpokládáme, že takový automatizovaný systém může usnadnit vyhledávání příslušné literatury a pomoci identifikovat tok znalostí pro určitou kategorii papírů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Finding the lineage of a research topic is crucial for understanding the prior state of the art and advancing scientific displacement. The deluge of scholarly articles makes it difficult to locate the most relevant previous work. It causes researchers to spend a considerable amount of time building up their literature list. Citations play a crucial role in discovering relevant literature. However, not all citations are created equal. The majority of the citations that a paper receives provide contextual and background information to the citing papers. In those cases, the cited paper is not central to the theme of citing papers. However, some papers build upon a given paper, further the research frontier. In those cases, the concerned cited paper plays a pivotal role in the citing paper. Hence, the nature of citation the former receives from the latter is significant. In this work, we discuss our investigations towards discovering significant citations of a given paper. We further show how we can leverage significant citations to build a research lineage via a significant citation graph. We demonstrate the efficacy of our idea with two real-life case studies. Our experiments yield promising results with respect to the current state-of-the-art in classifying significant citations, outperforming the earlier ones by a relative margin of 20 points in terms of precision. We hypothesize that such an automated system can facilitate relevant literature discovery and help identify knowledge flow for a particular category of papers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nalezení rodokmenu výzkumného tématu je klíčové pro pochopení předchozího stavu umění a postupujícího vědeckého posunu. Záplava odborných článků ztěžuje vyhledávání nejvýznamnějších předchozích prací a způsobuje, že výzkumníci tráví značné množství času sestavováním seznamu literatury. Citace hrají významnou roli při objevování relevantní literatury. Nicméně ne všechny citace jsou vytvořeny stejně. Většina citací, které redakce obdrží, slouží k poskytování kontextuálních a podkladových informací citujícím listinám a nejsou ústředním tématem těchto listin. Některé práce jsou však pro citující noviny stěžejní a inspirují nebo brzdí výzkum v citujících novinách. Z toho vyplývá, že podstata citace, kterou prvně jmenovaný obdrží od druhého, je významná. V tomto rozpracovaném dokumentu diskutujeme o naší předběžné myšlence vytvořit rodokmen pro daný výzkum prostřednictvím identifikace významných citací. Předpokládáme, že takový automatizovaný systém může usnadnit vyhledávání příslušné literatury a pomoci identifikovat tok znalostí alespoň pro určitou kategorii papírů. Distálním cílem této práce je zjistit skutečný dopad výzkumné práce nebo zařízení mimo přímá citační čísla.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Finding the lineage of a research topic is crucial for understanding the prior state of the art and advancing scientific displacement. The deluge of scholarly articles makes it difficult to locate the most relevant prior work and causes researchers to spend a considerable amount of time building up their literature list. Citations play a significant role in discovering relevant literature. However, not all citations are created equal. A majority of the citations that a paper receives are for providing contextual, and background information to the citing papers and are not central to the theme of those papers. However, some papers are pivotal to the citing paper and inspire or stem up the research in the citing paper. Hence the nature of citation the former receives from the latter is significant. In this work in progress paper, we discuss our preliminary idea towards establishing a lineage for a given research via identifying significant citations. We hypothesize that such an automated system can facilitate relevant literature discovery and help identify knowledge flow for at least a certain category of papers. The distal goal of this work is to identify the real impact of research work or a facility beyond direct citation counts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Speciální zasedání SummDial o sumarizaci dialogů a setkání více stran se konalo prakticky v rámci konference SIGDial 2021 dne 29. července 2021. SummDial @ SIGDial 2021 si kladl za cíl spojit komunity zabývající se řečí, dialogem a sumarizací, aby se podpořilo vzájemné opylování myšlenek a podpořily diskuse/spolupráce při pokusu o tento zásadní a aktuální problém. Když pandemie omezila většinu našich osobních interakcí, současný scénář donutil lidi přejít na virtuální, což vyústilo v zahlcení informacemi z častých dialogů a setkání ve virtuálním prostředí. Shrnutí by mohlo pomoci snížit kognitivní zátěž účastníků, nicméně sumarizace projevů více stran přináší vlastní soubor výzev. Speciální zasedání SummDial si kladlo za cíl využít komunitní zpravodajství k nalezení efektivních řešení a zároveň brainstorming o budoucnosti intervencí umělé inteligence na zasedáních a dialozích. O výsledcích zvláštního zasedání informujeme v tomto článku. Speciální sekci SummDial jsme uspořádali pod záštitou projektu H2020 European Live Translator (ELITR) financovaného EU.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The SummDial special session on summarization of dialogues and multi-party meetings was held virtually within the SIGDial 2021 conference on July 29, 2021. SummDial @ SIGDial 2021 aimed to bring together the speech, dialogue, and summarization communities to foster cross-pollination of ideas and fuel the discussions/collaborations to attempt this crucial and timely problem. When the pandemic has restricted most of our in-person interactions, the current scenario has forced people to go virtual, resulting in an information overload from frequent dialogues and meetings in the virtual environment. Summarization could help reduce the cognitive burden on the participants; however, multi-party speech summarization comes with its own set of challenges. The SummDial special session aimed to leverage the community intelligence to find effective solutions while also brainstorming the future of AI interventions in meetings and dialogues. We report the findings of the special session in this article. We organized the SummDial special session under the aegis of the EU-funded H2020 European Live Translator (ELITR) project.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento výstup představuje tři podrobné případové studie pro každou z hlavních tematických oblastí úkolu SSHOC 3.1 "Vícejazyčné terminologie", jejichž cílem je prozkoumat přístupy NLP a MT s ohledem na poskytování zdrojů a nástrojů pro podporu vícejazyčného přístupu k obsahu SSH v různých jazycích a zlepšení vyhledávání pro nerodilé mluvčí.  Soubor vícejazyčných metadatových konceptů, vícejazyčných slovníků a automaticky extrahovaných vícejazyčných terminologií byl dodán jako volně dostupná data, plně odpovídající zásadám FAIR prosazovaným v rámci EOSC, která lze nalézt prostřednictvím VLO a dalších služeb CLARIN a SSHOC.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The deliverable presents three detailed case studies for each of the main topical areas of SSHOC Task 3.1 “Multilingual Terminologies'' aiming to investigate NLP and MT approaches  in  view  of  providing resources  and  tools  to  foster  multilingual  access  to  SSH  content  across  different  languages  and improve  discovery  by  non-native  speakers.  A  set  of  multilingual  metadata  concepts,  multilingual vocabularies  and  automatically  extracted  multilingual  terminologies  has  been  delivered  as  freely, openly available data, fully corresponding to the FAIR  principles promoted within the EOSC, findable through the VLO and other CLARIN and SSHOC services.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozpoznávání ručně psané hudby je náročná úloha, které může vést ke zlepšení dostupnosti archivních rukopisů nebo usnadnění hudební kompozice. Moderních metody strojového učení však nelze na tuto úlohu snadno aplikovat kvůli omezené dostupnosti kvalitních trénovacích dat. Ruční anotace takových dat je drahá, a proto není v potřebném měřítku proveditelná. Tento problém již byl v jiných oblastech vyřešen trénováním  na automaticky generovaných syntetických datech. V tomto článku používáme stejný přístup k rozpoznávání ručně psané hudby a představujeme metodu generování syntetických snímků ručně psaných hudebních zápisů a ukazujeme, že trénování na těchto datech vede k výborným výsledkům.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Handwritten music recognition is a challenging task that could be of great use if mastered, e.g., to improve the accessibility of archival manuscripts or to ease music composition. Many modern machine learning techniques, however, cannot be easily applied to this task because of the limited availability of high-quality training data. Annotating such data manually is expensive and thus not feasible at the necessary scale. This problem has already been tackled in other fields by training on automatically generated synthetic data. We bring this approach to handwritten music recognition and present a method to generate synthetic handwritten music images (limited to monophonic scores) and
show that training on such data leads to state-of-the-art results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nedávný výbuch falešných informací na sociálních
média vedla k intenzivnímu výzkumu automatických modelů fake news detection a ověřovačů faktů. Falešné zprávy a dezinformace,
vzhledem ke své zvláštnosti a rychlému šíření, představovaly mnoho
zajímavé výzvy v oblasti zpracování přirozeného jazyka (NLP)
a komunity Machine Learning (ML). Přípustná literatura
ukazuje, že neotřelé informace zahrnují moment překvapení,
což je hlavní charakteristika pro zesílení a
viralita dezinformací. Román a emocionální informace
přitahuje okamžitou pozornost čtenáře. Emoce jsou
prezentace určitého pocitu nebo sentimentu. Sentiment pomáhá
jedince, který by vyjadřoval své emoce prostřednictvím výrazu a
proto spolu tyto dvě věci souvisejí. Tedy novinka v novince
a následně zjištění emočního stavu a pocitu
čtečka vypadá jako tři klíčové ingredience, těsně spojené
s dezinformacemi. V tomto dokumentu navrhujeme hluboký multiúkol
učící se model, který společně provádí detekci novot, emocí
rozpoznávání, předpovídání nálad a odhalování dezinformací.
Náš navrhovaný model dosahuje nejmodernějších parametrů (SOTA) pro detekci falešných zpráv na třech srovnávacích datasetech,
viz. ByteDance, Fake News Challenge(FNC), a Covid-Stance
s 11,55%, 1,58% a 21,76% zlepšením přesnosti,
respektive. Navrhovaný přístup také ukazuje na účinnost
rámec jednoho úkolu se ziskem přesnosti 11,53, 28,62,
a 14,31 procentního bodu u výše uvedených tří souborů údajů. Zdrojový kód je dostupný na https://github.com/Nish-19/MultitaskFake-News-NES</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The recent explosion in false information on social
media has led to intensive research on automatic fake news detection models and fact-checkers. Fake news and misinformation,
due to its peculiarity and rapid dissemination, have posed many
interesting challenges to the Natural Language Processing (NLP)
and Machine Learning (ML) community. Admissible literature
shows that novel information includes the element of surprise,
which is the principal characteristic for the amplification and
virality of misinformation. Novel and emotional information
attracts immediate attention in the reader. Emotion is the
presentation of a certain feeling or sentiment. Sentiment helps
an individual to convey his emotion through expression and
hence the two are co-related. Thus, Novelty of the news item
and thereafter detecting the Emotional state and Sentiment of
the reader appear to be three key ingredients, tightly coupled
with misinformation. In this paper we propose a deep multitask
learning model that jointly performs novelty detection, emotion
recognition, sentiment prediction, and misinformation detection.
Our proposed model achieves the state-of-the-art(SOTA) performance for fake news detection on three benchmark datasets,
viz. ByteDance, Fake News Challenge(FNC), and Covid-Stance
with 11.55%, 1.58%, and 21.76% improvement in accuracy,
respectively. The proposed approach also shows the efficacy over
the single-task framework with an accuracy gain of 11.53, 28.62,
and 14.31 percentage points for the above three datasets. The source code is available at https://github.com/Nish-19/MultitaskFake-News-NES</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Falešné zprávy nebo dezinformace jsou informace nebo příběhy záměrně vytvořené s cílem oklamat nebo uvést čtenáře v omyl. V dnešní době se platformy sociálních médií staly zralým důvodem k dezinformacím a během několika minut je rozšířily, což vedlo k chaosu, panice a potenciálním zdravotním rizikům mezi lidmi. Rychlé šíření a plodný nárůst šíření falešných zpráv a dezinformací vytváří časově nejkritičtější výzvy pro komunitu zpracování přirozeného jazyka (NLP). Z příslušné literatury vyplývá, že přítomnost momentu překvapení v příběhu je silnou hnací silou rychlého šíření dezinformací, které přitahuje okamžitou pozornost a vyvolává ve čtenáři silné emocionální podněty. Falešné příběhy nebo falešné informace jsou psány, aby vzbudily zájem a aktivovaly emoce lidí, aby je šířili. Falešné příběhy mají tedy vyšší úroveň novosti a emočního obsahu než pravdivé příběhy. Z toho vyplývá, že novost zpravodajského příspěvku a rozpoznání emočního stavu čtenáře po přečtení příspěvku jsou dva klíčové úkoly, které úzce souvisejí s odhalováním dezinformací. Předchozí literatura nezkoumala detekci dezinformací vzájemným učením pro detekci novot a rozpoznávání emocí podle našeho nejlepšího vědomí. Naše současná práce tvrdí, že společné učení novosti a emocí z cílového textu je pádným argumentem pro odhalování dezinformací. V tomto dokumentu navrhujeme hluboký víceúčelový vzdělávací rámec, který společně provádí detekci novot, rozpoznávání emocí a detekci dezinformací. Náš hluboký multitask model dosahuje nejmodernějších výkonů (SOTA) pro detekci falešných zpráv na čtyřech srovnávacích datasetech, viz. ByteDance, FNC, Covid-Stance a FNID s přesností 7,73%, 3,69%, 7,95% a 13,38%. Z hodnocení vyplývá, že náš víceúkolový vzdělávací rámec zlepšuje výkon oproti jednoúkolovému rámci pro čtyři datové soubory o 7,8 %, 28,62 %, 11,46 % a 15,66 % celkového zisku přesnosti. Tvrdíme, že textová novinka a emoce jsou dva klíčové aspekty, které je třeba zvážit při vývoji automatického mechanismu detekce falešných zpráv. Zdrojový kód je dostupný na adrese https://github.com/Nish-19/Misinformation-Multitask-Attention-NE.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Fake news or misinformation is the information or stories intentionally created to deceive or mislead the readers. Nowadays, social media platforms have become the ripe grounds for misinformation, spreading them in a few minutes, which led to chaos, panic, and potential health hazards among people. The rapid dissemination and a prolific rise in the spread of fake news and misinformation create the most time-critical challenges for the Natural Language Processing (NLP) community. Relevant literature reveals that the presence of an element of surprise in the story is a strong driving force for the rapid dissemination of misinformation, which attracts immediate attention and invokes strong emotional stimulus in the reader. False stories or fake information are written to arouse interest and activate the emotions of people to spread it. Thus, false stories have a higher level of novelty and emotional content than true stories. Hence, Novelty of the news item and recognizing the Emotional state of the reader after reading the item seems two key tasks to tightly couple with misinformation Detection. Previous literature did not explore misinformation detection with mutual learning for novelty detection and emotion recognition to the best of our knowledge. Our current work argues that joint learning of novelty and emotion from the target text makes a strong case for misinformation detection. In this paper, we propose a deep multitask learning framework that jointly performs novelty detection, emotion recognition, and misinformation detection. Our deep multitask model achieves state-of-the-art (SOTA) performance for fake news detection on four benchmark datasets, viz. ByteDance, FNC, Covid-Stance and FNID with 7.73%, 3.69%, 7.95% and 13.38% accuracy gain, respectively. The evaluation shows that our multitask learning framework improves the performance over the single-task framework for four datasets with 7.8%, 28.62%, 11.46%, and 15.66% overall accuracy gain. We claim that textual novelty and emotion are the two key aspects to consider while developing an automatic fake news detection mechanism. The source code is available at https://github.com/Nish-19/Misinformation-Multitask-Attention-NE.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jednou z časově nejkritičtějších výzev pro komunitu zpracování přirozeného jazyka (NLP) je boj proti šíření falešných zpráv a dezinformací. Stávající přístupy k odhalování dezinformací využívají modely neurální sítě, statistické metody, jazykové znaky, strategie ověřování faktů atd. Zdá se však, že hrozba falešných zpráv s příchodem humorných a neobvykle kreativních jazykových modelů sílí. Z příslušné literatury vyplývá, že jedním z hlavních rysů virality falešných zpráv je přítomnost momentu překvapení v příběhu, který přitahuje okamžitou pozornost a vyvolává ve čtenáři silné emocionální podněty. Při této práci tuto myšlenku zužitkujeme a navrhujeme jako dva úkoly související s automatickou detekcí dezinformací detekci textových novinek a předvídání emocí. Pro detekci novot používáme znovu účelové textové obnosy a k třídění falešných informací využíváme modely trénované na rozsáhlých datových souborech obnosů a emocí. Naše výsledky korelují s myšlenkou, protože dosahujeme nejmodernějších výsledků (SOTA) ve čtyřech rozsáhlých souborech dezinformačních dat (7,92%, 1,54%, 17,31% a 8,13% zlepšení z hlediska přesnosti). Doufáme, že naše současná sonda bude motivovat komunitu k dalšímu výzkumu odhalování dezinformací v tomto směru. Zdrojový kód je dostupný na GitHub.2</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>One of the most time-critical challenges for the Natural Language Processing (NLP) community is to combat the spread of fake news and misinformation. Existing approaches for misinformation detection use neural network models, statistical methods, linguistic traits, fact-checking strategies, etc. However, the menace of fake news seems to grow more vigorous with the advent of humongous and unusually creative language models. Relevant literature reveals that one major characteristic of the virality of fake news is the presence of an element of surprise in the story, which attracts immediate attention and invokes strong emotional stimulus in the reader. In this work, we leverage this idea and propose textual novelty detection and emotion prediction as the two tasks relating to automatic misinformation detection. We re-purpose textual entailment for novelty detection and use the models trained on large-scale datasets of entailment and emotion to classify fake information. Our results correlate with the idea as we achieve state-of-the-art (SOTA) performance (7.92%, 1.54%, 17.31% and 8.13% improvement in terms of accuracy) on four large-scale misinformation datasets. We hope that our current probe will motivate the community to explore further research on misinformation detection along this line. The source code is available at the GitHub.2</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Počítačová lingvistika vytváří modely, které jsou užitečné při zpracování a generování jazyka a které mohou zlepšit naše porozumění jazykovým jevům. Z pohledu počítačového zpracování představují jazyková data výzvu hlavně kvůli jejich různému stupni idiosynkrazie (nečekané vlastnosti, které má jen několik podobných objektů) a kvůli všudypřítomným nekompozicionálním jevům, jako jsou víceslovné výrazy (jejichž význam nelze jednoduše odvodit z významů jednotlivých částí, např. angl. red tape, by and large, to pay a visit a to pull one’s leg).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Computational linguistics builds models that can usefully process and produce language and that can increase our understanding of linguistic phenomena. From the computational perspective, language data are particularly challenging notably due to their variable degree of idiosyncrasy (unexpected properties shared by few peer objects), and the pervasiveness of non-compositional phenomena such as multiword expressions (whose meaning cannot be straightforwardly deduced from the meanings of their components, e.g. red tape, by and large, to pay a visit and to pull one’s leg) and constructions (conventional associations of forms and meanings). Additionally, if models and methods are to be consistent and valid across languages, they have to face specificities inherent either to particular languages, or to various linguistic traditions.
These challenges were addressed by the Dagstuhl Seminar 21351 entitled "Universals of Linguistic Idiosyncrasy in Multilingual Computational Linguistics", which took place on 30-31 August 2021. Its main goal was to create synergies between three distinct though partly overlapping communities: experts in typology, in cross-lingual morphosyntactic annotation and in multiword expressions. This report documents the program and the outcomes of the seminar. We present the executive summary of the event, reports from the 3 Working Groups and abstracts of individual talks and open problems presented by the participants.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jedním z nejobtížnějších aspektů současné sumarizace zpráv z jednoho dokumentu je to, že souhrn často obsahuje "extrinsické halucinace", tj. fakta, která nejsou obsažena ve zdrojovém dokumentu a která jsou často odvozena prostřednictvím znalosti světa. To způsobuje, že se sumarizační systémy chovají spíše jako otevřené jazykové modely se sklonem k halucinacím faktů, které jsou chybné. V tomto článku tento problém zmírňujeme pomocí doplňkových zdrojových dokumentů, které pomáhají při řešení úlohy. Představujeme novou datovou sadu MiRANews a srovnáváme stávající sumarizační modely.  Na rozdíl od vícedokumentové sumarizace, která se zabývá více událostmi z několika zdrojových dokumentů, se stále zaměřujeme na generování souhrnu pro jeden dokument. Pomocí analýzy dat ukazujeme, že na vině nejsou jen modely: více než 27 % faktů uvedených ve zlatých souhrnech MiRANews je lépe podloženo v pomocných dokumentech než v hlavních zdrojových článcích. Analýza chyb vygenerovaných souhrnů z předtrénovaných modelů dotrénovaných na MiRANews ukazuje, že to má na modely ještě větší vliv: asistovaná sumarizace snižuje počet halucinací o 55 % ve srovnání s modely pro sumarizaci jednotlivých dokumentů trénovanými pouze na hlavním článku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>One of the most challenging aspects of current single-document news summarization is that the summary often contains `extrinsic hallucinations', i.e., facts that are not present in the source document, which are often derived via world knowledge. This causes summarisation systems to act more like open-ended language models tending to hallucinate facts that are erroneous. In this paper, we mitigate this problem with the help of multiple supplementary resource documents assisting the task. We present a new dataset MiRANews and benchmark existing summarisation models.  In contrast to multi-document summarization, which addresses multiple events from several source documents, we still aim at generating a summary for a single document. We show via data analysis that it's not only the models which are to blame: more than 27% of facts mentioned in the gold summaries of MiRANews are better grounded on assisting documents than in the main source articles. An error analysis of generated summaries from pretrained models fine-tuned on MiRANews reveals that this has an even bigger effects on models: assisted summarisation reduces 55% of  hallucinations when compared to single-document summarisation models trained on the main article only.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme model AggGen (vyslovuje se "again"), který do neuronových systémů pro převod dat na text znovu zavádí dvě explicitní fáze plánování vět: řazení a agregaci vstupů. Na rozdíl od předchozích prací využívajících plánování vět je náš model stále end-to-end: AggGen provádí plánování vět současně s generováním textu tím, že se učí latentní zarovnání (prostřednictvím sémantických faktů) mezi vstupní reprezentací a cílovým textem. Experimenty na datech ze soutěží WebNLG a E2E ukazují, že díky použití zarovnání na základě faktů je náš přístup interpretovatelnější, expresivnější, odolnější vůči šumu a snadněji kontrolovatelný, přičemž si zachovává výhody end-to-end systémů z hlediska plynulosti. Náš kód je k dispozici na adrese https://github.com/XinnuoXu/AggGen.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present AggGen (pronounced ‘again’) a data-to-text model which re-introduces two explicit sentence planning stages into neural data-to-text systems: input ordering and input aggregation. In contrast to previous work using sentence planning, our model is still end-to-end: AggGen performs sentence planning at the same time as generating text by learning latent alignments (via semantic facts) between input representation and target text. Experiments on the WebNLG and E2E challenge data show that by using fact-based alignments our approach is more interpretable, expressive, robust to noise, and easier to control, while retaining the advantages of end-to-end systems in terms of fluency. Our code is available at https://github.com/XinnuoXu/AggGen.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci popisujeme naše systémové předkládání úkolu SemEval 2021 11: NLP Contribution Graph Challenge. Vyzkoušíme všechny tři dílčí úkoly výzvy a podáme zprávu o výsledcích. Cílem dílčího úkolu 1 je identifikovat přispívající věty v dané publikaci. Dílčí úkol č. 2 vyplývá z dílčího úkolu č. 1, jehož cílem je vyjmout vědecký termín a predikovat věty z označených přispívajících vět. Závěrečný dílčí úkol 3 spočívá ve vyjmutí trojic (předmět, predikát, objekt) z frází a jejich kategorizaci do jedné nebo více definovaných informačních jednotek. Sdíleným úkolem NLPContributionGraph organizátoři formalizovali vytvoření odborně zaměřeného grafu nad odbornými články NLP jako automatizovaný úkol. Mezi naše přístupy patří klasifikační model založený na BERT pro identifikaci přispívajících vět ve výzkumné publikaci, analýza závislosti založená na pravidlech pro extrakci frází, následovaná modelem podle CNN pro klasifikaci informačních jednotek a soubor pravidel pro extrakci trojice. Kvantitativní výsledky ukazují, že pátou, pátou a sedmou příčku získáváme ve třech fázích hodnocení. Naše kódy jsou dostupné na https://github.com/HardikArora17/SemEval-2021-INNOVATORS.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this work, we describe our system submission to the SemEval 2021 Task 11: NLP Contribution Graph Challenge. We attempt all the three sub-tasks in the challenge and report our results. Subtask 1 aims to identify the contributing sentences in a given publication. Subtask 2 follows from Subtask 1 to extract the scientific term and predicate phrases from the identified contributing sentences. The final Subtask 3 entails extracting triples (subject, predicate, object) from the phrases and categorizing them under one or more defined information units. With the NLPContributionGraph Shared Task, the organizers formalized the building of a scholarly contributions-focused graph over NLP scholarly articles as an automated task. Our approaches include a BERT-based classification model for identifying the contributing sentences in a research publication, a rule-based dependency parsing for phrase extraction, followed by a CNN-based model for information units classification, and a set of rules for triples extraction. The quantitative results show that we obtain the 5th, 5th, and 7th rank respectively in three evaluation phases. We make our codes available at https://github.com/HardikArora17/SemEval-2021-INNOVATORS.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek obsahuje popis sdílených úloh na WAT 2021 podle našeho tým "NLPHut". Zúčastnili jsme se úlohy multimodálního překladu angličtina→hindština, úlohy multimodálního překladu angličtina→malajština a úlohy vícejazyčného překladu z indických jazyků. Použili jsme nejmodernější model Transformeru
s jazykovými značkami v různých nastaveních
pro překladatelskou úlohu a navrhli jsme
nové "regionálně specifické" generování titulků využívající kombinaci obrazových
CNN a LSTM pro hindštinu a malajalamštinu. Náš příspěvek v angličtině→malajálamštině Multimodální
překladatelské úlohy (překlad pouze textu a titulků) a na druhém místě v úloze multimodálního překladu angličtina→hindština (překlad pouze textu a titulků). Naše příspěvky si vedly dobře také v indických vícejazyčných překladových úlohách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper provides the description of
shared tasks to the WAT 2021 by our
team “NLPHut”. We have participated
in the English→Hindi Multimodal translation task, English→Malayalam Multimodal translation task, and Indic Multilingual translation task. We have used
the state-of-the-art Transformer model
with language tags in different settings
for the translation task and proposed a
novel “region-specific” caption generation
approach using a combination of image
CNN and LSTM for the Hindi and Malayalam image captioning. Our submission
tops in English→Malayalam Multimodal
translation task (text-only translation, and
Malayalam caption), and ranks secondbest in English→Hindi Multimodal translation task (text-only translation, and Hindi
caption). Our submissions have also performed well in the Indic Multilingual translation tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení technologických možností překladatele ve 21. století a projektů věnujících se výzkumu a hodnocení strojového překladu na ÚFAL.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presentation of the technological possibilities of the translator in the 21st century and projects dedicated to research and evaluation of machine translation at ÚFAL.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška se zaměřovala na rozdíly mezi strojovým a lidským překladem – poznáme na první pohled, co přeložil člověk a co stroj? Existuje jen jeden správný překlad? Jak se hodnotí kvalita překladu?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The lecture focused on the differences between machine and human translation - can we tell at a glance what is translated by a human and what by a machine? Is there only one correct translation? How is the quality of a translation assessed?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek poskytuje stručný přehled možných metod, jak zjistit, že referenční překlady byly ve skutečnosti vytvořeny posteditací strojového překladu. Využity byly dvě metody založené na automatických metrikách: 1. rozdíl BLEU mezi podezřelým MT a některým jiným
dobrým MT a 2. rozdíl BLEU s využitím dalších referencí. Tyto dvě metody odhalily
podezření, že oficiální reference z WMT 2020 je ve skutečnosti posteditovaný strojový překlad. Toto podezření bylo potvrzeno při manuální analýze zjištěním konkretních dokladů o posteditačním postupu. Také byla vytvořena typologie
posteditačních změn, kde jsou uvedeny některé chyby nebo změny provedené posteditorem, nebo také chyby převzaté ze strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper provides a quick overview of possible methods how to detect that reference translations were actually created by post-editing
an MT system. Two methods based on automatic metrics are presented: BLEU difference
between the suspected MT and some other
good MT and BLEU difference using additional references. These two methods revealed
a suspicion that the WMT 2020 Czech reference is based on MT. The suspicion was confirmed in a manual analysis by finding concrete proofs of the post-editing procedure in
particular sentences. Finally, a typology of
post-editing changes is presented where typical errors or changes made by the post-editor
or errors adopted from the MT are classified.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme vítězný systém z Multilingual Lexical Normalization (MultiLexNorm) Shared Task na W-NUT 2021 (van der Goot et al., 2021a), který vyhodnocuje lexikálně-normalizační systémy na 12 datasetech sociálních médií v 11 jazycích. Naše řešení zakládáme na předtrénovaném jazykovém modelu ByT5 (Xue et al., 2021a), který dále trénujeme na syntetických datech a poté dotrénováváme na autentických normalizačních datech. Náš systém dosahuje nejlepších výsledků s velkým náskokem v intrinsic hodnocení a také nejlepších výsledků v extrinsic vyhodnocení prostřednictvím syntaktické analýzy. Zdrojový kód je uvolněn na https://github.com/ufal/multilexnorm2021 a natrénované modely na https://huggingface.co/ufal.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the winning entry to the Multilingual Lexical Normalization (MultiLexNorm) shared task at W-NUT 2021 (van der Goot et al., 2021a), which evaluates lexical-normalization systems on 12 social media datasets in 11 languages. We base our solution on a pre-trained byte-level language model, ByT5 (Xue et al., 2021a), which we further pre-train on synthetic data and then fine-tune on authentic normalization data. Our system achieves the best performance by a wide margin in intrinsic evaluation, and also the best performance in extrinsic evaluation through dependency parsing. The source code is released at https://github.com/ufal/multilexnorm2021 and the fine-tuned models at https://huggingface.co/ufal.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Evropa je mnohojazyčná společnost, v níž se hovoří desítkami jazyků. Jedinou možností, jak mnohojazyčnost umožnit a využít, jsou jazykové technologie (LT), tj. technologie zpracování přirozeného jazyka a řeči. Popisujeme evropskou jazykovou síť (European Language Grid, ELG), která se má vyvinout v primární platformu a tržiště LT v Evropě tím, že poskytne jednu zastřešující platformu pro evropský LT prostor, včetně výzkumu a průmyslu, která umožní všem zúčastněným stranám nahrávat, sdílet a distribuovat své služby, produkty a zdroje. Na konci projektu, který v roce 2022 vytvoří právní subjekt, poskytne ELG přístup k cca. 1300 služeb pro všechny evropské jazyky a tisíce datových souborů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Europe is a multilingual society, in which dozens of languages are spoken. The only option to enable and to benefit from multilingualism is through Language Technologies (LT), i.e., Natural Language Processing and Speech Technologies. We describe the European Language Grid (ELG), which is targeted to evolve into the primary platform and marketplace for LT in Europe by providing one umbrella platform for the European LT landscape, including research and industry, enabling all stakeholders to upload, share and distribute their services, products and resources. At the end of our EU project, which will establish a legal entity in 2022, the ELG will provide access to approx. 1300 services for all European languages as well as thousands of data sets.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Abstrakt je pouze v angličtině.

Interpreters facilitate multi-lingual meetings but the affordable set of languages is often smaller than what is needed. Automatic simultaneous speech translation can extend the set of provided languages. We investigate if such an automatic system should rather follow the original speaker, or an interpreter to achieve better translation quality at the cost of increased delay. To answer the question, we release Europarl Simultaneous Interpreting Corpus (ESIC), 10 hours of recordings and transcripts of European Parliament speeches in English, with simultaneous interpreting into Czech and German. We evaluate quality and latency of speaker-based and interpreter-based spoken translation systems from English to Czech. We study the differences in implicit simplification and summarization of the human interpreter compared to a machine translation system trained to shorten the output to some extent. Finally, we perform human evaluation to measure information loss of each of these approaches.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Interpreters facilitate multi-lingual meetings but the affordable set of languages is often smaller than what is needed. Automatic simultaneous speech translation can extend the set of provided languages. We investigate if such an automatic system should rather follow the original speaker, or an interpreter to achieve better translation quality at the cost of increased delay.
To answer the question, we release Europarl Simultaneous Interpreting Corpus (ESIC), 10 hours of recordings and transcripts of European Parliament speeches in English, with simultaneous interpreting into Czech and German. We evaluate quality and latency of speaker-based and interpreter-based spoken translation systems from English to Czech. We study the differences in implicit simplification and summarization of the human interpreter compared to a machine translation system trained to shorten the output to some extent. Finally, we perform human evaluation to measure information loss of each of these approaches.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Klasifikace předmětných článků je důležitým problémem
Schosimilar Document Processing to address the huge information overload in the scholarly space. Tento dokument popisuje
přístup našeho týmu CUNI-NU pro Biocreative VII-Track 5 challenge: Litcovid multilabel topic classification for
COVID-19 literatura [1]. Cílem dotčeného úkolu je automatizovat
manuální nakládání s biomedicínskými předměty do sedmi různých
značek, konkrétně pro datové úložiště LitCovid. Naše nejlepší
model používá dokument SPECTER [2]
zápatí pro reprezentaci abstraktních, a tituly vědeckých
články následované mechanismem Dual-Attention [3] do perform multilabel kategorizace. Dosáhli jsme významného lepší výkon než základní metody. Děláme
náš kód dostupný na https://github.com/Nid989/
CUNI-NU-Biocreative-Track5</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Subject-Article classification is an important problem in
Scholarly Document Processing to address the huge information overload in the scholarly space. This paper describes the
approach of our team CUNI-NU for the Biocreative VII-Track
5 challenge: Litcovid multi-label topic classification for
COVID-19 literature [1]. The concerned task aims to automate
the manual curation of biomedical articles into seven distinct
labels, specifically for the LitCovid data repository. Our best
performing model makes use of the SPECTER [2] document
embeddings for representing abstract, and titles of scientific
articles followed by a Dual-Attention [3] mechanism to perform the multi-label categorization. We achieve significantly
better performance than the baseline methods. We make our code available at https://github.com/Nid989/
CUNI-NU-Biocreative-Track5</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje znalostního vícejazyčného konverzačního agenta "MyWelcome Agent", který funguje jako osobní asistent migrantů v kontextu jejich přijímání a integrace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a knowledge-driven multilingual conversational agent (referred to as ``MyWelcome Agent'') that acts as personal assistant for migrants in the contexts of their reception and integration. In order to also account for tasks that go beyond communication and require advanced service coordination skills, the architecture of the proposed platform separates the dialogue management service from the agent behavior including the service selection and planning. The involvement of genuine agent planning strategies in the design of personal assistants, which tend to be limited to dialogue management tasks, makes the proposed agent significantly more versatile and intelligent. To ensure high quality verbal interaction, we draw upon state-of-the-art multilingual spoken language understanding and generation technologies.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Metriky srozumitelnosti textu hodnotí, kolik úsilí musí čtenář vynaložit na porozumění danému textu. Používají se např. K výběru vhodných materiálů pro četbu pro různé úrovně znalostí studentů nebo k zajištění efektivního přenosu důležitých informací (např. V případě nouze). Flesch Reading Ease je natolik globálně používaný vzorec, že je dokonce integrován do textového procesoru MS. Jeho konstanty jsou však závislé na jazyce. Původní vzorec byl vytvořen pro angličtinu. Doposud byl přizpůsoben několika evropským jazykům, bengálštině a hindštině. Tento článek popisuje českou adaptaci, přičemž jazykově závislé konstanty jsou optimalizovány algoritmem strojového učení pracujícím na paralelních korpusech češtiny s angličtinou, ruštinou, italštinou a francouzštinou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Text readability metrics assess how much effort a reader must put into comprehending a given text. They are e.g., used to choose appropriate readings for different student proficiency levels, or to make sure that crucial information is efficiently conveyed (e.g., in an emergency). Flesch Reading Ease is such a globally used formula that it is even integrated into the MS Word Processor. However, its constants are language dependent. The original formula was created for English. So far it has been adapted to several European languages, Bangla, and Hindi. This paper describes the Czech adaptation, with the language-dependent constants optimized by a machine-learning algorithm working on parallel corpora of Czech with English, Russian, Italian, and French.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Adaptovali jsme na češtinu čtyři klasické metriky srozumitelnosti na základě dat InterCorp (paralelní korpus s ručním zarovnáním vět)a CzEng 2.0 (velký paralelní korpus procházených webových textů) a algoritmu optimize.curve z knihovny SciPy. Adaptované metriky jsou: Flesch Reading Ease, Flesch-Kincaid Grade Level, Coleman-Liau Index a Automated Readability Index. Popisujeme podrobnosti postupu a předkládáme uspokojivé výsledky. Kromě toho diskutujeme citlivost těchto metrik na textové parafráze a korelaci skóre srozumitelnosti s empiricky pozorovaným porozuměním čtenému a také adaptaci Flesch Reading Ease na češtinu z ruštiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We have fitted four classic readability metrics to Czech, using InterCorp (a parallel corpus with manual sentence alignment), CzEng 2.0 (a large parallel corpus of crawled web texts),  and the optimize.curve fit algorithm from the SciPy library. The adapted metrics are: Flesch Reading Ease, Flesch-Kincaid Grade Level, Coleman-Liau Index, and Automated Readability Index. We describe the details of the procedure and present satisfactory results. Besides, we discuss the sensitivity of these metrics to text paraphrases and correlation of readability scores with empirically observed reading comprehension, as well as the adaptation of Flesch Reading Ease to Czech from Russian.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přestože přístupy založené na neuronových sítích výrazně zvýšily plynulost strojově generovaného textu, jsou náchylné k chybám v přesnosti, jako je vynechávání částí vstupu (opomenutí) nebo generování výstupu, který nemá oporu v žádném vstupu (halucinace). Tento problém se projevuje zejména při menším množství trénovacích dat, generování delších textů nebo šumu v trénovacích datech. Všechny tyto podmínky jsou pro úlohy generování přirozeného jazyka (NLG) velmi časté.

V této přednášce ukážu několik příkladů přístupů zaměřených na vytváření přesnějších výstupů v rámci systému NLG nebo na odhalování chyb ve výstupech systému NLG. Konkrétně se zaměřím na neuronové architektury sequence-to-sequence s předtrénovanými jazykovými modely a jejich rozšíření. Budu se zabývat především generováním textu ze strukturovaných dat, ale uvedu i srovnání se situací při sumarizaci nebo generování dialogových odpovědí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>While neural-network-based approaches have significantly increased the fluency of machine-generated text, they are prone to accuracy errors, such as leaving out parts of the input (omissions) or generating output that is not grounded in any input (hallucination). This problem becomes especially apparent with lower amounts of training data, generation of longer texts, and noise in the training data. All of these conditions are very common for natural language generation (NLG) tasks.

In this talk, I will show several example approaches aiming to produce more accurate outputs within an NLG system, or to detect errors in an NLG system output. I will specifically focus on sequence-to-sequence neural architectures with pretrained language models and their
extensions. I will mostly discuss generation of text from structured data, but I will also make comparisons to the situation in summarization or dialogue response generation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kdo se bojí umělé inteligence.
Přijďte do tematického prostoru Goethe-Institutu a zjistěte, jak s pomocí robotů vzniká literatura!

https://www.goethe.de/ins/cz/cs/ver.cfm?event_id=22345514</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Who's afraid of artificial intelligence?
Come to the Goethe-Institut thematic space and find out how literature is made with the help of robots!

https://www.goethe.de/ins/cz/en/ver.cfm?event_id=22345514</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Hodnotící kampaň Mezinárodní konference o překladu z mluveného jazyka
(IWSLT 2021) letos představila čtyři společné 
úkoly: (i) Simultánní překlad mluvené řeči, (ii) Offline překlad mluvené řeči, (iii) Vícejazyčný
překlad řeči, (iv) překlad řeči s malým množstvím zdrojů. Celkem se zúčastnilo 22 týmů
alespoň na jedné z úloh. Tento článek popisuje každou společnou úlohu, data a hodnocení.
metriky a uvádí výsledky obdržených podání.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The evaluation campaign of the International
Conference on Spoken Language Translation
(IWSLT 2021) featured this year four shared
tasks: (i) Simultaneous speech translation, (ii)
Offline speech translation, (iii) Multilingual
speech translation, (iv) Low-resource speech
translation. A total of 22 teams participated
in at least one of the tasks. This paper de-
scribes each shared task, data and evaluation
metrics, and reports results of the received submissions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá konstrukcemi s kategoriálním slovesem v češtině. Částěčně reviduje pravidla pro formování syntaktické struktury těchto konstrukcí navržená v rámci funkčního generativního popisu, a to zejména s ohledem na roli fakultativních volných doplnění kategoriálních sloves v těchto konstrukcích. Provedená analýza ukazuje, že v případech, kdy kategoriální slovesa neposkytují dostatečný počet valenčních doplnění pro povrchové vyjádření sémantických participantů jména, využívají participanty pro své povrchové vyjádření i fakultativní volná doplnění slovesa. Distribuce slovesných a jmenných doplnění analyzovaná v 1 600 konstrukcích s kategoriálním slovesem ukázala, že povrchové vyjádření participantů jména skrze fakultativní volná doplnění kategoriálního slovesa je v češtině silně preferováno (88% fakultativních volných doplnění slovesa oproti 12% valenčních doplnění jména).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper addresses Czech light verb constructions, partly revising principles of their syntactic structure formation formulated within the Functional Generative Description. It argues that obligatoriness of valency complementations should be reflected in these principles. Namely, the role of optional valency complementations of light verbs played in this process has been analyzed. This analysis has shown that in the cases where light verbs do not provide a sufficient number of valency complementations for the surface expression of semantic participants of predicative nouns, semantic participants of nouns make use of optional verbal complementations. In such cases, semantic participants can be expressed on the surface, either as optional verbal complementation or as nominal complementation. The distribution of verbal and nominal complementations have been observed in 1,600 light verb constructions extracted from the Czech National Corpus, with the result that the surface expression of these participants through the optional verbal complementations is strongly preferred (88% of verbal complementations and 12% of nominal ones).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Reflexiva představují velkou výzvu pro teoretický i lexikografický popis jazyka. Vzhledem ke změnám morfo-syntaktických vlastností sloves, která s sebou užití reflexiv nese, je jejich zachycení a popis vysoce relevantní pro slovesnou valenci. V češtině fungují reflexiva buď jako reflexivní osobní zájmeno, nebo jako morfém tvořící slovesné lemma či slovesný tvar. V tomto příspěvku se zabýváme těmi jazykovými jevy, které jsou kódovány reflexivním osobním zájmenem, tj. reflexivitou a reciprocitou.
Lexikografické znázornění těchto dvou jazykových jevů zpracováváme v rámci Valenčního slovníku českých sloves VALLEX. Reprezentace ve VALLEXu využívá rozdělení lexikonu na datovou a gramatickou komponentu; bere v úvahu, že reflexivita i reciprocita jsou podmíněny sémantickými vlastnostmi sloves, ovšem morfologické změny vyvolané těmito jevy jsou systémové a lze je popsat pomocí pravidel.
Zhruba třetina lexikálních jednotek obsažených v datové komponentě lexikonu má přiřazenu informaci o možném užití v reflexivních a/nebo recipročních konstrukcí, a to v podobě párů dotčených valenčních komplementů (2 039 u reflexivity a 2 744 u reciprocity). Dále je v gramatické komponentě formulován soubor pravidel pro odvození valenčních rámců pro syntakticky reflexní a reciproční konstrukce, která se aplikují na základové valenční rámce popisující nereflexní a nereferenční konstrukce (3 pravidla pro reflexivitu a 18 pravidel pro reciprocitu). V příspěvku se dále diskutují situace, kdy lexikální jednotky sloves vytvářejí nejednoznačné konstrukce, které mohou být vykládány buď jako reflexivní, nebo jako reciproční.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Reflexives, encoding a variety of meanings, pose a great challenge for both theoretical and lexicographic description. As they are associated with changes in morphosyntactic properties of verbs, their description is highly relevant for verb valency. In Czech, reflexives function as the reflexive personal pronoun and as verbal affixes. In this paper, we address those language phenomena that are encoded by the reflexive personal pronoun, i.e., reflexivity and reciprocity.
We introduce the lexicographic representation of these two language phenomena in the VALLEX lexicon, a valency lexicon of Czech verbs, accounting for the role of the reflexives with respect to the valency structure of verbs. This representation makes use of the division of the lexicon into a data component and a grammar component. It takes into account that reflexivity and reciprocity are conditioned by the semantic properties of verbs on the one hand and that morphosyntactic changes brought about by these phenomena are systemic on the other. 
About one third of the lexical units contained in the data component of the lexicon are assigned the information on reflexivity and/or reciprocity in the form of pairs of the affected valency complementations (2,039 on reflexivity and 2,744 on reciprocity). A set of rules is formulated in the grammar component (3 rules for reflexivity and 18 rules for reciprocity). These rules derive the valency frames underlying syntactically reflexive and reciprocal constructions from the valency frames describing non-reflexive and non-reciprocal constructions. Finally, the proposed representation makes it possible to determine which lexical units of verbs create ambiguous constructions that can be interpreted either as reflexive or as reciprocal.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V posledních desetiletích stále větší počet vystupujících umělců a technologů přivedl roboty na středové pódium při jejich vystoupeních. AI si přisvojila role komediálních improvizátorů, vypravěčů, herců, tanečníků a choreografů, čímž narušila tradiční obousměrnou lidskou herecko-lidskou diváckou interakci. Předložením této přednášky se snažíme zpochybnit myšlenku, že živé představení je specificky lidskou činností, a prozkoumat různé formy interakce mezi člověkem a umělou inteligencí v divadle. Jaký příběh Al vypráví? Jaké emoce to může vyvolat? Myslíte si, že umělá inteligence je schopna vytvořit příjemný divadelní scénář? Může se robot stát dramatikem? Existuje dokonalý divadelní večer, který vyprodukuje autonomní stroj?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Over the past decades an increasing number of performing artists and technologists have brought robots to the center stage in their performances. AI has taken roles such as comedy improvisers, storytellers, actors, dancers, and choreographers, disrupting the traditional two-way human actor-human audience interaction. By presenting this talk we try to challenge the idea that live performance is a specifically human activity and explore different forms of human-AI interactions in the theatre. What story does AI tell? What emotions can it generate? Do you think artificial intelligence is able to create an enjoyable theatre script? Can a robot become a playwright? Does the perfect theater evening exist, produced by an autonomous machine?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Šestý ročník workshopu Search-Oriented Conversational AI (SCAI 2021) byl uspořádán jako diskusní platforma o konverzační umělé inteligenci pro inteligentní přístup k informacím. Workshop byl koncipován jako multidisciplinární a spojil výzkumné pracovníky a odborníky z praxe napříč obory zpracování přirozeného jazyka (NLP), vyhledávání informací (IR), strojového učení (ML) a interakce člověk-počítač (HCI). Workshop zahrnoval čtyři sekce, na nichž zazněly pozvané přednášky, samostatnou posterovou sekci a sekci, na níž se diskutovalo o výsledcích soutěže v konverzačním zodpovídání otázek (SCAI-QReCC).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The 6th edition of the Search-Oriented Conversational AI workshop (SCAI 2021) was organised as a discussion platform on conversational AI for intelligent information access. The workshop was designed to be multidisciplinary, bringing together researchers and practitioners across the fields of natural language processing (NLP), information retrieval (IR), machine learning (ML) and human-computer interaction (HCI). The workshop included four sessions featuring invited talks, a separate poster session, and a session discussing the results of a shared task on conversational question answering (SCAI-QReCC).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme experimenty s automatickým odhalováním nekonzistentního chování na základě kontextu u dialogových systémů orientovaných na úkoly. Obohacujeme data bAbI/DSTC2 (Bordes et al., 2017) o automatickou anotaci nekonzistencí v dialogu a ukazujeme, že nekonzistence korelují s neúspěšnými dialogy. Předpokládáme, že použití omezené historie dialogů a předvídání dalšího tahu uživatele může zlepšit klasifikaci nekonzistencí. Zatímco obě hypotézy se potvrzují pro dialogový model založený na memory networks, neplatí pro trénování jazykového modelu GPT-2, který nejvíce těží z použití úplné historie dialogu a dosahuje 99% přesnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present experiments on automatically detecting inconsistent behavior of task-oriented dialogue systems from the context. We enrich the bAbI/DSTC2 data (Bordes et al., 2017) with automatic annotation of dialogue inconsistencies, and we demonstrate that inconsistencies correlate with failed dialogues. We hypothesize that using a limited dialogue history and predicting the next user turn can improve inconsistency classiﬁcation. While both hypotheses are conﬁrmed for a memory-networks-based dialogue model, it does not hold for a training based on the GPT-2 language model, which benefits most from using full dialogue history and achieves a 0.99 accuracy score.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předtrénované jazykové modely založené na attention, jako je GPT-2, přinesly značný pokrok v modelování dialogů "end-to-end". Pro dialog zaměřený na úkoly však představují také značná rizika, jako je nedostatečná podloženost fakty nebo rozmanitost odpovědí. Abychom tyto problémy vyřešili, zavádíme modifikované trénovací cíle pro dotrénování jazykového modelu a využíváme masivní augmentaci dat pomocí zpětného překladu, abychom zvýšili rozmanitost trénovacích dat. Dále zkoumáme možnosti kombinace dat z více zdrojů s cílem zlepšit výkonnost na cílové datové sadě. Naše příspěvky pečlivě vyhodnocujeme pomocí lidských i automatických metod. Náš model podstatně překonává základní model na datech MultiWOZ a vykazuje výkon konkurenceschopný se současným stavem techniky při automatickém i lidském hodnocení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Attention-based pre-trained language models such as GPT-2 brought considerable progress to end-to-end dialogue modelling. However, they also present considerable risks for task-oriented dialogue, such as lack of knowledge grounding or diversity. To address these issues, we introduce modified training objectives for language model finetuning, and we employ massive data augmentation via back-translation to increase the diversity of the training data. We further examine the possibilities of combining data from multiples sources to improve performance on the target dataset. We carefully evaluate our contributions with both human and automatic methods. Our model substantially outperforms the baseline on the MultiWOZ data and shows competitive performance with state of the art in both automatic and human evaluation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předtrénované jazykové modely založené na attention, jako je GPT-2, přinesly značný pokrok v end-to-end modelování dialogů. Pro dialog zaměřený na úkoly však představují také značná rizika, jako je nedostatečná korespondence s databází nebo nedostatek rozmanitosti odpovědí. Abychom tyto problémy vyřešili, zavádíme pro doladění jazykového modelu modifikované trénovací cíle a využíváme masivní rozšíření trénovacích dat pomocí zpětného překladu, čímž zvyšujeme jejich rozmanitost. Dále zkoumáme možnosti kombinace dat z více zdrojů s cílem zlepšit výkonnost na cílové datové sadě. Naše příspěvky pečlivě vyhodnocujeme pomocí ručních i automatických metod. Náš model dosahuje nejlepších výsledků na datové sadě MultiWOZ a vykazuje konkurenceschopný výkon při lidském hodnocení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Attention-based pre-trained language models such as GPT-2 brought considerable progress to end-to-end dialogue modelling. However, they also present considerable risks for task-oriented dialogue, such as lack of knowledge grounding or diversity. To address these issues, we introduce modified training objectives for language model finetuning, and we employ massive data augmentation via back-translation to increase the diversity of the training data. We further examine the possibilities of combining data from multiples sources to improve performance on the target dataset. We carefully evaluate our contributions with both human and automatic methods. Our model achieves state-of-the-art performance on the MultiWOZ data and shows competitive performance in human evaluation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Naše kniha „The Reality of Multi-Lingual Machine Translation“ pojednává o výhodách a nebezpečích používání více než dvou jazyků v systémech strojového překladu. I když se kniha zaměřuje na konkrétní úkol zpracování sekvencí a víceúkolového učení (multi-task learning), cílí i poněkud mimo oblast zpracování přirozeného jazyka. Strojový překlad je pro nás ukázkovým příkladem aplikací hlubokého učení, kde jsou lidské dovednosti a schopnosti učení brány jako laťka, kterou se mnozí snaží dosáhnout a překonat. Dokumentujeme, že některé z výdobytků pozorovaných v mnohojazyčném překladu mohou vyplývat z jednodušších důvodů, než je předpokládaný přenos znalostí napříč jazyky.

V první, spíše obecné části vás kniha provede motivací pro mnohojazyčnost, univerzálností hlubokých neuronových sítí zejména v úlohách typu sequence-to-sequence až ke komplikacím tohoto učení. Obecnou část uzavíráme varováním před příliš optimistickým a neopodstatněným vysvětlením zlepšení, které neuronové sítě demonstrují.

Ve druhé části se plně ponoříme do mnohojazyčných modelů, se zvlášť pečlivým zkoumáním učení přenosem (transfer learning) jako jednoho z přímočařejších přístupů využívajících další jazyky. Zkoumány jsou současné vícejazyčné techniky, včetně masivních modelů, a diskutovány jsou praktické aspekty nasazení mnohojazyčných systémů. Závěr knihy zdůrazňuje otevřený problém strojového porozumění a připomíná dva etické aspekty budování rozsáhlých modelů: inkluzivnost výzkumu a jeho ekologický dopad.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Our book "The Reality of Multi-Lingual Machine Translation" discusses the benefits and perils of using more than two languages in machine translation systems. While focused on the particular task of sequence-to-sequence processing and multi-task learning, the book targets somewhat beyond the area of natural language processing. Machine translation is for us a prime example of deep learning applications where human skills and learning capabilities are taken as a benchmark that many try to match and surpass. We document that some of the gains observed in multi-lingual translation may result from simpler effects than the assumed cross-lingual transfer of knowledge.

In the first, rather general part, the book will lead you through the motivation for multi-linguality, the versatility of deep neural networks especially in sequence-to-sequence tasks to complications of this learning. We conclude the general part with warnings against too optimistic and unjustified explanations of the gains that neural networks demonstrate.

In the second part, we fully delve into multi-lingual models, with a particularly careful examination of transfer learning as one of the more straightforward approaches utilizing additional languages. The recent multi-lingual techniques, including massive models, are surveyed and practical aspects of deploying systems for many languages are discussed. The conclusion highlights the open problem of machine understanding and reminds of two ethical aspects of building large-scale models: the inclusivity of research and its ecological trace.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V mediálním průmyslu se zaměření globálního zpravodajství může přes noc změnit. Existuje přesvědčivá
potřeba být schopni vyvinout nové systémy strojového překladu v krátkém časovém období, aby bylo možné efektivněji pokrýt rychle se vyvíjející příběhy. Jako součást stroje s nízkými zdroji
překladatelského projektu GOURMET jsme náhodně vybrali jazyk, pro který musel být systém
postaveno a vyhodnoceno za dva měsíce (únor a březen 2021). Vybraný jazyk byl
Paštština, indoíránský jazyk používaný v Afghánistánu, Pákistánu a Indii. V tomto období jsme
dokončili celý proces vývoje systému neuronového strojového překladu: procházení dat, čištění, zarovnání, vytváření testovacích sad, vývoj a testování modelů a jejich poskytování
uživatelským partnerům. V tomto článku popisujeme rychlý proces vytváření dat a experimenty
s transferovým učením a přípravou na paštskou angličtinu. Zjišťujeme, že začínáme od existujícího
velký model předem proškolený na 50 jazycích vede k mnohem lepším výsledkům BLEU než předtrénování na
jeden vysoce zdrojový jazykový pár s menším modelem. Uvádíme také lidské hodnocení
naše systémy, což naznačuje, že výsledné systémy fungují lépe než volně dostupné
komerční systém při překladu z angličtiny do paštštiny a podobně při
překlad z paštštiny do angličtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the media industry, the focus of global reporting can shift overnight. There is a compelling
need to be able to develop new machine translation systems in a short period of time, in order to more efficiently cover quickly developing stories. As part of the low-resource machine
translation project GoURMET, we selected a surprise language for which a system had to be
built and evaluated in two months (February and March 2021). The language selected was
Pashto, an Indo-Iranian language spoken in Afghanistan, Pakistan and India. In this period we
completed the full pipeline of development of a neural machine translation system: data crawling, cleaning, aligning, creating test sets, developing and testing models, and delivering them
to the user partners. In this paper we describe the rapid data creation process, and experiments
with transfer learning and pretraining for Pashto-English. We find that starting from an existing
large model pre-trained on 50 languages leads to far better BLEU scores than pretraining on
one high-resource language pair with a smaller model. We also present human evaluation of
our systems, which indicates that the resulting systems perform better than a freely available
commercial system when translating from English into Pashto direction, and similarly when
translating from Pashto into English.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku popisujeme, jak byla korpusová platforma TEITOK integrována s korpusovými platformami KonText a PML-TQ v LINDAT, aby poskytla vizualizaci dokumentů pro stávající i budoucí zdroje v LINDAT. TEITOK je online platforma pro vyhledávání, prohlížení a úpravy korpusů, kde jsou soubory korpusu ukládány jako anotované soubory TEI / XML. Integrace TEITOK také znamená, že zdroje LINDAT budou k dispozici ve formátu TEI / XML a lze je vyhledávat v CWB nad rámec stávajících nástrojů ústavu. Ačkoli integrace popsaná v tomto článku je specifická pro LINDAT, tato metoda by měla být použitelná pro integraci TEITOK nebo podobných nástrojů do existující korpusové architektury.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we describe how the TEITOK corpus platform was integrated with the KonText and PML-TQ corpus platforms at LINDAT to provide document visualization for both existing and future resources at LINDAT. TEITOK is an online platform for searching, viewing, and editing corpora, where corpus files are stored as annotated TEI/XML files. The TEITOK integration also means LINDAT resources will become available in TEI/XML format, and searchable in CWB on top of existing tools at the institute. Although the integration described in this paper is specific for LINDAT, the method should be applicable to the integration of TEITOK or similar tools into an existing corpus architecture.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Deep Universal Dependencies (hluboké univerzální závislosti) je sbírka treebanků poloautomaticky odvozených z Universal Dependencies. Obsahuje přídavné hloubkově-syntaktické a sémantické anotace. Verze Deep UD odpovídá verzi UD, na které je založena. Mějte však na paměti, že některé treebanky UD nebyly do Deep UD zahrnuty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Deep Universal Dependencies is a collection of treebanks derived semi-automatically from Universal Dependencies. It contains additional deep-syntactic and semantic annotations. Version of Deep UD corresponds to the version of UD it is based on. Note however that some UD treebanks have been omitted from Deep UD.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme GEM, živý benchmark pro generování přirozeného jazyka (NLG), jeho evaluaci a metriky. Měření pokroku v oblasti NLG se opírá o neustále se vyvíjející ekosystém automatizovaných metrik, datových sad a standardů lidské evaluace. Vzhledem k tomuto pohyblivému cíli se nové modely často stále vyhodnocují na odlišných anglocentrických korpusech s dobře zavedenými, ale chybnými metrikami. Tato nesouvislost ztěžuje identifikaci omezení současných modelů a příležitostí k pokroku. GEM toto omezení řeší a poskytuje prostředí, v němž lze modely snadno aplikovat na široký soubor úloh a v němž lze testovat evaluační strategie. Pravidelné aktualizace benchmarku pomohou výzkumu NLG stát se více mnohojazyčným a rozvíjet úlohu spolu s modely. Tento článek slouží jako popis dat pro sdílenou úlohu 2021 na souvisejícím workshopu GEM.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent anglo-centric corpora with well-established, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of tasks and in which evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for the 2021 shared task at the associated GEM Workshop.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek vysvětluje pojem „praktický soulad“, zahrnující rutinní neproblematický vztah hmotných artefaktů a textových předmětů, který poskytuje nezbytný základ pro práci ve třídě. Analýza videonahrávky interakce se zaměřuje na konkrétní případ tří studentů, kteří společně pracují se sdíleným notebookem a papírovým pracovním listem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article explicates the notion of ‘practical accord’, encompassing the routine nonproblematic relationship of material artifacts and textual objects that provides the necessary grounds for further classroom work. Practical accord consists of courses of action that are temporally aligned with structured objects such as series of pages, slides or questions, ordered as a sequence of steps. Grounded in the video-based analysis of a single case, the article argues that such practical accord can be a necessary requirement for an educative activity to take place in an orderly way in the classroom. The analysis focuses on a particular instance of three students working with a shared laptop and a paper worksheet, and losing their grasp of the relationship between the screen and the sheet. The identified practices used to get back ‘on the page’ include the verbal and gestural constitution of the screen and the sheet as two separate objects that are related through instructions provided on the screen, which serves as a link between two independent but interrelated numbering systems used to organise the on-screen material and the questions on the worksheet. The article concludes with a discussion of the notion of practical accord with regard to instructed action and gestalt contextures.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zaměřuje na sekvence opětovného sledování ve školní práci s digitální orální historií, tj. na účastníky provádějící druhé sledování videoklipu, který již jednou viděli. Podrobně zkoumám, jak účastníci zahajují přehrávání pomocí řeči a ztělesněného chování. K videoklipu přistupují jako ke strukturovanému objektu obsahujícímu již známý sled promluv vypravěče. Sekvence opakovaného sledování se uzavřou, když účastníci v rámci videa naleznou konkrétní promluvu, který mohou považovat za řešení dříve zjištěné nejednoznačnosti. K tomu vyhledávají relevantní promluvy manipulací s časovou značkou na obrazovce a stabilizují je zastavením klipu. Účastníci tak vykazují místní dočasnou kompetenci opětovně sledovat videoklipy a manipulovat s nimi jako s relevantními tématy a zdroji při řešení úkolů ve školní třídě.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper focuses on rewatching sequences in classroom work with digital oral history, i.e., participants conducting a second watching of a video clip that they have already seen. I provide a detailed examination of how the participants initiate rewatching using speech and embodied conduct. They treat the video clip as a structured object containing an already familiar succession of utterances by the narrator. The rewatching sequences are closed when the participants have identified a particular utterance within the video that they can treat as a solution to a previously encountered ambiguity. To do this, they search for the relevant utterances by manipulating the time marker on the screen and stabilize them by pausing the clip. The participants thus exhibit a local temporary competence in rewatching and manipulating video clips as topics and resources in classroom tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>O čem robot přemýšlí, co cítí a dokáže napsat divadelní hru? Střípky ze života umělé inteligence jejími vlastními slovy. Futuristický Malý princ z doby postpandemické.

Inscenace, po jejíž každé repríze následuje debata s odborníky na umělou inteligenci i divadelními tvůrci, je nejen svědectvím o současných schopnostech počítačových technologií, ale i poutavou vizí budoucího světa inspirující se v klasikách žánru sci-fi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>What does the robot think about, how does it feel and can it write a play? Snippets from the life of artificial intelligence in its own words. Futuristic Little Prince of the postpandemic era. 

Every reprise will be followed by a debate with experts in artificial intelligence and theater makers. The show is not only a testament to the current capabilities of computer technology, but also an engaging vision of the future world inspired by the sci-fi classics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Švandovo divadlo při příležitosti stoletého výročí premiéry hry R.U.R  Karla Čapka uvádí prezentaci unikátního projektu THEAITRE, který zkoumá, zda umělá inteligence dokáže napsat divadelní hru. Během několika měsíců počítač generoval obrazy ze života robota, který musí čelit radostem a strastem každodenního života. A odkryl nám, jak vnímá základní lidské otázky jako je narození, umírání, touha po lásce, hledání pracovních příležitostí, či stárnutí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>On the occasion of the centenary of Karel Čapek's play R.U.R, the Švanda Theatre prepared a presentation of a unique project THEAITRE that examines whether artificial intelligence can write a play. Within a few months, the computer generated images from the life of a robot that has to face the joys and sorrows of everyday life. And it revealed to us how it perceives basic human issues such as birth, dying, the desire for love, the search for jobs, or aging.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku představujeme metodu pro vytváření slovotvorných sítí pomocí přenosu informací z jiného jazyka. Navrhovaný algoritmus využívá existující slovotvornou síť a paralelní texty a vytváří síť s nízkou precision a středním recallem v jazyce, pro který nemusí být k dispozici manuální anotace. Recall výsledné sítě pak rozšířujeme tím, že ji využijeme k natrénování metody strojového učení a výsledný model aplikujeme na větší slovník, čímž získáme výsledek s obdobnou precision, ale vyšším recallem. Přístup je vyhodnocován proti existujícím slovotvorným sítím ve francouzštině, němčině a češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this article, we present a proof-of-concept method for creating word-formation networks by transferring information from another language. The proposed algorithm utilizes an existing word-formation network and parallel texts and creates a low-precision and moderate-recall network in a language, for which no manual annotations need to be available. We then extend the coverage of the resulting network by using it to train a machine-learning method and applying the resulting model to a larger lexicon, obtaining a moderate-precision and high-recall result. The approach is evaluated on French, German and Czech against existing word-formation networks in those languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>DeriNet je lexikální síť, která modeluje derivační vztahy ve slovníku češtiny. Uzly sítě odpovídají českým lexémům, zatímco hrany představují slovotvorné vztahy mezi odvozeným slovem a jeho základním slovem / slovy. Současná verze, DeriNet 2.1, obsahuje 1 039 012 lexemes (extrahovány ze slovníku MorfFlex CZ 2.0) spojených 782 814 derivačními relacemi, 50 533 ortografickými variantami, 1 952 vztahů skládání, 295 univerbizačními vztahy a 144 konverzními vztahy.
Ve srovnání s předchozí verzí obsahuje verze 2.1 anotace ortografických variant, plně automaticky generovanou anotaci hranic afixů (kromě kořenů anotovaných v 2.0), 202 affixoidů sloužících jako základ pro skládání, anotaci četností lexémů z korpusů, anotaci slovesných tříd a pilotní anotaci univerbace. Sada tagů pro slovní druhy byla převedena na Universal POS z projektu Universal Dependencies.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>DeriNet is a lexical network which models derivational relations in the lexicon of Czech. Nodes of the network correspond to Czech lexemes, while edges represent word-formational relations between a derived word and its base word / words. The present version, DeriNet 2.1, contains 1,039,012 lexemes (sampled from the MorfFlex CZ 2.0 ​dictionary) connected by 782,814 derivational, 50,533 orthographic variant, 1,952 compounding, 295 univerbation and 144 conversion relations.
Compared to the previous version, version 2.1 contains annotations of orthographic variants, full automatically generated annotation of affix morpheme boundaries (in addition to the roots annotated in 2.0), 202 affixoid lexemes serving as bases for compounding, annotation of corpus frequency of lexemes, annotation of verbal conjugation classes and a pilot annotation of univerbation. The set of part-of-speech tags was converted to Universal POS from the Universal Dependencies project.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme pilotní experimenty na dělení a identifikaci českých složených slov. Vytvořili jsme algoritmus měřící jazykovou podobnost dvou slov založený na nalezení nejkratšího
cesta skrze matici vzájemných odhadovaných korespondencí mezi dvěma fonologicky přepsanými řetězci.
Dále jsme vytvořili nástroj pro splitting neboli dělení složených slov (Czech Compound Splitter) pomocí frameworku Marian Neural Machine Translator, který byl vytrénován na datové sadě obsahující 1 164 ručně anotovaných sloučenin a zhruba 280 000 synteticky vytvořených kompozit. Ve splittingu kompozit dosáhlo první řešení přesnosti 28 % a druhé řešení 54 % na validačním datové sadě. V úloze identifikace kompozit dosáhl Czech Compound Splitter přesnosti 91%.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present pilot experiments on splitting and identifying Czech compound words. We created
an algorithm measuring the linguistic similarity of two words based on finding the shortest
path through a matrix of mutual estimated correspondences between two phonemic strings.
Additionally, a neural compound-splitting tool (Czech Compound Splitter) was implemented
by using the Marian Neural Machine Translator framework, which was trained on a data set
containing 1,164 hand-annotated compounds and about 280,000 synthetically created compounds.
In compound splitting, the first solution achieved an accuracy of 28% and the second solution
achieved 54% on a separate validation data set. In compound identification, the Czech Compound
Splitter achieved an accuracy of 91%.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jazykové domény vyžadující velmi pečlivé používání terminologie jsou hojné a odrážejí významnou část překladatelského průmyslu. V této práci představujeme metriku pro hodnocení kvality a konzistentnosti překladu terminologie se zaměřením na lékařskou (a konkrétně COVID-19) doménu pro pět jazykových párů: angličtinu do francouzštiny, čínštiny, ruštiny a korejštiny, a dále češtinu do němčiny. Uvádíme popisy a výsledky zúčastněných systémů, vyjadřujeme nutnost dalšího výzkumu jak pro adekvátnější zacházení s terminologií, tak pro správnou formulaci a vyhodnocení úlohy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Language  domains  that  require  very  carefuluse of terminology are abundant and reflect asignificant part of the translation industry.  Inthis work we introduce a benchmark for eval-uating the quality and consistency of terminol-ogy translation, focusing on the medical (andCOVID-19  specifically)  domain  for  five  lan-guage pairs:  English to French, Chinese, Rus-sian,  and  Korean,  as  well  as  Czech  to  Ger-man.   We  report  the  descriptions  and  resultsof  the  participating  systems,  commenting  onthe  need  for  further  research  efforts  towardsboth more adequate handling of terminologiesas  well  as  towards  a  proper  formulation  andevaluation of the task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme pilotní experiment zaměřený na harmonizaci různorodých datových zdrojů, které obsahují anotace související s koreferencí. Převedli jsme 17 existujících korpusů 11 jazyků do společného anotačního schématu, založeného na Universal Dependencies, a zveřejnili jsme podmnožinu této kolekce pod názvem CorefUD 0.1 v repozitáři LINDAT-CLARIAH-CZ (http://hdl.handle.net/11234/1-3510).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe a pilot experiment aimed at harmonizing diverse data resources that contain coreference-related annotations. We converted 17 existing datasets for 11 languages into a common annotation scheme based on Universal Dependencies, and released a subset of the resulting collection publicly under the name CorefUD 0.1 via the LINDAT-CLARIAH-CZ repository (http://hdl.handle.net/11234/1-3510).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CorefUD je kolekce existujících korpusů s anotací koreference, které jsme převedli na jednotné anotační schéma. Ve své současné verzi 0.1 CorefUD celkem obsahuje 17 korpusů pro 11 jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CorefUD is a collection of previously existing datasets annotated with coreference, which we converted into a common annotation scheme. In total, CorefUD in its current version 0.1 consists of 17 datasets for 11 languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CorefUD je kolekce existujících korpusů s anotací koreference, které jsme převedli na jednotné anotační schéma. Ve své současné verzi 0.2 CorefUD celkem obsahuje 17 korpusů pro 11 jazyků a v porovnání s verzí 0.1 obsahuje kvalitnejší automatickou morfo-syntaktickou anotaci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CorefUD is a collection of previously existing datasets annotated with coreference, which we converted into a common annotation scheme. In total, CorefUD in its current version 0.2 consists of 17 datasets for 11 languages, and compared to the version 0.1, the automatic morpho-syntactic annotation has improved.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládáme empirickou studii, která srovnává hlavy zmínek anotovaných ručně ve čtyřech koreferenčních datových sadách (pro nizozemštinu, angličtinu, polštinu a ruštinu) na jedné straně a hlavy vyplývající z automaticky predikovaných závislostních syntaktických stromů na straně druhé.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an empirical study that compares mention heads as annotated manually in four coreference datasets (for Dutch, English, Polish, and Russian) on one hand, with heads induced from dependency trees parsed automatically, on the other hand.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento dokument popisuje náš systém účasti na argumentačním textu chápajícím sdílený úkol pro AI Debater na NLPCC 2021 (http://www.fudan-disc.com/sharedtask/AIDebater21/tracks.html). Úkoly jsou motivovány k rozvoji autonomního diskusního systému. Počáteční pokus provedeme s trackem-3, konkrétně extrakcí argumentačních dvojic z recenzního posudku a vyvrácením, kdy extrahujeme argumenty z recenzního posudku a jejich odpovídající vyvrácení z autorových odpovědí. Ve srovnání s víceúkolovou základnou organizátorů zavádíme dvě významné změny: i) používáme vkládání tokenu ERNIE 2.0, které dokáže lépe zachytit lexikální, syntaktické a sémantické aspekty informací v tréninkových datech, ii) provádíme dvojí učení pozornosti, abychom zachytili dlouhodobé závislosti. Náš navrhovaný model dosahuje nejmodernějších výsledků s relativním zlepšením skóre F1 o 8,81% oproti základnímu modelu. Náš kód zveřejníme na adrese https://github.com/guneetsk99/ArgumentMining_SharedTask. Náš tým ARGUABLY je jedním z třetích oceněných týmů v Tracku 3 společného úkolu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes our participating system run to the argumentative text understanding shared task for AI Debater at NLPCC 2021 (http://www.fudan-disc.com/sharedtask/AIDebater21/tracks.html). The tasks are motivated towards developing an autonomous debating system. We make an initial attempt with Track-3, namely, argument pair extraction from peer review and rebuttal where we extract arguments from peer reviews and their corresponding rebuttals from author responses. Compared to the multi-task baseline by the organizers, we introduce two significant changes: (i) we use ERNIE 2.0 token embedding, which can better capture lexical, syntactic, and semantic aspects of information in the training data, (ii) we perform double attention learning to capture long-term dependencies. Our proposed model achieves the state-of-the-art results with a relative improvement of 8.81% in terms of F1 score over the baseline model. We make our code available publicly at https://github.com/guneetsk99/ArgumentMining_SharedTask. Our team ARGUABLY is one of the third prize-winning teams in Track 3 of the shared task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Překladový model z katalánštiny do okcitánštiny pro systém Marian.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Marian NMT model for Catalan to Occitan translation. The model was submitted to WMT'21 Shared Task on Multilingual Low-Resource Translation for Indo-European Languages as a CUNI-Primary system for Catalan to Occitan.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vícejazyčný překladový model z katalánštiny do rumunštiny, italštiny a okcitánštiny pro systém Marian.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Marian multilingual translation model from Catalan into Romanian, Italian and Occitan. This model is an updated version (trained for 2.1M updates) of the model submitted to WMT'21 Shared Task on Multilingual Low-Resource Translation for Indo-European Languages as a CUNI-Primary system for Catalan to Romanian and Italian (trained for 430k updates).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Lexikálně omezený strojový překlad umožňuje uživateli manipulovat s výstupní větou vynucením přítomnosti nebo nepřítomnosti určitých slov a frází. Přestože současné přístupy dokáží vynutit, aby se v překladu objevily specifikované termíny, často se snaží, aby povrchová forma omezeného slova souhlasila se zbytkem vygenerovaného výstupu. Ruční analýza ukazuje, že 46% chyb ve výstupu základního omezeného modelu překladu z angličtiny do češtiny souvisí s gramatickou shodou. Zkoumáme mechanismy, které umožňují neuronových strojový překlad k určení správné inflexe omezujících slov specifikovaných pomocí lemmat. Zaměřujeme se zejména na metody založené na tréninku modelu s omezeními, které jsou součástí vstupu. Naše experimenty na anglicko-českém jazykovém páru ukazují, že tento přístup zlepšuje překlad s omezením pomocí termínů a to jak v automatickém i ručním hodnocení, snížením počtu chyb v gramatické shodě. Náš přístup
tak odstraňuje inflexní chyby, aniž by zaváděl nové chyby nebo snižoval celkovou
kvalitu překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Lexically constrained machine translation allows the user to manipulate the output sentence by enforcing the presence or absence of certain words and phrases. Although current approaches can enforce terms to appear in the translation, they often struggle to make the constraint word form agree with the rest of the generated output. Our manual analysis shows that 46% of the errors in the output of a baseline constrained model for English to Czech translation are related to agreement. We investigate mechanisms to allow neural machine translation to infer the correct word inflection given lemmatized constraints. In particular, we focus on methods based on training
the model with constraints provided as part of the input sequence. Our experiments on the English-Czech language pair show that this approach improves the translation of constrained
terms in both automatic and manual evaluation
by reducing errors in agreement. Our approach
thus eliminates inflection errors, without introducing new errors or decreasing the overall
quality of the translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje příspěvek Univerzity Karlovy do soutěže ve vícejazyčném překladu indoeurópskych jazyků s nedostatečnými zdroji na konferenci WMT21.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes Charles University submission for  Multilingual Low-Resource Translation for Indo-European Languages shared task at WMT21. We competed in translation from Catalan into Romanian, Italian and Occitan. Our systems are based on shared multilingual model. We show that using joint model for multiple similar language pairs improves upon translation quality in each pair. We also demonstrate that chararacter-level bilingual models are competitive for very similar language pairs (Catalan-Occitan) but less so for more distant pairs. We also describe our experiments with multi-task learning, where aside from a textual translation, the models are also trained to perform grapheme-to-phoneme conversion.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje příspěvek Univerzity Karlovy do soutěže v překladu terminologie na konferenci WMT21.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This  paper  describes  Charles  University  submission for Terminology translation Shared Task at WMT21.  The objective of this task is to design a system which translates certain terms based on a provided terminology database, while preserving high overall translation quality. We competed in English-French language pair. Our approach is based on providing the desired translations alongside the input sentence and  training the model to use these provided terms. We lemmatize the terms both during the training and inference, to allow the model to learn how to produce correct surface forms of the words, when they differ from the forms provided in the terminology database. Our submission ranked second in Exact Match metric which evaluates the ability of the model to produce desired terms in the translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje účast týmu CUNI-MTIR v soutěži COVID-19 MLIA @ Eval Task 3.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the participation of our team (CUNIMTIR) in the COVID-19 MILA Machine Translation (MT) task. We present our implementation of four systems (English into French, German, Swedish and Spanish) in both constrained and unconstrained settings. We employ the Marian implementation of the Transformer model to train the constrained systems in the given training data (MLIA MT parallel data), while in the unconstrained systems, we use external medicaldomain training data for the base models and then fine-tune those models using MLIA MT data</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje účast týmu CUNI-MTIR v soutěži COVID-19 MLIA @ Eval Task 2.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The multi-lingual search task in MLIA Community Effort aims at improving
COVID-19 related information access for searchers in multi-lingual settings [3].
We choose in our participation to build a monolingual system where we index
the provided English documents and use the English queries for retrieval (monolingual system) then we design five runs in the monolingual settings.
As for the bilingual task, we design five runs where the documents are in English, and the queries are translated into English following the query-translation
approach.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dokáže AI tvořit umělecká díla? Hudbu, malířství, literaturu, film, divadlo? Nebo něco úplně nového? Dokáže neuronová síť vzbudit emoce? Zapojte stroje do tvorby spolu s garantem výzvy THEaiTRE.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Can AI create works of art? Music, painting, literature, film, theatre? Or something completely new? Can a neural net stir emotions? Engage the machines in the creation along with the THEaiTRE challenge guarantor.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Neuronové sítě jsou nejmodernější metodou strojového učení pro mnoho problémů v NLP. Jejich úspěch ve strojovém překladu a dalších úlohách NLP je fenomenální, ale jejich interpretovatelnost je náročná. Chceme zjistit, jak neuronové sítě reprezentují význam. Za tímto účelem navrhujeme prozkoumat rozložení významu ve vektorovém prostoru reprezentace slov v neuronových sítích trénovaných pro úlohy NLP. Dále navrhujeme zvážit různé teorie významu ve filosofii jazyka a najít metodologii, která by nám umožnila tyto oblasti propojit.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Neural networks are the state-of-the-art method of machine learning for many problems in NLP. Their success in machine translation and other NLP tasks is phenomenal, but their interpretability is challenging. We want to find out how neural networks represent meaning. In order to do this, we propose to examine the distribution of meaning in the vector space representation of words in neural networks trained for NLP tasks. Furthermore, we propose to consider various theories of meaning in the philosophy of language and to find a methodology that would enable us to connect these areas.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dataset MultiWOZ (Budzianowski et al.,2018) je často užíván na poměřování schopností generovat odpověď z kontextu v případě dialogových systému zaměřených na úkoly. V této práci identifikujeme nekonzistence v předzpracování dat a reportování tří metrik založených na evaluačním korpusu, tj., BLEU skóre, míry Inform a míry Success, v kontextu tohoto datasetu. Poukazujeme na několik problémů benchmarku MultiWOZ jako je neuspokojivé předzpracování dat, nedostatečné nebo nedostatečně specifikované evaluační metriky, nebo neohebná databáze. Ve spravedlivých podmínkách jsme znovu vyhodnotili 7 end-to-end a 6 policy optimization modelů a ukázali jsme, že jejich původně reportovaná skóre nemohou být přímo srovnávána. Abychom ulehčili porovnávání budoucích systémů, zveřejňujeme naše soběstačné standardizované evaluační skripty. Rovněž dáváme základní doporučení pro budoucí vyhodnocování založená na evaluačním korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The MultiWOZ dataset (Budzianowski et al.,2018) is frequently used for benchmarking context-to-response abilities of task-oriented dialogue systems. In this work, we identify inconsistencies in data preprocessing and reporting of three corpus-based metrics used on this dataset, i.e., BLEU score and Inform &amp; Success rates. We point out a few problems of the MultiWOZ benchmark such as unsatisfactory preprocessing, insufficient or under-specified evaluation metrics, or rigid database. We re-evaluate 7 end-to-end and 6 policy optimization models in as-fair-as-possible setups, and we show that their reported scores cannot be directly compared. To facilitate comparison of future systems, we release our stand-alone standardized evaluation scripts. We also give basic recommendations for corpus-based benchmarking in future works.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Se stále rostoucím tempem výzkumu
a velký objem vědecké komunikace,
stipendisty čeká nelehký úkol. Nejen musí
udržují krok s rostoucí literaturou v
svých vlastních a souvisejících oborů, musí vědci stále častěji také vyvracet pseudovědu a
dezinformace. Tyto potřeby motivovaly
rostoucí zaměření na výpočetní metody pro zlepšení vyhledávání, sumarizace a
analýzy odborných dokumentů. Nicméně
různé oblasti výzkumu vědeckého zpracování dokumentů zůstávají roztříštěné. K dosažení
širší komunitě NLP a AI/ML, společné úsilí v této oblasti a
umožnit sdílený přístup k publikovanému výzkumu, my
uspořádal 2. seminář o zpracovávání dokumentů (SDP) v rámci programu NAACL 2021 jako
virtuální událost (https://sdproc.org/2021/). The
SDP workshop sestával z výzkumné dráhy,
tři pozvané rozhovory a tři sdílené úkoly
(LongSumm 2021, SCIVER a 3C). Program byl zaměřen na NLP, získávání informací a dolování dat pro vědecké dokumenty s důrazem na identifikaci a
řešení otevřených výzev.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>With the ever-increasing pace of research
and high volume of scholarly communication,
scholars face a daunting task. Not only must
they keep up with the growing literature in
their own and related fields, scholars increasingly also need to rebut pseudo-science and
disinformation. These needs have motivated
an increasing focus on computational methods for enhancing search, summarization, and
analysis of scholarly documents. However, the
various strands of research on scholarly document processing remain fragmented. To reach
out to the broader NLP and AI/ML community, pool distributed efforts in this area, and
enable shared access to published research, we
held the 2nd Workshop on Scholarly Document Processing (SDP) at NAACL 2021 as a
virtual event (https://sdproc.org/2021/). The
SDP workshop consisted of a research track,
three invited talks and three Shared Tasks
(LongSumm 2021, SCIVER and 3C). The program was geared towards NLP, information retrieval, and data mining for scholarly documents, with an emphasis on identifying and
providing solutions to open challenges.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Švandovo divadlo chystá premiéru první české hry, jejíž dialogy „napsala“ umělá inteligence. Inscenace AI: Když robot píše hru vznikla ve spolupráci s Matematicko-fyzikální fakultou Univerzity Karlovy a připomíná sto let od uvedení hry R.U.R. od Karla Čapka, v níž slovo „robot“ poprvé zaznělo.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Švandovo Theatre is about to premiere the first Czech play, the dialogue of which was "written" by an artificial intelligence. "AI: When a robot writes a play" was created in collaboration with the Faculty of Mathematics and Physics of Charles University and commemorates a century since the release of the play R.U.R. by Karel Čapek, in which the word "robot" was first mentioned.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Seznámení uchazečů o různé projekty našimi zkušenostmi s koordinací projektu ELITR.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Describing our experience from the coordination of the EU project ELITR for grant applicants.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představil jsem výzkumný projekt ELITR a současný stav poznání v oblasti strojového překladu a překladu mluvené řeči.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I presented the research project ELITR and the state of the art in machine translation and speech translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení dvou aktuálních aplikovaných projektů ELITR a Bergamot a projektu základního výzkumu NEUREM3, realizovaných na ÚFALu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A brief description of two applied projects, ELITR and Bergamot, and one basic research project NEUREM3, run at UFAL</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představil jsem obor počítačové lingvistiky s důrazem na změnu paradigmatu zavedením neuronových sítí, zejména v oblasti strojového překladu a tlumočení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I presented the area of computational linguitics with a focus on the paradigm change brought by neural networks, in articular in the application area of machine translation and interpreting.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek na semináři pro studenty překladatelství a zejména tlumočnictví.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A presentation for students of translation studies and interpretring.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>542 / 5000
Translation results
Tento článek představuje systém automatického překladu řeči zaměřený na živé titulkování konferenčních prezentací. Popisujeme celkovou architekturu a klíčové komponenty zpracování. Důležitější je, že vysvětlujeme naši strategii budování komplexního systému pro koncové uživatele z mnoha jednotlivých komponent, z nichž každá byla testována pouze v laboratorních podmínkách. Systém je funkčním prototypem, který je rutinně testován v rozpoznávání anglické, české a německé řeči a současně je prezentován přeložený do 42 cílových jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents an automatic speech translation system aimed at live subtitling of conference presentations. We describe the overall architecture and key processing components. More importantly, we explain our strategy for building a complex system for end-users from numerous individual components, each of which has been tested only in laboratory conditions. The system is a working prototype that is routinely tested in recognizing English, Czech, and German speech and presenting it translated simultaneously into 42 target languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje praktické zkušenosti z testování systému pro simultánní překlad mluvené řeči z projevu řečníků a tlumočníků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes the practical experience from testing our complex system translating from the speech of speakers and interpreters.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato testovací sada strojového překladu obsahuje 2223 českých vět shromážděných v rámci projektu FAUST (https://ufal.mff.cuni.cz/grants/faust, http://hdl.handle.net/11234/1-3308). Každá původní věta obsahující šum byla normalizována (clean1 a clean2) a nezávisle na sobě přeložena do angličtiny dvěma překladateli.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This machine translation test set contains 2223 Czech sentences collected within the FAUST project (https://ufal.mff.cuni.cz/grants/faust, http://hdl.handle.net/11234/1-3308). Each original (noisy) sentence was normalized (clean1 and clean2) and translated to English independently by two translators.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve stati se analyzují na základě korpusových dat vybraná adjektiva tvořená od sloves příponou -lý a porovnávají se s dalšími typy slovesných adjektiv adjektiv z hlediska vzájemné zaměnitelnosti a významových rozdílů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Selected Czech adjectives derived from the verbs by the suffix -lý are analyzed and compared with the other types of deverbal adjectives and their mutual interchanges are presented in comparison with their independent meanings. The Russian system of deverbal adjectives is briefly demonstrated as to their position different from Czech system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Studie se zabývá tématem z české slovotvorby, konkrétně stanovenám směru tvoření ve dvojicích bezpříponového substantiva a příbuzného slovesa (např. skok - skočit, sůl - solit). Hledá kritéria, která by směr tvoření v těchto dvojicích pomohla určit. Zatímco korpusová frekvence je z důvodu formálně-významových asymetrií a významových posunů kritériem nespolehlivým, užitečným vodítkem jsou alternace v předponových i kořenných morfémech zkoumaných substantiv a sloves a také způsob tvoření vidových protějšků – toto kritérium se dá aplikovat na slovesa domácí i cizí, včetně neologismů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The study deals with a topic in Czech word formation, namely with the determination of the direction of formation in pairs of a suffixless noun and a related verb (e.g. skok "jump" - skočit "to jump", sůl "salt" - solit "to salt"). While corpus frequency is an unreliable criterion due to formal-meaning asymmetries and meaning shifts, alternations in the prefix and root morphemes of the studied nouns and verbs, as well as the way of formation of the aspectual counterparts, are useful clues - the latter criterion can be applied to both domestic and foreign verbs, including neologisms.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku sledujeme vztah slovosledu a aktuálního členění ve srovnání angličtiny s češtinou, a to ve dvou oblastech: umístění příslovečných určení času a místa v  ohniskové části věty a sémantického dosahu částic zvaných fokalizátory. Materiálem nám byl anotovaný paralelní korpus anglicko–český (PCEDT).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we study the relationship between word order and topic-focus articulation in comparison of English and Czech, namely in two areas: the position of proverbial determinations of time and place in the focal part of a sentence and the semantic range of particles called focalizers. We used the material of an annotated parallel English-Czech corpus (PCEDT).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení funkcionality repozitáře LINDAT/CLARIAH-CZ, tedy hlavně CLARIN/Dspace, která podporuje principy Open Science, v první řadě tzv. FAIR principy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Introducing the functionality of the LINDAT/CLARIAH-CZ repository, that is, mainly CLARIN/Dspace, which supports the principles of Open Science, primarily the so-called FAIR principles.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Proč a jakým způsobem integruje datový repozitář LINDATu služby podporující a využívající přímé citování dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Why and how the LINDAT data repository integrates services supporting and using direct data citation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Informace o centrálním repozitáři LINDAT/CLARIAH-cz, obzvláště z hlediska FAIR aspektů</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Information about the central repository of LINDAT/CLARIAH-cz, especially in terms of FAIR aspects</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přinosy zveřejňování dat, obzvláště pomocí kvalitních certrifikovaných repozitářů dobře integrovaných s další infrastrukturou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The benefits of data publishing, especially with high-quality certrified repositories well integrated with other infrastructure.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představíme repozitář pro jazyková data a jiná data primárně z humanitních oborů. Repozitář je provozován velkou výzkumnou infrastrukturou (VVI) LINDAT/CLARIAH-CZ a umožňuje snadné, bezpečné a dlouhodobé uchování, sdílení a přímé citování vědeckých dat. Repozitář jsme vlastním vývojem rozšířili o modul, který umožňuje datům přiřadit libovolnou licenci a tuto případně i online podepsat. Stručně vysvětlíme, proč je takové řešení nutné. Věnujeme se také FAIR aspektům repozitáře a tomu, proč je mnohem lepší svá výzkumná data uložit k nám, než je sdílet pomocí své vlastní webové stránky a jinými ad hoc metodami.

Součástí repozitáře je také řešení pro výběr co nejotevřenější možné licence pro daná data: Public License Selector. uživatel při ukládání dat odpovídá na otázky, které jej navigují bludištěm veřejných licencí k nejvhodnější možnosti pro daná data nebo software, jež chce sdílet.

Obě propojená řešení, která představíme, jsou open source, vyvíjená veřejně v rámci VVI LINDAT/CLARIAH-CZ (https://github.com/ufal/clarin-dspace, https://github.com/ufal/public-license-selector) a jsou používána v řadě instalací v celé Evropě.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We will present a repository for language data and other data primarily from the humanities. The repository is operated by the large research infrastructure (LRI) LINDAT/CLARIAH-CZ and allows for easy, safe and long-term preservation, sharing and direct quoting of scientific data. We have expanded the repository by our own development to include a module that allows data to assign any license and sign this, if necessary, online. We will briefly explain why such a solution is necessary. We also address FAIR aspects of the repository, and why it is far better to store our research data with us than to share it with our own website and other ad hoc methods.  

The repository also includes a solution for selecting the most open possible license for the data: Public License Selector. The user answers questions when saving data, navigating it through a maze of public licenses to the most appropriate option for the data or software they want to share.  

The two connected solutions we will present are open source, developed publicly under LRI LINDAT/CLARIAH-CZ (https://github.com/ufal/clarin-dspace, https://github.com/ufal/public-license-selector) and used in a variety of installations across Europe.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení funkcionality repozitáře LINDAT/CLARIAH-CZ, tedy hlavně CLARIN/Dspace, která podporuje principy Open Science, v první řadě tzv. FAIR principy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Introducing the functionality of the LINDAT/CLARIAH-CZ repository, that is, mainly CLARIN/Dspace, which supports the principles of Open Science, primarily the so-called FAIR principles.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Technická zprava shrnuje pravidla pro zařazení německých synonym do synonymního slovníku SynSemClass.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This report presents a guideline for including German synonyms into the multilingual SynSemClass lexicon.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>SynSemClass lexicon 3.5 zkoumá s ohledem na kontextově založenou slovesnou synonymii, sémantickou „ekvivalenci“ českých, anglických a německých sloves a jejich valenční chování v paralelních česko-anglických a německo-anglických jazykových zdrojích.
SynSemClass 3.5 je vícejazyčná ontologie typu události založená na třídách synonymních významů sloves, doplněná sémantickými rolemi a odkazy na existující sémantické lexikony.
Kromě již použitých odkazů na položky PDT-Vallex, EngVallex, CzEngVallex, FrameNet, VerbNet, PropBank, Ontonotes a English WordNet pro české a anglické záznamy jsou nové odkazy na německé jazykové lexikální zdroje, jako je Woxikon, E-VALBU a GUP, využívány pro německé slovesné záznamy.
Německá část lexikonu byla vytvořena v rámci projektu Multilingual Event-Type-Anchored Ontology for Natural Language Understanding (META-O-NLU) dvěma spolupracujícími týmy - týmem Univerzity Karlovy, Matematicko-fyzikální fakulty, Ústavu formální a aplikované lingvistiky, Praha (ÚFAL), Česká republika a týmem Německého výzkumného centra pro umělou inteligenci (DFKI) Speech and Language Technology, Berlín, Německo.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The SynSemClass synonym verb lexicon version 3.5 investigates, with respect to contextually-based verb synonymy, semantic ‘equivalence’ of Czech, English and German verb senses and their valency behavior in parallel Czech-English and German-English language resources. 
SynSemClass3.5 is a multilingual event-type ontology based on classes of synonymous verb senses, complemented with semantic roles and links to existing semantic lexicons.
Apart of the already used links to PDT-Vallex, EngVallex, CzEngVallex, FrameNet, VerbNet, PropBank, Ontonotes, and English WordNet for Czech and English entries the new links to German language lexical resources are exploited for German verb entries, such as Woxikon, E-VALBU, and GUP. 
The German part of the lexicon has been created within the project Multilingual Event-Type-Anchored Ontology for Natural Language Understanding (META-O-NLU) by two cooperating teams - by the team of the Charles University, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics, Prague (ÚFAL), Czech Republic and the team of the German Research Center for Artificial Intelligence (DFKI) Speech and Language Technology, Berlin, Germany.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>PDT-Vallex je český valenční lexikon propojený s reálnými texty v několika českých korpusech a je také součástí PDT-C 1.0. PDT-Vallex 4.0 obsahuje 14528 valenčních rámců pro 8498 sloves, která se vyskytla v PDT-C 1.0. PDT-Vallex 4.0 je rozšířenou verzí původního PDT-Vallexu, který obsahoval 7121 slovesných záznamů s 11933 rámci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>PDT-Vallex is a Czech valence lexicon linked to real texts in several Czech corpora and is also part of PDT-C 1.0. PDT-Vallex 4.0 contains 14528 valence frames for 8498 verbs that occurred in PDT-C 1.0. PDT-Vallex 4.0 is an expanded version of the original PDT-Vallex, which contained 7121 verb entries with 11933 frames.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku prezentujeme výsledky automatického srovnání valenčních rámců vzájemně propojených adjektivních a slovesných lexikálních jednotek, obsažených ve valenčních slovnících NomVallex a VALLEX. Rozlišujeme devět derivačních typů deverbálních adjektiv a zkoumáme, zda vykazují systémové valenční chování, nebo spíše nesystémové valenční chování. K projevům nesystémového valenčního chování patří změny v počtu valenčních doplnění a především nesystémové formy aktantů, zvláště nesystémová předložková skupina.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present results of an automatic comparison of valency frames of interlinked adjectival and verbal lexical units based on the valency lexicons NomVallex and VALLEX. We distinguish nine derivational types of deverbal adjectives and examine whether they tend to display systemic valency behavior, or rather the non-systemic one. The non-systemic valency behavior includes changes in the number of valency complementations and, more dominantly, non-systemic forms of actants, especially a prepositional group.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme metodu pro rozšíření pokrytí Slovníku českých diskurzních konektorů (CzeDLex) pomocí anotační projekce. Využíváme dva jazykové zdroje: (i) Penn Discourse Treebank 3.0 jako zdroj ručně anotovaných diskurzních vztahů v angličtině a (ii) Pražský česko -anglický závislostí korpus 2.0 jako překlad anglických textů do češtiny a propojení mezi tokeny v obou jazycích. Přestože byl CzeDLex původně extrahován z velkého českého korpusu, vedla prezentovaná metoda k přidání řady nových konektorů a nových užití (diskurzních typů) pro již přítomné položky ve slovníku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a method for extending coverage of the Lexicon of Czech Discourse Connectives – CzeDLex – using annotation projection. We take advantage of two language resources: (i) the Penn Discourse Treebank 3.0 as a source of manually annotated discourse relations in English, and (ii) the Prague Czech–English Dependency Treebank 2.0 as a translation of the English texts to Czech and a link between tokens on the two language sides. Although CzeDLex was originally extracted from a large Czech corpus, the presented method resulted in an addition of a number of new connectives and new types of usages (discourse types) for already present entries in the lexicon.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CzeDLex 1.0 je první produkční verze slovníku českých diskurzních konektorů, navazující na 3 předchozí verze vývojové. Slovník obsahuje konektory částečně automaticky extrahované z Pražského diskurzního korpusu 2.0 (PDiT 2.0) a z dalších zdrojů. Všechna slovníková hesla byla ručně zkontrolována, přeložena do angličtiny a doplněna dalšími lingvistickými informacemi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CzeDLex 1.0 is the first production version of a lexicon of Czech discourse connectives, following three previous  development versions. The lexicon contains connectives partially automatically extracted from the Prague Discourse Treebank 2.0 (PDiT 2.0) and other resources. All entries in the lexicon have been manually checked, translated to English and supplemented with additional linguistic information.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládáme experimenty zaměřené na predikci významu explicitních mezivětných diskurzních vztahů v češtině a angličtině, s využitím hlubokého učení (BERT) pro predikci významů a anotační projekce z angličtiny do češtiny pro zvětšení množství trénovacích dat pro češtinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present experiments in predicting a discourse sense for explicit inter-sentential discourse relations in Czech and English, using embedding and deep learning (fine-tuned BERT) to predict the senses, and annotation projection from English to Czech to increase the size of training data for Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pokoušíme se osvětlit rozličné způsoby, kterými jazyky udávají datum a čas, a možnosti, které máme, když se příslušné konstrukce snažíme jednotně zachytit ve formalismu Universal Dependencies. Probíráme příklady z několika jazykových rodin a navrhujeme jejich anotaci. Doufáme, že tento (nebo podobný) návrh by se mohl v budoucnosti stát součástí anotačních pravidel UD, což by přispělo k větší konzistenci treebanků UD. Současné anotace mají ke konzistenci daleko, jak přehledně ukazujeme v dodatcích k této studii.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We attempt to shed some light on the various ways how languages specify date and time, and on the options we have when trying to annotate them uniformly across Universal Dependencies. Examples from several language families are discussed, and their annotation is proposed. Our hope is to eventually make this (or similar) proposal an integral part of the UD annotation guidelines, which would help improve consistency of he UD treebanks. The current annotations are far from consistent, as can be seen from the survey we provide in appendices to this paper.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies (UD, univerzální závislosti) je mnohojazyčná kolekce korpusů, morfologicky a syntakticky anotovaných v jednotném stylu. Představujeme volitelnou rovinu hloubkově-syntaktické anotace v UD, zvanou Enhanced Universal Dependencies (rozšířené univerzální závislosti). Podáváme přehled rozšířených anotací ve verzi 2.8 a zvažujeme dvě možná budoucí pokračování: poloautomatické přidávání známých typů rozšíření do nových jazyků a přidávání nových typů rozšíření.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies (UD) is a multilingual collection of corpora featuring morphological and syntactic annotation in a unified style. We discuss an optional layer of deep-syntactic annotation in UD, called Enhanced Universal Dependencies. We survey the existing enhanced representation as of release 2.8 and consider two possible future expansions: semi-automatic addition of existing enhancement types to new languages, and addition of new enhancement types.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představuji Universal Dependencies, mezinárodní komunitní projekt, v jehož rámci vznikla morfologická a syntaktická anotační pravidla aplikovatelná na všechny přirozené jazyky světa. Probírám modely pro automatickou syntaktickou analýzu a jejich využití v digitálních humanitních studiích: v lingivistice, výuce jazyků, dokumentaci ohrožených jazyků, jazykové typologii a historickém vývoji jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I present Universal Dependencies, a worldwide community project to design morphological and syntactic annotation guidelines applicable to all the languages of the world. I discuss automatic parsing models and their use in digital humanities: linguistics, language teaching, documentation of endangered languages, linguistic typology and language change.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představím Universal Dependencies, mezinárodní komunitní projekt a kolekci morfosyntakticky anotovaných datových sad (treebanků) pro více než 100 jazyků. Tato kolekce je neocenitelným zdrojem pro různé lingvistické studie od gramatických konstrukcí v jednom jazyce k jazykové typologii, dokumentaci ohrožených jazyků a historickému vývoji jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I will present Universal Dependencies, an international community project and a collection of morphosyntactically annotated data sets (“treebanks”) for more than 100 languages. The collection is an invaluable resource for various linguistic studies, ranging from grammatical constructions within one language to language typology, documentation of endangered languages, and historical evolution of language.

From the engineering perspective, UD treebanks serve as training data for automatic parsers that can be subsequently used to analyze previously unseen text. The parsed output is an intermediate representation between the input text and its underlying meaning. It is helpful in foreign language learning as well as for automatic extraction of semantic relations (answers to “who did what to whom”). I will thus discuss these semantic aspects in the last part of my talk; in particular, I will look at extensions of UD that have been proposed and that focus more on deep-syntactic and semantic relations expressed in natural language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Deep Universal Dependencies (hluboké univerzální závislosti) je sbírka treebanků poloautomaticky odvozených z Universal Dependencies. Obsahuje přídavné hloubkově-syntaktické a sémantické anotace. Verze Deep UD odpovídá verzi UD, na které je založena. Mějte však na paměti, že některé treebanky UD nebyly do Deep UD zahrnuty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Deep Universal Dependencies is a collection of treebanks derived semi-automatically from Universal Dependencies. It contains additional deep-syntactic and semantic annotations. Version of Deep UD corresponds to the version of UD it is based on. Note however that some UD treebanks have been omitted from Deep UD.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento balíček obsahuje data použitá v soutěži IWPT 2021. Obsahuje trénovací, vývojová a testovací (vyhodnocovací) datové množiny. Data jsou založena na podmnožině vydání 2.7 Universal Dependencies (http://hdl.handle.net/11234/1-3424), ale některé treebanky obsahují další obohacené anotace nad rámec UD 2.7.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This package contains data used in the IWPT 2021 shared task. It contains training, development and test (evaluation) datasets. The data is based on a subset of Universal Dependencies release 2.7 (http://hdl.handle.net/11234/1-3424) but some treebanks contain additional enhanced annotations. Moreover, not all of these additions became part of Universal Dependencies release 2.8 (http://hdl.handle.net/11234/1-3687), which makes the shared task data unique and worth a separate release to enable later comparison with new parsing algorithms. The package also contains a number of Perl and Python scripts that have been used to process the data during preparation and during the shared task. Finally, the package includes the official primary submission of each team participating in the shared task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je čtrnácté vydání treebanků UD, verze 2.8.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the fourteenth release of UD Treebanks, Version 2.8.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je patnácté vydání treebanků UD, verze 2.9.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the fifteenth release of UD Treebanks, Version 2.9.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V monografii předkládáme celistvý popis forem a funkcí vybraných okolnostních určení založený na detailní analýze rozsáhlých korpusových dat. Usilujeme o komplexnost popisu co do šíře i co do hloubky: analyzujeme všechny v korpusech dostupné příklady a pro každý stanovujeme jeho funkci a formu. Výsledkem je podrobný seznam okolnostních významů a jejich jemnějších významových podtypů včetně seznamu formálních realizací, doložený reálnými příklady. Analýza se soustředí na
popis určení prostorových a časových jakožto jádra okolnostních významů.
Kniha se skládá ze čtyř částí.
- Část I: Teoretický úvod do analýzy okolnostních určení (repertoár funktorů, hranice mezi funktory a subfunktory, principy determinace subfunktoru, synonymie a nejednoznačnost ve vztahu forma-funkce, stanovení třídy sekundárních předložek).
- Část II: Podrobný a komplexní popis prostorových a časových určení
- Část III: Dvě srovnávací studie.
- Část IV: Přehled navržených funktorů a subfunktorů</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The necessity of a subdivision of the functors as bearers of the main semantic features in the Functional Generative Description as well as in the annotation scenario for the Prague Dependency Treebanks into more grained units called there subfunctors is presented here. The discussion is focused on the spatial and temporal functors as the most frequently occurring modification types.  The book consists of the four parts.
- Part I: Theoretical introduction to the description of adverbials analysis (repertory of functors, boundary between functors and subfunctors, principles of subfunctor determination, synonymy and ambiguity in form-function relation, determination of the class of secondary prepositions).
- Part II: Corpus-based, detailed and comprehensive description of spatial and temporal modification
- Part III: Two comparative studies.
- Part IV: Overview of proposed functors and subfunctors</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládaný Valenční slovník českých sloves (též VALLEX 4.0) poskytuje informace o valenční struktuře českých sloves v jejich jednotlivých významech, které charakterizuje pomocí glos a příkladů; tyto údaje jsou doplněny o další syntaktické a sémantické charakteristiky. VALLEX 4.0 zachycuje 4 659 českých sloves, která odpovídají 11 030 lexikálním jednotkám, tedy vždy „danému slovesu v daném významu“ (což odpovídá 6 829 lexikálním jednotkám, sdruží-li se vidové protějšky do lexémů).

VALLEX 4.0 poskytované informace obohacuje o charakteristiku sloves vyjadřujících reflexivní a reciproční významy – jde jednak o slovesa, která tyto rysy obsahují ve svém lexikálním významu, jednak o slovesa vyjadřující reciprocitu nebo reflexivitu pomocí specifických syntaktických konstrukcí. VALLEX 4.0 takto charakterizuje 2 909 sloves ve 4 453 lexikálních jednotkách, která vyjadřují či mohou vyjadřovat reciproční vztahy (což odpovídá 2 744 lexikálním jednotkám, sdruží-li se vidové protějšky do lexémů), a 2 291 sloves ve 3 287 lexikálních jednotkách s možností vyjadřovat reflexivitu (jde o 2 039 lexikálních jednotek pro vidové protějšky sdružené do lexémů).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Valency Dictionary of Czech Verbs (also VALLEX 4.0) provides information on the valency structure of Czech verbs in their individual meanings, characterized by glosses and examples; these data are supplemented by other syntactic and semantic characteristics. VALLEX 4.0 captures 4,659 Czech verbs, which correspond to 11,030 lexical units, "a given verb in a given meaning" (which corresponds to 6,829 lexical units if the mode counterparts are grouped into lexemes).

VALLEX 4.0 enriches the information with the characteristics of verbs expressing reflexive and reciprocal meanings - these are both verbs that contain these features in their lexical meaning, and verbs expressing reciprocity or reflexivity using specific syntactic constructions. VALLEX 4.0 thus characterizes 2,909 verbs in 4,453 lexical units, which express or can express reciprocal relations (corresponding to 2,744 lexical units if the visual counterparts are grouped into lexemes), and 2,291 verbs in 3,287 lexical units with the possibility of expressing reflexivity (these are 2,039 lexical units for mode counterparts grouped into lexemes).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto textu na vybraných příkladech představujeme fungování slovníku VALLEX v obou jeho složkách: datové i pravidlové. Předpokládáme, že metodologie zde užitá je užitečná pro další lexikografickou práci. Navrhly jsme také způsob, jak na příkladech z korpusů ověřit vhodnost distinkcí užitých ve VALLEXu. S ohledem na frekvenci jednotlivých typů delimitace významů v textech jsme na příkladu slovesa projet / projíždět navrhly i jistá zjednodušení na škále významů jakožto proces potřebný pro případné budoucí úpravy hesel ve slovníku VALLEX.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this text we present the VALLEX dictionary in both its components: data and rule. We assume that the methodology used here is useful for further lexicographic work. We also proposed a way to verify the suitability of the distinctions used in VALLEX on examples from corpora. With regard to the frequency of individual types of delimitation of meanings in texts, we also proposed certain simplifications on the scale of meanings on the example of the verb projet / projíždět as a process necessary for possible future modifications of passwords in the VALLEX dictionary.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme druhou soutěž IWPT v automatické analýze prostého textu do struktur Enhanced Universal Dependencies. Uvádíme podrobnosti o mírách použitých při vyhodnocení, jakož i o datových sadách použitých pro učení a vyhodnocení. Srovnáváme přístupy jednotlivých týmů, které se soutěže zúčastnily, a rozebíráme výsledky, mimo jiné i ve srovnání s výsledky prvního ročníku této soutěže.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe the second IWPT task on end-to-end parsing from raw text to Enhanced Universal Dependencies. We provide details about the evaluation metrics and the datasets used for training and evaluation. We compare the approaches taken by participating teams and discuss the results of the shared task, also in comparison with the first edition of this task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek přináší diskusi o homonymii českých podstatných jmen s různým nebo kolísavým rodem. Lemata s tímto typem homonymie jsou v novém vydání slovníku MorfFlex považována za různá. Ukazujeme, že rozdělení paradigmat podle rodu je nejen zbytečné, ale také nepraktické. Proto tomuto druhu hononymie říkáme „umělé“.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents a discussion about homonymy of Czech nouns with different or varying genders. The lemmas with this type of homonymy are treated in the new release of the dictionary MorfFlex as separated. We show that the separation of paradigms according to the gender is not only superfluous, but also clumsy, because it forces to make a choice when it is not necessary. That’s why we call this type of homonymy “artificial”.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Koncept textové srozumitelnosti: jako jazyková kompetence vyjádřená v úrovních CEFR, jako čtenářská gramotnost ve studiích OECD PIAAC, jako souhrn textových vlastností.  
 Výhody, které přináší: úspora času, úspora peněz, společenská soudržnost, důvěra ve veřejné instituce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Readability as (1) language competence expressed by CEFR levels, (2) literacy/reading comprehension level in OECD PIAAC studies, (3) a set of textual features. Benefits of clarity: time and cost savings, social cohesion, trust in public administration</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příručka snadných jazyků v Evropě popisuje historické pozadí, principy a postupy Easy Language ve 21 evropských zemích. Pojem Easy Language odkazuje na upravené formy standardních jazyků, jejichž cílem je usnadnit čtení a porozumění.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Handbook of Easy Languages in Europe describes the historical background, the principles and the practices of Easy Language in 21 European countries. The notion of Easy Language refers to modified forms of standard languages that aim to facilitate reading and comprehension.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>EngVallex 2.0 je aktualizovaná verze slovníku EngVallex. Jedná se o anglický protějšek valenčního slovníku PDT-Vallex, který používá stejný pohled na valenci, valenční rámečky a popis povrchové formy slovních argumentů. EngVallex obsahuje také odkazy na PropBank (anglický predicate-argument lexicon). Slovník EngVallex je plně propojen s anglickou stranou PCEDT paralelního treebanku, což je ve reanotovaný PTB korpus ve stylu anotace Prague Dependency Treebank. EngVallex je k dispozici ve formátu XML a také ve formě k online vyhledávání s příklady z PCEDT. EngVallex 2.0 je stejný datový soubor jako EngVallex ve vydání PCEDT 3.0, ale vydává se samostatně na základě benevolentnější licence, čímž se předejde nutnosti pro uživatele získat LDC licenci, která je vázána na PCEDT 3.0 jako celek.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>EngVallex 2.0 as a slightly updated version of EngVallex. It is the English counterpart of the PDT-Vallex valency lexicon, using the same view of valency, valency frames and the description of a surface form of verbal arguments. EngVallex contains links also to PropBank (English predicate-argument lexicon). The EngVallex lexicon is fully linked to the English side of the PCEDT parallel treebank(s), which is in fact the PTB re-annotated using the Prague Dependency Treebank style of annotation. The EngVallex is available in an XML format in our repository, and also in a searchable form with examples from the PCEDT. EngVallex 2.0 is the same dataset as the EngVallex lexicon packaged with the PCEDT 3.0 corpus, but published separately under a more permissive licence, avoiding the need for LDC licence which is tied to PCEDT 3.0 as a whole.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Funkce na generování stop-slov v 110 jazycích, na základě morfologického značkování Universal Dependencies</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Functions to generate stop-word lists in 110 languages, in a way consistent across all the languages supported. The generated lists are based on the morphological tagset from the Universal Dependencies.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení úlohy Argumentation Mining a náčrt anotačního schématu k trénovacím datům z pohledu právních expertů - potenciálních přispěvatelů textů a anotátorů. Širší kontext úlohy, současné možnosti NLP.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presentation of the Argumentation Mining task and an outline of the annotation scheme to train it, explained to law experts - potential text contributors and annotators. A broader context of the task, state of the art in NLP.</seg>
            </tuv>
        </tu>
        
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce shrneme hlavní vlastnosti slovotvorné sítě DeriNet vyvíjené pro češtinu v Ústavu formální a aplikované lingvistiky MFF UK. Nastíníme rozmanitost podobně zaměřených databází vznikajících pro jiné jazyky, jakož i neskromné vyhlídky na jejich syntézu ve stopách Universal Dependencies. Dále se budeme věnovat dvěma tématům, kterými se v souvislosti s DeriNetem zabýváme v poslední době. První souvisí s aplikovatelností tzv. paradigmatického přístupu k odvozování v češtině, kontrastovaného se stromovým přístupem zvoleným v DeriNetu. Druhým tématem je segmentace českých lemmat na morfémy; jakkoli je morfém jedním z “nejklasičtějších” lingvistických pojmů, jeho rigorózní datové uchopení se zdá být překvapivě nesnadnou výzvou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this talk, we will summarize the main features of the word formation network DeriNet developed for Czech at the Institute of Formal and Applied Linguistics, MFF UK. We will also discuss two topics that have been recently addressed in connection with DeriNet. The first is related to the applicability of the so-called paradigmatic approach to Czech word formation, in contrast with the tree approach taken in DeriNet. The second topic is the segmentation of Czech lemmas into morphemes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Práce se zabývá tvorbou agentních jmen v češtině, zejména kompeticí mezi osmi nejfrekventovanějšími agentními příponami. Na základě metod strojového učení je v práci vyčíslen vliv různých formálně-lingvistických vlastností na výslednou volbu přípony při tvorbě agentního substantiva.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The work deals with the formation of agent nouns in Czech, especially the rivalry among the eight most frequent agent suffixes. We use machine learning methods to calculate the influence of several different formal-linguistic properties on the resulting suffix selected when an agent noun is coined.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje projekt ParlaMint z hlediska jeho cílů, úkolů, účastníků, výsledků a aplikačního potenciálu. Projekt vytvořil jazykové korpusy ze zasedání národních parlamentů 17 zemí, celkem téměř půl miliardy slov. Korpusy jsou rozděleny na subkorpusy související s COVID (od listopadu 2019) a referenční korpusy (do října 2019). Korpusy jsou jednotně kódovány podle schématu ParlaMint se stejnými lingvistickými anotacemi podle Universal Dependencies. Ukázky korpusů a konverzních skriptů jsou dostupné z GitHub úložiště projektu. Kompletní korpusy je volně dostupné ke stažení přes repozitář CLARIN.SI a přes concordancery NoSketch Engine a KonText i přes rozhraní Parlameter pro procházení a analýzu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper outlines the ParlaMint project from the perspective of its goals, tasks, participants, results and applications potential. The project produced language corpora from the sessions of the national parliaments of 17 countries, almost half a billion words in total. The corpora are split into COVID-related subcorpora (from November 2019) and reference corpora (to October 2019). The corpora are uniformly encoded according to the ParlaMint schema with the same Universal Dependencies linguistic annotations. Samples of the corpora and conversion scripts are available from the project’s GitHub repository. The complete corpora are openly available via the CLARIN.SI repository for download, and through the NoSketch Engine and KonText concordancers as well as through the Parlameter interface for exploration and analysis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>ParlaMint je vícejazyčný soubor srovnatelných korpusů parlamntních debat. Obsahuje parlamentní data těchto zemí: Belgie, Bulharsko, Česko, Dánsko, Francie, Chorvatsko, Island, Itálie, Litva, Lotyšsko, Maďarsko, Nizozemí, Polsko, Slovinsko, Španělsko, Turecko, UK.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>ParlaMint is a multilingual set of comparable corpora containing parliamentary debates. It contains parliament debates from these countries: Belgium, Bulgaria, Czech Republic, Denmark, Croatia, France, Iceland, Italy, Lithuania, Latvia, Hungary, Netherlands, Poland, Slovenia, Spain, Turkey, UK</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>ParlaMint je vícejazyčný soubor srovnatelných
 korpusů parlamntních debat.
Obsahuje parlamentní data těchto zemí: Belgie, Bulharsko, Česko, Dánsko, Chorvatsko, Island, Itálie, Litva, Lotyšsko, Maďarsko, Nizozemí, Polsko, Slovinsko, Španělsko, Turecko, UK.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>ParlaMint is a multilingual set of comparable
 corpora containing parliamentary debates.
It contains parliament debates from these countries: Belgium, Bulgaria, Czech Republic, Denmark, Croatia, Iceland, Italy, Lithuania, Latvia, Hungary, Netherlands, Poland, Slovenia, Spain, Turkey, UK</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>ParlaMint.ana je vícejazyčný soubor srovnatelných lingvisticky anotovaných korpusů parlamntních debat.

Obsahuje parlamentní data těchto zemí: Belgie, Bulharsko, Česko, Dánsko, Chorvatsko, Island, Itálie, Litva, Lotyšsko, Maďarsko, Nizozemí, Polsko, Slovinsko, Španělsko, Turecko, UK.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>ParlaMint.ana is a multilingual set of comparable linguistically annotated corpora containing parliamentary debates.

It contains parliament debates from these countries: Belgium, Bulgaria, Czech Republic, Denmark, Croatia, Iceland, Italy, Lithuania, Latvia, Hungary, Netherlands, Poland, Slovenia, Spain, Turkey, UK</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>ParlaMint.ana je vícejazyčný soubor srovnatelných lingvisticky anotovaných korpusů parlamntních debat.
Obsahuje parlamentní data těchto zemí: Belgie, Bulharsko, Česko, Dánsko, Francie, Chorvatsko, Island, Itálie, Litva, Lotyšsko, Maďarsko, Nizozemí, Polsko, Slovinsko, Španělsko, Turecko, UK.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>ParlaMint.ana is a multilingual set of comparable linguistically annotated corpora containing parliamentary debates.
It contains parliament debates from these countries: Belgium, Bulgaria, Czech Republic, Denmark, France, Croatia, Iceland, Italy, Lithuania, Latvia, Hungary, Netherlands, Poland, Slovenia, Spain, Turkey, UK</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek zkoumá nedávné pokroky v analýze Index Thomisticus Treebank, který zahrnuje středověké latinské texty Tomáše Akvinského. Výzkum se zaměřuje na dva typy proměnných. Na jedné straně zkoumá, jaký vliv má větší soubor dat na výsledky parsování, na druhé straně jsou analyzovány výkony nových parserů s ohledem na méně aktuální nástroje. Termínem srovnání pro určení efektivního pokroku v parsování jsou výsledky při parsování Index Thomisticus Treebank popsané v předchozí práci. Nejprve je nejvýkonnější parser z těch, kterých se týkala tato studie, testován na větším souboru dat, než byl ten původně použitý. Poté jsou vyhodnoceny i některé kombinace parserů, které byly vyvinuty v téže studii, přičemž je posouzeno, že více trénovacích dat vede k přesnějším výkonům. Nakonec, abychom prozkoumali, jaký vliv mají nově dostupné nástroje na výsledky parsování, trénujeme, testujeme a vyhodnocujeme dva neuronové parsery vybrané mezi těmi, které dosáhly nejlepších výsledků ve sdílené úloze CoNLL 2018. Naše experimenty dosahují dosud nejvyšší dosažené míry přesnosti v automatickém syntaktickém rozboru Index Thomisticus Treebank a latiny celkově.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper investigates the recent advances in parsing the Index Thomisticus Treebank, which encompasses Medieval Latin texts by Thomas Aquinas. The research focuses on two types of variables. On the one hand, it examines the impact that a larger dataset has on the results of parsing; on the other hand, performances of new parsers are analysed with respect to less recent tools. Term of comparison to determine the effective parsing advances are the results in parsing the Index Thomisticus Treebank described in a previous work. First, the best performing parser among those concerned in that study is tested on a larger dataset than the one originally used. Then, some parser combinations that were developed in the same study are evaluated as well, assessing that more training data result in more accurate performances. Finally, to examine the impact that newly available tools have on parsing results, we train, test, and evaluate two neural parsers chosen among those best performing in the CoNLL 2018 Shared Task. Our experiments reach the highest accuracy rates achieved so far in automatic syntactic parsing of the Index Thomisticus Treebank and of Latin overall.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>O umělé inteligenci slyšíme v poslední době víc a víc. Hraje šachy, překládá texty, řídí auta... Ale zvládne umělá inteligence tvořit umění, například napsat divadelní hru? Co od ní v budoucnosti můžeme očekávat? Do jakých oblastí lidského života zasáhne? A co to vlastně je, ta umělá inteligence?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We've been hearing more and more about artificial intelligence lately. It plays chess, translates texts, drives cars... But can artificial intelligence create art, such as writing a play? What can we expect from it in the future? What areas of human life will it interfere with? And what exactly is this artificial intelligence?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>O umělé inteligenci slyšíme v poslední době víc a víc. Hraje šachy, překládá texty, řídí auta... Ale zvládne umělá inteligence tvořit umění, například napsat divadelní hru? Co od ní v budoucnosti můžeme očekávat? Do jakých oblastí lidského života zasáhne? A co to vlastně je, ta umělá inteligence?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We've been hearing more and more about artificial intelligence lately. It plays chess, translates texts, drives cars... But can artificial intelligence create art, such as writing a play? What can we expect from it in the future? What areas of human life will it interfere with? And what exactly is this artificial intelligence?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Měkké aspekty projektu THEaiTRE (humanitní vědy, PR, diskuse, diváci...)</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Soft aspects of THEaiTRE (humanities, PR, discussions, spectators...)</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento projekt kombinující počítačovou lingvistiku a divadelnictví má za cíl přímým způsobem reflektovat nástup a zkoumat roli umělé inteligence ve společnosti a v umění a možnost její spolupráce s člověkem. Navrhujeme spojit teatrologický výzkum s komputačně lingvistickým výzkumem, kde aplikace poznatků a výsledků z těchto oborů povede k sestavení a natrénování systému pro automatické generování scénářů divadelních her. Tento systém následně bude naším týmem interaktivně využit pro vytvoření hry, která bude realizována hereckým souborem ke 100. výročí premiéry dramatu R.U.R. Vzniklé dílo a jeho přijetí diváky budeme zpětně zkoumat v rámci obou vědních oborů a získané poznatky využijeme pro další zdokonalení systému, který zpřístupníme na webu, a pro realizaci dalších her.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This project, combining computational linguistics and theatre, aims to directly reflect the onset and explore the role of artificial intelligence in society and art and the possibility of its cooperation with humans. We propose to combine theatrological research with computational linguistic research: the application of results from these fields will lead to the creation and training of a system for automatic generation of theatre play scripts. This system will be interactively used by our team to create a play that will be put on by a theatre on the occasion of the 100th anniversary of the R.U.R drama premiere. The resulting work of art and its acceptance by viewers will be examined, and the knowledge gained will be used to improve the system and to put on another play.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Hackování jazykového modelu GPT-2
Sestavení webové aplikace
Vytváří se scénář divadelní hry
Představení hry na jevišti</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Hacking the GPT-2 language model
Building a web application
Generating a theatre play script
Performing the play on stage</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme dosavadní stav řešení projektu:
1. Nástroj THEaiTRobot
2. Tvorba scénáře
3. Realizace divadelní hry</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the status of the project solution to date:
1. THEaiTRobot tool
2. Script making
3. Realization of the play</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>O umělé inteligenci slyšíme v poslední době víc a víc. Hraje šachy, překládá texty, řídí auta... Ale zvládne umělá inteligence tvořit umění, například napsat divadelní hru? Co od ní v budoucnosti můžeme očekávat? Do jakých oblastí lidského života zasáhne? A co to vlastně je, ta umělá inteligence?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We've been hearing more and more about artificial intelligence lately. It plays chess, translates texts, drives cars... But can artificial intelligence create art, such as writing a play? What can we expect from it in the future? What areas of human life will it interfere with? And what exactly is this artificial intelligence?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>O umělé inteligenci slyšíme v poslední době víc a víc. Hraje šachy, překládá texty, řídí auta... Ale zvládne umělá inteligence tvořit umění, například napsat divadelní hru? Co od ní v budoucnosti můžeme očekávat? Do jakých oblastí lidského života zasáhne? A co to vlastně je, ta umělá inteligence?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We've been hearing more and more about artificial intelligence lately. It plays chess, translates texts, drives cars... But can artificial intelligence create art, such as writing a play? What can we expect from it in the future? What areas of human life will it interfere with? And what exactly is this artificial intelligence?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nástroj THEaiTRobot 1.0 umožňuje uživateli interaktivně generovat scénáře pro jednotlivé divadelní scény.
Nástroj je založen na jazykovém modelu GPT-2 XL.
Při vytváření skriptu tímto způsobem jsme narazili na řadu problémů. Některé problémy se nám podařilo vyřešit různými úpravami, ale některé z nich je třeba vyřešit v budoucí verzi.

THEaiTRobot 1.0 byl použit k vytvoření první hry THEaiTRE, "AI: Když robot píše hru".</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The THEaiTRobot 1.0 tool allows the user to interactively generate scripts for individual theatre play scenes.
The tool is based on GPT-2 XL generative language model.
We encountered numerous problems when generating the script in this way. We managed to tackle some of the problems with various adjustments, but some of them remain to be solved in a future version.

THEaiTRobot 1.0 was used to generate the first THEaiTRE play, "AI: When a robot writes a play".</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme první verzi systému pro interaktivní tvorbu divadelních scénářů. Systém je založen na základním modelu GPT-2 s několika úpravami, se zaměřením na konkrétní problémy, se kterými jsme se setkali v praxi. Popisujeme i další problémy, se kterými jsme se setkali, ale plánujeme je řešit až v budoucí verzi systému. Předložený systém byl použit k vytvoření scénáře divadelní hry, která měla premiéru v únoru 2021.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the first version of a system for interactive generation of theatre play scripts. The system is based on a vanilla GPT-2 model with several adjustments, targeting specific issues we encountered in practice. We also list other issues we encountered but plan to only solve in a future version of the system. The presented system was used to generate a theatre play script premiered in February 2021.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Informujeme o AI: Když robot píše hru, divadelní hře s převážně uměle vytvořeným scénářem. Popisujeme nástroj THEaiTRobot 1.0, který byl použit ke generování scénáře. Diskutujeme o různých problémech, se kterými se v procesu setkáváme, včetně těch, které jsme do určité míry vyřešili, i těch, které plánujeme vyřešit v budoucí verzi systému.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We inform about AI: When a Robot Writes a Play, a theatre play with a mostly artificially generated script. We describe the THEaiTRobot 1.0 tool, which was used to generate the script. We discuss various issues encountered in the process, including those that we solved to some extent as well as those which we plan to solve in a future version of the system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V únoru 2021 jsme zinscenovali první divadelní hru, pro kterou bylo 90% scénáře automaticky generováno systémem umělé inteligence.

Systém THEaiTRobot je založen na jazykovém modelu GPT-2, který vytvořilo konsorcium OpenAI a který je doplněn o automatický překlad. Model jsme museli různě upravovat, zejména abychom se vyhnuli opakování a zapomínání kontextu, a abychom se drželi omezeného souboru postav. Jako vstup do systému jsme použili krátké úvodní výzvy (scénické nastavení a prvních pár řádků dialogu), připravené dramaturgem, které THEaiTRobot rozšířil do celých scén. Scénář byl následně posteditován a uveden na scénu. Recenze většinou zaznamenaly, že AI neumí napsat dobrou hru (zatím), ale uznaly, že představení bylo hlavně zajímavé a zábavné.

Se svým přístupem jsme čelili mnoha omezením. Mohli jsme generovat pouze jednotlivé scény nezávisle, s omezeným počtem postav a s charaktery, které se často náhodně zaměňují a slučují. Systém také nevidí za text scénáře, chybí mu porozumění vztahu scénáře k tomu, co se děje na jevišti. V současné době pracujeme na nové verzi systému, která by měla některé problémy zlepšit a zároveň dále minimalizovat množství lidského vlivu. Měl by také do procesu začlenit koncept dramatických situací.

Projekt THEaiTRE souvisí s dalšími podobnými pokusy, jako je hra Životní styl Richarda a rodiny, muzikál Za plotem, krátký film Sluneční jaro nebo představení divadelní skupiny Improbotics, které do jisté míry využívají automaticky generovaný obsah. Naše hra mezi těmito projekty vyniká tím, že je poměrně dlouhá (60 minut) a zároveň má velmi vysoký podíl automaticky generovaného obsahu (90%).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In February 2021, we staged the first theatre play for which 90% of the script was automatically generated by an artificial intelligence system.

The THEaiTRobot system is based on the GPT-2 language model, created by the OpenAI consortium, complemented with automated translation. We had to adapt the model in various ways, especially to avoid repetitiveness and forgetting of context, and to stick to a limited set of characters. As input for the system, we used short starting prompts (scene setting and first few lines of dialogue), prepared by a dramaturge, which were expanded into full scenes by THEaiTRobot. The script was then post-edited and put on stage. Reviews mostly noted that AI cannot really write a good play (yet), but acknowledged that the performance was mostly interesting and entertaining to watch.

We faced numerous limitations with our approach. We could only generate individual scenes independently, with a limited number of characters, and with the character personalities often randomly switching and merging. Also, the system does not see beyond the text of the script, lacking the understanding of the relation of the script to what is happening on stage. We are currently working on a new version of the system, which should improve on some of the issues, while also further minimizing the amount of human influence. It should also incorporate the concept of dramatic situations into the generation process.

The THEaiTRE project is related to other similar attempts, such as the play Lifestyle of the Richard and Family, the musical Beyond the Fence, the short movie Sunspring, or the performances of the Improbotics theatre group, all of which use automatically generated content to some extent. Our play stands out among these projects by being rather long (60 minutes) while having a very high proportion of automatically generated content (90%).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Panel sleduje české divadelní představení „AI: When a Robot Writes a Play“, které bylo napsáno s pomocí systému AI a – namluveno robotem – pojednává o hledání sounáležitosti v odcizeném světě. S tvůrci hry a dalšími umělci pracujícími s umělou inteligencí prozkoumáme, do jaké míry mohou systémy umělé inteligence imitovat lidskou tvořivost a co tato imitace vypovídá o našem chápání umění a společnosti. Kromě strachu z toho, že počítače nahradí člověka, chceme diskutovat o tom, jaké nové možnosti může umělec nabídnout umění a jak umělci mohou kriticky a originálně přemýšlet o nových technologiích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The panel follows the Czech theater performance “AI: When a Robot Writes a Play”, which was written by the help of an AI system and – narrated by a robot – deals with the quest for belonging in an alienated world. With the makers of the play and other artists working with AI, we will explore to what extent AI systems can imitate human creativity and what this imitation says about our understanding of art and society. Beyond the fear of computers replacing humans, we want to discuss what new possibilities AI can offer to art and how artists can think critically and originally about new technologies.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>O čem robot přemýšlí, co cítí a dokáže napsat divadelní hru? Střípky ze života umělé inteligence jejími vlastními slovy. Futuristický Malý princ z doby postpandemické.

Inscenace, po jejíž každé repríze následuje debata s odborníky na umělou inteligenci i divadelními tvůrci, je nejen svědectvím o současných schopnostech počítačových technologií, ale i poutavou vizí budoucího světa inspirující se v klasikách žánru sci-fi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>What does the robot think about, how does it feel and can it write a play? Snippets from the life of artificial intelligence in its own words. Futuristic Little Prince of the postpandemic era. 

Every reprise will be followed by a debate with experts in artificial intelligence and theater makers. The show is not only a testament to the current capabilities of computer technology, but also an engaging vision of the future world inspired by the sci-fi classics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>R.U.R. Karla Čapka byla první divadelní hra napsaná člověkem o robotech (a lidech). Premiéru měl 25. ledna 1921. O sto let později, se všemi současnými pokroky ve zpracování přirozeného jazyka a umělých neuronových sítích, jsme v našem projektu THEaiTRE tuto myšlenku otočili. 25. ledna 2021 budeme mít premiéru "AI: When a robot writes a play" ("AI: robot píše hru"), divadelní hru o lidech (a robotech), kterou napsala naše umělá inteligence THEaiTRobot. Umožňují to počítačoví lingvisté, kteří pro tento unikátní výzkumný projekt spojují své síly s divadelními odborníky. O co v tomto projektu jde a jak byl vyvinut? Jakým výzvám jsme čelili? Jak ten umělou inteligencí vytvořený scénář vypadá? ...a opravdu chceme, aby umělá inteligence vytvářela umění?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Karel Čapek's R.U.R. was the first theatrical play written by a human about robots (and humans). It premiered on 25 January 1921. A hundred years later, with all the current advances in natural language processing and artificial neural networks, we have turned the idea around in our THEaiTRE project. On 25 January 2021, we will premiere "AI: When a robot writes a play" ("AI: Když robot píše hru"), a theatre play about humans (and robots) written by our artificial intelligence called THEaiTRobot. It is made possible by computational linguists joining forces with theatre experts for this unique research project. What is this project about and how has it been developed? What challenges have we faced? What does the AI-generated script look like? ...and do we actually want AI to create art?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Invertování R.U.R. s umělou inteligencí.
Čapkovo výročí.
Vytváření divadelních her.
Příklady výstupů.
Pohled divadelního experta.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Inverting R.U.R. with Artificial Intelligence.
Čapek anniversary.
Generating Theatre Plays.
Output Examples.
View of a Theatre Expert.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Překladový model z katalánštiny do okcitánštiny pro systém Marian. Je to multi-task model kromě okcitánskeho překladu produkující rovněž fonémický zápis katalánskeho zdroje.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Marian NMT model for Catalan to Occitan translation. It is a multi-task model, producing also a phonemic transcription of the Catalan source. The model was submitted to WMT'21 Shared Task on Multilingual Low-Resource Translation for Indo-European Languages as a CUNI-Contrastive system for Catalan to Occitan.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Data použity pro experiment Ptakopět s odchozím strojovým překladem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The dataset used for the Ptakopět experiment on outbound machine translation. It consists of screenshots of web forms with user queries entered. The queries are available also in a text form. The dataset comprises two language versions: English and Czech. Whereas the English version has been fully post-processed (screenshots cropped, queries within the screenshots highlighted, dataset split based on its quality etc.), the Czech version is raw as it was collected by the annotators.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce popisuje soutěžní systém pro vícejazyčný strojový překlad LMU Mnichov do WMT 2021 pro úlohu č. 1, která studuje překlad mezi 6 jazyky (chorvatština, maďarština, estonština, srbština, makedonština, angličtina) ve 30 směrech. Zkoumáme, do jaké míry mohou dvojjazyčné překladové systémy ovlivňovat mnohojazyčné překladové systémy. Konkrétně jsme natrénovali 30 dvojjazyčných překladových systémů, které pokrývají všechny jazykové dvojice, a použili jsme techniky pro rozšíření dat, jako je zpětný překlad a destilace znalostí.. Náš nejlepší překladový systém má o 5 až 6 BLEU vyšší skóre než silný základní systém poskytovaný organizátory (Goyal et al.,2021). Jak je vidět v žebříčku Dynalabu, náš systém je jediným systémem, který využívá pouze korpus poskytnutý organizátory a nepoužívá žádné předtrénované modely</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the submission of LMU Munich to the  WMT  2021  multilingual machine translation task for small track #1, which studies translation between 6 languages (Croatian,  Hungarian,  Estonian,  Serbian,  Macedonian, English) in 30 directions. We investigate the extent to which bilingual translation systems can influence multilingual translation systems.   More specifically,  we trained 30 bilingual translation systems, covering all language pairs, and used data augmentation techniques such as back-translation and knowledge distillation to improve the multilingual translation systems.  Our best translation system scores 5 to 6 BLEU higher than a strong baseline system provided by the organizers (Goyal et al.,2021).   As  seen  in  the  Dynalab  leaderboard, our  submission  is  the  only  fully  constrained submission that uses only the corpus provided by  the  organizers  and  does  not  use  any  pre-trained models</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Argument těžby se zaměřuje na struktury v přirozeném jazyce související s tlumočením a přesvědčováním, které jsou pro vědeckou komunikaci stěžejní. Většina vědecké rozpravy zahrnuje interpretaci experimentálních důkazů a snahu přesvědčit ostatní vědce, aby přijali stejné závěry. Různé studie o těžbě argumentů se sice zabývaly studentskými eseji a zpravodajskými články, ale těch, které se zaměřují na vědecký diskurz, je stále málo. Tento dokument zkoumá stávající práci v oblasti argumentační těžby odborného diskurzu a poskytuje přehled o aktuálních modelech, datech, úkolech a aplikacích. Určujeme řadu klíčových výzev, před nimiž stojí argumentační těžba ve vědecké oblasti, a navrhujeme některá možná řešení a budoucí směry.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Argument mining targets structures in natural language related to interpretation and persuasion which are central to scientific communication. Most scholarly discourse involves interpreting experimental evidence and attempting to persuade other scientists to adopt the same conclusions. While various argument mining studies have addressed student essays and news articles, those that target scientific discourse are still scarce. This paper surveys existing work in argument mining of scholarly discourse, and provides an overview of current models, data, tasks, and applications. We identify a number of key challenges confronting argument mining in the scientific domain, and suggest some possible solutions and future directions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku představujeme novou architekturu pro obnovu diakritiky založenou na kontextualizovaných vektorových reprezentacích, konkrétně BERT, a vyhodnocujeme ji ve dvanácti jazycích s diakritikou. Dále jsme provedli detailní chybovou analýzu v češtině, jazyce s bohatou morfologií a vysokou úrovní diakritizace. Zejména jsme ručně anotovali všechny chybné predikce a ukázali jsme, že zhruba 44% z chybně určené diakritizace nepředstavují skutečné chyby, nýbrž z 19% paralelní přijatelné varianty nebo dokonce systémové opravy diakritizace indukované různými chybami v datech (25%). Nakonec jsme také detailně kategorizovali skutečné chyby systému. Zdrojový kód jsme vydali zde: https://github.com/ufal/bert-diacritics-restoration.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We propose a new architecture for diacritics restoration based on contextualized embeddings, namely BERT, and we evaluate it on 12 languages with diacritics. Furthermore, we conduct a detailed error analysis on Czech, a morphologically rich language with a high level of diacritization. Notably, we manually annotate all mispredictions, showing that roughly 44% of
them are actually not errors, but either plausible variants (19%), or the system corrections of erroneous data (25%). Finally, we categorize the real errors in detail. We release the code at https://github.com/ufal/bert-diacritics-restoration.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Citlivost modelů hlubokého neuronového učení k šumu na vstupu je známý a výrazný problém. Při strojovém zpracování přirozeného jazyka se výkon modelu často zhoršuje při přirozeně se vyskytujícím šumu, například při překlepech a pravopisných chybách. Aby se tomuto problém zabránilo, modely často využívají data s uměle vytvořenými chybami. Ovšem množství a typ takto generovaného šumu bylo dosud určováno libovolně. My proto navrhujeme modelovat chyby statisticky z korpusů pro opravy gramatiky. Předkládáme pečlivou evaluaci několika současných nástrojů strojového zpracování textu co do robustnosti v několika jazycích a úlohách, včetně morfo-syntaktické analýzy, rozpoznávání pojmenovaných entit, neuronového strojového překladu, podmnožiny úloh v GLUE a porozumění textu. Dále srovnáváme dva přístupy pro zamezení zhoršení výkonu: a) trénování modelů za použití dat se šumem zavedeným pomocí našeho modelu; a b) redukci vstupního šumu pomocí externího nástroje pro kontrolu gramatiky. Zdrojový kód je vydán na adrese https://github.com/ufal/kazitext.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Sensitivity of deep-neural models to input noise is known to be a challenging problem. In NLP, model performance often deteriorates with naturally occurring noise, such as spelling errors. To mitigate this issue, models may leverage artificially noised data. However, the amount and type of generated noise has so far been determined arbitrarily. We therefore propose to model the errors statistically from grammatical-error-correction corpora. We present a thorough evaluation of several state-of-the-art NLP systems' robustness in multiple languages, with tasks including morpho-syntactic analysis, named entity recognition, neural machine translation, a subset of the GLUE benchmark and reading comprehension. We also compare two approaches to address the performance drop: a) training the NLP models with noised data generated by our framework; and b) reducing the input noise with external system for natural language correction. The code is released at https://github.com/ufal/kazitext.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje výsledky soutěže strojových překladačů pro indoevropské jazyky a dalších úloh automatického překladu
a post-editační úlohy, které byly uspořádány v rámci konference o strojovém překladu (WMT) 2021.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the news
translation task, the multilingual low-resource
translation for Indo-European languages, the
triangular translation task, and the automatic
post-editing task organised as part of the Conference on Machine Translation (WMT) 2021.
In the news task, participants were asked to
build machine translation systems for any of
10 language pairs, to be evaluated on test
sets consisting mainly of news stories. The
task was also opened up to additional test
suites to probe specific aspects of translation. In the Similar Language Translation
(SLT) task, participants were asked to develop systems to translate between pairs of
similar languages from the Dravidian and Romance family as well as French to two sim-
ilar low-resource Manding languages (Bambara and Maninka). In the Triangular MT
translation task, participants were asked to
build a Russian to Chinese translator, given
parallel data in Russian-Chinese, Russian-
English and English-Chinese. In the mul-
tilingual low-resource translation for Indo-
European languages task, participants built
multilingual systems to translate among Romance and North-Germanic languages. The task was designed to deal with the translation of documents in the cultural heritage domain for relatively low-resourced languages.
In the automatic post-editing (APE) task, participants were asked to develop systems capable to correct the errors made by an unknown
machine translation systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento dokument představuje výsledky sdílených úkolů z 8. workshopu o překladech do asijských jazyků (WAT2021). WAT2021 se účastnilo 28 týmů a 24 týmů předložilo výsledky překladů pro lidské hodnocení. Obdrželi jsme také 5 článků. Zhruba 2100 výsledků překladů bylo odevzdáno na automatickém hodnotícím serveru a vybraná podání byla vyhodnocena ručně.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the shared
tasks from the 8th workshop on Asian translation (WAT2021). For the WAT2021, 28 teams
participated in the shared tasks and 24 teams
submitted their translation results for the human evaluation. We also accepted 5 research
papers. About 2,100 translation results were
submitted to the automatic evaluation server,
and selected submissions were manually evaluated.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V krátkém vstupu představíme projekt THEaiTRE a řekneme jak roboti píšou divadelní hry.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In a short sketch, we present the THEaiTRE project and say how robots write plays.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Diskuze o inscenaci a projektu Švandova divadla AI: Když robot píše hru, uskutečněná po on-line premiéře 18. února 2021.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Discussion about the production and project of the Švanda Theater AI: When a Robot Writes a Play held after the on-line premiere on February 18, 2021.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V debatě navazující na inscenaci „AI: Když robot píše hru‟ se probírá otázka, která stála u zrodu celého projektu THEaiTRE, zda AI dokáže napsat divadelní hru. Jak to vidí experti z nejrůznějších oborů IT? Jak to vidí divadelní tvůrci, kteří se s textem, jež AI z 90% vytvořila, museli potýkat? Jak to má AI s kreativitou? A k jakým horizontům se AI ubírá nejen v oblasti umění?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the debate following the production of AI: When a Robot Writes a Play, the question that was present from the very beginning of the entire THEaiTRE project is discussed, and that is whether AI can write a play. How do experts from various fields of IT see it? How do theatre creators who had to deal with the text that AI created from 90% see it? How does AI work with creativity? And what other horizons does AI go to besides the field of art?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V debatě navazující na inscenaci „AI: Když robot píše hru‟ se probírá otázka, která stála u zrodu celého projektu THEaiTRE, zda AI dokáže napsat divadelní hru. Jak to vidí experti z nejrůznějších oborů IT? Jak to vidí divadelní tvůrci, kteří se s textem, jež AI z 90% vytvořila, museli potýkat? Jak to má AI s kreativitou? A k jakým horizontům se AI ubírá nejen v oblasti umění?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the debate following the production of AI: When a Robot Writes a Play, the question that was present from the very beginning of the entire THEaiTRE project is discussed, and that is whether AI can write a play. How do experts from various fields of IT see it? How do theatre creators who had to deal with the text that AI created from 90% see it? How does AI work with creativity? And what other horizons does AI go to besides the field of art?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Při příležitosti oslavy sto let od prvního uvedení Čapkovy hry R. U. R. (která přinesla světu mj. slovo „robot“) navrhl Tomáš Studeník  v roce 2019 divadelní kus, jehož autorem by nebyl člověk, ale chytrý software – tzv. umělá inteligence (Artificial Intelligence – AI). Na základě této myšlenky vznikl tým složený z informatiků Matematicko-fyzikální fakulty Univerzity Karlovy (v čele s počítačovým lingvistou Rudolfem Rosou), divadelníků ze Švandova divadla a studentů DAMU (vedený ředitelem Švandova divadla, režisérem a pedagogem Danielem Hrbkem). Ve vzájemné kooperaci a debatě se rozhodli odpovědět na otázku, zda je umělá inteligence schopna napsat divadelní hru a jak se při zpracování takového úkolu bude chovat. Projekci této neobvyklé divadelní hry uvedeme v sále Mozartea Arcidiecézního muzea Olomouc.

Inscenace, po jejíž každé repríze následuje debata s odborníky na umělou inteligenci i divadelními tvůrci, je nejen svědectvím o současných schopnostech počítačových technologií, ale i poutavou vizí budoucího světa inspirující se v klasikách žánru sci-fi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In 2019, the innovator Tomáš Studeník came up with an idea to celebrate one hundred years since the first performance of Karel Čapeks play RUR (which brought the word "robot" into the world) in an unconventional way – by a theatre play written not by a human, but by artificial intelligence. Based on this idea, a team was formed consisting of computer scientists from Charles University Faculty of Mathematics and Physics (led by computational linguist Rudolf Rosa) and theatre-makers from the Švanda Theater and students of The Academy of Performing Arts in Prague (led by the head of the Švanda Theater, director and pedagogue Daniel Hrbek). By mutual cooperation and debate, they decided to answer the question of whether artificial intelligence is able to write a play and how it will behave when processing such a task.

Every reprise will be followed by a debate with experts in artificial intelligence and theater makers. The show is not only a testament to the current capabilities of computer technology, but also an engaging vision of the future world inspired by the sci-fi classics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Automatické hodnocení kvality strojového překladu (MT) bylo zkoumáno po několik desetiletí. Strojový překlad mluveného jazyka (SLT), zejména simultánní, musí zvážit další kritéria a nemá standardní postup hodnocení a široce využívanou sadu nástrojů. Abychom zaplnili tuto mezeru, představujeme SLTev, open-source nástroj pro komplexní hodnocení SLT. SLTev informuje o kvalitě, latenci a stabilitě výstupu kandidáta SLT na základě časově vyznačeného přepisu a překladu odkazu do cílového jazyka. Pokud jde o kvalitu, spoléháme na SacreBLEU, která poskytuje MT hodnotící opatření, jako je chrF nebo BLEU. Pro latenci navrhujeme dvě nové bodovací techniky. V zájmu stability rozšiřujeme dříve definovaná opatření normalizovaným flickerem v naší práci. Navrhujeme také nové zprůměrování starších metod. V projektu IWSLT 2020 SHARED TASK byla použita předběžná verze programu SLTev. Navíc se rozšiřuje sbírka testovacích datových souborů, které jsou přímo přístupné přes SLTev, pro hodnocení systémů srovnatelných napříč papíry.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Automatic evaluation of Machine Translation(MT)  quality  has  been  investigated  over  several  decades.   Spoken  Language  Translation (SLT), especially when simultaneous, needs to consider additional criteria and does not have a standard evaluation procedure and a widely used  toolkit.    To  fill  the  gap,  we  introduce SLTev, an open-source tool for assessing SLT in a comprehensive way.   SLTev reports the quality,  latency,  and  stability  of  an  SLT  candidate output based on the time-stamped transcript  and  reference  translation  into  a  target language.  For quality, we rely on sacreBLEU which provides MT evaluation measures such as chrF or BLEU. For latency, we propose two new scoring techniques.  For stability, we extend  the  previously  defined  measures  with  a normalized Flicker in our work.  We also propose a new averaging of older measures. A preliminary version of SLTev was used in the IWSLT  2020 SHARED  TASK.  Moreover, a  growing  collection  of  test  datasets  directly accessible by SLTev are provided for system evaluation comparable across papers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme koncept rozšíření vícejazyčného slovesného slovníku, který bude zahrnovat také němčinu. V tomto slovníku jsou zatím zahrnuta česká a anglická slovesa a seskupena podle významu a sémantických vlastností. Položky jsou dále propojeny s externími lexikálními zdroji jako VerbNet a PropBank. V tomto článku představujeme náš plán zahrnout také německá slovesa jako kandidáty propojené s existujícími anglickými a českými slovesy.
Také identifikujeme specifické německé lexikální zdroje, se kterými bude slovník provázán.
Cílem této pilotní studie malého rozsahu je poskytnout návrh na rozšíření již existujícího
lexikálního zdroje o nový jazyk.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the concept of extending a multilingual verb lexicon
also to include German. In this lexicon, verbs are grouped by meaning
and by semantic properties (following frame semantics) to form multilingual
classes, linking Czech and English verbs. Entries are further linked
to external lexical resources like VerbNet and PropBank. In this paper,
we present our plan also to include German verbs, by experimenting
with word alignments to obtain candidates linked to existing English
entries, and identify possible approaches to obtain semantic role information.
We further identify German-specific lexical resources to link to.
This small-scale pilot study aims to provide a blueprint for extending a
lexical resource with a new language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pod širým nebem promítneme záznam online premiéry první české divadelní hry, jejíž dialogy vygenerovala umělá inteligence. Následuje beseda s tvůrci unikátního díla o radostech a strastech každodenního života z pohledu robota. Netradiční představení Švandova divadla v Praze vzniklo ve spolupráci s DAMU a Matematicko-fyzikální fakultou Univerzity Karlovy a na Strži bude k vidění ještě před uvedením hry před diváky smíchovské scény. Účast přislíbili: Daniel Hrbek (hudba a režie), Martina Kinská (dramaturgyně), Rudolf Rosa (počítačový lingvista)... a zástupce hereckého souboru Švandova divadla.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the open air, we will show a recording of the online premiere of the first Czech play the dialogue of which was generated by artificial intelligence. Next is a sit-down with the creators of a unique work on the joys and tribulations of everyday life from the robot's point of view. The unconventional performance of the Švandova Theatre in Prague was created in collaboration with DAMU and the Mathematical Physics Faculty of Charles University, and will be on view at Strž before the play is released in front of the audience of the Smíchov scene. Daniel Hrbek (music and direction), Martina Kinská (dramaturgy), Rudolf Rosa (computer linguist)... and a representative of the acting ensemble of the Švanda Theatre.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>O čem robot přemýšlí, co cítí a dokáže napsat divadelní hru? Střípky ze života umělé inteligence jejími vlastními slovy. Futuristický Malý princ z doby postpandemické.

Inscenace, po jejíž každé repríze následuje debata s odborníky na umělou inteligenci i divadelními tvůrci, je nejen svědectvím o současných schopnostech počítačových technologií, ale i poutavou vizí budoucího světa inspirující se v klasikách žánru sci-fi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>What does the robot think about, how does it feel and can it write a play? Snippets from the life of artificial intelligence in its own words. Futuristic Little Prince of the postpandemic era. 

Every reprise will be followed by a debate with experts in artificial intelligence and theater makers. The show is not only a testament to the current capabilities of computer technology, but also an engaging vision of the future world inspired by the sci-fi classics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>THEaiTRE
Je výzkumný projekt oslavující sté výročí premiéry divadelní hry R.U.R od autora Karla Čapka, ve které bylo slovo „robot“ poprvé použito v roce 1921. Na oslavu tohoto 100 let starého jubilea se spojil Institut formální a aplikované lingvistiky na Matematicko-fyzikální fakultě Univerzity Karlovy se Švandovým divadlem na Smíchově s organizátory Hackathons CEE Hacks, aby čelili svým robotům s novou výzvou, dosud nevídanou v plném rozsahu - aby umělá inteligence napsala scénář.
První fáze projektu vyvrcholila premiérou divadelní inscenace AI: Když robot píše hru s cílem vytvořit scénář složený z dialogů vytvořených umělou inteligencí. V současné době se připravuje druhá fáze, která bude uspořádána na rok 2022, kdy bude představen další premiér s propracovanějším jazykovým modelem.
Tento projekt financovala Technologická agentura České republiky ve spolupráci s prg.ai a Divadelní fakultou Akademie múzických umění v Praze.
Rudolf Rosa promluví o technickém zázemí projektu (jak umělá inteligence vytváří scénář), David Košťák promluví o vytvoření tohoto konkrétního scénáře a Daniel Hrbek shrne proces tvorby hry.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>THEaiTRE  
Is a research project celebrating the 100th year anniversary of the premiere of the theater play R.U.R by author Karel Čapek, in which, the word “robot” was used for the first time in 1921. To celebrate this 100 years old jubilee Institute of Formal and Applied Linguistics  at the Faculty of Mathematics and Physics of the Charles University joined together with Švanda Theater in Smíchov and the organizers of Hackathons CEE Hacks to face their robots with a new challenge, not previously seen in its full extent - to make artificial intelligence write a screenplay.
The first phase of the project culminated with the premiere of a theater production AI: When a robot writes a play with the purpose to create a screenplay made up of dialogues generated by artificial intelligence. The second phase is currently in the making, staged for 2022 when another premier with a more sophisticated linguistic model will be introduced.
This project was funded by the Technology Agency of the Czech Republic in cooperation with prg.ai and the Theater Faculty of the Academy of Performing Arts in Prague. 
Rudolf Rosa will talk about the technical background of the project (how artificial intelligence generates a screenplay), David Košťák will talk about the creation of this particular screenplay and Daniel Hrbek will summarize the process of the play´s production.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>O čem robot přemýšlí, co cítí a dokáže napsat divadelní hru? Střípky ze života umělé inteligence jejími vlastními slovy. Futuristický Malý princ z doby postpandemické.

Příběh robota, který po smrti svého mistra zůstal vydán napospas nejrůznějším vzorkům z lidské společnosti​, balancuje na hranici absurdní černé komedie a existenciálního dramatu. 

„Autobiografická“ hra umělé inteligence o hledání blízkosti ve světě, který již nějakou dobu nezná či neumí prostý kontakt, a v němž je tak tou nejhůře zdolatelnou vzdáleností cesta jednoho člověka k druhému.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>What does the robot think about, how does it feel and can it write a play? Snippets from the life of artificial intelligence in its own words. Futuristic Little Prince of the postpandemic era. 

The story of the robot, which after the death of its master was left at the mercy of various individuals of human society, balances on a thin line between absurd black comedy with existential drama.  

An "autobiographical" play written by artificial intelligence that talks about the search for closeness of someone in a world where people have not known or are not able to make simple contact with each other for some time, and in which the path of one person to another is the hardest to cover.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Citace jsou pro vědecký diskurz klíčové. Vedle poskytování dodatečných kontextů výzkumným dokumentům působí citace jako sledovatelé směru výzkumu v určité oblasti a jako důležité měřítko pro pochopení dopadu výzkumné publikace. S rychlým růstem publikací ve výzkumu začínají být velmi důležitá automatizovaná řešení pro identifikaci účelu a vlivu citací. Úloha 3C Citation Context Classification Task organizovaná v rámci druhého semináře o zpracovávání dokumentů Schopodobně @ NAACL 2021 je sdíleným úkolem pro řešení výše uvedených problémů. V tomto příspěvku představujeme náš tým, IITP-CUNI@3C, který se hlásí ke sdíleným úkolům 3C. Pro úkol A, citační kontextovou klasifikaci účelu, navrhujeme neurální víceúčelový vzdělávací rámec, který využívá strukturální informace výzkumných prací a vztah mezi citačním kontextem a citovaným dokumentem pro citační klasifikaci. Pro úkol B, citační kontext ovlivňuje klasifikaci, používáme sadu jednoduchých funkcí pro klasifikaci citací na základě jejich vnímaného významu. Dosahujeme srovnatelného výkonu s ohledem na systémy s nejlepšími výsledky v úkolu A a nahradili jsme většinovou základní linii v úkolu B velmi jednoduchými funkcemi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Citations are crucial to a scientific discourse. Besides providing additional contexts to research papers, citations act as trackers of the direction of research in a field and as an important measure in understanding the impact of a research publication. With the rapid growth in research publications, automated solutions for identifying the purpose and influence of citations are becoming very important. The 3C Citation Context Classification Task organized as part of the Second Workshop on Scholarly Document Processing @ NAACL 2021 is a shared task to address the aforementioned problems. In this paper, we present our team, IITP-CUNI@3C’s submission to the 3C shared tasks. For Task A, citation context purpose classification, we propose a neural multi-task learning framework that harnesses the structural information of the research papers and the relation between the citation context and the cited paper for citation classification. For Task B, citation context influence classification, we use a set of simple features to classify citations based on their perceived significance. We achieve comparable performance with respect to the best performing systems in Task A and superseded the majority baseline in Task B with very simple features.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>S rychlým růstem publikací ve výzkumu nabývají na významu automatizovaná řešení, která mají řešit přetížení vědeckých informací. Správné určení záměru citací je jedním z takových úkolů, který vyhledává aplikace od předpovídání vědeckého dopadu, hledání šíření myšlenek, přes sumarizaci textu až po vytvoření informativnějších citačních indexů. V této probíhající práci využíváme informace z citovaného dokumentu a prokazujeme, že to pomáhá při efektivní klasifikaci citačních záměrů. Navrhujeme neurální víceúčelový vzdělávací rámec, který využívá strukturální informace z výzkumných prací a vztah mezi citačním kontextem a citovaným dokumentem pro citační klasifikaci. Naše počáteční experimenty se třemi datovými soubory pro klasifikaci referenčních citací ukazují, že se začleněním citovaných papírových informací (názvu) náš neurální model dosahuje na datovém souboru ACL-ARC nového stavu s absolutním nárůstem F1 skóre o 5,3% oproti předchozímu nejlepšímu modelu. Náš přístup rovněž předčí podání v rámci 3C Shared task: Citation Context Classification s nárůstem o 8 % resp. 3,6 % oproti předchozímu nejlepšímu skóre Public F1-macro a Private F1-macro.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>With the rapid growth in research publications, automated solutions to tackle scholarly information overload is growing  more  relevant.  Correctly  identifying  the  intent  of  the  citations  is  one  such  task  that  finds  applications  ranging from predicting scholarly impact, finding idea propagation, to text summarization to establishing more informative citation indexers. In this in-progress work, we leverage the cited paper’s information and demonstrate that this helps in the effective classification of citation intents. We propose a neural multi-task learning framework that harnesses the structural information of the research papers and the relation between the citation context and the  cited  paper  for  citation  classification.  Our  initial  experiments  on  three  benchmark  citation  classification  datasets show that with incorporating cited paper information (title), our neural model achieves a new state of the art on the ACL-ARC dataset with an absolute increase of 5.3% in the F1 score over the previous best model. Our approach also outperforms the submissions made in the 3C Shared task: Citation Context Classification with an increase of 8% and 3.6%over the previous best Public F1-macro and Private F1-macro scores respectively.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Korpus ParCzech 3.0 je třetí verze ParCzech, který obsahuje stenografické protokoly sedmého volebního období (2013-2017) a současného 8. období (2017-březen 2021). Protokoly jsou v jejich původním HTML formátu, v Parla-CLARIN  TEI formátu a ve formátu vhodném pro automatické rozpoznávání řeči. Korpus je automaticky obohacen o morfologii, syntax a jmenné entity programy UDPipe 2 a NameTag 2. Audio soubory jsou zarovnané s texty v anotovaných TEI souborech</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The ParCzech 3.0 corpus is the third version of ParCzech consisting of stenographic protocols that record the Chamber of Deputies’ meetings held in the 7th term (2013-2017) and the current 8th term (2017-Mar 2021). The protocols are provided in their original HTML format, Parla-CLARIN TEI format, and the format suitable for Automatic Speech Recognition. The corpus is automatically enriched with the morphological, syntactic, and named-entity annotations using the procedures UDPipe 2 and NameTag 2. The audio files are aligned with the texts in the annotated TEI files.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Uvádíme ParCzech 3.0, mluvený korpus záznamů jednání Poslanecké sněmovny Parlamentu České republiky z období od 25. listopadu 2013 do 1. dubna 2021.

Na rozdíl od předchozích mluvených korpusů češtiny zachováváme nejen ortografii, ale také všechna dostupná metadata (identitu mluvčích, pohlaví, hypertextové odkazy, příslušnosti, politické strany atd.) a doplňujeme je automatickou morfologickou a syntaktickou anotací a rozpoznáním pojmenovaných entit. Korpus je kódován ve formátu TEI, který umožňuje přímočaré a mnohostranné využití.

Díky bohatým metadatům a anotaci je korpus relevantní pro široké spektrum výzkumníků od inženýrů v oblasti rozpoznávání řeči až po teoretické lingvisty zkoumající rétorické vzorce z rozsáhlých materiálů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present ParCzech 3.0, a speech corpus of the Czech parliamentary speeches from The Czech Chamber of Deputies which took place from 25th November 2013 to 1st April 2021.

Different from previous speech corpora of Czech, we preserve not just orthography but also all the available metadata (speaker identities, gender, web pages links, affiliations committees, political groups, etc.) and complement this with automatic morphological and syntactic annotation, and named entities recognition. The corpus is encoded in the TEI format which allows for a straightforward and versatile exploitation.

The rather rich metadata and annotation make the corpus relevant for a~wide audience of researchers ranging from engineers in the speech community to theoretical linguists studying rhetorical patterns at scale.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>AI: Když robot píše hru.
Oslava výročí Karla Čapka.
Jak je to uděláno.
Příklady výstupů.
Pohled dramaturga.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>AI: When a robot writes a play.
Celebrating the anniversary of Karel Čapek.
How it's done.
Examples of outputs.
View of a dramaturge.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>O čem robot přemýšlí, co cítí a dokáže napsat divadelní hru? Střípky ze života umělé inteligence jejími vlastními slovy. Futuristický Malý princ z doby postpandemické.

Inscenace, po jejíž každé repríze následuje debata s odborníky na umělou inteligenci i divadelními tvůrci, je nejen svědectvím o současných schopnostech počítačových technologií, ale i poutavou vizí budoucího světa inspirující se v klasikách žánru sci-fi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>What does the robot think about, how does it feel and can it write a play? Snippets from the life of artificial intelligence in its own words. Futuristic Little Prince of the postpandemic era. 

Every reprise will be followed by a debate with experts in artificial intelligence and theater makers. The show is not only a testament to the current capabilities of computer technology, but also an engaging vision of the future world inspired by the sci-fi classics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje anglicko-německé a anglicko-hauské systémy z Edinburské univerzity pro sdílenou úlohu na WMT 2021 týkající se překladu zpráv.
En-De systémy budujeme ve třech fázích: korpusové filtrování, zpětný překlad a jemné ladění.
Pro En-Ha používáme iterativní zpětný překlad
přístup na vrchol předtrénovaných modelů En-De
a zkoumat mapování vkládání slovní zásoby.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the University of Edinburgh’s constrained submissions of English-German and English-Hausa systems to the
WMT 2021 shared task on news translation.
We build En-De systems in three stages: corpus filtering, back-translation, and fine-tuning.
For En-Ha we use an iterative back-translation
approach on top of pre-trained En-De models
and investigate vocabulary embedding mapping.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies (UD) je formalismus pro morfologickou a syntaktickou anotaci, který byl dosud použit při tvorbě treebanků pro více než 100 přirozených jazyků. V tomto článku nastiňujeme lingvistickou teorii UD, která staví na dlouhé tradici typologicky orientovaných gramatických teorií. V centru stojí gramatické vztahy mezi slovy, které objasňují, jaké morfosyntaktické strategie používají různé jazyky k zakódování struktury predikátů a jejich argumentů. Morfologické rysy a slovní druhy popisují vlastnosti slov. Tvrdíme, že tato teorie poskytuje dobrý základ pro mezijazykově konzistentní anotaci typologicky rozmanitých jazyků způsobem, který podporuje jak počítačové porozumění přirozenému jazyku, tak širší jazykovědné studie.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies (UD) is a framework for morphosyntactic annotation of human language, which to date has been used to create treebanks for more than 100 languages. In this article, we outline the linguistic theory of the UD framework, which draws on a long tradition of typologically oriented grammatical theories. Grammatical relations between words are centrally used to explain how predicate-argument structures are encoded morphosyntactically in different languages while morphological features and part-of-speech classes give the properties of words. We argue that this theory is a good basis for cross-linguistically consistent annotation of typologically diverse languages in a way that supports computational natural language understanding as well as broader linguistic studies.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dialogové systémy orientované na úkoly obvykle vyžadují manuální anotaci dialogových slotů v trénovacích datech, jejichž získání je nákladné.
Navrhujeme metodu, která tento požadavek eliminuje:
K identifikaci potenciálních kandidátů na sloty využíváme slabou supervizi z existujících modelů pro lingvistickou anotaci a poté automaticky identifikujeme doménově relevantní sloty pomocí clusterovacích algoritmů.
Dále používáme výslednou anotaci slotů k natrénování taggeru založeného na neuronové síti, který je schopen provádět tagování slotů bez lidského zásahu. 
Tento tagger je trénován výhradně na výstupech naší metody, a není tedy závislý na žádných označených datech.

Náš model vykazuje špičkový výkon v označování slotů bez anotovaných trénovacích dat na čtyřech různých dialogových doménách. Kromě toho jsme zjistili, že anotace slotů zjištěné naším modelem výrazně zlepšují výkonnost end-to-end modelu pro generování odpovědi v dialogu v porovnání s modelem, který anotace slotů vůbec nepoužívá.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Task-oriented dialogue systems typically require manual annotation of dialogue slots in training data, which is costly to obtain.
We propose a method that eliminates this requirement:
We use weak supervision from existing linguistic annotation models to identify potential slot candidates, then automatically identify domain-relevant slots by using clustering algorithms.
Furthermore, we use the resulting slot annotation to train a neural-network-based tagger that is able to perform slot tagging with no human intervention. 
This tagger is trained solely on the outputs of our method and thus does not rely on any labeled data.

Our model demonstrates state-of-the-art performance in slot tagging without labeled training data on four different dialogue domains. Moreover, we find that slot annotations discovered by our model significantly improve the performance of an end-to-end dialogue response generation model, compared to using no slot annotation at all.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Porozumění textu je jednou z klíčových dovedností, které se učíme během školních let. Jednak je  základem úspěšného porozumění čtenářská gramotnost, jednak textové vlastnosti, tj. jeho srozumitelnost. Srozumitelnosttak představuje
důležitý problém - jak recepce (lidé nemusí být schopni plně porozumět textu kvůli)
k jeho nízké čitelnosti), tak produkce (lidé nemusí být schopni vytvořit dobře srozumitelný
text). Současná studie představuje experimentální metodu měření srozumitelnosti českých textů
různých žánrů na základě triangulace (1) míry správných odpovědí na porozumění
otázky, (2) intratextových rysů a (3) subjektivního hodnocení čitelnosti textu. Metoda
je podrobně diskutována a jednotlivé kroky jsou ilustrovány na příkladu probíhajícího výzkumu
v rámci projektu Lingvistické faktory pochopitelnosti v českých administrativních a vzdělávacích textech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Text comprehension is one of the key skills that are learned during the school years. On the one
hand, reading literacy is fundamental for successful comprehension, on the other hand, comprehension
success is determined by textual features, i.e. its readability. Text readability thus presents an
important issue — from both the reception (people may not be able to comprehend a text fully due
to its low readability) and production perspective (people may not be able to produce a well readable
text). The current study presents an experimental method for measuring readability of Czech texts
of various genres based on the triangulation of (1) the rate of correct answers on comprehension
questions, (2) intra-textual features, and (3) subjective assessment of text readability. The method
is discussed in depth and individual steps are illustrated on an example of a research in progress
under the project Linguistic Factors of Readability in Czech Administrative and Educational Texts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Toto je rozpracovaný dokument o hodnotící
analýze různých přístupů k nakládání s reprezentacemi faktů a postojů v lingvistice. Cílem
této srovnávací analýzy je najít nejvhodnější přístup pro vývoj schématu anotací za účelem vybudování slovníku postojových výrazů v dalších fázích projektu. Mezi analyzovanými přístupy
jsou Appraisal Theory, schéma vyvinuté
Bednárkovou a také analýza sentimentu a techniky opinion miningu a argument miningu. Výsledky tohoto článku by měly být považovány za první
krok ve výzkumu rozlišování faktů a postojů v
diplomatických projevech Rady bezpečnosti OSN.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This is a work-in-progress paper on evaluative
analysis of different approaches to managing representations of facts and attitudes in linguistics. The aim of
this comparative analysis is to find the most suitable approach for developing an annotation scheme in order to
build a dictionary of attitudinal expressions during the further stages of the project. Among the approaches analyzed
are the Appraisal Theory, the scheme developed by
Bednarek, as well as sentiment analysis and opinion
mining techniques, and argument mining. The
results of the current paper should be considered as the first
step in the research of distinguishing facts and attitudes in
diplomatic speeches of the United Nations Security Council.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Strojový překlad je úkol automatického překládání textu z jednoho jazyka do druhého. V posledních letech tomuto oboru dominovala řešení založená na neuronových sítích.
V této prezentaci uvedu krátký úvod k tématu a zmíním několik postupů, které lze použít ke zlepšení konečné kvality překladu. Podělím se také o své zkušenosti z účasti na jedné z nejdéle probíhajících mezinárodních strojově překladatelských soutěží.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Machine Translation is a task of automatically translating text from one language
into another. In recent years, solutions based on neural networks have dominated
the field.
In this talk I will give a short introduction to the topic and mention a few tricks
that can be used to improve the final quality of translation. I will also share my
experience from participating in one of the longest-running international Machine
Translation competitions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci ukazujeme, že automaticky generované otázky a odpovědi mohou být použity k hodnocení kvality strojových překladatelských systémů. V návaznosti na nedávnou práci na hodnocení abstraktní sumarizace textu navrhujeme novou metriku pro systémové hodnocení strojového překladu, porovnáme ji s ostatními nejmodernějšími řešeními a prokážeme její robustnost provedením experimentů pro různé překladatelské směry.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we show that automatically-generated questions and answers can be used to evaluate the quality of Machine Translation systems. Building on recent work on the evaluation of abstractive text summarization, we propose a new metric for system-level Machine Translation evaluation, compare it with other state-of-the-art solutions, and show its robustness by conducting experiments for various translation directions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto dokumentu popisujeme naši účast ve WMT 2021 Metrics Shared Task. Automaticky generované otázky a odpovědi používáme k hodnocení kvality systémů strojového překladu. Naše experimenty jsou založeny na nedávno navržené metodě MTEQA. Pokusy s vyhodnocovacími datovými soubory WMT20 ukazují, že na systémové úrovni dosahuje MTEQA výkonu srovnatelného s jinými nejmodernějšími řešeními, přičemž zohledňuje je velmi omezené informace z celého překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we describe our submission to the WMT 2021 Metrics Shared Task. We use the automatically-generated questions and answers to evaluate the quality of Machine Translation (MT) systems. Our submission builds upon the recently proposed MTEQA framework. Experiments on WMT20 evaluation datasets show that at the system-level the MTEQA metric achieves performance comparable with other state-of-the-art solutions, while considering only a certain amount of information from the whole translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Řešení úloh z benchmarku SCAN, který testuje schopnost neuronových sítí využít kompozicionalitu. Návrh nové úlohy, kde prezentované řešení selhává.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We address the compositionality challenge presented by the SCAN benchmark. Using data augmentation and a modification of the standard seq2seq architecture with attention, we achieve SOTA results on all the relevant tasks from the benchmark, showing the models can generalize to words used in unseen contexts. We propose an extension of the benchmark by a harder task, which cannot be solved by the
proposed method.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>In most of neural machine translation distillation or stealing scenarios, the highest-scoring hypothesis of the target model (teacher) is used to train a new model (student). If reference translations are also available, then better hypotheses (with respect to the references) can be oversampled and poor hypotheses either removed or undersampled. This paper explores the sampling method landscape (pruning, hypothesis oversampling and undersampling, deduplication and their combination) with English to Czech and English to German MT models using standard MT evaluation metrics. We show that careful oversampling and combination with the original data leads to better performance when compared to training only on the original or synthesized data or their direct combination.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Ve většině scénářů destilace nebo krádeže neuronových strojových překladů se k výcviku nového modelu (studenta) používá hypotéza s nejvyšším bodovým ohodnocením cílového modelu (učitele). Jsou-li k dispozici i referenční překlady, pak lze lepší hypotézy (s ohledem na odkazy) přetížit a špatné hypotézy buď odstranit, nebo podtrhnout. Tento dokument zkoumá prostředí metody odběru vzorků (prořezávání, hypotetické nadměrné vzorkování a nedostatečné vzorkování, deduplikace a jejich kombinace) s anglickými až českými a anglickými až německými modely MT pomocí standardních metrik hodnocení MT. Ukazujeme, že pečlivé nadměrné vzorkování a kombinace s původními údaji vede k lepším výsledkům ve srovnání se školením pouze o původních nebo syntetizovaných údajích nebo jejich přímé kombinaci.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek zkoumá vlyv odhadování kvality překladu na slovní úrovni, zpětného překladu a automatického parafrázování na kvalitu odchozího překladu do jazyka, kterému jeho uživatelé nerozumí, a subjektivní důvěru uživatelů v kvalitu tohoto překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Translating text into a language unknown to the text’s author, dubbed outbound translation, is a modern need for which the user experience has significant room for improvement, beyond the basic machine translation facility. We demonstrate this by showing three ways in which user confidence in the outbound translation, as well as its overall final quality, can be affected: backward translation, quality estimation (with alignment) and source paraphrasing. In this paper, we describe an experiment on outbound translation from English to Czech and Estonian. We examine the effects of each proposed feedback module and further focus on how the quality of machine translation systems influence these findings and the user perception of success. We show that backward translation feedback has a mixed effect on the whole process: it increases user confidence in the produced translation, but not the objective quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Testujeme přirozené očekávání, že použití strojového překladu (MT) v profesionálním překladu ušetří lidem čas. Poslední takovou studii provedli Sanchez-Torron a Koehn (2016) s frázovým MT, čímž uměle snížili kvalitu překladu. My se oproti tomu zaměřujeme na vysoce kvalitní neuronový strojový překlad (NMT), který převyšuje kvalitu frázového MT a také ho přijala většina překladatelských společností.
Prostřednictvím experimentální studie, do níž se zapojilo přes 30 profesionálních překladatelů pro anglicko-český překlad, zkoumáme vztah mezi výkonem NMT a časem a kvalitou post-editace. Ve všech modelech jsme zjistili, že lepší systémy MT skutečně vedou k menšímu počtu změn vět v tomto průmyslovém prostředí. Vztah mezi systémovou kvalitou a dobou post-editace však není jednoznačný a na rozdíl od výsledků frázového MT není BLEU stabilním prediktorem času post-editace či konečné výstupní kvality.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We test the natural expectation that using MT in professional translation saves human processing time. The last such study was carried out by Sanchez-Torron and Koehn (2016) with phrase-based MT, artificially reducing the translation quality. In contrast, we focus on neural MT (NMT) of high quality, which has become the state-of-the-art approach since then and also got adopted by most translation companies.
Through an experimental study involving over 30 professional translators for English -> Czech translation, we examine the relationship between NMT performance and post-editing time and quality. Across all models, we found that better MT systems indeed lead to fewer changes in the sentences in this industry setting. The relation between system quality and post-editing time is however not straightforward and, contrary to the results on phrase-based MT, BLEU is definitely not a stable predictor of the time or final output quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>End-to-end neurální systémy automatického rozpoznávání řeči dosáhly v poslední době nejmodernějších výsledků, ale vyžadují velké datové sady a rozsáhlé výpočetní zdroje.
Přenosové učení bylo navrženo k překonání těchto obtíží i napříč jazyky, např. německý ASR trénovaný podle anglického modelu.
Experimentujeme s mnohem méně příbuznými jazyky, přičemž pro české ASR znovu používáme anglický model. Pro zjednodušení převodu navrhujeme používat přechodnou abecedu, češtinu bez přízvuků, a dokládáme, že jde o vysoce efektivní strategii. Technika je užitečná i na samotných českých datech, ve stylu tréninku „coarse-to-fine“.
Dosahujeme podstatného zkrácení doby tréninku a také word error rate (WER).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>End-to-end neural automatic speech recognition systems achieved recently  state-of-the-art results but they require large datasets and extensive computing resources.
Transfer learning has been proposed to overcome these difficulties even across languages, e.g., German ASR trained from an English model.
We experiment with much less related languages, reusing an English model for Czech ASR. To simplify the transfer, we propose to use an intermediate alphabet, Czech without accents, and we document that it is a highly effective strategy. The technique is also useful on Czech data alone, in the style of "coarse-to-fine" training.
We achieve substantial reductions in training time as well as word error rate (WER).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje náš účastnický systém ve úloze Explainable quality estimation of 2nd Workshop on Evaluation &amp; Comparison of NLP Systems. Úkolem odhadu kvality (QE, neboli bezreferenční hodnocení) je předpovídat kvalitu výstupu MT v čase odvození bez přístupu k referenčním překladům. V této navrhované práci nejprve vytvoříme model odhadu kvality na úrovni slov a poté tento model doladíme pro QE na úrovni vět. Námi navržené modely dosahují téměř nejmodernějších výsledků. V QE na úrovni slov se umístíme na 2. a 3. místě v testovaných sadách Ro-En a Et-En pod dohledem. V QE na úrovni vět dosahujeme relativního zlepšení 8,86% (Ro-En) a 10,6% (Et-En) ve smyslu Pearsonova korelačního koeficientu oproti základnímu modelu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes our participating system in the shared task Explainable quality estimation of 2nd Workshop on Evaluation &amp; Comparison of NLP Systems. The task of quality estimation (QE, a.k.a. reference-free evaluation) is to predict the quality of MT output at inference time without access to reference translations. In this proposed work, we first build a word-level quality estimation model, then we finetune this model for sentence-level QE. Our proposed models achieve near state-of-the-art results. In the word-level QE, we place 2nd and 3rd on the supervised Ro-En and Et-En test sets. In the sentence-level QE, we achieve a relative improvement of 8.86% (Ro-En) and 10.6% (Et-En) in terms of the Pearson correlation coefficient over the baseline model.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku zachycujeme výsledky pilotní studie zaměřené na intenzifikátory: absolutně, naprosto a úplně. Vycházíme ze tří korpusů současné češtiny - psaného korpusu SYN2020, webového korpusu ONLINE-ARCHIVE a mluveného korpusu ORTOFON 1. Na základě paralelní anotace náhodného vzorku všech vybraných intenzifikátorů sledujeme funkce a významy těchto výrazů v kontextu.Cílem studie je vymezit vlastnosti, které jsou relevantní pro slovnědruhové určení zkoumaných výrazů a připravit podklady pro jejich disambiguaci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present a preliminary study of three intensifiers (absolutně, naprosto, úplně) based on data from three different corpora, a written corpus SYN2020, a web corpus ONLINE-ARCHIVE, and a spoken corpus ORTOFON 1. Providing a parallel annotation of a random sample of each intensifier, we focus on their functions and meanings in context. We analyse their properties in order to define those features which are relevant to their word class assignment, and to prepare grounds for the future disambiguation tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Výzkum implicitních a explicitních diskurzních vztahů v češtině, založený na srovnání dat korpusové analýzy (korpus PDiT-EDA 1.0, vytvořený pro tento výzkum) a výsledků psycholingvistických experimentů. Studie se zaměřuje na jednolitvé sémantické typy diskurzních vztahů a na jejich mezivětnou a vnitrovětnou realizaci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Research of implicit and explicit discourse relations in Czech, based on a comparison of corpus analysis data (corpus PDiT-EDA 1.0, created for this research) and the results of psycholinguistic experiments. The study focuses on different semantic types of discourse relations and on their inter-sentential and intra-sentential realization.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek je zaměřen na analýzu diskurzních konektorů a dalších diskurzních jevů v češtině na základě korpusových dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the paper, we study discourse connectives and other discourse phenomena in Czech based on corpus data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje výsledky WMT21 Metrics Shared Task. Účastníci byli požádáni o hodnocení výstupů překladatelských systémů soutěžících v překladatelské úloze WMT21 News pomocí automatických metrik pro dvě různé oblasti: zpravodajství a TED talks. Všechny metriky byly hodnoceny podle toho, jak dobře korelují na úrovni systému a segmentu s lidským hodnocením. Na rozdíl od předchozích ročníků jsme letos získali vlastní lidská hodnocení na základě expertního lidského hodnocení prostřednictvím vícerozměrných metrik kvality (MQM). Toto nastavení mělo několik výhod:
(i) ukázalo se, že expertní hodnocení je spolehlivější, (ii) byli jsme schopni vyhodnotit všechny metriky na dvou různých doménách s použitím překladů stejných systémů MT, (iii) během vývoje systému jsme přidali 5 dalších překladů pocházejících ze stejného systému. Kromě toho jsme navrhli tři
sady výzev, které hodnotí robustnost všech automatických metrik. Předkládáme rozsáhlou analýzu toho, jak dobře metriky fungují na třech jazykových dvojicích: angličtina→němčina, angličtina→ruština a čínština→angličtina. Dále ukazujeme vliv různých referenčních překladů metriky a porovnáváme naši expertní anotaci MQM s DA skóre získanými pomocí WMT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the WMT21
Metrics Shared Task. Participants were asked
to score the outputs of the translation systems competing in the WMT21 News Translation Task with automatic metrics on two different domains: news and TED talks. All metrics were evaluated on how well they correlate at the system- and segment-level with
human ratings. Contrary to previous years’
editions, this year we acquired our own human ratings based on expert-based human evaluation via Multidimensional Quality Metrics
(MQM). This setup had several advantages:
(i) expert-based evaluation has been shown
to be more reliable, (ii) we were able to
evaluate all metrics on two different domains
using translations of the same MT systems,
(iii) we added 5 additional translations coming from the same system during system development. In addition, we designed three
challenge sets that evaluate the robustness of
all automatic metrics. We present an extensive analysis on how well metrics perform
on three language pairs: English→German,
English→Russian and Chinese→English. We
further show the impact of different reference
translations on reference-based metrics and
compare our expert-based MQM annotation
with the DA scores acquired by WMT.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dokument prezentuje nový, sjednocený morfologický popis číslovek a zájmen, který byl zkompilován pro nejnovější vydání Pražských závislostních korpusů (Prague Dependency Treebank – Consolidated 1.0) a jeho nedílnou součást je morfologický slovník MorfFlex. Na základě zkušeností s anotací skutečných dat a s užíváním morfologického slovníku byly navrženy konkrétní změny. Pro oba slovní druhy byl navržen nový soubor podtypů, založený zejména na morfologickém kritériu a jeho kombinaci se sémantickými vlastnostmi a dalšími relevantními rysy, jako je ne/určitost u číslovek a posesivita, reflexivita a klitičnost u zájmen. Každý podtyp má specifickou hodnotu na 2. pozici morfologické značky, která slouží také jako ukazatel použitelnosti dalších kategorií ve značce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents a novel and unified morphological description of numerals and pronouns, as compiled for the newest edition of the Prague Dependency Treebank (Prague Dependency Treebank – Consolidated 1.0) and its integral part the morphological dictionary MorfFlex. On the basis of considerable experience with real data annotation and the use of the morphological dictionary, particular changes were proposed. For both of the parts of speech a new set of subtypes was proposed, based mainly on the morphological criterion and its combination with semantic properties and other relevant features, such as definiteness in numerals and possessivity, reflexivity and clitichood in pronouns. Each subtype has a specific value at the 2nd position of the morphological tag which serves also as an indicator of the applicability of other tag categories.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme neautoregresivní přístup k opravě gramatiky založený na znacích s automaticky generovanými transformacemi znaků. Nedávno byla jako alternativa k současným systémům pro opravu gramatiky typu enkodér-dekodér navržena klasifikace korekčních oprav jednotlivých slov. Ukazujeme, že náhrada celých slov může být neoptimální a může vést k explozi počtu pravidel pro opravy typu překlepů, diakritizační opravy a opravy v morfologicky bohatých jazycích, a proto navrhujeme metodu pro generování transformací znaků z korpusu pro opravu gramatiky. Dále jsme natrénovali znakové transformační modely pro češtinu, němčinu a ruštinu a dosáhli jsme solidních výsledků a dramatického zrychlení ve srovnání s autoregresivními systémy. Zdrojový kód je zveřejněn zde: https://github.com/ufal/wnut2021_character_transformations_gec.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We propose a character-based non-autoregressive GEC approach, with automatically generated character transformations. Recently, per-word classification of correction edits has proven an efficient, parallelizable alternative to current encoder-decoder GEC systems. We show that word replacement edits may be suboptimal and lead to explosion of rules for spelling, diacritization and errors in morphologically rich languages, and propose a method for generating character transformations from GEC corpus. Finally, we train character transformation models for Czech, German and Russian, reaching solid results and dramatic speedup compared to autoregressive systems. The source code is released at https://github.com/ufal/wnut2021_character_transformations_gec.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme RobeCzech, jednojazyčnou RoBERTu, (jazykový model) trénovaný pouze na českých datech. RoBERTa je robustně optimalizovaný přístup pro předtrénování založený na Transformeru. V příspěvku ukazujeme, že RobeCzech výrazně překonává podobně velké vícejazyčné i české kontextualizované modely a zlepšuje současné výsledky v pěti vyhodnocovaných úlohách automatického jazykového zpracování, přičemž dosahuje nejlepších známých výsledků ve čtyřech z nich. Model RobeCzech je veřejně dostupný zde: https://hdl.handle.net/11234/1-3691 a zde: https://huggingface.co/ufal/robeczech-base.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present RobeCzech, a monolingual RoBERTa language representation model trained on Czech data. RoBERTa is a robustly optimized Transformer-based pretraining approach. We show that RobeCzech considerably outperforms equally-sized multilingual and Czech-trained contextualized language representation models, surpasses current state of the art in all five evaluated NLP tasks and reaches state-of-the-art results in four of them. The RobeCzech model is released publicly at https://hdl.handle.net/11234/1-3691 and https://huggingface.co/ufal/robeczech-base.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Derivations (UDer) je kolekce harmonizovaných lexikalních sítí zachycujících slovotvorné, zvl. derivační vztahy jednotlivých jazyků v témže anotačním schématu. Základní datovou strukturou tohoto anotačního schématu jsou zakořeněné stromy, ve kterých uzly odpovídají lexémům a hrany reprezentují derivační, příp. kompozitní vztahy. Stávající verze UDer v1.1 obsahuje 31 harmonizovaných zdrojů pokrývajících 21 různých jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Derivations (UDer) is a collection of harmonized lexical networks capturing word-formation, especially derivational relations, in a cross-linguistically consistent annotation scheme for many languages. The annotation scheme is based on a rooted tree data structure, in which nodes correspond to lexemes, while edges represent derivational relations or compounding. The current version of the UDer collection contains thirty-one harmonized resources covering twenty-one different languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>DeriNet.RU je lexikální síť inspirovaná formétem českého DeriNetu. Jako taková zachycuje základní vztahy pro tvorbu slov v ruštině. DeriNet.RU je vytvořen na podkladě slovníku z velkého korpusu Araneum Russicum Maius a na kombinaci gramatické komponenty, modelu strojového učení využívajícího metodu Random Forest a algoritmu Maximum Spanning Tree.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>DeriNet.RU is DeriNet-like lexical network of that captures core word-formation relations for Russian. It is based on a top of large corpus Araneum Russicum Maius corpus and a novel combination of a grammar-based component, supervised machine-learning model of Random Forest, and Maximum Spanning Tree algorithm.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Během našeho setkání se zaměříme na budoucnost divadla, scénických umění a rituálů ve věku algoritmů, dat a různých pokusů virtualizovat každý atom a okamžik našich žitých zkušeností. Přežijí naše autentické způsoby vyjadřování a lidská zkušenost spojená s údivem, experimentováním a vzdorem v metaverzu, v systémech řízených strojovým učením a umělou inteligencí? Anebo se stanou předvídatelnými efekty nových infrastruktur? Vzniká posthumanistická estetika, osvobozujeme novou tvořivost?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We will meet to reflect upon the future of theater, performance arts and rituals in an age of algorithms, data and various attempts to virtualize every atom and moment of our lived experiences. Will our genuine expressions and experience of human agency in wondering, experimenting and even disobedience survive in the metaverse, ML and AIs driven systems or become predictable effects of new infrastructures? Is there an emerging posthuman aesthetics and creativity that we are liberating?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V krátkém vstupu představíme projekt THEaiTRE a uvedeme ukázku z vygenerovaného scénáře.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We briefly introduce the THEaiTRE project and present a sample from the generated script.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nejmodernější kontextové vložení získáváme z velkých jazykových modelů dostupných pouze pro několik jazyků. U ostatních se musíme naučit reprezentace pomocí mnohojazyčného modelu. Probíhá diskuse o tom, zda lze vícejazyčné vložení sladit do prostoru sdíleného v mnoha jazycích. Ortogonální strukturální sonda (Limisiewicz a Mareček, 2021) nám umožňuje odpovědět na tuto otázku pro specifické jazykové rysy a naučit se projekci založenou pouze na jednojazyčných komentovaných datových souborech. Hodnotíme syntaktické (UD) a lexikální (WordNet) strukturální informace zakódované v mBERT kontextové reprezentaci pro devět různých jazyků. Pozorujeme, že u jazyků úzce spjatých s angličtinou není nutná žádná transformace. Vyhodnocená informace je zakódována ve sdíleném mezijazyčném vkládacím prostoru. Pro ostatní jazyky je výhodné použít ortogonální transformaci naučenou samostatně pro každý jazyk. Úspěšně aplikujeme naše zjištění na nulovou a málo natočenou analýzu přes jazyk.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>State-of-the-art contextual embeddings are obtained from large language models available only for a few languages. For others, we need to learn representations using a multilingual model. There is an ongoing debate on whether multilingual embeddings can be aligned in a space shared across many languages. The novel Orthogonal Structural Probe (Limisiewicz and Mareček, 2021) allows us to answer this question for specific linguistic features and learn a projection based only on mono-lingual annotated datasets. We evaluate syntactic (UD) and lexical (WordNet) structural information encoded inmBERT's contextual representations for nine diverse languages. We observe that for languages closely related to English, no transformation is needed. The evaluated information is encoded in a shared cross-lingual embedding space. For other languages, it is beneficial to apply orthogonal transformation learned separately for each language. We successfully apply our findings to zero-shot and few-shot cross-lingual parsing.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nejmodernější kontextové vložení získáváme z velkých jazykových modelů dostupných pouze pro několik jazyků. U ostatních se musíme naučit reprezentace pomocí mnohojazyčného modelu. Probíhá diskuse o tom, zda lze vícejazyčné vložení sladit do prostoru sdíleného v mnoha jazycích.
Ortogonální strukturální sonda (Limisiewicz a Mareček, 2021) nám umožňuje odpovědět na tuto otázku pro specifické jazykové rysy a naučit se projekci založenou pouze na jednojazyčných komentovaných datových souborech. Hodnotíme syntaktické (UD) a lexikální (WordNet) strukturální informace zakódované v mBERT kontextové reprezentaci pro devět různých jazyků.
Pozorujeme, že u jazyků úzce spjatých s angličtinou není nutná žádná transformace. Vyhodnocená informace je zakódována ve sdíleném mezijazyčném vkládacím prostoru. Pro ostatní jazyky je výhodné použít ortogonální transformaci naučenou samostatně pro každý jazyk. Úspěšně aplikujeme naše zjištění na nulovou a málo natočenou analýzu přes jazyk.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>State-of-the-art contextual embeddings are obtained from large language models available only for a few languages. For others, we need to learn representations using a multilingual model. There is an ongoing debate on whether multilingual embeddings can be aligned in a space shared across many languages.
The novel Orthogonal Structural Probe (Limisiewicz and Mareček, 2021) allows us to answer this question for specific linguistic features and learn a projection based only on mono-lingual annotated datasets. We evaluate syntactic (UD) and lexical (WordNet) structural information encoded inmBERT's contextual representations for nine diverse languages.
We observe that for languages closely related to English, no transformation is needed. The evaluated information is encoded in a shared cross-lingual embedding space. For other languages, it is beneficial to apply orthogonal transformation learned separately for each language. We successfully apply our findings to zero-shot and few-shot cross-lingual parsing.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vzhledem k nedávnému úspěchu předcvičených modelů v NLP byla velká pozornost věnována interpretaci jejich vyjádření. Jedním z nejvýraznějších přístupů je strukturální sondování (Hewitt a Manning, 2019), kde se provádí lineární projekce slovních vložek s cílem přiblížit topologii závislostních struktur. Při této práci zavedeme nový typ strukturálního sondování, kdy se lineární projekce rozloží na 1. izomorfní prostorovou rotaci, 2. lineární škálování, které určí a změří nejdůležitější rozměry. Kromě syntaktické závislosti hodnotíme naši metodu na dvou neotřelých úkolech (lexikální hypernymie a pozice ve větě). Společně cvičíme sondy pro více úkolů a experimentálně ukazujeme, že lexikální a syntaktické informace jsou v reprezentacích odděleny. Díky ortogonálnímu omezení jsou navíc Strukturální sondy méně náchylné k memorování.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>With the recent success of pre-trained models in NLP, a significant focus was put on interpreting their representations. One of the most prominent approaches is structural probing (Hewitt and Manning, 2019), where a linear projection of word embeddings is performed in order to approximate the topology of dependency structures. In this work, we introduce a new type of structural probing, where the linear projection is decomposed into 1. iso-morphic space rotation; 2. linear scaling that identifies and scales the most relevant dimensions. In addition to syntactic dependency, we evaluate our method on two novel tasks (lexical hypernymy and position in a sentence). We jointly train the probes for multiple tasks and experimentally show that lexical and syntactic information is separated in the representations. Moreover, the orthogonal constraint makes the Structural Probes less vulnerable to memorization.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vzhledem k nedávnému úspěchu předcvičených modelů v NLP byla velká pozornost věnována interpretaci jejich vyjádření. Jedním z nejvýraznějších přístupů je strukturální sondování (Hewitt a Manning, 2019), kde se provádí lineární projekce slovních vložek s cílem přiblížit topologii závislostních struktur. Při této práci zavedeme nový typ strukturálního sondování, kdy se lineární projekce rozloží na 1. izomorfní prostorovou rotaci, 2. lineární škálování, které určí a změří nejdůležitější rozměry. Kromě syntaktické závislosti hodnotíme naši metodu na dvou neotřelých úkolech (lexikální hypernymie a pozice ve větě). Společně cvičíme sondy pro více úkolů a experimentálně ukazujeme, že lexikální a syntaktické informace jsou v reprezentacích odděleny. Díky ortogonálnímu omezení jsou navíc Strukturální sondy méně náchylné k memorování</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>With the recent success of pre-trained models in NLP, a significant focus was put on interpreting their representations. One of the most prominent approaches is structural probing (Hewitt and Manning, 2019), where a linear projection of word embeddings is performed in order to approximate the topology of dependency structures. In this work, we introduce a new type of structural probing, where the linear projection is decomposed into 1. iso-morphic space rotation; 2. linear scaling that identifies and scales the most relevant dimensions. In addition to syntactic dependency, we evaluate our method on two novel tasks (lexical hypernymy and position in a sentence). We jointly train the probes for multiple tasks and experimentally show that lexical and syntactic information is separated in the representations. Moreover, the orthogonal constraint makes the Structural Probes less vulnerable to memorization.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ačkoli mobilita a pohyb získaly v poslední době na významu v interakcionistickém výzkumu společenského jednání, není příliš známo, jaké důsledky má pohyb pro konkrétní vývoj interakčních epizod. Prostřednictvím dvou veřejně přístupných videoklipů zachycujících situace „silniční zuřivosti“ popisujeme a analyzujeme stěžejní rysy práce rukou při eskalaci a úpadku emocionálně nabité interakce mezi účastníky silničního provozu. Vyhýbáme se apriorně kognitivistickému postoji a v detailech ukazujeme, jak může být práce rukou sama o sobě konstituentem vzteku a že může vést k otevřenému konfliktu na hranici fyzického násilí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Although mobility and movement has recently gained importance within interactionist studies of social action, not much is known about the consequentiality of being on the move for the particular unfolding of interactional episodes. Utilising two publicly accessible video clips of ‘road rage’ situations, we describe and analyse the centrality of hand-work in the escalation and decline of an emotionally charged interaction between members of traffic. Avoiding an a priori cognitivist stance, we show in detail how the work of hands can be constitutive of anger itself, and that it can lead to open conflict on the boundary of physical violence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládaná studie se zabývá kvalitativní analýzou specifické skupiny vztahů textové koherence, a to takovými vztahy, které byly v anotaci Pražského diskurzního korpusu označeny jako pragmatické či „nepravé“, na rozdíl od většinově se vyskytujících vztahů sémantických.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The study presents a qualitative analysis of pragmatic discourse relations as annotated on Czech
texts of the Prague Dependency Treebank. A detailed study of these relations (as opposed to semantic
relations) with their widest contexts reveals a considerable diversity and shows some space
for improvement in the annotation scheme.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek zkoumá možnosti a limity lokální analýzy textové koherence s ohledem na jevy globální koherence a vyšší výstavby textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present article investigates possibilities and limits of local (shallow) analysis of discourse coherence with respect to the phenomena of global coherence and higher composition of texts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pandemie Covid-19 vyvolala globální poptávku po přesných a aktuálních informacích, které často pocházejí z angličtiny a je třeba je přeložit. K natrénování systému strojového překladu pro tak úzké téma využíváme doménová trénovací data v jiných jazycích, a to jak z příbuzných, tak ze vzdálených jazykových rodin. Experimentujeme s různými rozvrhy učení pomocí metody transfer learning a pozorujeme, že přenos prostřednictvím více než jednoho pomocného jazyka přináší největší zlepšení. Porovnáváme výstupy s mnohojazyčným trénováním a nacházíme lepší výsledky při použití transfer learningu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Covid-19 pandemic has created a global demand for accurate and up-to-date information which often originates in English and needs to be translated. To train a machine translation system for such a narrow topic, we leverage in-domain training data in other languages both from related and unrelated language families. We experiment with different transfer learning schedules and observe that transferring via more than one auxiliary language brings the most improvement. We compare the performance with joint multilingual training and report superior results of the transfer learning approach.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme naše dva NMT systémy, které byly odeslány do soutěže WMT2021 v anglicko-českém překladu zpráv: CUNI-DocTransformer (document-level CUBBITT) a CUNI-Marian-Baselines. První z nich vylepšujeme lepším předzpracováním segmentace vět a následným zpracováním pro opravu chyb v číslech a jednotkách. Druhý z nich používáme při experimentech s různými variantami techniky backtranslation.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe our two NMT systems submitted to the WMT2021 shared task in English-Czech news translation: CUNI-DocTransformer (document-level CUBBITT) and CUNI-Marian-Baselines.
We improve the former with a better sentence-segmentation pre-processing
 and a post-processing for fixing errors in numbers and units.
We use the latter for experiments with various backtranslation techniques.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme příspěvek Charles-UPF pro sdílenou úlohu o hodnocení přesnosti generovaných textů na konferenci INLG 2021. Náš systém dokáže automaticky detekovat chyby pomocí kombinace systému pro generování přirozeného jazyka založeného na pravidlech a předtrénovaných jazykových modelů. Nejprve využíváme systém pro generování přirozeného jazyka založený na pravidlech, který generuje fakta odvozená ze vstupních dat. Pro každou větu, kterou vyhodnocujeme, vybereme podmnožinu faktů, které jsou relevantní na základě měření sémantické podobnosti s danou větou. Nakonec dotrénujeme předtrénovaný jazykový model na anotovaných datech spolu s relevantními fakty pro jemnou detekci chyb. Na testovací sadě dosahujeme 69% výtěžnosti (recall) a 75% přesnosti (precision) s modelem natrénovaným na mixu dat anotovaných lidmi a syntetických dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our Charles-UPF submission for the Shared Task on Evaluating Accuracy in Generated Texts at INLG 2021. Our system can detect the errors automatically using a combination of a rule-based natural language generation (NLG) system and pretrained language models (LMs). We first utilize a rule-based NLG system to generate sentences with
facts that can be derived from the input. For each sentence we evaluate, we select a subset of facts which are relevant by measuring semantic similarity to the sentence in question. Finally, we finetune a pretrained language model on annotated data along with the relevant facts for fine-grained error detection. On the test set, we achieve 69% recall and 75% precision with a model trained on a mixture of human-annotated and synthetic data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek představuje poloautomatickou metodu pro konstruování slovotvorných sítí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article presents a semi-automatic method for the construction of word-formation networks focusing particularly on derivation. The proposed approach applies a sequential pattern mining technique to construct useful morphological features in an unsupervised manner. The features take the form of regular expressions and later are used to feed a machine-learned ranking model. The network is constructed by applying the learned model to sort the lists of possible base words and selecting the most probable ones. This approach, besides relatively small training set and a lexicon, does not require any additional language resources such as a list of vowel and consonant alternations, part-of-speech tags etc. The proposed approach is evaluated on lexeme sets of four languages, namely Polish, Spanish, Czech, and French. The conducted experiments demonstrate the ability of the proposed method to construct linguistically adequate word-formation networks from small training sets. Furthermore, the performed feasibility study shows that the method can further benefit from  the interaction with a human language expert within the  active learning framework.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška představuje výsledky na konferenci WMT21 o překladu s velmi malým množstvím zdrojů a bez paralelních dat a soutěží příspěvek, se kterým se účastil tým LMU Mnichov. Dále se probíráme designová rozhodnutími, která museli účastníci soutěže udělat a spekulujeme o tom, jaké by mohly být budoucí směry ve strojovém překladu překladů s malými zdroji.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk presents overall task findings and LMU Munich submission to the very low resource and unsupervised translation at WMT21. Further, we discuss design decisions made both by LMU and other shared task participants and speculate what the future directions in machine translation of low resource translation might be.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme závěry soutěžního ve strojovém překladu bez paralelních data nebo s velmi málo paralelními daty na WMT2021. V rámci tohoto úkolu se komunita zabývala strojovým překladem s velmi málo paralelními daty  mezi němčinou a hornolužickou srbštinou, překladem bez paralelních dat mezi němčinou a dolnolužickou srbštinou a překladem s málo paralelními daty mezi ruštinou a čuvašštinou, všemi menšinovými jazyky s aktivními jazykovými komunitami pracujícími na zachování těchto jazyků. Díky tomu se nám podařilo získat většinu digitálních dat dostupných pro tyto jazyky a nabídnout je účastníkům úkolu. Soutěžního úkolu se účastnilo celkem šest týmů. Článek představuje úkoly a výsledky a pojednává o osvědčených postupech do budoucna.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the findings of the WMT2021 Shared Tasks in Unsupervised MT and Very Low Resource Supervised MT. Within the task, the community studied very low resource translation between German and Upper Sorbian, unsupervised translation between German and Lower Sorbian and low resource translation between Russian and Chuvash, all minority languages with active language communities working on preserving the languages, who are partners in the evaluation. Thanks to this, we were able to obtain most digital data available for these languages and offer them to the task participants. In total, six teams participated in the shared task. The paper discusses the background, presents the tasks and results, and discusses best practices for the future.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme naše systémy pro soutěžní úkol v strojovém překladu bez paralelní dat a s velmi málo paralelními data na WMT21: mezi němčinou a hornolužickou srbštinou, němčinou a dolnolužickou srbštinou a ruštinou a čuvašštinou. Naše nízkozdrojové systémy (němčina↔hornolužická srbština, ruština↔čuvaština) jsou předtrénovány na párech příbuzných jazyků s dostatkem data. Tyto systémy jsme dotrénovali pomocí dostupných autentických paralelních dat a dále vylepšili opakovaným zpětným překladem. Německo↔dolnolužický systém je inicializován nejlepším hornolužickosrbským systémem a vylepšen opakovaným zpětným překladem pouze za použití jednojazyčných dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our submissions to the WMT21 shared task in Unsupervised and Very Low-Resource machine translation between German and Upper Sorbian, German and Lower Sorbian, and Russian and Chuvash. Our low-resource systems (German↔Upper Sorbian, Russian↔Chuvash) are pre-trained on high-resource pairs of related languages. We fine-tune those systems using the available authentic parallel data and improve by iterated back-translation. The unsupervised German↔Lower Sorbian system is initialized by the best Upper Sorbian system and improved by iterated back-translation using monolingual data only.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pozorujeme, že různé druhy chyb, kterých se dopouštějí systémy pro generování přirozeného jazyka, jsou velmi málo reportovány v literatuře. To je problém, protože chyby jsou důležitým ukazatelem toho, kde by se systémy měly ještě zlepšit. Pokud autoři uvádějí pouze celkové metriky výkonnosti, zůstává výzkumná komunita v nevědomosti o konkrétních nedostatcích v nejmodernějších přístupech. Vedle kvantifikace rozsahu nedostatečného vykazování chyb tento článek poskytuje doporučení pro identifikaci, analýzu a vykazování chyb.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We observe a severe under-reporting of the different kinds of errors that Natural Language Generation systems make. This is a problem, because mistakes are an important indicator of where systems should still be improved. If authors only report overall performance metrics, the research community is left in the dark about the specific weaknesses that are exhibited by ‘state-of-the-art’ research. Next to quantifying the extent of error under-reporting, this position paper provides recommendations for error identification, analysis and reporting.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme scénář divadelní hry AI: When a Robot Writes a Play (AI: Když robot píše hru), kterou napsala umělá inteligence v rámci projektu THEaiTRE.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the script of the theatre play AI: When a Robot Writes a Play (AI: Když robot píše hru), which was written by artificial intelligence within the THEaiTRE project.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška úvodem krátce představí některé úlohy, kterými se zabývá počítačová lingvistika, například automatickou opravu překlepů/gramatiky či automatický větný rozbor.
Hlavní pozornost bude věnována úloze strojového překladu, zejména vývoji různých typů překladačů z angličtiny do češtiny během posledního desetiletí. Dnešní nejlepší překladače jsou založeny na technologiích umělé inteligence, konkrétně hlubokých neuronových sítí, a kvalita výsledného překladu se blíží úrovni profesionální překladatelské agentury. Některé překladové chyby jsou způsobeny překládáním jednotlivých vět nezávisle, což řeší tzv. document-level machine translation.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The lecture will briefly introduce some of the tasks dealt with in computer linguistics, such as automatic correction of misspellings/grammar or automatic sentence analysis.
The main focus will be on the role of machine translation, in particular the development of different types of English-to-Czech translators over the last decade. Today's best translators are based on artificial intelligence technologies, namely deep neural networks, and the quality of the resulting translation is close to that of a professional translation agency. Some translation errors are caused by translating sentences independently, which is being solved by so-called document-level machine translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Existují desítky datových zdrojů pro různé jazyky, ve kterých je ručně anotovaná koreference – vztah mezi dvěma nebo více výrazy, které odkazují na tutéž entitu v reálném světě. Dalo by se předpokládat, že takové výrazy obvykle tvoří syntakticky významné jednotky; avšak rozsahy koreferenčních výrazů (zmínek) byly ve většině projektů anotovány prostě vymezením intervalů tokenů, tj. nezávisle na jakékoli syntaktické reprezentaci. Tvrdíme, že by bylo z dlouhodobého hlediska výhodné, kdyby se k sobě anotace syntaxe a koreference přiblížily. Představujeme pilotní empirickou studii, která se zaměřuje na případy, kde koreferenční zmínky pasují nebo naopak nepasují na automaticky přiřazené syntaktické stromy, které odpovídají standardu Universal Dependencies. Studie zahrnuje 8 datových sad pro 7 různých jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>One can find dozens of data resources for various languages in which coreference – a relation between two or more expressions that refer to the same real-world entity – is manually annotated. One could also assume that such expressions usually constitute syntactically meaningful units; however, mention spans have been annotated simply by delimiting token intervals in most coreference projects, i.e., independently of any syntactic representation. We argue that it could be advantageous to make syntactic and coreference annotations convergent in the long term. We present a pilot empirical study focused on matches and mismatches between hand-annotated linear mention spans and automatically parsed syntactic trees that follow Universal Dependencies conventions. 8 datasets for 7 different languages are included in the study.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vysoký výkon velkých předcvičených jazykových modelů (LLM), jako je BERT (Devlin et al., 2019) na úkoly NLP, vyvolal otázky ohledně jazykových schopností BERT a v tom, jak se liší od lidských. V tomto příspěvku přistupujeme k této otázce zkoumáním znalostí BERT o lexikálních sémantických vztazích. Zaměřujeme se na hypernymii, vztah „je-a“, který spojuje slovo s nadřazenou kategorií. Jednoduše používáme metodiku nabádání
zeptejte se BERTe, co je hypernym daného slova. Zjistili jsme, že v prostředí, kde jsou všechny hypernymy uhodnutelné pomocí výzvy, BERT zná hypernymy s přesností až 57%. Navíc BERT s výzvou překonává ostatní modely bez dozoru pro hypernomické objevování i v neomezeném scénáři. Předpovědi a výkon BERT jsou však zapnuty
soubor dat obsahující neobvyklé hyponymy a
hypernymy naznačují, že jeho znalosti o hypernymii jsou stále omezené.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The high performance of large pretrained language models (LLMs) such as BERT (Devlin et al., 2019) on NLP tasks has prompted questions about BERT’s linguistic capabilities, and how they differ from humans’. In this paper, we approach this question by examining BERT’s knowledge of lexical semantic relations. We focus on hypernymy, the “is-a” relation that relates a word to a superordinate category. We use a prompting methodology to simply
ask BERT what the hypernym of a given word is. We find that, in a setting where all hypernyms are guessable via prompting, BERT knows hypernyms with up to 57% accuracy. Moreover, BERT with prompting outperforms other unsupervised models for hypernym discovery even in an unconstrained scenario. However, BERT’s predictions and performance on
a dataset containing uncommon hyponyms and
hypernyms indicate that its knowledge of hypernymy is still limited.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>BERTScore (Zhang et al., 2020), nedávno navržená automatická metrika kvality strojového překladu, používá BERT (Devlin et al., 2019), velký předškolený jazykový model pro hodnocení kandidátských překladů s ohledem na zlatý překlad. BERTScore využívá sémantických a syntaktických schopností BERT a snaží se vyhnout chybám dřívějších přístupů, jako je BLEU, místo toho hodnotí kandidátské překlady na základě jejich sémantické podobnosti se zlatou větou. BERT však není neomylný; zatímco jeho výkon v oblasti úkolů NLP obecně nastoluje nový stav, studie specifických syntaktických a sémantických jevů ukázaly, kde se výkon BERT odchyluje od výkonu lidí obecněji.
To přirozeně vyvolává otázky, kterými se v tomto dokumentu zabýváme: jaké jsou silné a slabé stránky BERTScore? Souvisejí s
známé slabiny na straně BERT? Zjistili jsme, že BERTScore sice dokáže odhalit, když se kandidát liší od odkazu v důležitých obsahových slovech, ale je méně citlivý na menší chyby, zejména pokud je kandidát lexikálně nebo stylisticky podobný odkazu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>BERTScore (Zhang et al., 2020), a recently proposed automatic metric for machine translation quality, uses BERT (Devlin et al., 2019), a large pre-trained language model to evaluate candidate translations with respect to a gold translation. Taking advantage of BERT’s semantic and syntactic abilities, BERTScore seeks to avoid the flaws of earlier approaches like BLEU, instead scoring candidate translations based on their semantic similarity to the gold sentence. However, BERT is not infallible; while its performance on NLP tasks set a new state of the art in general, studies of specific syntactic and semantic phenomena have shown where BERT’s performance deviates from that of humans more generally.
This naturally raises the questions we address in this paper: what are the strengths and weaknesses of BERTScore? Do they relate to
known weaknesses on the part of BERT? We find that while BERTScore can detect when a candidate differs from a reference in important content words, it is less sensitive to smaller errors, especially if the candidate is lexically or stylistically similar to the reference.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Architektury založené na transformaci sekvence na sekvenci sice dosahují nejmodernějších výsledků na velkém množství úloh zpracování přirozeného jazyka, přesto však mohou při trénovaní trpět problémem přeučení. V praxi se tomu obvykle předchází buď použitím regularizačních metod (např. dropout, L2-regularizace), nebo poskytnutím obrovského množství trénovacích dat. Navíc je známo, že Transformer a další architektury mají problém s generováním velmi dlouhých sekvencí. Například ve strojovém překladu dosahují neuronové systémy horších výsledků na velmi dlouhých sekvencích než předchozí překladové metody založené na frázovém překladu (Koehn and Knowles, 2017).
Předkládáme výsledky, které naznačují, že problém může být také v rozdílech mezi rozložením délek v trénovacích a validačních datech v kombinaci s výše uvedenou tendencí neuronových sítí přeučit se na trénovacích datech. Na jednoduché úloze editace řetězců a strojovém překladu demonstrujeme, že kvalita modelu Transformer výrazně klesá, když zpracovává sekvence délky odlišné od délek v trénovacích datech. Dále ukazujeme, že pozorovaný pokles kvality je způsoben spíše délkou hypotézy, která odpovídá délkám v trénovacích datech, než délkou vstupní sekvence.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Transformer-based sequence-to-sequence architectures, while achieving state-of-the-art results on a large number of NLP tasks, can still suffer from overfitting during training. In practice, this is usually countered either by applying regularization methods (e.g. dropout, L2-regularization) or by providing huge amounts of training data. Additionally, Transformer and other architectures are known to struggle when generating very long sequences. For example, in machine translation, the neural-based systems perform worse on very long sequences when compared to the preceding phrase-based translation approaches (Koehn and Knowles, 2017).
  We present results which suggest that the issue might also be in the mismatch between the length distributions of the training and validation data combined with the aforementioned tendency of the neural networks to overfit to the training data. We demonstrate on a simple string editing task and a machine translation task that the Transformer model performance drops significantly when facing sequences of length diverging from the length distribution in the training data. Additionally, we show that the observed drop in performance is due to the hypothesis length corresponding to the lengths seen by the model during training rather than the length of the input sequence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek přináší zprávu o průběhu a výsledcích 47. ročníku Olympiády v českém jazyce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article brings an overview and results of the 47th year of the Czech Language Olympics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku uvádíme Uniformní reprezentaci významu (UMR), navrženou k anotaci sémantického obsah textu. UMR je primárně založena na Abstract Meaning Representation (AMR), anotačním rámci původně určeném pro angličtinu, ale čerpá i z jiných významových reprezentací. UMR rozšiřuje AMR do dalších jazyků, obzvláště morfologicky složité jazyky s omezenými jazykovými zdroji. UMR také přidává do AMR funkce, které jsou kritické pro sémantiku a zlepšuje AMR navržením doprovodné dokumentární reprezentace, která zachycuje jazykové jevy jako je koreference a také časové a modální závislosti, které potenciálně přesahují hranice vět.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we present Uniform Meaning Representation (UMR), a meaning representation designed to annotate the semantic content of a text. UMR is primarily based on Abstract Meaning Representation (AMR), an annotation framework initially designed for English, but also draws from other meaning representations. UMR extends AMR to other languages, particularly morphologically complex, low-resource languages. UMR also adds features to AMR that are critical to semantic interpretation and enhances AMR by proposing a companion document-level representation that captures linguistic phenomena such as coreference as well as temporal and modal dependencies that potentially go beyond sentence boundaries.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zpráva o konání semináře Jazykovědného sdružení České republiky na počest Světly Čmejrkové a Františka Daneše.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Report on the seminar of the Linguistic Association of the Czech Republic in honour of Světla Čmejrková and František Daneš.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slovní asociace je důležitou součástí lidského jazyka. Existuje mnoho technik pro zachycení sémantických vztahů mezi slovy, ale jejich schopnost modelovat slovní asociace je zřídka testována v reálné aplikaci. V tomto článku hodnotíme tři modely zaměřené na různé typy slovních asociací: model pro vkládání slov pro synonymii, bodový model vzájemných informací pro slovní spojení a model závislosti pro společné vlastnosti slov. Kvalitu navrhovaných modelů testují lidé v angličtině a češtině v online verzi slovně-asociační hry „Codenames“.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Word association is an important part of human language. Many techniques for capturing semantic relations between words exist, but their ability to model word associations is rarely tested in a real application. In this paper, we evaluate three models aimed at different types of word associations: a word-embedding model for synonymy, a point-wise mutual information model for word collocations, and a dependency model for common properties of words. The quality of the proposed models is tested on English and Czech by humans in an online version of the word-association game “Codenames”.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jazyky, pro které není k dispozici dostatek zdrojů, představují mimořádné příležitosti, ale také obtíže při počítačovém zpracování. Nově vydaný korpus ručně anotovaných částí jorubské Bible připravuje cestu pro závislostní analýzu jorubštiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Low-resource languages present enormous NLP opportunities as well as varying degrees of difficulties. The newly released treebank of hand-annotated parts of the Yorùbá Bible provides an avenue for dependency analysis of the Yorùbá language; the application of a new grammar formalism to the language. In this paper, we discuss our choice of Universal Dependencies, important dependency annotation decisions considered in the creation of the first annotation guidelines for Yorùbá and results of our parsing experiments. We also lay the foundation for future incorporation of other domains with the initial test on Yorùbá Wikipedia articles and highlighted future directions for the rapid expansion of the treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Sborník příspěvků z mezinárodní konference Prague Visual History and Digital Humanities Conference (PraViDCo 2020), která proběhla v lednu 2020 k 10. výročí Centra vizuální historie Malach.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Given the occasion of celebrating the 10th year of the Malach CVH existence, we decided to substantially extend the program of our annual January anniversary conference, which we newly entitled Prague Visual History and Digital Humanities Conference (PraViDCo). By adopting this name, we desired to express in the broadest possible sense the thematic and methodological scope representative of the interdisciplinarity which, as we believe, is the very essence of the Malach CVH.

The second day of the conference (January 28, 2020) gave the floor to the contributions collected from mostly junior researchers from all over the world, who swiftly responded to our open call for papers and which you can find in a written form in this book, entitled Malach Center for Visual History on its 10th Anniversary. We hope that readers will appreciate this brief glimpse into the full palette of different topics, methodologies and approaches that Malach Center for Visual History came into contact with throughout the first ten years of its existence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Abstraktivní sumarizaci je notoricky těžké hodnotit, protože standardní metriky založené na překryvu slov jsou nedostatečné. Představujeme novou evaluační metriku, která je založena na vážení obsahu na úrovní faktů, tj. vztažení faktů z dokumentu k faktům ze shrnutí. Vycházíme z předpokladu, že dobré shrnutí bude odrážet všechna relevantní fakta, tj. ta, která jsou obsažena v referenčním shruntí vytvořeném člověkem. Na potvrzení této hypotézy ukazujeme, že naše váhy velmi dobře korelují s lidským hodnocením a jsou srovnatelné s nedávnou manuální metrikou Hardyho et al. (2019), založenou na zvýrazňování částí textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Abstractive summarisation is notoriously hard to evaluate since standard word-overlap-based metrics are insufficient. We introduce a new evaluation metric which is based on fact-level content weighting, i.e. relating the facts of the document to the facts of the summary. We follow the assumption that a good summary will reflect all relevant facts, i.e. the ones present in the ground truth (human-generated refer- ence summary). We confirm this hypothe- sis by showing that our weightings are highly correlated to human perception and compare favourably to the recent manual highlight- based metric of Hardy et al. (2019).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem naší práce je navrhnout vhodné morfologické značky pro popis indonéštiny v rámci Universal Dependencies a aplikovat tyto značky na existující indonéský závislostní korpus.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The objectives of our work are to propose the relevant Universal Dependencies morphological features for Indonesian dependency treebank and to apply the proposed features to an existing treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dávat smysl jazyku není snadný úkol. Tím spíš, že máme tak intimní zážitek -- stromy, se kterými se pravidelně setkáváme v každodenní komunikaci, nám mohou znesnadnit výhled na les. V této situaci nám počítače mohou pomoci udělat krok zpět a podívat se na velké množství textu z odstupu padesáti tisíc stop.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Making sense of language is no easy task. All the more since we have such an intimate experience of it -- the trees that we regularly encounter in everyday communication may make it hard to see the forest. In this situation, computers can help us take a step back and look at large quantities of text from a fifty-thousand-foot view.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje vyjádření týmu („ODIANLP“) k WAT 2020. Účastnili jsme se úkolu English→Hindi Multimodal a Indic. Pro úlohu překladu jsme použili nejmodernější model Transformeru a pro úlohu Hindi Image Captioning jsme použili Inception ResNetV2. Naše podání dosahuje nejlepších výsledků ve směru English→Hindi Multimodal a Odia↔English. Naše návrhy si vedly dobře i při vícejazyčných úkolech (Indic).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the team (“ODIANLP”)’s submission to WAT 2020. We have participated in the English→Hindi Multimodal task and Indic task. We have used the state-of-the-art Transformer model for the translation task and Inception ResNetV2 for the Hindi Image Captioning task. Our submission tops in English→Hindi Multimodal task in its track and Odia↔English translation tasks. Also, our submissions performed well in the Indic Multilingual tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příprava paralelních korpusů je náročným úkolem, zejména pro jazyky, které trpí nedostatečným zastoupením v digitálním světě. Ve vícejazyčné zemi, jako je Indie, je potřeba takových paralelních korpusů přísná pro několik jazyků s nízkými zdroji. V této práci poskytujeme rozšířený anglicko-odijský paralelní korpus OdiEnCorp 2.0 zaměřený zejména na systémy Neural Machine Translation (NMT), které pomohou přeložit angličtinu ↔ Odia. OdiEnCorp 2.0 zahrnuje stávající anglicko-odijské korpusy a sbírku jsme rozšířili o několik dalších metod získávání dat: paralelní škrábání dat z mnoha webů, včetně Odia Wikipedia, ale také optické rozpoznávání znaků (OCR) pro extrakci paralelních dat ze skenovaných obrázků. Náš přístup k extrakci dat založený na OCR pro vytváření paralelního korpusu je vhodný pro jiné jazyky s nízkými zdroji, které nemají online obsah. Výsledný OdiEnCorp 2.0 obsahuje 98 302 vět a 1,69 milionu anglických a 1,47 milionu žetonů Odia. Pokud je nám známo, OdiEnCorp 2.0 je největší Odia-anglický paralelní korpus pokrývající různé domény a je volně dostupný pro nekomerční a výzkumné účely.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The preparation of parallel corpora is a challenging task, particularly for languages that suffer from under-representation in the digital world. In a multi-lingual country like India, the need for such parallel corpora is stringent for several low-resource languages. In this work, we provide an extended English-Odia parallel corpus, OdiEnCorp 2.0, aiming particularly at Neural Machine Translation (NMT) systems which will help translate English↔Odia. OdiEnCorp 2.0 includes existing English-Odia corpora and we extended the collection by several other methods of data acquisition: parallel data scraping from many websites, including Odia Wikipedia, but also optical character recognition (OCR) to extract parallel data from scanned images. Our OCR-based data extraction approach for building a parallel corpus is suitable for other low resource languages that lack in online content. The resulting OdiEnCorp 2.0 contains 98,302 sentences and 1.69 million English and 1.47 million Odia tokens. To the best of our knowledge, OdiEnCorp 2.0 is the largest Odia-English parallel corpus covering different domains and available freely for non-commercial and research purposes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku představujeme první treebank Universal Dependencies (UD) pro spisovnou albánštinu, sestávající z 60 vět vybraných z albánské Wikipedie, anotovaných lematy, univerzálními značkami slovních druhů, morfologickými rysy a syntaktickými závislostmi. Kromě představení treebanku jako takového probíráme vybrané jazykové jevy v albánštině, jejichž analýza v UD není na první pohled jasná, včetně jádrových argumentů, zacházení s nepřímými předměty, zájmenných příklonek, konstrukcí s genitivem, preartikulovaných adjektiv a způsobových sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we introduce the first Universal Dependencies (UD) treebank for standard Albanian, consisting of 60 sentences collected from the Albanian Wikipedia, annotated with lemmas, universal part-of-speech tags, morphological features and syntactic dependencies. In addition to presenting the treebank itself, we discuss a selection of linguistic constructions in Albanian whose analysis in UD is not self-evident, including core arguments and the status of indirect objects, pronominal clitics, genitive constructions, prearticulated adjectives, and modal verbs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme PERIN, nový permutačně invariantní přístup k sémantickému parsingu věty na graf. PERIN je všestranná, anotačně a jazykově nezávislá architektura pro univerzální modelování sémantických struktur. Náš systém se zúčastnil shared tasku CoNLL 2020, Cross-Framework Meaning Representation Parsing (MRP 2020), kde byl hodnocen v pěti různých frameworcích (AMR, DRG, EDS, PTG a UCCA) napříč čtyřmi jazyky. PERIN byl jedním z vítězů. Zdrojový kód a předtrénované modely jsou k dispozici na adrese http://www.github.com/ufal/perin.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present PERIN, a novel permutation-invariant approach to sentence-to-graph semantic parsing. PERIN is a versatile, cross-framework and language independent architecture for universal modeling of semantic structures. Our system participated in the CoNLL 2020 shared task, Cross-Framework Meaning Representation Parsing (MRP 2020), where it was evaluated on five different frameworks (AMR, DRG, EDS, PTG and UCCA) across four languages. PERIN was one of the winners of the shared task. The source code and pretrained models are available at http://www.github.com/ufal/perin.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Mnohojazyčnost je kulturním úhelným kamenem Evropy a je pevně zakotvena v evropských smlouvách, včetně úplné jazykové rovnosti. Jazykové bariéry ovlivňující obchodní, mezijazykovou a mezikulturní komunikaci jsou však stále všudypřítomné. Jazykové technologie (LT) jsou mocným prostředkem k prolomení těchto bariér. I když v posledním desetiletí vzniklo množství různých iniciativ, které vytvořily množství přístupů a technologií přizpůsobených specifickým potřebám Evropy, stále existuje obrovská míra roztříštěnosti. Zároveň se umělá inteligence (AI) stala stále důležitějším konceptem v oblasti evropských informačních a komunikačních technologií. Už několik let AI – včetně mnoha příležitostí, synergií, ale i mylných představ – zastiňuje všechna ostatní témata. Představujeme přehled evropského prostředí jazykových technologí (LT), popisujeme programy financování, činnosti, akce a výzvy v jednotlivých zemích s ohledem na LT, včetně současného stavu v průmyslu a na trhu LT. Představujeme stručný přehled hlavních činností souvisejících s LT na úrovni EU za posledních deset let a představujeme strategické pokyny s ohledem na čtyři klíčové rozměry.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Multilingualism is a cultural cornerstone of Europe and firmly anchored in the European Treaties including full language equality. However, language barriers impacting business, cross-lingual and cross-cultural communication are still omnipresent. Language Technologies (LTs) are a powerful means to break down these barriers. While the last decade has seen various initiatives that created a multitude of approaches and technologies tailored to Europe’s specific needs, there is still an immense level of fragmentation. At the same time, AI has become an increasingly important concept in the European Information and Communication Technology area. For a few years now, AI – including many opportunities, synergies but also misconceptions – has been overshadowing every other topic. We present an overview of the European LT landscape, describing funding programmes, activities, actions and challenges in the different countries with regard to LT, including the current state of play in industry and the LT market. We present a brief overview of the main LT-related activities on the EU level in the last ten years and develop strategic guidance with regard to four key dimensions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>S 24 oficiálními jazyky EU a mnoha dalšími jazyky může být mnohojazyčnost v Evropě a inkluzivní jednotný digitální trh umožněn pouze prostřednictvím jazykových technologií (LT). Evropské LT podnikání ovládají stovky malých a středních podniků a několik velkých hráčů. Mnohé z nich jsou světové, s technologiemi, které předčí globální hráče. Evropské podnikání v LT je však také roztříštěné – v závislosti na národních státech, jazycích, vertikálech a odvětvích, což výrazně brzdí jeho dopad. Projekt evropské jazykové sítě (European Language Grid, ELG) řeší tuto roztříštěnost tím, že stanoví ELG jako primární platformu pro LT v Evropě. Skupina ELG je rozšiřitelná cloudová platforma, která umožňuje snadnou integraci přístupu ke stovkám komerčních i nekomerčních LT pro všechny evropské jazyky, včetně provozních nástrojů a služeb, jakož i datových souborů a zdrojů. Jakmile bude plně funkční, umožní komerční i nekomerční evropské komunitě LT ukládat a nahrávat své technologie a soubory dat do systému ELG, zavádět je do sítě a propojovat s dalšími zdroji. Skupina ELG podpoří vícejazyčný jednotný digitální trh směrem k prosperující evropské LT komunitě a vytvoří nová pracovní místa a příležitosti. Kromě toho projekt ELG organizuje dvě otevřené výzvy až pro 20 pilotních projektů. Zřizuje také 32 národních kompetenčních center a Evropskou radu LT pro osvětové a koordinační účely.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>With 24 official EU and many additional languages, multilingualism in Europe and an inclusive Digital Single Market can only be enabled through Language Technologies (LTs). European LT business is dominated by hundreds of SMEs and a few large players. Many are world-class, with technologies that outperform the global players. However, European LT business is also fragmented – by nation states, languages, verticals and sectors, significantly holding back its impact. The European Language Grid (ELG) project addresses this fragmentation by establishing the ELG as the primary platform for LT in Europe. The ELG is a scalable cloud platform, providing, in an easy-to-integrate way, access to hundreds of commercial and non-commercial LTs for all European languages, including running tools and services as well as data sets and resources. Once fully operational, it will enable the commercial and non-commercial European LT community to deposit and upload their technologies and data sets into the ELG, to deploy them through the grid, and to connect with other resources. The ELG will boost the Multilingual Digital Single Market towards a thriving European LT community, creating new jobs and opportunities. Furthermore, the ELG project organises two open calls for up to 20 pilot projects. It also sets up 32 national competence centres and the European LT Council for outreach and coordination purposes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Strojový překlad:

Některé metody automatického simultánního převodu řeči dlouhé formy umožňují revize výstupů,
přesnost obchodování pro nízkou latenci. Zavádění těchto systémů
uživatele čeká problém s prezentací titulků v omezeném prostoru, jako jsou dva řádky na televizní obrazovce. The
titulky musí být zobrazeny okamžitě, postupně a s
dostatečný čas na čtení. Poskytujeme algoritmus pro
titulkování. Dále navrhujeme způsob, jak odhadnout
celková využitelnost kombinace automatického překladu a titulkování měřením kvality, latence a
stabilitu na testovací sadě a navrhnout vylepšené opatření
kvůli zpoždění překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Some methods of automatic simultaneous translation of a long-form speech allow revisions of outputs,
trading accuracy for low latency. Deploying these systems
for users faces the problem of presenting subtitles in a limited space, such as two lines on a television screen. The
subtitles must be shown promptly, incrementally, and with
adequate time for reading. We provide an algorithm for
subtitling. Furthermore, we propose a way how to estimate
the overall usability of the combination of automatic translation and subtitling by measuring the quality, latency, and
stability on a test set, and propose an improved measure
for translation latency.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento abstrakt je pouze v angličtině:

This paper is an ELITR system submission for the non-native speech translation task at IWSLT 2020. We describe systems for offline ASR, real-time ASR, and our cascaded approach to offline SLT and real-time SLT. We select our primary candidates from a pool of pre-existing systems, develop a new end-to-end general ASR system, and a hybrid ASR trained on non-native speech. The provided small validation set prevents us from carrying out a complex validation, but we submit all the unselected candidates for contrastive evaluation on the test set.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper is an ELITR system submission for the non-native speech translation task at IWSLT 2020. We describe systems for offline ASR, real-time ASR, and our cascaded approach to offline SLT and real-time SLT. We select our primary candidates from a pool of pre-existing systems, develop a new end-to-end general ASR system, and a hybrid ASR trained on non-native speech. The provided small validation set prevents us from carrying out a complex validation, but we submit all the unselected candidates for contrastive evaluation on the test set.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zkoumáme vliv trénování modelů NMT na více cílových jazyků. Předpokládáme, že integrace více jazyků a zvýšení jazykové rozmanitosti povedou k silnějšímu zastoupení syntaktických a sémantických rysů zachycených modelem. Testujeme naši hypotézu na dvou různých architekturách NMT: široce používaná architektura transformátorů a architektura Attention Bridge. Trénujeme modely na datech Europarl a kvantifikujeme úroveň syntaktických a sémantických informací objevených modely pomocí tří různých metod: úkoly lingvistického průzkumu SentEval, analýza struktur pozornosti týkající se inherentních informací o frázích a závislostech a strukturální sonda na kontextových reprezentacích slov . Naše výsledky ukazují, že s rostoucím počtem cílových jazyků model Attention Bridge stále více získává určité jazykové vlastnosti, včetně některých syntaktických a sémantických aspektů věty, zatímco transformátorské modely jsou do značné míry nedotčeny. Posledně uvedené platí také pro frázovou strukturu a syntaktické závislosti, které se při zvyšování jazykové rozmanitosti při výcviku překladu nejeví ve větných vyjádřeních. To je docela překvapivé a může to naznačovat relativně malý vliv gramatické struktury na porozumění jazyku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We investigate the effect of training NMT models on multiple target languages. We hypothesize that the integration of multiple languages and the increase of linguistic diversity will lead to a stronger representation of syntactic and semantic features captured by the model. We test our hypothesis on two different NMT architectures: The widely-used Transformer architecture and the Attention Bridge architecture. We train models on Europarl data and quantify the level of syntactic and semantic information discovered by the models using three different methods: SentEval linguistic probing tasks, an analysis of the attention structures regarding the inherent phrase and dependency information and a structural probe on contextualized word representations. Our results show evidence that with growing number of target languages the Attention Bridge model increasingly picks up certain linguistic properties including some syntactic and semantic aspects of the sentence whereas Transformer models are largely unaffected. The latter also applies to phrase structure and syntactic dependencies that do not seem to be developing in sentence representations when increasing the linguistic diversity in training to translate. This is rather surprising and may hint on the relatively little influence of grammatical structure on language understanding.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V posledních letech dominovaly hluboké neuronové sítě v oblasti zpracování přirozeného jazyka (NLP). Modely trénované od konce do konce mohou dělat úkoly stejně zručně jako nikdy předtím a rozvíjet vlastní vyjádření jazyka. Působí však jako černé skříňky, které je velmi těžké interpretovat. To vyžaduje kontrolu, do jaké míry jsou jazykové koncepce v souladu s tím, co se modely učí. Používají neuronové sítě morfologii a syntaxi stejně jako lidé, když mluví o jazyce? Nebo si vyvinou vlastní způsob?
V naší přednášce pootevřeme neurální černou skříňku a zanalyzujeme vnitřní reprezentace vstupních vět s ohledem na jejich morfologické, syntaktické a sémantické vlastnosti. Zaměříme se na embedinky slov i kontextové embedinky a sebepozornost modelů Transformer (BERT, NMT). Ukážeme jak řízené, tak i neřízené postupy analýzy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In recent years, deep neural networks dominated the area of Natural Language Processing (NLP). End-to-end-trained models can do tasks as skillfully as never before and develop their own language representations. However, they act as black boxes that are very hard to interpret. This calls for an inspection to what extent the linguistic conceptualizations are consistent with what the models learn. Do neural networks use morphology and syntax the way people do when they talk about language? Or do they develop their own way?
In our talk, we will half-open the neural black-box and analyze the internal representations of input sentences with respect to their morphological, syntactic, and semantic properties. We will focus on word embeddings as well as contextual embeddings and self-attentions of the Transformer models (BERT, NMT). We will show both supervised and unsupervised analysis approaches.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této knize prozkoumáme architektury a modely neuronových sítí, které se používají pro zpracování přirozeného jazyka (NLP). Analyzujeme jejich interní reprezentace (vkládání slov, skryté stavy, mechanismus pozornosti a kontextové vkládání) a zkoumáme, jaké vlastnosti mají tyto reprezentace a jaké jazykově interpretovatelné rysy se v nich objevují. K prezentaci přehledu modelů a reprezentací a jejich jazykových vlastností používáme vlastní experimentální výsledky i výsledky publikované jinými výzkumnými týmy.

Na začátku vysvětlíme základní pojmy hlubokého učení a jeho využití v NLP a probereme podrobnosti nejvýznamnějších neurálních architektur a modelů. Poté nastíníme koncept interpretovatelnosti, různé pohledy na ni a představíme základní supervizované a nekontrolované metody, které se používají pro tlumočení
modely neuronových sítí.

Další část je věnována statickému vkládání slov. Ukážeme různé metody pro vkládání vizualizace prostoru, analýzu komponent a vkládání transformací prostoru pro interpretaci. Předem připravená vložení slov obsahují informace o morfologii i lexikální sémantice. Když jsou embedings trénovány pro konkrétní úkol, embeddings mají tendenci být organizovány podle informací, které jsou pro daný úkol důležité (např. Emoční polarita pro analýzu sentimentu).

Analyzujeme také mechanismy pozornosti, ve kterých můžeme sledovat vážené vazby mezi reprezentacemi jednotlivých tokenů. Ukazujeme, že mezijazykové pozornosti většinou spojují vzájemně odpovídající tokeny; v některých případech se však mohou velmi lišit od tradičních slovních spojení. Hlavně se zaměřujeme na sebepozornost v sítích typu Transformer. Některé hlavy spojují tokeny s určitými syntaktickými vztahy. To motivovalo vědce odvodit syntaktické stromy ze sebepozorování a porovnat je s lingvistickými anotacemi. Shrneme množství syntaxe v pozornost napříč vrstvami několika modelů NLP. Rovněž poukazujeme na skutečnost, že pozornost může být někdy velmi zavádějící a může nést velmi odlišné informace, o kterých bychom si na základě zúčastněných tokenů mysleli.

V poslední části se podíváme na kontextové vektorové representace slov a jazykové vlastnosti, které zachycují. Představují jasné vylepšení oproti statickým vektorovým reprezentacím slov, zejména pokud jde o zachycení morfologických a syntaktických znaků. Zdá se však, že některé vyšší jazykové abstrakce, jako je sémantika, se v současných kontextových vloženích odrážejí jen velmi slabě nebo vůbec.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this book, we explore neural-network architectures and models that are used for Natural Language Processing (NLP). We analyze their internal representations (word-embeddings, hidden states, attention mechanism, and contextual embeddings) and review what properties these representations have and what kinds of linguistically interpretable features emerge in them. We use our own experimental results, as well as the results published by other research teams to present an overview of models and representations and their linguistic properties.

In the beginning, we explain the basic concepts of deep learning and its usage in NLP and discuss details of the most prominent neural architectures and models. Then, we outline the concept of interpretability, different views on it, and introduce basic supervised and unsupervised methods that are used for interpreting trained
neural-network models.

The next part is devoted to static word embeddings. We show various methods for embeddings space visualization, component analysis and embedding space transformations for interpretation. Pretrained word embbedings contain information about both morphology and lexical semantics. When the embbedings are trained for a specific task, the embeddings tend to be organised by the information that is important for the given task (e.g. emotional polarity for sentiment analysis).

We also analyze attention mechanisms, in which we can observe weighted links between representations of individual tokens. We show that the cross-lingual attentions mostly connect mutually corresponding tokens; however, in some cases, they may be very different from the traditional word-alignments. We mainly focus on self-attentions in Transformers. Some heads connect tokens with certain syntactic relations. This motivated researchers to infer syntactic trees from the self-attentions and compare them to the linguistic annotations. We summarize the amount of syntax in
the attentions across the layers of several NLP models. We also point out the fact that attentions might sometimes be very misleading and may carry very different information from which we would think based on the attended tokens.

In the last part, we look at contextual word embeddings and the linguistic features they capture. They constitute a clear improvement over static word embeddings, especially in terms of capturing morphological and syntactic features. However, some higher linguistic abstractions, such as semantics, seem to be reflected in the current contextual embeddings only very weakly or not at all.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>„Roboti žít nebudou, mohou jen zemřít,“ říká stavitel Alquist, postava z divadelní hry R.U.R. Autorem těchto slov ovšem tentokrát není Karel Čapek, ale stroj, umělá inteligence, která na počest slavného díla píše drama o vztahu člověka a robota. Má být uvedena ve Švandově divadle 25. ledna, kdy od premiéry R.U.R. uplyne právě 100 let.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>"The robots will not live, they can only die," says the builder Alquist, a character in the R.U.R. play. The author of these words, however, this time is not Karel Čapek, but a machine, an artificial intelligence, writing a drama about the relationship of man and robot in honor of the famous work. It is to be shown at the Švanda Theatre on January 25th, when it will be 100 years since the premiere of R.U.R.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rychlý růst tvory škodlivého softwaru (malware) v posledním tesetiletí a počet útoků způsobených malwarem na síťová prostředí, jako je Internet, dokládá potřebu výzkumu bezpečnosti počítačových sítí a digitálně zaměřené forenzní vědy. V naší studii překládáme metodu, které identifikuje "druhy" rodin malware, které jsou pokročilé, záměrně matoucí a strukturně pestré. Navrhujeme hybridní techniku, která při klasifikaci malwarových rodin kombinuje detekci signatur s metodami založenými na strojovém učení. Metoda využívá profilové skryté markovské modely (PHMM) k behaviorálnímu popisu malwarových druhů. Tento článek vysvětluje proces modelování a trénování vícečetného zarovnání sekvencí. Vzhledem k tomu, že ne všechny části souboru jsou škodlivé, je cílem rozlišit škodlivé a neškodné části. Zaměřením na škodlivé části roste šance na detekci malwaru. Experimentální výsledky ukazují, že svou úspěšností naše metoda překonává ostatní přístupy založené na skrytých markovských řetězcích. Všechny doprovodné materiály k textu, zejména zdrojové kódy, datové sady a kompletní výsledky, jsou veřejně dostupné na Internetu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The rapid growth of malicious software (malware) production in recent decades and the increasing number of threats posed by malware to network environments, such as the Internet and intelligent environments, emphasize the need for more research on the security of computer networks in information security and digital forensics. The method presented in this study identifies “species” of malware families, which are more sophisticated, obfuscated, and structurally diverse. We propose a hybrid technique combining aspects of signature detection with machine learning-based methods to classify malware families. The method is carried out by utilizing Profile Hidden Markov Models (PHMMs) on the behavioral characteristics of malware species. This paper explains the process of modeling and training an advanced PHMM using sequences obtained from the extraction of each malware family’s paramount features, and the canonical sequences created in the process of Multiple Sequence Alignment (MSA) production. Due to the fact that not all parts of a file are malicious, the goal is to distinguish the malicious portions from the benign ones and place more emphasis on them in order to increase the likelihood of malware detection by having the least impact from the benign portions. Based on “consensus sequences”, the experimental results show that our proposed approach outperforms other HMM-based techniques even when limited training data is available. All supplementary materials including the code, datasets, and a complete list of results are available for public access on the Web.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje podrobnou analýzu první soutěže v end-to-end generování přirozeného jazyka (NLG) a na základě jejích výsledků naznačuje směr budoucího výzkumu. Cílem této soutěže úkolu bylo posoudit, zda moderní end-to-end systémy NLG mohou generovat komplexnější výstup, jsou-li natrénovány z dat lexikálně bohatších, syntakticky složitějších a zahrnujících různé diskurzní jevy. S použitím nových automatických a lidských metrik porovnáváme 62 systémů zaslaných do soutěže 17 institucemi, které zahrnují širokou škálu přístupů, včetně architektur strojového učení – kde většina implementací jsou modely typu sequence-to-sequence (seq2seq) – i systémů založených na gramatických pravidlech a šablonách. Systémy založené na architektuře seq2seq ukázaly v této souteži velký potenciál pro NLG. Zjistili jsme, že seq2seq systémy mají obecně vysoké skóre, pokud jde o metriky založené na překryvu slov a lidské hodnocení přirozenosti/plynulosti; vítězný systém Slug (Juraska et al., 2018) je založený na seq2seq. Základní modely typu seq2seq však často nedokážou správně vyjádřit vstupný reprezentaci významu, pokud postrádají silný mechanismus sémantické kontroly použitý během dekódování. Modely seq2seq mohou být navíc překonány ručně vytvořenými systémy z hlediska celkové kvality, jakož i složitosti, délky a rozmanitosti výstupů. Tento výzkum ovlivnil, inspiroval a motivoval řadu nedávných studií mimo původní soutěž, které v článku rovněž shrnujeme.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper provides a comprehensive analysis of the first shared task on End-to-End Natural Language Generation (NLG) and identifies avenues for future research based on the results. This shared task aimed to assess whether recent end-to-end NLG systems can generate more complex output by learning from datasets containing higher lexical richness, syntactic complexity and diverse discourse phenomena. Introducing novel automatic and human metrics, we compare 62 systems submitted by 17 institutions, covering a wide range of approaches, including machine learning architectures – with the majority implementing sequence-to-sequence models (seq2seq) – as well as systems based on grammatical rules and templates. Seq2seq-based systems have demonstrated a great potential for NLG in the challenge. We find that seq2seq systems generally score high in terms of word-overlap metrics and human evaluations of naturalness – with the winning Slug system (Juraska et al., 2018) being seq2seq-based. However, vanilla seq2seq models often fail to correctly express a given meaning representation if they lack a strong semantic control mechanism applied during decoding. Moreover, seq2seq models can be outperformed by hand-engineered systems in terms of overall quality, as well as complexity, length and diversity of outputs. This research has influenced, inspired and motivated a number of recent studies outwith the original competition, which we also summarise as part of this paper.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Velkým problémem v evaluaci systémů pro generování textu z dat (D2T) je jak změřit sémantickou přesnost vygenerovaného textu, tj. jeho věrnost vstupním datům. Navrhujeme novou metriku pro evaluaci sémantické přesnosti D2T generování založené na neuronovém modelu předtrénovaném na úlohu automatické jazykové inference (NLI). Pomocí modelu NLI kontrolujeme, zda generovaný text vyplývá (entailment) z dat a opačně, což nám dovoluje odhalit vynechané části dat nebo halucinované (daty nepodložené) části vygenerovaného textu. Vstupní data pro model NLI převádíme do textu pomocí triviálních šablon. Naše experimenty na dvou běžně užívaných datových sadách pro D2T ukazují, že naše metrika dokáže dosáhnout vysoké přesnosti při identifikaci chybných výstupů generátorů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A major challenge in evaluating data-to-text (D2T) generation is measuring the semantic accuracy of the generated text, i.e. its faithfulness to the input data. We propose a new metric for evaluating the semantic accuracy of D2T generation based on a neural model pretrained for natural language inference (NLI). We use the NLI model to check textual entailment between the input data and the output text in both directions, allowing us to reveal omissions or hallucinations. Input data are converted to text for NLI using trivial templates. Our experiments on two recent D2T datasets show that our metric can achieve high accuracy in identifying erroneous system outputs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce představuje výsledky překladatelských úloh týkajících se zpravodajských textů a podobného úkolu překladů jazyků, obojí organizováno v rámci Conference on Machine Translation (WMT) 2020. V úkolu týkajícím se zpravodajských textů byli účastníci požádáni o sestavení systémů strojového překladu pro kterýkoli z 11 jazykových párů, které budou hodnoceny na testovacích souborech sestávajících hlavně z reportáží. Úloha byla také otevřena pro další testovací sady, aby se daly  zkoumat specifické aspekty překladu. V další úloze účastníci sestavili systémy strojového překladu pro překládání mezi úzce příbuznými páry jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the news translation task and the similar language translation task, both organised
alongside the  Conference on Machine Translation (WMT) 2020. In the news task, participants
were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories.  The task was also opened up
to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built 
machine translation systems for translating between closely related pairs of languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Umělá inteligence zvládá leccos.
Píše burzovní zprávy, zvládá
zábavnou konverzaci, objedná za
nás stůl v restauraci… Tentokrát ale
AI zašla ještě dál a dostala se zase o kousek blíž člověku – výzkumný
projekt THEaiTRE učí svého robota
napsat divadelní hru. O tom, jak
proces probíhá, co k tomu autory
vedlo a jaká jsou úskalí AI scenáristy
jsme si povídali s Rudolfem Rosou,
Danielem Hrbkem a Tomášem
Studeníkem, kteří se na unikátním
počinu podílí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Artificial intelligence can do things. She writes stock reports, handles entertaining conversation, orders a table for us at a restaurant ... But this time AI has gone further and gotten a little bit closer to humans -- the THEaiTRE research project is teaching its robot to write a play. We talked with Rudolf Rosa, Daniel Hrbek and Tomáš Studeník, who are involved in the unique feat, about how the process is going, what led the authors to it, and what the challenges of the AI writer are.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přestože nové neuronové modely typu sequence-to-sequence neurální značně zlepšily kvalitu syntézy řeči, dosud neexistuje systém schopný rychlého trénování, rychlé inference a zároveň vysoce kvalitní syntézy. Navrhujeme dvojici sítí typu učitel-student, která je schopna vysoce kvalitní syntézy spektrogramu rychleji než v reálném čase, s nízkými požadavky na výpočetní zdroje a rychlým trénováním. Ukazujeme, že vrstvy typu self-attention nejsou pro generování vysoce kvalitní řeči nutné. Jak v učitelské, tak ve studentské síti využíváme jednoduché konvoluční bloky s reziduálním propojením; používáme pouze jednu vrstvu attention v učitelském modelu. Ve spojení s hlasovým kodérem MelGAN byla hlasová kvalita našeho modelu hodnocena signifikantně lépe než Tacotron 2. Náš model může být efektivně trénován na jednom GPU a může běžet v reálném čase i na CPU. Zdrojový kód i zvukové ukázky poskytujeme na našem úložišti na GitHubu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>While recent neural sequence-to-sequence models have greatly improved the quality of speech synthesis, there has not been a system capable of fast training, fast inference and high-quality audio synthesis at the same time. We propose a student-teacher network capable of high-quality faster-than-real-time spectrogram synthesis, with low requirements on computational resources and fast training time. We show that self-attention layers are not necessary for generation of high quality audio. We utilize simple convolutional blocks with residual connections in both student and teacher networks and use only a single attention layer in the teacher model. Coupled with a MelGAN vocoder, our model's voice quality was rated significantly higher than Tacotron 2. Our model can be efficiently trained on a single GPU and can run in real time even on a CPU. We provide both our source code and audio samples in our GitHub repository.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V češtině – podobně jako v jiných slovanských jazycích – může mít klitické reflexivum funkci slovotvorného formantu, derivujícího lexikální reciproční slovesa, tedy  slovesa obsahující rys vzájemnosti již ve svém lexikálním významu. V článku rozlišuji mezi lexikálními recipročními slovesy, u nichž rys vzájemnosti vyjadřuje klitické reflexivum (např. nenávidět se ← nenávidět, slíbit si ← slíbit), a lexikálními recipročními slovesy, u nichž má klitické reflexivum jinou funkci (např. oddělit se ← oddělit). Lexikální reciproka prvního typu vytvářejí konstrukce, u nichž se participanty ve vztahu vzájemnosti vyjadřují typicky v subjektové pozici a v pozici nepřímého objektu, vyjádřeného instrumentálem. Ukazuji však, že tato slovesa tvoří sémantické skupiny, které se do značné míry sémanticky překrývají s dalšími typy lexikálních recipročních sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In Czech (similarly as in other Slavic languages), the clitic reflexives serve – among others – as a derivational means deriving lexical reciprocal verbs, i.e., those verbs that encode mutuality directly in their lexical meaning. Here I draw a line between those lexical reciprocal verbs with which the reflexives introduce mutuality (nenávidět se ‘to hate each other’ ← nenávidět ‘to hate sb’ and slíbit si ‘to promise sth to each other’ ← slíbit ‘to promise sth to sb’) and those with which the reflexives have another function (oddělit se ‘to separate from each other’ ← oddělit ‘to separate sb/sth from sb/sth’). I show that lexical reciprocal verbs of the former type uniformly form the so-called discontinuous constructions with the nominative subject and comitative indirect object (e.g., Petr si slíbil s Marií věrnost. ‘Peter and Mary promised fidelity to each other.’) and that they fall into several semantic classes, which, however, semantically overlap to a great extent with lexical reciprocal verbs of other types.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Česká verbální jména (zakončená sufixy -ní/-tí) označující událost (např. poučení jako proces) nebo abstraktní výsledek děje (např. poučení jako předaná informace) se z hlediska valence liší především tím, jak v povrchové struktuře vyjadřují svá valenční doplnění. V nominálních konstrukcích je povrchové vyjádření valenčních doplnění verbálních jmen označujících událost odvoditelné z valenční struktury jejich základových sloves. Oproti tomu verbální substantiva označující abstraktní výsledek děje užívají k povrchovému vyjádření svých valenčních doplnění vedle systémových morfematických forem i formy nesystémové. Ve verbonominálních konstrukcích, v nichž je syntaktickým centrem kategoriální sloveso, verbální jména označující událost systematicky uplatňují na povrchu jako subjekt slovesa participant Agens (v širokém slova smyslu). Oproti tomu verbální jména označující abstraktní výsledek děje mohou často na povrchu jako rozvití slovesa vyjádřit i své další sémantické participanty a výběr různých kategoriálních sloves jim často umožňuje perspektivizovat označovanou situaci pokaždé z hlediska jiného participantu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Comparing valency behavior of two selected types of Czech verbal nouns (ending in -ní/-tí), i.e., those denoting actions, e.g., poučení ‘instructing’, and those denoting abstract results of actions, e.g., poučení ‘advice’, we can observe that they differ especially in the surface syntactic expression of their valency complementations. In nominal constructions, the surface expression of valency complementations of verbal nouns denoting actions can be typically derived from valency structure of their base verbs. In contrast, verbal nouns denoting abstract results of actions often express their valency complementations on the surface in a non-systemic way. In verbonominal constructions, the syntactic center of which is a light verb, verbal nouns denoting actions systematically employ their participant Agent (in a broad sense) on the surface as subject provided by the light verb. In contrast, verbal nouns denoting abstract results of actions can often express more semantic participants on the surface as verbal modifications and the choice of light verbs can change the perspective from which situations denoted by these verbal nouns are viewed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku představujeme soupis jazykových prostředků, které vyjadřují sémantický rys vzájemnosti v českých verbálních konstrukcích. Na základě výsledků nedávných typologických průzkumů rozlišujeme prostředky lexikální a prostředky gramatické. Lexikální prostředky pro vyjádření vzájemnosti zahrnují omezenou skupinu inherentně recipročních sloves s rysem vzájemnosti v lexikálním významu. Gramatické prostředky realizující syntaktickou operaci reciprocalizace se primárně uplatňují u velké skupiny sloves, která označujeme jako syntaktická reciproční slovesa; tyto prostředky zahrnují (i) zvratné osobní reflexivum se/sebe, si/sobě, sebou, (ii) výraz jeden - druhý a (iii) příslovce. Zatímco použití gramatických prostředků je pro vyjádření vzájemnosti u syntakticky recipročních sloves nutné, pro inherentně reciproční slovesa je typicky volitelné. V textu podrobně popisujeme různé funkce těchto jazykových prostředků ve vyjadřování vzájemnosti v češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we introduce an inventory of language means that express the semantic feature of mutuality in Czech verbal constructions. Based on the results of recent typological surveys, we distinguish between lexical and grammatical means. The lexical means include a limited group of inherently reciprocal verbs with the feature of mutuality in their lexical meaning. The grammatical means involved in the syntactic operation of reciprocalization are primarily used by a large group of verbs to which we refer as syntactic reciprocal verbs; these means involve (i) the reflexive personal pronoun se/sebe, si/sobě, sebou, (ii) the expression jeden – druhý ‘each other’, and (iii) adverbials. While the use of the grammatical means is obligatory for expressing mutuality with syntactic reciprocal verbs, it is often only optional for inherently reciprocal verbs. We thoroughly describe various functions that these language means have in encoding mutuality in Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku se věnujeme způsobům vyjadřování vzájemnost v českých konstrukcích s kategoriálními slovesy. Reciprocitu do těchto konstrukcí vnáší prediktivní substantiva, neboť ta představují jejich sémantické jádro. Zaměřujeme se na reciproční konstrukce s kategoriálními slovesy, které vznikají odvozením syntaktickou operací reciprokalizace. Ukazujeme, že komplexní mapování sémantických participantů na valenční doplnění  charakteristické pro reciprocitu se zachovává i v recipročních konstrukcích s kategoriálními slovesy. Hlavní rozdíl mezi recipročními nominálními konstrukcemi a recipročními konstrukcemi s kategoriálním slovesem spočívá v morfosyntaktickém vyjádření reciprokalizovaných participantů. Ukazujeme, že povrchové syntaktické změny v recipročních konstrukcích s kategoriálními slovesy jsou natolik pravidelné, že je lze popsat pomocí pravidel, a to  pravidel pro hlubokou a povrchovou tvorbu syntaktických struktury s kategoriálním slovesem a pravidel pro tvoření recipročních konatrukcí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we draw attention to reciprocity in Czech light verb constructions – a language phenomenon, which has not been discussed yet. Reciprocity is contributed to light verb constructions by predictive nouns, as they are the nouns that represent the semantic core of these constructions. Here we focus on reciprocal light verb constructions derived by the syntactic operation of reciprocalization. We show that the complex mapping of semantic participants onto valency complementations, characteristic of reciprocalization, is reflected in reciprocal light verb constructions in the same way as in reciprocal nominal constructions. The main difference between reciprocal nominal constructions and reciprocal light verb constructions lies in the morphosyntactic expression of reciprocalized participants. We demonstrate that surface syntactic changes in reciprocal light verb constructions are regular enough to be described on the rule basis: the rule based generation of reciprocal light verb constructions requires a cooperation of two sets of rules – rules for deep and surface syntactic structure formation of light verb constructions and rules for capturing reciprocity.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valenční slovníky obvykle popisují valenční chován sloves v jejich nereflexivním a nerecipročním užití, přestože reflexivní a reciproční konstrukce lze považovat za běžné morfosyntaktické formy sloves. Pro oba tyto typy jsou charakteristické pravidelné změny v morfosyntaktické struktuře sloves, které lze popsat gramatickými pravidly; ovšem možnost tyto konstrukce tvořit musí být vyznačena ve slovníku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Valency lexicons usually describe valency behavior of verbs in non-reflexive and non-reciprocal constructions. However, reflexive and reciprocal constructions are common morphosyntactic forms of verbs. Both of these constructions are characterized by regular changes in morphosyntactic properties of verbs, thus they can be described by grammatical rules. On the other hand, the possibility to create reflexive and/or reciprocal constructions cannot be trivially derived from the morphosyntactic structure of verbs as it is conditioned by their semantic properties as well. A large-coverage valency lexicon allowing for rule based generation of all well formed verb constructions should thus integrate the information on reflexivity and reciprocity. In this paper, we propose a semi-automatic procedure, based on grammatical constraints on reflexivity and reciprocity, detecting those verbs that form reflexive and reciprocal constructions in corpus data. However, exploitation of corpus data for this purpose is complicated due to the diverse functions of reflexive markers crossing the domain of reflexivity and reciprocity. The list of verbs identified by the previous procedure is thus further used in an automatic experiment, applying word embeddings for detecting semantically similar verbs. These candidate verbs have been manually verified and annotation of their reflexive and reciprocal constructions has been integrated into the valency lexicon of Czech verbs VALLEX.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ročník 2020 soutěže (společné úlohy) na Konferenci o počítačovém učení jazyka (CoNLL) byl věnován generování významových reprezentací (Meaning Representation Parsing, MRP) napříč formalismy a jazyky. Podmínky soutěže byly rozšířením podobné soutěže z předcházejícího roku. Pět různých přístupů k reprezentaci větného významu v podobě orientovaných grafů bylo zastoupeno v anglických trénovacích a testovacích datech pro tuto úlohu a uloženo v jednotném formátu pro abstraktní reprezentaci a serializaci grafů. U čtyř z těchto formalismů byla k dispozici další trénovací a testovací data v jednom dalším jazyce. Soutěže se zúčastnilo osm týmů, z nichž dva nejsou zahrnuty v oficiální výsledkové listině, protože jejich řešení bylo doručeno po termínu nebo zahrnovalo přídavná trénovací data. Technické informace o soutěži, včetně zaslaných řešení, oficiálních výsledků, odkazů na podpůrné zdroje a software jsou k dispozici na webu soutěže na: http://mrp.nlpl.eu</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a uniform graph abstraction and serialization; for four of these representation frameworks, additional training and evaluation data was provided for one additional language per framework. The task received submissions from eight teams, of which two do not participate in the official ranking because they arrived after the closing deadline or made use of additional training data. All technical information regarding the task, including system submissions, official results, and links to supporting resources and software are available from the task web site at: http://mrp.nlpl.eu</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>NameTag je nástroj pro rozpoznávání pojmenovaných entit. NameTag identifikuje vlastní jména v textu a klasifikuje je do definovaných kategorií, jako například jména osob, míst, organizací, apod.

NameTag 2 také rozpoznává vnořené pojmenované entity. V roce 2019 dosahuje úrovně poznání (state of the art) v češtině, angličtině, nizozemštině a španělštině; a velmi blízce také v němčině (Straková a kol., 2019).

NameTag je dostupný jako NameTag Online Demo a jako webová služba NameTag Web Service na LINDAT/CLARIN.

Modely jsou volně použitelné pro nekomerční užití a jsou distrubuovány pod licencí CC BY-NC-SA, ale některé z dat použité pro vytvoření modelů mohou vyžadovat další licenční podmínky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>NameTag is a tool for named entity recognition (NER). NameTag identifies proper names in text and classifies them into predefined categories, such as names of persons, locations, organizations, etc.

NameTag 2 also recognizes nested named entities (nested NER) and as of 2019, it achieves state of the art in Czech, English, Dutch and Spanish and nearly state of the art in German (Straková et al. 2019).

NameTag is available as an online demo NameTag Online Demo and web service NameTag Web Service hosted by LINDAT/CLARIN.

The linguistic models are free for non-commercial use and distributed under CC BY-NC-SA license, although for some models the original data used to create the model may impose additional licensing conditions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V rozhovoru pro český rozhlas popisujeme, jak probíhá generování divadelní hry v projektu THEaiTRE.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In an interview for the Czech radio Český rozhlas, we describe how we generate a theatre play in the THEaiTRE project.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nedávný pokrok v neuronovém strojovém překladu směřuje k větším sítím trénovaným na stále větším množství hardwarových zdrojů.
V důsledku toho jsou modely NMT nákladné na trénování, a to jak finančně, kvůli nákladům na elektřinu a hardware, tak ekologicky, kvůli uhlíkové stopě.
Zvláště to platí v transferu znalostí při trénování modelu "rodiče" před přenesením znalostí do požadovaného modelu "dítě".
V tomto článku navrhujeme jednoduchou metodu opakovaného použití již natrénovaného modelu pro různé jazykové páry, u nichž není nutné upravovat modelovou architekturu.
Náš přístup nepotřebuje samostatný model pro každou zkoumanou dvojici jazyků, jak je to typické v rámci přenosového učení u neuronového strojového překladu. Abychom ukázali použitelnost naší metody, recyklujeme model Transformeru, který natrénovali jiní vyzkumníci a použijeme ho pro různé jazykové páry.
Naše metoda dosahuje lepší kvality překladu a kratších časů konvergence, než když trénujeme z náhodné inicializace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Recent progress in neural machine translation is directed towards larger neural networks trained on an increasing amount of hardware resources.
As a result, NMT models are costly to train, both financially, due to the electricity and hardware cost, and environmentally, due to the carbon footprint.
It is especially true in transfer learning for its additional cost of training
the ``parent'' model before transferring knowledge and training the desired ``child'' model.
In this paper, we propose a simple method of re-using an already trained model for different language pairs where there is no need for modifications in model architecture.
Our approach does not need a separate parent model for each investigated language pair, as it is typical in NMT transfer learning. To show the applicability of our method, we recycle a Transformer model trained by different researchers and use it to seed models for different language pairs. 
We achieve better translation quality and shorter convergence times than when training from random initialization.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Genderová zaujatost ve strojovém překladu se může projevit při výběru genderových modulací na základě falešných genderových korelací. Například vždy překládat lékaře jako muže a sestry jako ženy. To může být obzvláště škodlivé, protože modely se stávají populárnějšími a jsou zaváděny v rámci komerčních systémů. Naše práce představuje největší důkaz tohoto jevu ve více než 19 systémech předložených WMT ve čtyřech různých cílových jazycích: češtině, němčině, polštině a ruštině.K dosažení tohoto cíle používáme WinoMT, nedávnou automatickou testovací sadu, která zkoumá genderovou korektnost a zkreslení při překladu z angličtiny do jazyků s gramatickým pohlavím. Bývalí pracovníci WinoMT se starají o dva nové jazyky testované ve WMT: polštinu a češtinu. Zjistili jsme, že všechny systémy důsledně používají nepravdivé korelace v datech spíše než smysluplné kontextové informace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Gender  bias  in  machine  translation  can  manifest when choosing gender inflections based on  spurious  gender  correlations.    For  example,  always  translating  doctors  as  men  and nurses  as  women.    This  can  be  particularly harmful as models become more popular and deployed  within  commercial  systems. Our work presents the largest evidence for the phenomenon  in  more  than  19  systems  submitted to the WMT over four diverse target languages:  Czech, German, Polish, and Russian.To achieve this, we use WinoMT, a recent automatic test suite which examines gender coreference and bias when translating from English to languages with grammatical gender. We ex-tend  WinoMT  to  handle  two  new  languages tested in WMT: Polish and Czech. We find that all  systems  consistently  use  spurious  correlations  in  the  data  rather  than  meaningful  contextual information.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CLARIN je evropská výzkumná infrastruktura, která poskytuje přístup k digitálním jazykovým zdrojům a nástrojům z celé Evropy i mimo ni výzkumným pracovníkům v humanitních a sociálních vědách. Tento dokument se zaměřuje na CLARIN jako platformu pro sdílení jazykových zdrojů. Přibližuje nabídku služeb pro agregaci jazykových úložišť a návrh hodnot pro řadu komunit, které těží z větší viditelnosti svých údajů a služeb v důsledku integrace do CLARIN. Zvýšená jemnost jazykových zdrojů slouží celé komunitě společenských a humanitních věd (SSH) a podporuje výzkumné komunity, které usilují o spolupráci založenou na virtuálních sbírkách pro určitou oblast. Dokument se také zabývá širším prostředím platforem služeb založených na jazykových technologiích, které mají potenciál stát se silným souborem interoperabilních zařízení pro nejrůznější využití.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CLARIN is a European Research Infrastructure providing access to digital language resources and tools from across Europe and beyond to researchers in the humanities and social sciences. This paper focuses on CLARIN as a platform for the sharing of language resources. It zooms in on the service offer for the aggregation of language repositories and the value proposition for a number of communities that benefit from the enhanced visibility of their data and services as a result of integration in CLARIN. The enhanced findability of language resources is serving the social sciences and humanities (SSH) community at large and supports research communities that aim to collaborate based on virtual collections for a specific domain. The paper also addresses the wider landscape of service platforms based on language technologies which has the potential of becoming a powerful set of interoperable facilities to a variety of communities of use.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této studii zkoumáme možné výhody využití informací z eye trackeru pro analýzu závislostní syntaxe na anglické části Dundee corpu. Abychom toho dosáhli, zavedeme parsing jako úlohu značkování sekvencí a pak rozšiřujeme neurální model pro značkování sekvencí o rysy z eye trackeru. Poté experimentujeme s různými nastaveními analyzátorů od lexikalizovaného parsingu po delexikalizovaný parser. Naše experimenty ukazují, že u lexikalizovaného parseru, i když jsou zlepšení pozitivní, nejsou statisticky významná, zatímco náš delexikalizovaný parser statisticky významně překonává baseline, kterou jsme stanovili. Analyzujeme také přínos různých rysů z eye trackeru k různým nastavením analyzátoru a zjišťujeme, že rysy z eye trackeru obsahují informace, které se svou povahou doplňují, což znamená, že rozšíření analyzátoru o různé rysy z eye trackeru seskupené dohromady poskytuje lepší výkon než jakýkoli jednotlivý prvek.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we explore the potential benefits of leveraging eye-tracking information for dependency parsing on the English part of the Dundee corpus. To achieve this, we cast dependency parsing as a sequence labelling task and then augment the neural model for sequence labelling with eye-tracking features. We then experiment with a variety of parser setups ranging from lexicalized parsing to a delexicalized parser. Our experiments show that for a lexicalized parser, although the improvements are positive they are not significant whereas our delexicalized parser significantly outperforms the baseline we established. We also analyze the contribution of various eye-tracking features towards the different parser setups and find that eye-tracking features contain information which is complementary in nature, thus implying that augmenting the parser with various gaze features grouped together provides better performance than any individual gaze feature.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme náš příspěvek do společné úlohy SIGTYP 2020 v předpovídání typologických rysů. Náš systém je patří do omezené části soutěže, neboť používá pouze databázi WALS. Zkoumáme dva přístupy. Jednodušší z nich je založen na odhadu korelace mezi hodnotami rysů u stejného jazyka pomocí podmíněných pravděpodobností a vzájemné informace. Druhý přístup je založen na neuronovém prediktoru, který využívá vektorovou reprezentaci jazyků předpočítanou na rysech z WALS. Ve výsledném systému oba přístupy kombinujeme s využitím jejich vlastního odhadu důvěryhodnosti předpovědi. Na testovacích datech dosahujeme úspěšnosti 70,7 %.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our submission to the SIGTYP 2020 Shared Task on the prediction of typological features. We submit a constrained system, predicting typological features only based on the WALS database. We investigate two approaches. The simpler of the two is a system based on estimating correlation of feature values within languages by computing conditional probabilities and mutual information. The second approach is to train a
neural predictor operating on precomputed language embeddings based on WALS features. Our submitted system combines the two approaches based on their self-estimated confidence scores. We reach the accuracy of 70.7% on the
test data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Deep Universal Dependencies (hluboké univerzální závislosti) je sbírka treebanků poloautomaticky odvozených z Universal Dependencies. Obsahuje přídavné hloubkově-syntaktické a sémantické anotace. Verze Deep UD odpovídá verzi UD, na které je založena. Mějte však na paměti, že některé treebanky UD nebyly do Deep UD zahrnuty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Deep Universal Dependencies is a collection of treebanks derived semi-automatically from Universal Dependencies. It contains additional deep-syntactic and semantic annotations. Version of Deep UD corresponds to the version of UD it is based on. Note however that some UD treebanks have been omitted from Deep UD.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Více než 50 let se výzkumníci pokouší naučit počítače číst hudební notaci, kterýžto obor se nazývá rozpoznávání notopisu (Optical Music Recognition, OMR). Do tohoto oboru je však pro začínající výzkumníky stále obtížné proniknout, obzvlášť pokud nemají nezanedbatelné hudební znalosti: je málo materiálů do problematiky uvádějících, a navíc se sám obor průběžně nedokáže shodnout na tom, jak sebe sama definovat a jak vybudovat sdílenou terminologii. V článku se těmto nedostatkům věnujeme: (1) formulujeme robustní definici OMR a vztahů k příbuzným oborům, (2) analyzujeme, jak OMR invertuje proces zapisování hudby, aby získalo z dokumentu popis hudební notace a hudební sémantiky, a (3) předkládáme taxonomii OMR, především novou taxonomii aplikací. Dále diskutujeme, jak hluboké učení ovlivňuje současný výzkum OMR v kontrastu s předchozími přístupy. Na základě tohoto článku by měl čtenář získat základní porozumění OMR: jeho cílům, jeho vnitřní struktuře, vztahům k ostatním oborům, stavu poznání a výzkumných příležitostí, které OMR poskytuje.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>For over 50 years, researchers have been trying to teach computers to read music notation, referred to as Optical Music Recognition (OMR). However, this field is still difficult to access for new researchers, especially those without a significant musical background: Few introductory materials are available, and, furthermore, the field has struggled with defining itself and building a shared terminology. In this work, we address these shortcomings by (1) providing a robust definition of OMR and its relationship to related fields, (2) analyzing how OMR inverts the music encoding process to recover the musical notation and the musical semantics from documents, and (3) proposing a taxonomy of OMR, with most notably a novel taxonomy of applications. Additionally, we discuss how deep learning affects modern OMR research, as opposed to the traditional pipeline. Based on this work, the reader should be able to attain a basic understanding of OMR: its objectives, its inherent structure, its relationship to other fields, the state of the art, and the research opportunities it affords.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek představil zkoumání „fenoménu opakování“ na příkladu opakovaného sledování videoklipu během práce studentů s digitálním učebním materiálem v malých skupinách, kdy měli k dispozici jeden počítač a vyplňovali papírový list otázkami. První sledování (W1) videoklipů bylo většinou dostačující, ale občas byla vložena sekvence opakovaného sledování (W2), která řešila problémy a nejasnosti. Příspěvek se zabývá detailní analýzou těchto interakčních sekvencí opakovaného sledování.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>My paper presents an investigation of “the phenomenon of repetition” in case of re-watching a video clip. I noticed this practice in a corpus of video data recorded in a classroom setting. Students worked with an e-learning material in small groups with one computer device to their disposal, filling a paper sheet with questions. First watching (W1) of video clips was mostly sufficient, but a sequence of re-watching (W2) was occasionally inserted to deal with troubles and ambiguities. After W1, the interactional status of video clip is changed: it becomes a structured object. The clip is (re)inspected with transparent vision acquired through W1. W2 as part of the classroom activity is oriented by the question on the paper sheet, and transparent vision emerges as a novel (and temporary) interactional competence. Practices of working with video clips point to the properties of video clip as an object in interaction, and they inform us of the work involved in constituting occasioned practical objectivities.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek se zabývá automatickou morfologickou segmentací českých lemmat obsažených v derivační síti DeriNet. Popis derivačních vztahů mezi základními a odvozenými lemmaty, a dělení lemmat na sekvence morfémů, jsou dva blízce propojené formální modely popisující vznik slov. Proto navrhujeme novou segmentační metodu, která využívá existence derivační sítě. Naše řešení překonává dosavadní metody segmentace pro češtinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper deals with automatic morphological segmentation of Czech lemmas contained in the word-formation network DeriNet. Capturing derivational relations between base and derived lemmas, and segmenting lemmas into sequences of morphemes are two closely related formal models of how words come into existence. Thus we propose a novel segmentation method that benefits from the existence of the network; our solution constitutes new state-of-the-art for the Czech language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku představujeme novou vyhlepšenou verzi vyhledávače a vizualizátoru slovotvorných sítí DeriSearch.

Slovotvorné sítě jsou datové sady zachycující derivační, kompoziční a jiné slovotvorné vztahy mezi slovy. Jsou reprezentovatelné pomocí orientovaných grafů, ve kterých uzly představují slova a orientované hrany mezi nimi vyjadřují slovotvorné vztahy. Některé sítě navíc obsahují další lingvistické anotace, například segmentaci slov na morfémy nebo identifikaci slovotvorných procesů.

Sítě pro morfologicky bohaté jazyky s produktivním odvozováním a skládáním mají velké komponenty souvislosti, které se obtížně vizualizují. Například v DeriNetu 2.0, jedné ze sítí pro češtinu, je 1/8 slovníku obsažena v komponentách souvislosti velkých přes 500 slov. V síti Word Formation Latin pro latinu je přes 10 000 slov (1/3 slovníku) v jediné komponentě.

S nedávným vydáním souboru slovotvorných sítí pro více jazyků Universal Derivations potřeba nástroje pro vyhledávání a vizualizaci takto komplexních dat dále vzrůstá.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we introduce a new and improved version of DeriSearch, a search engine and visualizer for word-formation networks.

Word-formation networks are datasets that express derivational, compounding and other word-formation relations between words. They are usually expressed as directed graphs, in which nodes correspond to words and edges to the relations between them. Some networks also add other linguistic information, such as morphological segmentation of the words or identification of the processes expressed by the relations.

Networks for morphologically rich languages with productive derivation or compounding have large connected components, which are difficult to visualize. For example, in the network for Czech, DeriNet 2.0, connected components over 500 words large contain 1/8 of the vocabulary, including its most common parts. In the network for Latin, Word Formation Latin, over 10 000 words (1/3 of the vocabulary) are in a single connected component.

With the recent release of the Universal Derivations collection of word-formation networks for several languages, there is a need for a searching and visualization tool that would allow browsing such complex data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Autoři se zabývají právními otázkami týkajícími se tvorby a používání jazykových modelů. Článek začíná vysvětlením vývoje jazykových technologií. Autoři analyzují technologický postup v rámci autorského práva, práv s ním souvisejících a práva na ochranu osobních údajů. Autoři se věnují také komerčnímu využití jazykových modelů. Hlavním argumentem autorů je, že právní omezení vztahující se na jazykové údaje obsahující materiály a osobní údaje chráněné autorským právem se obvykle nevztahují na jazykové modely. Jazykové modely nejsou obvykle považovány za odvozená díla. Vzhledem k široké škále jazykových modelů není tato pozice absolutní.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The authors address the legal issues relating to the creation and use of language models. The article begins with an explanation of the development of language technologies. The authors analyse the technological process within the framework copyright, related rights and personal data protection law. The authors also cover commercial use of language models. The authors’ main argument is that legal restrictions applicable to language data containing copyrighted material and personal data usually do not apply to language mod-els. Language models are not normally considered derivative works. Due to a wide range of language models, this position is not absolute.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve Švandově divadle se chystá nová hra. Premiéru bude mít už v lednu. Ale nikdo ještě úplně neví, o čem přesně bude. Její scénář totiž nevzniká v hlavě žádného člověka. Generuje ho umělá inteligence.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>There's a new play coming up at the Švanda Theatre. It will premiere in January. But no one quite knows what it will be about yet. Its script doesn't originate in any human's mind. It's generated by artificial intelligence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>„Large Scale Colloquial Persian Dataset“ (LSCP) je hierarchicky uspořádán do asemantické taxonomie, která se zaměřuje na víceúčelové neformální porozumění perskému jazyku jako komplexní problém. LSCP zahrnuje 120 milionů vět z 27 milionů příležitostných perských tweetů se svými závislostními vztahy ve syntaktické anotaci, tagy řeči, polaritu sentimentu a automatický překlad původních perských vět do pěti různých jazyků (EN, CS, DE, IT, HI).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Large Scale Colloquial Persian Dataset" (LSCP) is hierarchically organized in asemantic taxonomy that focuses on multi-task informal Persian language understanding as a comprehensive problem. LSCP includes 120M sentences from 27M casual Persian tweets with its dependency relations in syntactic annotation, Part-of-speech tags, sentiment polarity and automatic translation of original Persian sentences in five different languages (EN, CS, DE, IT, HI).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozpoznávání jazyků v posledních letech významně pokročilo pomocí moderních metod strojového učení, jako je deep learning a měřítka s bohatými anotacemi. Výzkum je však ve formálních jazycích s nízkými zdroji stále omezený. Skládá se z
významná mezera v popisu hovorového jazyka, zejména pro ty s nízkými zdroji, jako je perština. Aby bylo možné tuto mezeru zacílit
pro jazyky s nízkými zdroji navrhujeme „Large Scale Colloquial Persian Dataset“ (LSCP). LSCP je hierarchicky uspořádán do a
sémantická taxonomie, která se zaměřuje na víceúčelové neformální porozumění perskému jazyku jako komplexní problém. To zahrnuje uznání několika sémantických aspektů ve větách na lidské úrovni, které přirozeně zachycuje z vět z reálného světa. Věříme, že další vyšetřování a zpracování, stejně jako aplikace nových algoritmů a metod, může posílit obohacení počítačového porozumění a zpracování jazyků s nízkými zdroji. Navrhovaný korpus se skládá ze 120 milionů vět vycházejících z 27 milionů tweetů anotovaných stromem analýzy, tagy řeči, polaritou sentimentu a překladem do pěti různých jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Language recognition has been significantly advanced in recent years by means of modern machine learning methods such as deep
learning and benchmarks with rich annotations. However, research is still limited in low-resource formal languages. This consists of
a significant gap in describing the colloquial language especially for low-resourced ones such as Persian. In order to target this gap
for low resource languages, we propose a “Large Scale Colloquial Persian Dataset” (LSCP). LSCP is hierarchically organized in a
semantic taxonomy that focuses on multi-task informal Persian language understanding as a comprehensive problem. This encompasses
the recognition of multiple semantic aspects in the human-level sentences, which naturally captures from the real-world sentences. We
believe that further investigations and processing, as well as the application of novel algorithms and methods, can strengthen enriching
computerized understanding and processing of low resource languages. The proposed corpus consists of 120M sentences resulted from
27M tweets annotated with parsing tree, part-of-speech tags, sentiment polarity and translation in five different languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci navrhujeme algoritmus pro indukci morfologických sítí pro perštinu a turečtinu. Algoritmus využívá slovníky s morfematickou segmentací. Výsledná síť zachycuje jak derivační, tak flektivní relace. Algoritmus pro indukci sítě vychází buď z automaticky rozlišených afixů a kořenů, nebo z jednoduché klasifikační heuristiky. Obě varianty jsou empiricky vyhodnoceny. Pro perštinu používáme vlastní velký ručně segmentovaný slovník, pro turečtinu menší slovník publikovaný dříve. Ručně anotovaná data jsou algoritmem využita pro inicializaci sítě, která je následně rozšířena o formy pozorované v korpusech. Slovní formy, které nebyly přítomny v ručně anotovaných datech, segmentujeme řízenou i neřízenou verzí segmentačního nástroje Morfessor a nástrojem MorphSyn. Experimentální výsledky ukazují, jak inicializace ručně segmentovanými daty ovlivňuje finální kvalitu vygenerovaných sítí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this work, we propose an algorithm that induces morphological networks for Persian and Turkish. The algorithm uses morpheme-segmented lexicons for the two languages. The resulting networks capture both derivational and inflectional relations. The network induction algorithm can use either manually annotated lists of roots and affixes, or simple heuristics to distinguish roots from affixes. We evaluate both variants empirically. We use our large hand-segmented set of word forms in the experiments with Persian, which is contrasted with employing only a very limited manually segmented lexicon for Turkish that existed previously. The network induction algorithm uses gold segmentation data for initializing the networks, which are subsequently extended with additional corpus attested word forms that were unseen in the segmented data. For this purpose, we use existing morpheme-segmentation tools, namely supervised and unsupervised version of Morfessor, and (unsupervised) MorphSyn.
The experimental results show that the accuracy of segmented initial data influences derivational network quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku je představena COSTRA 1.0, dataset komplexních transformací vět. Dataset je určen ke studiu větných embeddingů nad rámec jednoduchých výměn slov nebo standardních parafrází.  COSTRA 1.0 obsahuje pouze věty v češtině, ale metoda konstrukce je univerzální a plánujeme ji použít i pro jiné jazyky.

Dataset obsahuje 4262 unikátních vět s průměrnou délkou 10 slov, ilustrujících 15 typů úprav, jako je zjednodušení, zobecnění nebo formální a neformální jazykové variace. Doufáme, že s tímto datovým souborem bychom měli být schopni otestovat sémantické vlastnosti větných embeddingů a možná dokonce najít nějaké topologicky zajímavé '' kostry '' v prostoru větných embeddingů. Předběžná analýza s využitím mnohojazyčných větných embeddingů LASER naznačuje, že nevykazuje požadované vlastnosti</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present COSTRA 1.0, a dataset of complex sentence transformations. The dataset is intended for the study of sentence-level embeddings beyond simple word alternations or standard paraphrasing. This first version of the dataset is limited to sentences in Czech but the construction method is universal and we plan to use it also for other languages.

The dataset consists of 4,262 unique sentences with an average length of 10 words, illustrating 15 types of modifications, such as simplification, generalization, or formal and informal language variation. The hope is that with this dataset, we should be able to test semantic properties of sentence embeddings and perhaps even to find some topologically interesting ''skeleton'' in the sentence embedding space. A preliminary analysis using LASER, multi-purpose multi-lingual sentence embeddings suggests that the LASER space does not exhibit the desired properties</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku představujeme nový dataset pro testování geometrických vlastností prostorů vět. Zaměřujeme se zejména na to, jak jsou v rámci větných embeddingů interpretovány komplexní jevy, jako jsou parafrázy, časy nebo zobecnění.
Dataset je přímým rozšířením Costra 1.0, kterou jsme obohatili o další vět a jejich porovnání.
Ukazujeme, že dostupným předtrénovaným větným embeddingům chybí základní předpoklad, aby synonymní věty byly zanořeny blíže k sobě než věty s výrazně odlišným významem.
Na druhou stranu se zdá, že některé embeddingy respektují lineární pořadí větných jevů jako je styl (formálnost a jednoduchost jazyka) nebo čas (minulost do budoucnosti).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present a new dataset for testing geometric properties of sentence embeddings spaces. In particular, we concentrate on examining how well sentence embeddings capture complex phenomena such paraphrases, tense or generalization. 
The dataset is a direct expansion of Costra 1.0, which we extended with more sentences and sentence comparisons.
We show that available off-the-shelf embeddings do not possess essential attributes such as having synonymous sentences embedded closer to each other than similar sentences with a significantly different meaning. 
On the other hand, some embeddings appear to capture the linear order of sentence aspects such as style (formality and simplicity of the language) or time (past to future).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Porovnáváme dva základní přístupy k vícejazyčnému vyhledávání informací: překlad dokumentů (DT) a překlad dotazů (QT). Naše experimenty jsou prováděny na datech CLEF eHealth 2013–2015, které obsahují anglické dokumenty a dotazy v několika evropských jazycích. S použitím Statistického strojového překladu (SMT) a Neurálního strojového překladu (NMT) a trénujeme několik systémů strojového překladu pro překlad neanglických dotazů do angličtiny (QT) a anglických dokumentů do jazyků dotazů (DT). Výsledky ukazují, že kvalita QT pomocí SMT je dostatečná k překonání výsledků vyhledávání s DT pro všechny jazyky. NMT pak dále zvyšuje kvalitu překladu a kvalitu vyhledávání pro QT i DT pro většinu jazyků, QT ale stále poskytuje obecně lepší výsledky vyhledávání než DT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a thorough comparison of two principal approaches to Cross-Lingual Information Retrieval: document translation (DT) and query translation (QT). Our experiments are conducted using the cross-lingual test collection produced within the CLEF eHealth information retrieval tasks in 2013–2015 containing English documents and queries in several European languages. We exploit the Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) paradigms and train several domain-specific and task-specific machine translation systems to translate the non-English queries into English (for the QT approach) and the English documents to all the query languages (for the DT approach). The results show that the quality of QT by SMT is sufficient enough to outperform the retrieval results of the DT approach for all the languages. NMT then further boosts translation quality and retrieval quality for both QT and DT for most languages, but still, QT provides generally better retrieval results than DT.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Výzkumní pracovníci Univerzity Karlovy, Švandova divadla a Akademie múzických umění v Praze v současné době pracují na zajímavém výzkumném projektu, který spojuje umělou inteligenci a robotiku s divadlem. Hlavním cílem jejich projektu je využít umělou inteligenci k vytvoření inovativního divadelního představení, které by mělo mít premiéru v lednu 2021.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Researchers at Charles University, Švanda Theater and the Academy of Performing Arts in Prague are currently working on an intriguing research project that merges artificial intelligence and robotics with theater. Their project's main objective is to use artificial intelligence to create an innovative theatrical performance, which is expected to premiere in January 2021.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme přístup k vícejazyčné syntéze řeči, který využívá koncepce meta-učení  – generování parametrů na základě kontextu – a produkuje přirozeně znějící vícejazyčnou řeč s využitím více jazyků a méně trénovacích dat než předchozí přístupy. Náš model je založen na Tacotronu 2 s plně konvolučním enkodérem vstupního textu, jehož váhy jsou predikovány samostatnou sítí – generátorem parametrů. Pro zlepšení klonování hlasu napříč jazyky náš model používá adversariální klasifikaci mluvčího s vrstvou obracející gradienty, která z enkodéru odstraňuje informace specifické pro daného mluvčího. Provedli jsme dva experimenty, abychom náš model porovnali s baseliny používajícími různé úrovně sdílení parametrů napříč jazyky a přitom vyhodnotili: 1) stabilitu a výkonnost při trénování na malém množství dat, 2) přesnost výslovnosti a kvalitu hlasu při code-switchingu (změně jazyka uprostřed věty). Pro trénování jsme použili dataset CSS10 a náš nový malý dataset založený na nahrávkách Common Voice v pěti jazycích. Ukazujeme, že náš model efektivně sdílí informace napříč jazyky a podle subjektivní evaluace vytváří přirozenější a přesnější vícejazyčnou řeč než baseliny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce an approach to multilingual speech synthesis which uses the meta-learning concept of contextual parameter generation and produces natural-sounding multilingual speech using more languages and less training data than previous approaches. Our model is based on Tacotron 2 with a fully convolutional input text encoder whose weights are predicted by a separate parameter generator network. To boost voice cloning, the model uses an adversarial speaker classifier with a gradient reversal layer that removes speaker-specific information from the encoder. We arranged two experiments to compare our model with baselines using various levels of cross-lingual parameter sharing, in order to evaluate: (1) stability and performance when training on low amounts of data, (2) pronunciation accuracy and voice quality of code-switching synthesis. For training, we used the CSS10 dataset and our new small dataset based on Common Voice recordings in five languages. Our model is shown to effectively share information across languages and according to a subjective evaluation test, it produces more natural and accurate code-switching speech than the baselines.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem workshopu je nabídnout platformu pro diskuse o stavu a budoucnosti hodnocení systémů generování přirozeného jazyka (NLG). Workshop vyzýval k pubilkaci jak archivních článků, tak abstraktů, zaměřených na evaluaci NLG včetně osvědčených postupů lidského hodnocení, kvalitativních studií, kognitivního zkreslení při lidském hodnocení atd. Sešlo se 12 přihlášených prezentací, bylo přijato deset článků a abstraktů, které byly na workshopu prezentovány jako postery. Tento sborník obsahuje pět archivních článků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of the workshop is to offer a platform for discussions on the status and the future of the evaluation of Natural Language Generation (NLG) systems. The workshop invited archival papers and abstracts on NLG evaluation including best practices of human evaluation, qualitative studies, cognitive bias in human evaluations etc.  The workshop received twelve submissions. Ten papers and abstracts were accepted and were presented as posters at the workshop. This proceedings volume contains the five archival papers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve svém projevu shrnu naše nedávné aktivity v překladu textu a řeči. Začnu projektem EU ELITR (https://elitr.eu/), kde se zaměřujeme na vysoce mnohojazyčný živý překlad řeči, který upozorňuje na technické problémy (pravděpodobně je všechny znáte), ale dotknu se i toho, jak získat lepší vstupy od koncových uživatelů, aby byla možná lepší kvalita překladu (naše aktivita na "odchozím překladu" v projektu EU Bergamot, https://browser.mt/). Na závěr požádám o spolupráci na svém celkovém úkolu základního výzkumu: správně určit význam a modelovat ho lidštějším způsobem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In my talk, I will summarize our recent activities in text and speech
translation. I will start with the EU project ELITR (https://elitr.eu/) where we
are aiming at highly multi-lingual live translation of speech, highlighting the
technical challenges (you probably know all of them) but I will also touch upon
getting a better input from the end users so that better translation quality is
possible (our activity on 'outbound translation' in the EU project Bergamot,
https://browser.mt/). Finally, I will ask for collaboration on my
overall basic-research quest: to get the meaning right and to model it in a more
human-like way.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Strojový překlad:

Projekt ELITR (European Live Translator)
usiluje o vytvoření systému pro překlad řeči
pro současné titulkování konferencí
a on-line schůzky až do 43
jazyků. Technologii testuje společnost
Nejvyššího kontrolního úřadu ČR a prostřednictvím alfaview®, německého online
konferenční systém. Další cíle projektu
mají posunout úroveň dokumentů a vícejazyčný strojový překlad, automatický
rozpoznávání řeči a shrnutí setkání.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>ELITR (European Live Translator) project
aims to create a speech translation system
for simultaneous subtitling of conferences
and online meetings targetting up to 43
languages. The technology is tested by
the Supreme Audit Office of the Czech Republic and by alfaview®, a German online
conferencing system. Other project goals
are to advance document-level and multilingual machine translation, automatic
speech recognition, and meeting summarization.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>MorfFlex CZ 2.0 je český morfologický slovník, který původně vyvinul Jan Hajič jako slovník kontroly pravopisu a lemmatizace. MorfFlex je seznam trojic lemma-značka-slovní forma. Pro každou slovní formu je kompletní morfologická informace kódována poziční značkou. Slovní formy jsou uspořádány do skupin (paradigma instancí nebo paradigmat ve zkratce) podle jejich formálního morfologického chování. Paradigma (množina slovník forem) je identifikováno jedinečným lemmatem. Kromě tradičních morfologických kategorií obsahuje popis také některé sémantické, stylistické a odvozené informace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>MorfFlex CZ (the latest version is MorfFlex CZ 2.0)  is the Czech morphological dictionary developed originally by Jan Hajič as a spelling checker and lemmatization dictionary. MorfFlex is a flat list of lemma-tag-wordform triples. For each wordform, full inflectional information is coded in a positional tag. Wordforms are organized into entries (paradigm instances or paradigms in short) according to their formal morphological behavior. The paradigm (set of wordforms) is identified by a unique lemma. Apart from traditional morphological categories, the description also contains some semantic, stylistic and derivational  information.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Bohatě anotovaný a žánrově diverzifikovaný jazykový zdroj, Prague Dependency Treebank - Consolidated 1.0 (PDT-C 1.0) je konsolidovaným vydáním stávajících PDT-korpusů s českými texty, jednotně anotovanými podle standardního anotačního schématu PDT. Korpusy zahrnuté do vydání PDT-C: Prague Dependency Treebank (psané noviny a texty časopisů tří žánrů); Česká část Prague Czech-English Dependency Treebank (přeložené finanční texty, z angličtiny), Prague Dependency Treebank of Spoken Czech (mluvená data, včetně audia a přepisu a anotace rekonstrukce řeči); PDT-Faust (texty generované uživateli). Rozdíl oproti samostatně publikovaným původním korpusům lze stručně popsat následovně: korpusy jsou publikovány v jednom balíčku, aby bylo umožněno jejich snadnější zpracování; data jsou doplněna o manuální lingvistickou anotaci na morfologické rovině a je přiložena nová verze morfologického slovníku; je přiložen společný valenční lexikon pro všechny čtyři původní části. Dokumentace poskytuje dva nástroje pro procházení a úpravy  (TrEd a MEd) a korpus je také k dispozici online pro vyhledávání pomocí PML-TQ.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A richly annotated and genre-diversified language resource, The Prague Dependency Treebank – Consolidated 1.0 (PDT-C 1.0, or PDT-C in short in the sequel) is a consolidated release of the existing PDT-corpora of Czech data, uniformly annotated using the standard PDT scheme. PDT-corpora included in PDT-C: Prague Dependency Treebank (the original PDT contents, written newspaper and journal texts from three genres); Czech part of Prague Czech-English Dependency Treebank (translated financial texts, from English), Prague Dependency Treebank of Spoken Czech (spokem data, including audio and transcripts and multiple speech reconstruction annotation); PDT-Faust (user-generated texts). The difference from the separately published original treebanks can be briefly described as follows: it is published in one package, to allow easier data handling for all the datasets; the data is enhanced with a manual linguistic annotation at the morphological layer and new version of morphological dictionary is enclosed; a common valency lexicon for all four original parts is enclosed. Documentation provides two browsing and editing desktop tools (TrEd and MEd) and the corpus is also available online for searching using PML-TQ.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme bohatě anotovaný a žánrově diverzifikovaný jazykový zdroj Prague Dependency Treebank-Consolidated 1.0 (PDT-C 1.0), jehož účelem je - jak tomu vždy bylo u rodiny Pražských závislostních korpusů - sloužit jako trénovací data pro různé typy úkolů NLP i pro jazykově orientovaný výzkum. PDT-C 1.0 obsahuje čtyři různé datové soubory s českými texty, jednotně anotované podle standardního schématu PDT. Texty pocházejí z různých zdrojů: novinové články, český překlad Wall Street Journal, přepsané dialogy a malé množství uživatelem vytvořených krátkých, často nestandardních jazykových segmentů, které se zadávají do webového překladače. Celkem obsahuje strom kolem 180 000 vět s jejich morfologickou, povrchovou a hlubokou syntaktickou anotací.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a richly annotated and genre-diversified language resource, the Prague Dependency Treebank-Consolidated 1.0 (PDT-C 1.0), the purpose of which is - as it always been the case for the family of the Prague Dependency Treebanks - to serve both as a training data for various types of NLP tasks as well as for linguistically-oriented research. PDT-C 1.0 contains four different datasets of Czech, uniformly annotated using the standard PDT scheme. The texts come from different sources: daily newspaper articles, Czech translation of the Wall Street Journal, transcribed dialogs and a small amount of user-generated, short, often non-standard language segments typed into a web translator. Altogether, the treebank contains around 180,000 sentences with their morphological, surface and deep syntactic annotation. The diversity of the texts and annotations should serve well the NLP applications as well as it is an invaluable resource for linguistic research, including comparative studies regarding texts of different genres. The corpus is publicly and freely available.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přestavujeme Ústav formální a aplikované lingvistiky u příležitosti relokace jeho časti do nové trojské budovy Impakt.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the Institute of formal and applied linguistics at the occasion of the relocation of its part to the new Impakt building in Troja.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve vzpomínkovém článku se vyčíslují několikerá setkání dvou významných českých lingvistů na poli editorském, pedagogickém, ale zejména jde o jejich styčné body v článcích gramatických.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the memorial number the meetings of two famous Czech linguists are described and evaluated with the focus on their grammatical contributions shared by them.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje hlavní principy Funkčního generativního popisu, jak ho navrhl Petr Sgall a jeho spolupracovníci, současně však podává i stručnou charakteristiku vybraných gramatických jevů popsaných v rámci tohoto přístupu. K těmto jevům patří zejména slovesná a substantivní valence a dále aktuální členění věty, zkoumané zejména ve vztahu k negaci a presupozici. Dále je věnována pozornost třídění elips a souvisejícímu jevu zvanému všeobecný aktant. Hlavní principy FGP byly aplikovány, ověřovány a dále zdokonalovány při budování jazykových zdrojů, rovněž v článku krátce popsaných, zejména v rodině Pražských závislostních korpusů a ve valenčních slovnících.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the present contribution, the main constituting features of the Functional Generative Description as proposed by Petr Sgall and his collaborators are introduced together with short characteristics of selected Czech grammatical phenomena within this framework. These phenomena include mainly the verbal and nominal valency and related issues and topic-focus articulation, esp. in relation to negation and presupposition. Further, attention is paid to the categorization of deletions and the related phenomenon of a general participant. Main tenets of FGD have been applied, verified and further refined in the Prague Dependency Treebanks family and valency lexicons, which are briefly characterized here as well.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku analyzujeme adverbiální určení s časovým významem se zvláštním důrazem na jejich formální realizaci. Tradiční klasifikace významů podle toho na jakou otázku adverbiální určení odpovídá (Kdy?, Od kdy?, Do kdy?, Jak dlouho?, Jak často?) vyžaduje přesnější subkategorizaci. Pro primární časové výrazy, které odpovídají na otázky Kdy? a Jak dlouho?, je navržen systém subfunktorů, zatímco pro sekundární významy Od kdy?, Do kdy?, nejsou odpovídající funktory rozděleny na subfunktory. Významy spojené s opakováním a frekvencí zde nejsou diskutovány, protože patří k popisu hranice mezi lexikonem a gramatikou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The system of adverbials with temporal meaning is analyzed here with specific emphasis on their surface expressions. The traditional classification for the meanings applied as responses for the question When?, Since when?, Till when?, How long?, How often? requires a more precise subcategorization with regard to systemic behavior of the adverbials. For the primary temporal expressions answering the questions When? and How long?, a system of subfunctors is proposed, while for the secondary meanings Since when?, Till when?, the corresponding functors are not splitted into subfunctors. The meanings connected with the repetition and the frequency are not discussed here as they belong to the description of the boundary between lexicon and grammar.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku se zaměřujeme na souhru reciprocity a valence. V češtině lze -- s ohledem na reciprocitu -- vymezit tři skupiny sloves: inherentně reciproční slovesa, odvozená inherentně reciproční slovesa a slovesa, který rys vzájemnosti ve svém významu nenesou, ale reciprocitu umožňují. Ukazujeme, že slovesa těchto tří skupin vyžadují jinou reprezentaci ve slovníku a v gramatice a že reflexivum se v jejich konstrukcích má různou slovnědruhovou klasifikací. Dále se soutředíme na ty aspekty reciprocity, které přesahují jazykový význam.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The notion of reciprocity is analyzed in linguistics from many different points of view. In this contribution, we focus on an interplay between reciprocity and valency. In Čzech (as in other languages), three groups of reciprocal verbs can be delimited: inherently reciprocal verbs, derived inherently reciprocal verbs and verbs without reciprocal feature in their lexical meaning that nevertheless allow reciprocity of some of their valency complementations. We show that verbs from the three given groups require different representation in lexicon and grammar and that in their syntactically reciprocalized constructions the clitic reflexive is of different part-of-speech classification. Moreover, they exhibit different types of ambiguity. Finally, we mention those aspects of reciprocity that cross the boundary between linguistically structured meaning and cognitive content.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek přináší přehled způsobů zachycení větného významu v jedenácti hloubkově-syntaktických formalismech, od takových, které jsou založené na lingvistických teoriích vyvíjených řadu desetiletí, až po jednodušší přístupy motivované zpracováním přirozeného jazyka. Nastiňujeme nejdůležitější charakteristiky každého formalismu a poté podrobněji rozebíráme způsoby zachycení konkrétních jazykových jevů napříč všemi formalismy. Snažíme se přitom objasnit společné rysy i rozdíly.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article gives an overview of how sentence meaning is represented in eleven deep-syntactic frameworks, ranging from those based on linguistic theories elaborated for decades to rather lightweight NLP-motivated approaches. We outline most important characteristics of each framework and then discuss how particular language phenomena are treated across those frameworks, while trying to shed light on commonalities as well as differences.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Autorka předkládá analýzu jisté kategorie v aktuálním členění věty, která se v češtině nazývá kulisa a zahrnuje místní a časové určení věty. Ukazuje se, že pokud má jít o kulisu, je toto určení v češtině součástí základu (tématu) věty a stojí na jejím začátku. V angličtině pak vzhledem k zásadě gramatického slovosledu může takové určení stát i na konci věty, v tom případě ovšem není nositelem větného přízvuku. Materiálem analýzy je anotovaný česko-anglický paralelní korpus PCEDT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The author presents analysis of a certain category of the functional sentence perspective, called in Czech "kulisa" (setting), and comprising local and temporal modifications of the sentence. The analysis demonstrates that in Czech, such a setting is a part of the topic (theme) of the sentence and stands at its beginning. In English, due to its grammatical word order, such a setting may be placed at the end of the sentence. In such a case it is not a bearer of the sentential pitch accent.  The analysis is based on the data from the annotated Czech-English corpus PCEDT.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nekrolog věnovaný životu a dílu zesnulého profesora Petra Sgalla, zakladatele počítačové lingvistiky v Československu, člena Pražského lingvistického kroužku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An obituary devoted to the life and work of the late Professor Petr Sgall, the founder of computational linguistics in Czechoslovakia, a member of the Prague Linguistic Circle.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Analýza tzv. fokalizátorů, tj. částic jako jsou anglické also, only, even, a jejich českých protějšků také, jenom, dokonce, vycházející z údajů anglicko-českého paralelního korpusu PCEDT, zaměřená na (i) za jakých podmínek lze o těchto fokalizátorech říci, že slouží jako diskurzní konektory, (ii) které konkrétní diskurzní vztahy jsou dotyčnými fokalizátory značeny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The analysis of the so-called focalizers, i.e. particles such as E. also, only, even, and their Czech counterparts také, jenom, dokonce, based on the data from the English–Czech parallel corpus PCEDT, focused on (i) in which respects and under which conditions these focalizers may  be said to serve as discourse connectives, (ii) which particular discourse relations are indicated by the  focalizers in question.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Naše podrobná analýza dat z anglicko-českého anotovaného paralelního korpusu PCEDT potvrzuje hypotézu, že anglické částice also, only a even i jejich české ekvivalenty hrají v zásadě diskurzivní roli explicitních konektorů, i když jiným způsobem a v jiné míře.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Our detailed analysis of the data from the English–Czech annotated parallel corpus PCEDT confirms the hypothesis that the particles also, only and even as well as their Czech equivalents play basically a discoursive role of explicit connectives, though in a different way and to a different extent.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek představí službu Evropské komise pro strojový překlad - eTranslation</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk will introduce a machine translation service - eTranslation</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Každodenně vzniká na světě nepřeberné množství dat, které mají různou formu a jsou z různých oblastí lidské činnosti. Značnou část tvoří data textová, a to v různých jazycích, např. novinové zprávy, odborné publikace, blogposty, ale i soukromá korespondence, konverzace na sociálních sítích a další. V oblasti lékařství a péče o zdraví tomu není jinak. Širokou škálu útvarů tvoří na jedné straně vysoce důvěryhodné a recenzované publikace pro odborníky, na straně druhé jsou to zcela nekontrolované rady na ten či onen neduh od virtuálních přátel, či dokonce záměrné dezinformace. Zvláštní místo pak mají lékařské zprávy typicky psané velmi specifickým jazykem.   

Příspěvek poskytne přehled výzkumu v oblasti zpracování přirozeného jazyka se zaměřením na různé typy textů z oblasti lékařství a zdraví, a
to zejména v kontextu evropských projektů Khresmoi a KConnect, které byly řešeny na Ústavu formální a aplikované lingvistiky, MFF UK.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Every day, a vast amount of data is created in the world, in various forms and areas of human activity. A significant part consists of textual data in various languages, such as news reports, professional publications, blog posts, but also private correspondence, conversations on social networks and more. This is no different in the field of medicine and health care. A wide range of data consists on the one hand of highly credible and peer-reviewed publications for experts, on the other hand they are completely uncontrolled advices on this or that illness from virtual friends, or even deliberate misinformation. A special place is given to medical reports typically written in a very specific language.

The paper will provide an overview of research in the field of natural language processing with a focus on various types of texts in the field of medicine and health, especially in the context of European projects Khresmoi and KConnect, which were addressed at the Institute of Formal and Applied Linguistics, MFF UK.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>SynSemClass je slovník česko-anglických slovesných synonym. Základními hesly ve slovníku jsou dvojjazyčné česko-anglické slovesné synonymní třídy, v nichž jsou obsažena synonymní česká a anglická slovesa (členy třídy), reprezentovaná jako valenční rámce (tj. slovesné významy), jejichž pojetí vychází z teorie Funkčně generativního popisu jazyka. Sémantická ekvivalence jednotlivých členů třídy byla stanovena na základě jejich kontextového valenčního chování usouvztažněného k situačně-kognitivnímu obsahu (sémantickým rolím). Synonymické vztahy jsou ve slovníku chápány volně, jednotlivé členy třídy jsou ve vztahu nikoli striktní (úplné) synonymie, ale ve vztahu významové podobnosti, tj. částečné synonymie. Předností slovníku je použití paralelního česko-anglického korpusu, PCEDT(https://catalog.ldc.upenn.edu/LDC2012T08 ) jako hlavního zdroje jazykových dat, které umožňuje tzv. "bottom-up" přístup, tj. od praxe k teorii.  Předností slovníku je rovněž propojení všech členů jednotlivých synonymních tříd s dalšími lexikálními zdroji, a to s hesly valenčních slovníků (PDT-Vallex - http://hdl.handle.net/11858/00-097C-0000-0023-4338-F, EngVallex- http://hdl.handle.net/11858/00-097C-0000-0023-4337-2), CzEngVallex - http://hdl.handle.net/11234/1-1512 a Vallex -  http://ufal.mff.cuni.cz/vallex/3.5/), a s hesly sémantických  databází (FrameNet - https://framenet.icsi.berkeley.edu/fndrupal/, VerbNet  - http://verbs.colorado.edu/verbnet/index.html, PropBank - http://verbs.colorado.edu/%7Empalmer/projects/ace.html, Ontonotes - http://verbs.colorado.edu/html_groupings/ a Wordnet - https://wordnet.princeton.edu/).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The SynSemClass synonym verb lexicon is a result of a project investigating semantic ‘equivalence’ of verb senses and their valency behavior in parallel Czech-English language resources, i.e., relating verb meanings with respect to contextually-based verb synonymy. The lexicon entries are linked to PDT-Vallex (http://hdl.handle.net/11858/00-097C-0000-0023-4338-F), EngVallex (http://hdl.handle.net/11858/00-097C-0000-0023-4337-2), CzEngVallex (http://hdl.handle.net/11234/1-1512), FrameNet (https://framenet.icsi.berkeley.edu/fndrupal/), VerbNet (http://verbs.colorado.edu/verbnet/index.html), PropBank (http://verbs.colorado.edu/%7Empalmer/projects/ace.html), Ontonotes (http://verbs.colorado.edu/html_groupings/), and English Wordnet (https://wordnet.princeton.edu/). Part of the dataset are files reflecting interannotator agreement.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>SynSemClass je slovník česko-anglických slovesných synonym. Základními hesly ve slovníku jsou dvojjazyčné česko-anglické slovesné synonymní třídy, v nichž jsou obsažena synonymní česká a anglická slovesa (členy třídy), reprezentovaná jako valenční rámce (tj. slovesné významy), jejichž pojetí vychází z teorie Funkčně generativního popisu jazyka. Sémantická ekvivalence jednotlivých členů třídy byla stanovena na základě jejich kontextového valenčního chování usouvztažněného k situačně-kognitivnímu obsahu (sémantickým rolím). Synonymické vztahy jsou ve slovníku chápány volně, jednotlivé členy třídy jsou ve vztahu nikoli striktní (úplné) synonymie, ale ve vztahu významové podobnosti, tj. částečné synonymie. Předností slovníku je použití paralelního česko-anglického korpusu, PCEDT(https://catalog.ldc.upenn.edu/LDC2012T08 ) jako hlavního zdroje jazykových dat, které umožňuje tzv. "bottom-up" přístup, tj. od praxe k teorii.  Předností slovníku je rovněž propojení všech členů jednotlivých synonymních tříd s dalšími lexikálními zdroji, a to s hesly valenčních slovníků (PDT-Vallex - http://hdl.handle.net/11858/00-097C-0000-0023-4338-F, EngVallex- http://hdl.handle.net/11858/00-097C-0000-0023-4337-2), CzEngVallex - http://hdl.handle.net/11234/1-1512 a Vallex -  http://ufal.mff.cuni.cz/vallex/3.5/), a s hesly sémantických  databází (FrameNet - https://framenet.icsi.berkeley.edu/fndrupal/, VerbNet  - http://verbs.colorado.edu/verbnet/index.html, PropBank - http://verbs.colorado.edu/%7Empalmer/projects/ace.html, Ontonotes - http://verbs.colorado.edu/html_groupings/ a Wordnet - https://wordnet.princeton.edu/).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>he SynSemClass synonym verb lexicon is a result of a project investigating semantic ‘equivalence’ of verb senses and their valency behavior in parallel Czech-English language resources, i.e., relating verb meanings with respect to contextually-based verb synonymy. The lexicon entries are linked to PDT-Vallex (http://hdl.handle.net/11858/00-097C-0000-0023-4338-F), EngVallex (http://hdl.handle.net/11858/00-097C-0000-0023-4337-2), CzEngVallex (http://hdl.handle.net/11234/1-1512), FrameNet (https://framenet.icsi.berkeley.edu/fndrupal/), VerbNet (http://verbs.colorado.edu/verbnet/index.html), PropBank (http://verbs.colorado.edu/%7Empalmer/projects/ace.html), Ontonotes (http://verbs.colorado.edu/html_groupings/), and English Wordnet (https://wordnet.princeton.edu/). Part of the dataset are files reflecting interannotator agreement.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slovník synonym propojuje anglická slovesa s jejich překladovými českými protějšky. Vychází z valence, tak jak je zpracovaná ve valenčních slovnících PDT_Vallex, EngVallex a CzEngVallex, a je založen na datech PCEDT. Sdružuje slovesná synonyma podle významu a chování ohledně sémantických rolí. Hesla jsou propojena s existujícími slovníky pro angličtinu i češtinu (FrameNet, VALLEX, VerbNet, PropBank, OntoNotes, WordNet).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This lexicon stores cross-lingual semantically similar verb senses in synonym classes extracted from a richly annotated parallel corpus, the Prague Czech-English Dependency Treebank. When building the lexicon, we make use of predicate-argument relations (valency) and link them to semantic roles; in addition, each entry is linked to several external lexicons of more or less “semantic” nature, namely FrameNet, WordNet, VerbNet, OntoNotes and PropBank, and Czech VALLEX. The aim is to provide a linguistic resource that can be used to compare semantic roles and their syntactic properties and features across languages within and across synonym groups (classes, or ’synsets’), as well as gold standard data for automatic NLP experiments with such synonyms, such as synonym discovery, feature mapping, etc. However, perhaps the most important goal is to eventually build an event type ontology that can be referenced and used as a human-readable and human-understandable “database” for all types of events, processes and states. While the current paper describes primarily the content of the lexicon, we are also presenting a preliminary design of a format compatible with Linked Data, on which we are hoping to get feedback during discussions at the workshop. Once the resource (in whichever form) is applied to corpus annotation, deep analysis will be possible using such combined resources as training data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento dokument shrnuje závěry tříleté studie o slovesných synonymech v překladu, založené na syntaktických i sémantických kritériích. Primárními jazykovými zdroji jsou stávající české a anglické lexikální a korpusové zdroje, konkrétně valenční lexikony ve stylu Pražského závislostního korpusu, FrameNet, VerbNet, PropBank, WordNet a paralelní Pražský česko-anglický závislostní korpus. Výsledný lexikon slovesných synonym (dříve nazývaný CzEngClass, nyní SynSemClass) a všechny související zdroje spojené se stávajícími lexikony  jsou veřejně a volně dostupné. Projekt samotný sice předpokládá ruční práci s anotacemi, ale předpokládáme, že výsledný zdroj (spolu se stávajícími) použijeme jako nezbytný zdroj pro vývoj automatických metod rozšíření takového lexikonu nebo vytvoření podobných lexikonů pro další jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper summarizes findings of a three-year study on
verb synonymy in translation based on both syntactic and semantic criteria
and reports on recent results extending this work. Primary language
resources used are existing Czech and English lexical and corpus
resources, namely the Prague Dependency Treebank-style valency lexicons,
FrameNet, VerbNet, PropBank, WordNet and the parallel Prague
Czech-English Dependency Treebank, which contains deep syntactic
and partially semantic annotation of running texts. The resulting lexicon
(called formerly CzEngClass, now SynSemClass) and all associated
resources linked to the existing lexicons and corpora following from
this project are publicly and freely available. While the project proper
assumes manual annotation work, we expect to use the resulting resource
(together with the existing ones) as a necessary resource for developing
automatic methods for extending such a lexicon, or creating similar lexicons
for other languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Při zpracování valence adjektiv a substantiv ve valenčním slovníku je třeba rozhodnout, zda je možné jejich afirmativní a negované formy zpracovat v jednom hesle, např. (ne)závislý, (ne)závislost. Kromě rozdílů ve významu afirmativní a negované formy, např. volnost (svoboda) vs. nevolnost (nepříjemný tělesný stav), by k vydělení zvláštního hesla vedla také jejich odlišná valence (srov. anglické dependent on vs. independent of). Naše korpusová analýza ukazuje, že negované formy českých adjektiv a substantiv mají až na výjimky stejnou předložkovou valenci jako jejich afirmativní formy (např. (ne)spokojený s čím, (ne)spokojenost s čím), liší se však jejich frekvence. Při zpracování afirmativních a negovaných forem v jednom hesle je vedle běžných případů třeba jednotným způsobem ošetřit jak méně doložené negované formy a jejich valenci (např. nevděčný za něco), tak případy, kdy frekvence negované formy a její valence naopak převažuje nad afirmativní formou (nepostradatelný pro někoho).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>When treating valency of adjectives and nouns in a valency lexicon it is necessary to decide whether their affirmative and negative form can be captured in one entry, e.g., (ne)závislý ‘(in)dependent’ and (ne)závislost ‘(in)dependence’. There are two cases that would lead to creating a separate entry in the lexicon, namely a difference between the meaning of an affirmative form and the corresponding negative form, e.g., volnost ‘freedom’ vs. nevolnost ‘indisposition’, and a difference in their valency, cf. Eng. dependent on and independent of. Focusing on valency complementations expressed by a prepositional group, we show that valency of negative forms of Czech adjectives and nouns in used corpus data is, with few exceptions, the same as valency of the corresponding affirmative forms, e.g. (ne)spokojený s čím ‘(un)satisfied with sth’, (ne)spokojenost s čím  ‘(dis)satisfaction with sth’. However, it often differs as for its frequency. When capturing valency of affirmative and negative forms in one entry in a lexicon it is important to treat not only the common cases but also less frequent negative forms and their valency (e.g., nevděčný za něco ‘ungrateful for sth’) as well as the opposite cases in which the frequency of a negative form and its valency considerably outnumbers the frequency of the affirmative form (e.g., nepostradatelný pro někoho ‘indispensable to sb’).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valenční slovník NomVallex I. zachycuje valenci českých deverbativních substantiv, která alespoň v jednom ze svých významů patří k sémantické třídě Communication (např. dotaz, dotázání se – dotazování se), Mental Action (např. plán, plánování) nebo Psych State (např. nenávist, nenávidění), a to v celkovém počtu 248 lexémů zahrnujících 505 lexikálních jednotek. Problematiku substantivní valence zpracovává v teoretickém rámci Funkčního generativního popisu, přičemž se důsledně opírá o korpusový materiál (korpusy řady SYN Českého národního korpusu a korpus Araneum Bohemicum Maximum). Navazuje na valenční slovník VALLEX, odkud přebírá anotační schéma, v relevantních případech rozdělení základových slovesných lexémů do lexikálních jednotek a jejich přiřazení k sémantickým třídám. U substantiv zachycuje všechny jejich lexikální významy, přičemž rozlišuje základní kategoriální významy událost (např. žádání, dovtípení se), abstraktní výsledek děje (např. žádost), vlastnost (např. důvtip), substance (např. komunikace (silnice)) a kontejner (např. počet). S cílem demonstrovat valenční chování různých typů substantivních derivátů jsou do slovníku zařazeny jak kmenové deriváty (odvozené sufixy -ní/-tí, např. žádání, navrhování - navržení, namítání - namítnutí), tak kořenové deriváty (odvozené různými sufixy, včetně sufixu nulového, např. žádost, návrh, námitka), je-li možné je od příslušných základových sloves utvořit. Kritériem pro zařazení do slovníku je příslušnost k jedné ze tří výše uvedených sémantických tříd, označení události nebo abstraktního výsledku děje a projev nesystémového valenčního chování, zejména nesystémové formy valenčních doplnění. Valenci zachycuje v podobě valenčního rámce, který pro každé valenční doplnění uvádí funktor a množinu možných morfematických vyjádření, a příkladů, které se vyskytly v použitých korpusech. Jeho cílem je na korpusových datech ukázat, v jakých syntaktických strukturách se zkoumaná substantiva mohou vyskytovat, proto dokládá všechny kombinace aktantů (ve všech formách uvedených ve valenčním rámci), které se u daných substantiv v použitých korpusových datech vyskytly.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The NomVallex I. lexicon describes valency of Czech deverbal nouns belonging to three semantic classes, namely Communication (e.g. dotaz 'question'), Mental Action (e.g. plán 'plan') and Psych State (e.g. nenávist 'hatred'). It covers stem-nominals (e.g. dotazování 'asking') as well as root-nominals (e.g. dotaz 'question'). It captures all lexical meanings of the nouns, differentiating between basic “categorial” meanings of action (e.g. žádání (si) ‘asking’, dovtípení (se) ‘inferring’), abstract result of an action (e.g. žádost ‘request’), property / quality (e.g. důvtip ‘ingenuity’), material object (e.g. pohled ‘postcard’), or container / quantity (e.g. počet ‘number’). Nouns matching the following criteria were included in the lexicon: its semantic class is either Communication, Mental Action or Psych State, its categorial meaning is action or abstract result of an action, and it exhibits non-systemic valency behaviour (especially non-systemic forms of participants) in at least one of its meanings. In total, the lexicon includes 505 lexical units in 248 lexemes. Valency properties are captured in the form of valency frames, specifying valency slots and their morphemic forms. The lexicon aims to illustrate the full range of syntactic structures of noun phrases, and thus the syntactic behaviour of every lexical unit is exemplified with all combinations of its participants (in all forms specified in the valency frame) which were found in the corpus data (CNC SYNv8 and Araneum Bohemicum Maximum). The lexicon is created within the theoretical framework of Functional Generative Description and was inspired by the VALLEX lexicon; it adopts the VALLEX annotation scheme, and in relevant cases, deverbal nouns captured in NomVallex I. mirror the division of lexemes into lexical units and the assignment of lexical units to semantic classes of the base verbs captured in the VALLEX lexicon.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valenční slovník NomVallex I. zachycuje valenci českých deverbativních substantiv, která alespoň v jednom ze svých významů patří k sémantické třídě Communication (např. dotaz, dotázání se – dotazování se), Mental Action (např. plán, plánování) nebo Psych State (např. nenávist, nenávidění), a to v celkovém počtu 248 lexémů zahrnujících 505 lexikálních jednotek. Problematiku substantivní valence zpracovává v teoretickém rámci Funkčního generativního popisu, přičemž se důsledně opírá o korpusový materiál (korpusy řady SYN Českého národního korpusu a korpus Araneum Bohemicum Maximum). Navazuje na valenční slovník VALLEX, odkud přebírá anotační schéma, v relevantních případech rozdělení základových slovesných lexémů do lexikálních jednotek a jejich přiřazení k sémantickým třídám. U substantiv zachycuje všechny jejich lexikální významy, přičemž rozlišuje základní kategoriální významy událost (např. žádání, dovtípení se), abstraktní výsledek děje (např. žádost), vlastnost (např. důvtip), substance (např. komunikace (silnice)) a kontejner (např. počet). S cílem demonstrovat valenční chování různých typů substantivních derivátů jsou do slovníku zařazeny jak kmenové deriváty (odvozené sufixy -ní/-tí, např. žádání, navrhování - navržení, namítání - namítnutí), tak kořenové deriváty (odvozené různými sufixy, včetně sufixu nulového, např. žádost, návrh, námitka), je-li možné je od příslušných základových sloves utvořit. Kritériem pro zařazení do slovníku je příslušnost k jedné ze tří výše uvedených sémantických tříd, označení události nebo abstraktního výsledku děje a projev nesystémového valenčního chování, zejména nesystémové formy valenčních doplnění. Valenci zachycuje v podobě valenčního rámce, který pro každé valenční doplnění uvádí funktor a množinu možných morfematických vyjádření, a příkladů, které se vyskytly v použitých korpusech. Jeho cílem je na korpusových datech ukázat, v jakých syntaktických strukturách se zkoumaná substantiva mohou vyskytovat, proto dokládá všechny kombinace aktantů (ve všech formách uvedených ve valenčním rámci), které se u daných substantiv v použitých korpusových datech vyskytly.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The NomVallex I. lexicon describes valency of Czech deverbal nouns belonging to three semantic classes, namely Communication (e.g. dotaz 'question'), Mental Action (e.g. plán 'plan') and Psych State (e.g. nenávist 'hatred'). It covers stem-nominals (e.g. dotazování 'asking') as well as root-nominals (e.g. dotaz 'question'). It captures all lexical meanings of the nouns, differentiating between basic “categorial” meanings of action (e.g. žádání (si) ‘asking’, dovtípení (se) ‘inferring’), abstract result of an action (e.g. žádost ‘request’), property / quality (e.g. důvtip ‘ingenuity’), material object (e.g. pohled ‘postcard’), or container / quantity (e.g. počet ‘number’). Nouns matching the following criteria were included in the lexicon: its semantic class is either Communication, Mental Action or Psych State, its categorial meaning is action or abstract result of an action, and it exhibits non-systemic valency behaviour (especially non-systemic forms of participants) in at least one of its meanings. In total, the lexicon includes 505 lexical units in 248 lexemes. Valency properties are captured in the form of valency frames, specifying valency slots and their morphemic forms. The lexicon aims to illustrate the full range of syntactic structures of noun phrases, and thus the syntactic behaviour of every lexical unit is exemplified with all combinations of its participants (in all forms specified in the valency frame) which were found in the corpus data (CNC SYNv8 and Araneum Bohemicum Maximum). The lexicon is created within the theoretical framework of Functional Generative Description and was inspired by the VALLEX lexicon; it adopts the VALLEX annotation scheme, and in relevant cases, deverbal nouns captured in NomVallex I. mirror the division of lexemes into lexical units and the assignment of lexical units to semantic classes of the base verbs captured in the VALLEX lexicon.</seg>
            </tuv>
        </tu>
        
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CzeDLex je elektronický slovník českých diskurzních konektorů s daty pocházejícími z velkého korpusu anotovaného diskurzními vztahy. Jeho nová verze CzeDLex 0.6 přináší podstatně větší podíl ručně zpracovaných položek. Struktura slovníku byla upravena, aby umožňovala výskyt primárních konektorů s více položkami pro jeden diskurzní typ. Představujeme novou verzi slovníku a ukazujeme možnosti vyhledávání různých typů informací ve slovníku pomocí PML-Tree Query.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CzeDLex is an electronic lexicon of Czech discourse connectives with its data coming from a large treebank annotated with discourse relations. Its new version CzeDLex 0.6 is significantly larger with respect to manually processed entries. Also, its structure has been modified to allow for primary connectives to appear with multiple entries for a single discourse sense. We present the new version of the lexicon and demonstrate possibilities of mining various types of information from the lexicon using PML-Tree Query.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Mnohé lingvistické teorie a anotační projekty obsahují hloubkově-syntaktickou a/nebo sémantickou rovinu. I když většina z těchto projektů mířila na více než jeden jazyk, žádný z nich se ani zdaleka neblíží počtu jazyků, které jsou pokryté projektem Universal Dependencies (UD). Ve své přednášce nejprve proberu tzv. rozšířené univerzální závislosti (Enhanced Universal Dependencies, EUD), sadu sémanticky orientovaných rozšíření, která byla navržena v rámci projektu Universal Dependencies (ale v současné době jsou k dispozici pouze pro malý počet jazyků). Představím také předběžná pozorování z právě probíhající soutěže v automatickém větném rozboru do EUD (https://universaldependencies.org/iwpt20/). Ve druhé části představím další rozšíření, která navrhujeme v rámci projektu Deep UD a která přesahují rámec současných anotačních pravidel UD. Zaměřím se na dva aspekty: jak tato rozšíření mohou být užitečná při porozumění přirozenému jazyku strojem a do jaké míry je můžeme získat z povrchově syntaktické anotace automaticky, pro mnoho typologicky odlišných jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Many linguistic theories and annotation frameworks contain a deep-syntactic and/or semantic layer. While many of these frameworks have been applied to more than one language, none of them is anywhere near the number of languages that are covered in Universal Dependencies (UD). In my talk, I will first discuss Enhanced Universal Dependencies (EUD), a set of semantically-oriented enhancements that have been proposed within the framework of Universal Dependencies (but which are still available only for a small number of languages). I will also present some preliminary observations from the current shared task on parsing into EUD (https://universaldependencies.org/iwpt20/). In the second part, I will present some additional enhancements, called Deep UD, which extend beyond the official UD guidelines. I will focus on two aspects: how can these enhancements be useful for natural language understanding, and to what extent can they be obtained semi-automatically from the surface annotation for many typologically different languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies (univerzální závislosti) je mezinárodní projekt a komunita, která se snaží poskytnout morfologicky a syntakticky anotovaná data pro mnoho jazyků v jednotném anotačním stylu. Informujeme o českém grantovém projektu MANYLA, který byl jednou z hnacích sil UD.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is an international project and community that seeks to provide morphologically and syntactically annotated data for many languages, using a uniform annotation style. We report on the Czech grant project MANYLA that was one of the driving forces behind UD.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Po nejméně dvě desetiletí jsou syntakticky anotované korpusy (treebanky) důležitým zdrojem jak pro jazykovědný výzkum, tak pro vývoj počítačových aplikací, které potřebují porozumět přirozenému jazyku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>For at least two decades, syntactically annotated corpora (treebanks) have been instrumental both in linguistic research and in development of natural language understanding applications. Even though the application aspect somewhat diminished with the current surge of neural networks, the classical who-did-what-to-whom type of questions still cannot be answered without understanding the syntax of the sentence.
In order to facilitate the usage of treebanks, it is desirable that they capture same phenomena the same way, across languages and domains. This is exactly the goal of Universal Dependencies (UD): a community effort to define cross-linguistically applicable annotation guidelines for morphology and syntax, and to provide data annotated following those guidelines. In my talk, I will introduce UD, its main principles and the current state, and I will discuss some of the challenges that harmonization and multi-lingual annotation presents. In the last part of the talk, I will touch upon the latest development towards Enhanced UD and Deep UD.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pražské tektogramatické grafy (PTG) představují reprezentaci pro zachycení významu, která má kořeny v tektogramatické rovině Pražského závislostního korpusu (PDT) a je teoreticky podložena Funkčním generativním popisem jazyka (FGP). Ve své současné podobě byly PTG připraveny pro soutěž CoNLL 2020 v analýze významových reprezentací napříč formalismy (MRP). Jsou automaticky generovány z pražských závislostních korpusů a uloženy v grafovém formátu založeném na JSONu. Převod je částečně ztrátový; v tomto článku popisujeme, které části anotace byly do PTG zahrnuty a jak jsou v PTG reprezentovány.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Prague Tectogrammatical Graphs (PTG) is a meaning representation framework that originates in the tectogrammatical layer of the Prague Dependency Treebank (PDT) and is theoretically founded in Functional Generative Description of language (FGD). PTG in its present form has been prepared for the CoNLL 2020 shared task on Cross-Framework Meaning Representation Parsing (MRP). It is generated automatically from the Prague treebanks and stored in the JSON-based MRP graph interchange format. The conversion is partially lossy; in this paper we describe what part of annotation was included and how it is represented in PTG.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je třinácté vydání treebanků UD, verze 2.7.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the thirteenth release of UD Treebanks, Version 2.7.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je dvanácté vydání treebanků UD, verze 2.6.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the twelfth release of UD Treebanks, Version 2.6.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme systém pro předpovídání rysů z World Atlas of Language Structures (WALS), který se účastnil soutěže pořádané u příležitosti typologického workshopu SIGTYP 2020.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our system for prediction of typological features from the World Atlas of Language Structures (WALS), which participated in the shared task organized as a part of the SIGTYP 2020 workshop.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Parlament České republiky se skládá ze dvou komor: Poslanecké sněmovny (dolní komora) a Senátu (horní komora). V naší práci se zaměřujeme na agendu a dokumenty týkající se Poslanecké sněmovny. Konkrétně věnujeme zvláštní pozornost stenografickým protokolům, které zaznamenávají schůze Poslanecké sněmovny. Naším cílem je kontinuálně kompilovat protokoly do korpusu ParCzech kódovaného TEI a zpřístupnit ho uživatelsky přívětivějším způsobem, než tak činí Parlament ČR. V první fázi kompilace ParCzech obsahuje protokoly z let 2013+, které zpřístupňujeme a prohledáváme ve webové platformě TEITOK.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Parliament of the Czech Republic consists of two chambers: the Chamber of Deputies (Lower House) and the Senate (Upper House). In our work, we focus on agenda and documents that relate to the Chamber of Deputies. Namely, we pay particular attention to stenographic protocols that record the Chamber of Deputies’ meetings. Our overall goal is to continually compile the protocols into the TEI encoded corpus ParCzech and make the corpus accessible in a more user friendly way than the Parliament publishes the protocols. In the very first stage of the compilation, the ParCzech corpus consists of the 2013+ protocols that we make accessible and searchable in the TEITOK web-based platform.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Korpus ParCzech PS7 1.0 je úplně první část rodiny korpusů z Parlamentu České republiky. ParCzech PS7 1.0 obsahuje stenoprotokoly z Poslanecké sněmovny sedmého volebního období z let 2013-2017. Audio záznamy jsou přiloženy. Přepisy jsou poskytnuty v původním HTML formátu a navíc zkonvertovány do TEI-odvozeném formátu pro korpusového správce TEITOK. Korpus je automaticky obohacen o morfologii a jmenné entity programy MorphoDita a NameTag.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The ParCzech PS7 1.0 corpus is the very first member of the corpus family of data coming from the Parliament of the Czech Republic. ParCzech PS7 1.0 consists of stenographic protocols that record the Chamber of Deputies' meetings held in the 7th term between 2013-2017. The audio recordings are available as well. Transcripts are provided in the original HTML as harvested, and also converted into TEI-derived XML format for use in TEITOK corpus manager. The corpus is automatically enriched with the morphological and named-entity annotations using the procedures MorphoDita and NameTag.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Korpus ParCzech PS7 2.0 je druhá verze korpusu ParCzech PS7, který obsahuje stenografické protokoly sedmého volebního období v letech 2013-2017. Protokoly jsou v jejich puvodním HTML formátu, v TEI formátu a TEI-odvozeném formátu pro korpusového správce TEITOK. Audio záznamy jsou přiloženy. Korpus je automaticky obohacen o morfologii, syntax a jmenné entity programy UDPipe 2 a NameTag 2.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The ParCzech PS7 2.0 corpus is the second version of ParCzech PS7 consisting of stenographic protocols that record the Chamber of Deputies' meetings held in the 7th term between 2013-2017. The protocols are provided in their original HTML format, TEI format and TEI-derived format to make them searchable in the TEITOK corpus manager. Their audio recordings are available as well. The corpus is automatically enriched with the morphological, syntactic, and named-entity annotations using the procedures UDPipe 2 and NameTag 2.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ačkoli název této zprávy přebírá slovo „Manuál“ z předchozích verzí, její účel již primárně není sloužit jako návod pro anotátory. Spíše se pokouší popsat současný stav morfologické anotace ve vydání Prague Dependency Treebank - Consolidated 1.0 (PDT-C 1.0) Věříme, že pokyny mohou být užitečné pro uživatele dat PDT-C 1.0, stejně jako pro přípravu nových.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Although the title of this report inherits the word "Manual" from the previous versions, it is no more intended to guide the annotators. Rather it attempts to describe the current state of the morphological annotation in the Prague Dependency Treebank – Consolidated 1.0 (PDT-C 1.0) We believe that the guidelines can be of use to the users of the PDT-C 1.0 data, as well as for possible preparation of new data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Na základě analýzy bohatého materiálu Pražské databáze syntaktických forem a funkcí (ForFun), která obsahuje psané texty i přepisy dialogů, popisujeme funkce a formy prostorových určení a sledujeme rozdíly v jejich vyjádření v psané i mluvené podobě text. Zaměřujeme se na základní formální vyjádření příslovečných určení: (i) příslovce, (ii) závislá věta a (iii) předložková fráze. Pro každý formální výraz (i) - (iii) popisujeme charakteristické rysy prostorových určení v mluvené komunikaci, které jsme získali z databáze ForFun.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>. Based on an analysis of the rich material of the Prague Database of Syntactic Forms and Functions (ForFun), which contains written texts as well as transcriptions of dialogues, we describe functions and forms of spatial adverbials and observe differences in their expression in written and spoken text. We focus on basic formal expressions of adverbials: (i) an adverb, (ii) a dependent clause and (iii) a prepositional phrase. For each formal expression (i) – (iii), we describe the characteristic features of the spatial adverbials in the spoken communication that we gained from the ForFun database.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valenční slovník českých sloves (též VALLEX 4.0) poskytuje informace o valenční struktuře českých sloves v jejich jednotlivých významech, které charakterizuje pomocí glos a příkladů; tyto základní údaje jsou doplněny o některé další syntaktické a sémantické charakteristiky. VALLEX 4.0 zachycuje 4 700 českých sloves, která odpovídají více než 11 000 lexikálním jednotkám, tedy vždy danému slovesu v daném významu.
VALLEX je budován jako odpověď na potřebu rozsáhlého a kvalitního veřejně dostupného slovníku, který by obsahoval syntaktické a sémantické charakteristiky českých sloves a byl přitom široce uplatnitelný v aplikacích pro zpracování přirozeného jazyka. Při navrhování koncepce slovníku byl proto kladen důraz na všestranné využití jak pro lidského uživatele, tak pro automatické zpracování češtiny.
VALLEX 4.0 obohacuje informace z předchozích verzí o charakteristiku sloves vyjadřujících reflexivní a reciproční významy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>VALLEX 4.0 provides information on the valency structure (combinatorial potential) of verbs in their particular senses, which are characterized by glosses and examples. VALLEX 4.0 describes 4 700 Czech verbs in more than 11 000 lexical units, i.e., given verbs in the given senses.
VALLEX 4.0 is a is a collection of linguistically annotated data and documentation, resulting from an attempt at formal description of valency frames of Czech verbs. In order to satisfy different needs of different potential users, the lexicon is distributed (i) in a HTML version (the data allows for an easy and fast navigation through the lexicon) and (ii) in a machine-tractable form, so that the VALLEX data can be used in NLP applications.
češtiny.
VALLEX 4.0 provides (in addition to information from previous versions) also characteristics of verbs expressing reciprocity and reflexivity.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento balíček obsahuje data použitá v soutěži IWPT 2020. Obsahuje trénovací, vývojová a testovací (vyhodnocovací) datové množiny. Data jsou založena na podmnožině vydání 2.5 Universal Dependencies (http://hdl.handle.net/11234/1-3105), ale některé treebanky obsahují další obohacené anotace nad rámec UD 2.5.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This package contains data used in the IWPT 2020 shared task. It contains training, development and test (evaluation) datasets. The data is based on a subset of Universal Dependencies release 2.5 (http://hdl.handle.net/11234/1-3105) but some treebanks contain additional enhanced annotations. Moreover, not all of these additions became part of Universal Dependencies release 2.6 (http://hdl.handle.net/11234/1-3226), which makes the shared task data unique and worth a separate release to enable later comparison with new parsing algorithms. The package also contains a number of Perl and Python scripts that have been used to process the data during preparation and during the shared task. Finally, the package includes the official primary submission of each team participating in the shared task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje úlohu parsingu (syntaktické analýzy) do rozšířených Universal Dependencies, popisuje data použitá pro trénování a vyhodnocení, jakož i evaluační metriky. Stručně shrnujeme jednotlivé přístupy a probíráme výsledky úlohy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This overview introduces the task of parsing into enhanced universal dependencies, describes the datasets used for training and evaluation, and evaluation metrics. We outline various approaches and discuss the results of the shared task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Náš příspěvek chce představit novou verzi tzv. „pražského“ morfologického slovníku MorfFlex.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes the new version of the "Prague" morphological dictionary MorfFlex.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Metoda stylometrie nejčastějšími slovy neumožňuje přímé srovnání původních textů a jejich překladů, tj. Napříč jazyky. Například v dvojjazyčné česko-německé textové sbírce obsahující paralelní texty (originály a překlady v obou směrech spolu s českými a německými překlady z jiných jazyků) by autoři neshlukovali mezi jazyky, protože seznamy četných slov pro jakékoli české texty jsou zjevně bude se více podobat německému textu a naopak. Pokusili jsme se přijít s interlinguou, která by odstranila rysy specifické pro jazyk a případně zachovala jazykově nezávislé rysy signálu jednotlivého autora, pokud existují. Každý jazykový protějšek jsme označili, lemmatizovali a analyzovali odpovídajícím jazykovým modelem v UDPipe, který poskytuje jazykové označení, které je do značné míry vícejazyčné. Odstranili jsme výstup jazykově závislých položek, ale to samo o sobě moc nepomohlo. V dalším kroku jsme transformovali lemma obou jazykových protějšků na sdílená pseudolemata na základě velmi hrubého česko-německého glosáře s 95,6% úspěšností. Ukazujeme, že u stylometrických metod založených na nejčastějších slovech se můžeme obejít bez překladů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The method of stylometry by most frequent words does not allow direct comparison of original texts and their translations, i.e. across languages. For instance, in a bilingual Czech-German text collection containing parallel texts (originals and translations in both directions, along with Czech and German translations from other languages), authors would not cluster across languages, since frequency word lists for any Czech texts are obviously going to be more similar to each other than to a German text, and the other way round. We have tried to come up with an interlingua that would remove the language-specific features and possibly keep the linguistically independent features of individual author signal, if they exist. We have tagged, lemmatized, and parsed each language counterpart with the corresponding language model in UDPipe, which provides a linguistic markup that is cross-lingual to a significant extent. We stripped the output of language-dependent items, but that alone did not help much. As a next step, we transformed the lemmas of both language counterparts into shared pseudolemmas based on a very crude Czech-German glossary, with a 95.6% success. We show that, for stylometric methods based on the most frequent words, we can do without translations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vzájemná provázanost přejímání slov a slovotvorby (specificky derivace) je demonstrována na příkladu přípon -ismus a -ita,  které jsou uváděny mezi nejběžnějšími příponami v přejatých substantivech v češtině. Obě přípony odvozují abstraktní substantiva, nicméně v mnoha dalších ohledech se liší. Přípona -ismus se kombinuje se základy, které vytvářejí větší derivační rodiny než základy kombinované s -ita, ale i substantiva na -ita většinou sdílejí svůj kořen s několika dalšími deriváty. Analýzou vybraných derivátů a jejich vzájemných vztahů napříč velkého množství derivačních rodin ukazuji, že velikost a vnitřní struktura derivačních rodin může poskytnout informace o významu analyzovaných derivátů. Význam přípon je popsán pomocí vzorců, do kterých jsou zahrnuty relevantní deriváty s explicitně vyznačenými derivačními vztahy. S použitím těchto vzorců je možné vysvětlit sémantické nuance, které u přejaých slov v češtině zatím nebyly popsány.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The interplay between borrowing and word formation (in particular, derivation) is documented on the example of the suffixes -ismus and -ita,  which are listed among the most common suffixes in loan nouns in Czech. They are both used to form abstract nouns but differ in many aspects. The suffix -ismus combines with bases that form larger derivational families than those of -ita but still most nouns in -ita share their root with several other derivatives, too. By analysing selected derivatives and their mutual relations across a large amount of derivational families, I demonstrate that the size and inner structure of derivational families can provide significant knowledge about the meaning of the formations analysed. The meanings of the suffixes are described using patterns which involve the most relevant derivatives with explicitly marked derivational relations. Using the patterns, it is possible to explain semantic nuances that have not been described with loan words in Czech so far.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V návaznosti na aktuální diskuzi o derivačních paradigmatech ve slovotvorbě přednáška představí dvě případové studie, jejichž cílem je identifikace opakujících se vzorců (paradigmat) v derivační morfologii češtiny. První studie se zabývá odvozováním bezpříponových dějových substantiv, druhá studie slovotvorných chováním přejatých sloves v češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Concurring with the recent discussion on derivational paradigms (Bonami &amp; Strnadová 2018, Fernandéz-Alcaina &amp; Čermák 2018, Štekauer 2014, and others), I will present two case studies which aim at identification of repeating patterns (paradigms) in derivational morphology of Czech.

First, suffixless action nouns derived from verbs (skok 'jump') are analysed and contrasted with unmotivated suffixless nouns (noc 'night') from which denominal verbs are derived. The corpus data reveal that suffixless action nouns correspond mostly to a pair of verbs with aspect-changing suffixes (cf. skákat : skočit 'to jump.PFV|IPFV' > skok 'jump'). In contrast, verbs that are based on nouns use a prefix to change the aspect (noc 'night' > nocovat 'to stay.IPFV overnight' > přenocovat 'to stay.PFV overnight').

The difference between verbs with verbal roots vs. nominal roots is elaborated into two patterns which are exploited in the second case study on verbal morphology. Assuming that, according to my analysis, Czech native verbs with verbal roots prefer to change the aspect by substituting the suffix (navrhnout : navrhovat 'to propose.PFV|IPFV') while verbs with nominal roots attach a prefix (nocovat : přenocovat 'to stay.PFV|IPFV overnight'), a clear dominance of the prefixation pattern with loan verbs (e.g. kontrolovat : zkontrolovat 'to control.PFV|IPFV') over the suffixation pattern (riskovat : risknout 'to risk.PFV|IPFV') suggests that loan verbs in Czech resemble native denominal verbs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V češtině se nulový odvozovací sufix předpokládá u substantiv s významem děje nebo výsledku. Tato substantiva jsou považována za deverbální deriváty, protože tyto významy jsou primárně vyjadřovány slovesy. V příspěvku jsou substantiva s nulovou příponu analyzována ve vztahu k základovým slovesům i ve vztahu ke konkurenčním substantivům s nenulovými příponami.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In  Czech, derivational  zero  suffixes  are  assumed  to  occur  in  nouns  that  denote  an  action  or  a  result. These nouns are viewed as derived from verbs because the action meaning is primarily expressed by verbs. As verbs in Czech are obligatorily marked by a thematic suffix conveying grammatical aspect, deverbal  zero-suffix  nouns  belong  to  the  minority  of  derivations  that  have  a  simpler  morphemic structure  than  their  base  words  and  violate  thus  the  general  account  of  derivation  as  an  affix-adding process. In the paper, zero-derived nouns are analysed with respect to the motivating verbs and to competing nominalizations with overt suffixes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je komunitní projekt, jehož cílem je vytvořit mnohojazyčnou sbírku korpusů anotovaných způsobem, který je konzistentní napříč jazyky, v rámci závislostního lexikalistického přístupu. Anotace sestává z lingvisticky motivované segmentace na slova, z morfologické roviny obsahující lemmata, univerzální slovní kategorie a standardizované morfologické rysy, jakož i ze syntaktické roviny, která se zaměřuje na syntaktické vztahy mezi predikáty, argumenty a volnými rozvitími.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is an open community effort to create cross-linguistically consistent treebank annotation for many languages within a dependency-based lexicalist framework. The annotation consists in a linguistically motivated word segmentation; a morphological layer comprising lemmas, universal part-of-speech tags, and standardized morphological features; and a syntactic layer focusing on syntactic relations between predicates, arguments and modifiers. In this paper, we describe version 2 of the guidelines (UD v2), discuss the major changes from UD v1 to UD v2, and give an overview of the currently available treebanks for 90 languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Mezijazyčné vyhledávání informací (pro Elitr LangTools workshop při Eurosai 2020)</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Cross-Lingual Information Retrieval (for Elitr LangTools workshop at Eurosai 2020)</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nástroj Cross-Lingual Information Retrieval (CLIR) vám umožňuje vyhledávat v dokumentech v různých jazycích a pomocí vlastního jazyka zadávat vyhledávací dotaz i zobrazovat výsledky vyhledávání díky automatizovanému strojovému překladu.
V ukázce můžete vyhledávat v auditech a dalších dokumentech publikovaných českými a belgickými nejvyššími kontrolními institucemi. Demo funguje v angličtině, němčině, francouzštině a češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Cross-Lingual Information Retrieval (CLIR) tool allows you to search in documents in various languages, using your own language both to enter the search query as well as to display the search results, thanks to automated machine translation.
In the demo, you can search in audits and other documents published by the Czech and Belgian Supreme Audit Institutions. The demo works in English, German, French and Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento deliverable reportuje přípravu workshopu LangTools na kongres EUROSAI 2020, zaměřeného na prezentaci jazykových technologií zástupcům nejvyšších kontrolních úřadů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This deliverable reports on the preparation of the LangTools workshop at EUROSAI Congress 2020, aimed at presenting NLP Technologies to supreme audit institution (SAI) representatives.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme projekt THEaiTRE, ve kterém se snažíme počítačově vygenerovat scénář divadelní hry.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the THEaiTRE project, in which we are trying to computationally generate a theatre play script.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku představíme projekt THEaiTRE, který si dává za cíl automaticky vygenerovat scénář divadelní hry. Podíváme se, jak to děláme, jak se nám to zatím daří a na jaké problémy narážíme.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the paper, we introduce the THEaiTRE project, which aims to automatically generate a theatre play script. We look at how this is done, how successful we have been so far and what problems we are facing.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Umělá inteligence stále více zasahuje do našich životů a doplňuje či nahrazuje lidi v různých činnostech, jako je řízení auta, překlad textu či vytváření krátkých novinových zpráv. My jsme se rozhodli jí zadat opravdu náročný úkol: napsat novou divadelní hru. Jak se s tím popere? Zvládne umělá inteligence tvořit umělecká díla? A měli bychom jí to vůbec dovolit? Jaká by měla být role umělé inteligence ve společnosti?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Artificial intelligence is increasingly intruding into our lives, complementing or replacing people in various activities such as driving a car, translating text or making short news stories. We decided to give it a really challenging assignment: to write a new play. How's he going to deal with it? Can artificial intelligence create works of art? Should we even let it? What should be the role of artificial intelligence in society?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Karel Čapek psal v roce 1920 o robotech, jsou v dnešní době roboti schopni napsat divadelní hru o Karlu Čapkovi? Chceme, aby umělá inteligence zasahovala do umění? Jaké je povědomí společnosti o využívání robotiky? A jak vývoj nastane v budoucnu? Odpovědi na tyto otázky přinese právě projekt THEAITRE a Rudolf ve své prezentaci na InnoCampu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Karel Čapek wrote about robots in 1920, are robots capable of writing a play about Karel Čapek these days? Do we want artificial intelligence to interfere with art? What is society's awareness of the use of robotics? And what will happen in the future? It is THEAITRE and Rudolf that will provide answers to these questions in their presentation at the InnoCamp.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme THEaiTRE, začínající výzkumný projekt zaměřený na automatické generování scénářů divadelních her. Tento článek podává přehled související literatury a návrh přístupu, který plánujeme použít.
Konkrétně jde o generativní neuronové jazykové modely a metody hierarchického generování, s podporou automatické sumarizace a strojového překladu, doplněné o přístupy používající manuální lidské vstupy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present THEaiTRE, a starting research project aimed at automatic generation of theatre play scripts.
This paper reviews related work and drafts an approach we intend to follow.
We plan to adopt generative neural language models and hierarchical generation approaches, supported by summarization and machine translation methods, and complemented with a human-in-the-loop approach.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Projekt THEaiTRE se provokativně ptá, zda robot zvládne napsat divadelní hru. V uplynulých měsících vědci z MFF UK ve spolupráci s odborníky z DAMU, dramaturgy a herci programovali umělou inteligenci tak, aby zvládla generovat scénář hry. Experiment má plánovanou premiéru na 25. 1. ve Švandově divadle – přesně 100 let od premiéry hry Karla Čapka R.U.R., jejíž výročí tím zároveň připomíná.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The THEaiTRE project provocatively asks if a robot can write a play. Over the past few months, scientists at MFF UK, in collaboration with DAMU experts, dramatists and actors, have programmed artificial intelligence to be able to generate a play script. The experiment is scheduled to premiere at 25. 11 in the Švanda Theatre – exactly 100 years since the premiere of Karel Čapek's R.U.R. play, whose anniversary it also marks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Mnohé studie zkoumaly reprezentace vznikající v neuronových sítích trénovaných pro úkoly NLP a zkoumaly, jaké jazykové informace na úrovni slov mohou být v reprezentacích zakódovány.
V klasickém sondování je klasifikátor trénován na reprezentacích k získání cílové jazykové informace.
Hrozí však, že si klasifikátor pouze zapamatuje jazykové popisky pro jednotlivá slova, místo toho, aby z vyjádření vytěžil jazykové abstrakce, čímž by vykázal falešně pozitivní výsledky.
I když bylo vynaloženo značné úsilí na minimalizaci problému s memorizací, úkol skutečně změřit množství memorizace odehrávající se v klasifikaci byl zatím podceněn.
V naší práci navrhujeme jednoduchou obecnou metodu měření memorizačního efektu, založenou na symetrickém výběru srovnatelných sad viděných a neviděných slov pro trénování a testování.
Naši metodu lze použít k explicitní kvantifikaci množství memorování, které se děje, aby bylo možné zvolit adekvátní nastavení a výsledky sondování bylo možné spolehlivěji interpretovat.
To dokládáme ukázkou naší metody na případové studii sondování slovních druhů v natrénovaném enkodéru neuronového strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Multiple studies have probed representations emerging in neural networks trained for end-to-end NLP tasks and examined what word-level linguistic information may be encoded in the representations.
In classical probing, a classifier is trained on the representations to extract the target linguistic information.
However, there is a threat of the classifier simply memorizing the linguistic labels for individual words, instead of extracting the linguistic abstractions from the representations, thus reporting false positive results.
While considerable efforts have been made to minimize the memorization problem, the task of actually measuring the amount of memorization happening in the classifier has been understudied so far.
In our work, we propose a simple general method for measuring the memorization effect, based on a symmetric selection of comparable sets of test words seen versus unseen in training.
Our method can be used to explicitly quantify the amount of memorization happening in a probing setup, so that an adequate setup can be chosen and the results of the probing can be interpreted with a reliability estimate.
We exemplify this by showcasing our method on a case study of probing for part of speech in a trained neural machine translation encoder.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento dokument představuje výsledky WMT20
Metriky sdíleného úkolu. Účastníci byli dotázáni
k hodnocení výstupů překladatelských systémů
soutěžících v WMT20 News Translation s automatickými metrikami. Deset výzkumů
skupiny předložily 27 metrik, z nichž čtyři
jsou „metriky“ bez odkazů. Kromě toho jsme
vypočítali  pět základních metrik, včetně SENT BLEU, BLEU, TER a CHR F us-
SacreBLEU. Všechny metriky dobře korelují
na úrovni systému, dokumentu a segmentu s
oficiálním prekladem.
Předkládáme rozsáhlou analýzu vlivu
referenčních překladů o metrické spolehlivosti,
jak dobře automatické metriky hodnotí lidské preklady a také upozorňujeme na velké nesrovnalosti
mezi metrickým a lidským skóre při hodnocení 
systémem MT. Nakonec zkoumame,
zda můžeme použít automatické metriky k označení
nesprávného hodnocení lidí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the WMT20
Metrics Shared Task. Participants were asked
to score the outputs of the translation systems
competing in the WMT20 News Translation
Task with automatic metrics. Ten research
groups submitted 27 metrics, four of which
are reference-less “metrics”. In addition,
we computed five baseline metrics, including SENT BLEU, BLEU, TER and CHR F using the SacreBLEU scorer. All metrics were evaluated on how well they correlate at the
system-, document- and segment-level with
the WMT20 official human scores.
We present an extensive analysis on influence
of reference translations on metric reliability,
how well automatic metrics score human trans-
lations, and we also flag major discrepancies
between metric and human scores when eval-
uating MT systems. Finally, we investigate
whether we can use automatic metrics to flag
incorrect human ratings.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Čtení s porozuměním je značně studovaný úkol s obrovskými trénovacími datasety v angličtině. Tato práce se zaměřuje na tvorbu systému čtení s porozuměním pro češtinu, aniž by byla potřeba ručně anotovaná česká trénovací data. Nejprve jsme automaticky přeložili datasety SQuAD 1.1 a SQuAD 2.0 do češtiny, abychom vytvořili trénovací a validační data, která zveřejňujeme na http://hdl.handle.net/11234/1-3249. Poté jsme natrénovali a vyhodnotili několik referenčních modelů založených na architekturách BERT a XLM-RoBERTa. Náš hlavní příspěvek však spočívá v modelech mezijazykového přenosu. Model XLM-RoBERTa, trénovaný na anglických datech a vyhodnocený na češtině, dosahuje velmi konkurenceschopných výsledků, jen přibližně o 2 procenta horší než model trénovaný na přeložených českých datech. Tento výsledek je mimořádně dobrý, vezmeme-li v úvahu skutečnost, že model během trénování neviděl žádná česká data. Mezijazykový přenos je velmi flexibilní a je pomocí něj možné vytvořit model v jakémkoli jazyce, pro který máme dostatek čistých dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Reading comprehension is a well studied task, with huge training datasets in English. This work focuses on building reading comprehension systems for Czech, without requiring any manually annotated Czech training data. First of all, we automatically translated SQuAD 1.1 and SQuAD 2.0 datasets to Czech to create training and development data, which we release at http://hdl.handle.net/11234/1-3249. We then trained and evaluated several BERT and XLM-RoBERTa baseline models. However, our main focus lies in cross-lingual transfer models. We report that a XLM-RoBERTa model trained on English data and evaluated on Czech achieves very competitive performance, only approximately 2 percent points worse than a model trained on the translated Czech data. This result is extremely good, considering the fact that the model has not seen any Czech data during training. The cross-lingual transfer approach is very flexible and provides a reading comprehension in any language, for which we have enough monolingual raw texts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento dokument představuje výsledky sdílených
úkolů ze 7. workshopu o překladech do asijských jazyků (WAT2020). WAT2020 se účastnilo 20 týmů a 14 týmů předložilo výsledky překladů pro lidské hodnocení. Obdrželi jsme také 12 písemných podání k výzkumu, z nichž 7 bylo přijato
s výjimkou. Zhruba 500 výsledků překladů bylo
odevzdáno na automatickém hodnotícím serveru
a vybraná podání byla vyhodnocena ručně.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the shared
tasks from the 7th workshop on Asian transla­tion (WAT2020). For the WAT2020, 20 teams
participated in the shared tasks and 14 teams
submitted their translation results for the hu­man evaluation. We also received 12 research paper submissions out of which 7 were ac­cepted. About 500 translation results were
submitted to the automatic evaluation server,
and selected submissions were manually eval­uated.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Automatické metody a metriky, které hodnotí různá kritéria kvality automaticky generovaných textů, jsou důležité pro vývoj systémů NLG, protože vytvářejí opakovatelné výsledky a umožňují rychlý vývojový cyklus. Představujeme zde pokus automatizovat hodnocení přirozenosti textu, což je velmi důležitá charakteristika metod generování přirozeného jazyka. Namísto spoléhání se na lidské účastníky při hodnocení nebo označování textových vzorků navrhujeme automatizovat proces pomocí metriky lidské pravděpodobnosti, kterou definujeme, a diskriminačního postupu založeného na velkých předtrénovaných jazykových modelech s jejich rozděleními pravděpodobnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Automatic methods and metrics that assess various quality criteria of automatically generated texts are important for developing NLG systems because they produce repeatable results and allow for a fast development cycle. We present here an attempt to automate the evaluation of text naturalness which is a very important characteristic of natural language generation methods. Instead of relying on human participants for scoring or labeling the text samples, we propose to automate the process by using a human likeliness metric we define and a discrimination procedure based on large pretrained language models with their probability distributions. We analyze the text probability fractions and observe how they are influenced by the size of the generative and discriminative models involved in the process. Based on our results, bigger generators and larger pretrained discriminators are more appropriate for a better evaluation of text naturalness. A comprehensive validation procedure with human participants is required as follow up to check how well this automatic evaluation scheme correlates with human judgments.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Schopnost předpovědět délku vědecké práce může být užitečná v mnoha situacích. Tato práce definuje úlohu predikce délky papíru jako regresní problém a uvádí několik experimentálních výsledků pomocí populárních modelů strojového učení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Being able to predict the length of a scientific paper may be helpful in numerous situations. This work defines the paper length prediction task as a regression problem and reports several experimental results using popular machine learning models. We also create a huge dataset of publication metadata and the respective lengths in
number of pages. The dataset will be freely available and is intended to foster research in this domain. As future work, we would like to
explore more advanced regressors based on neural networks and big pretrained language models.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Automatic evaluation of various text quality criteria produced by data-driven intelligent methods is very common and useful because it is cheap, fast, and usually yields repeatable results. In this paper, we present an attempt to automate the human likeliness evaluation of the output text samples coming from natural language generation methods used to solve several tasks. We propose to use a human likeliness score that shows the percentage of the output samples from a method that look as if they were written by a human. Instead of having human participants label or rate those samples, we completely automate the process by using a discrimination procedure based on large pretrained language models and their probability distributions. As follow up, we plan to perform an empirical analysis of human-written and machine-generated texts to find the optimal setup of this evaluation approach. A validation procedure involving human participants will also check how the automatic evaluation correlates with human judgments.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Automatické vyhodnocení různých kritérií kvality textu vytvořených inteligentními metodami založenými na datech je velmi běžné a užitečné, protože je levné, rychlé a obvykle přináší opakovatelné výsledky. V tomto příspěvku prezentujeme pokus automatizovat hodnocení lidské pravděpodobnosti výstupních textových vzorků pocházejících z metod generování přirozeného jazyka používaných k řešení několika úkolů.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Poslední vývoj v učení neuronovými sítěmi výrazně zvýšilo kvalitu automaticky generovaných souhrnů a klíčových slov dokumentu s tím, že je třeba ještě větších trénovacích korpusů. V tomto příspěvku představujeme dvě velké datové sady pro sumarizaci textu (OAGSX) a generování klíčových slov bsahující 34 milionů, resp. 23 milionů záznamů. Data byla získána ze sítě Open Academic Graph obsahující výzkumné profily a publikace. Pečlivě jsme zpracovávali každý záznam a také zkoušeli
několik extraktivních a abstraktivních metod pro obě úlohy, abychom vytvořili základ pro další výzkum. Dále jsme ukázali výkon těchto metod
kontrolou jejich výstupu. Brzy bychom rdi užili modelování témat na dvou množinách, abychom vytvořili dvě podmnožiny článků ze specifičtějších
disciplín.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Recent developments in sequence-to-sequence learning with neural networks have considerably improved the quality of automatically generated text summaries and document keywords, stipulating the need for even bigger training corpora. Metadata of research articles are usually easy to find online and can be used to perform research on various tasks. In this paper, we introduce two huge datasets for text summarization (OAGSX) and keyword generation (OAGKX) research, containing 34 million and 23 million records, respectively. The data were retrieved from the Open Academic Graph which is a network of research profiles and publications. We carefully processed each record and also tried several extractive and abstractive methods of both tasks to create performance baselines for other researchers. We further illustrate the performance of those methods previewing their outputs. In the near future, we would like to apply topic modeling on the two sets to derive subsets of research articles from more specific disciplines.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V rámci hodnotící kampaně Mezinárodní konference o překladu mluveného jazyka (IWSLT 2020) bylo letos zařazeno šest náročných úloh: i) simultánní překlad řeči, ii) překlad videořeči, iii) offline překlad řeči, iv) překlad konverzační řeči, v) Open domain překlad a vi) překlad řeči nerodilých mluvčí. Tento dokument uvádí cíle každé trati, údaje a metriky hodnocení a informuje o výsledcích obdržených podání.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The evaluation campaign of the International Conference on Spoken Language Translation (IWSLT 2020) featured this year six challenge tracks: (i) Simultaneous speech translation, (ii) Video speech translation, (iii) Offline speech translation, (iv) Conversational speech translation, (v) Open domain translation, and (vi) Non-native speech translation. A total of teams participated in at least one of the tracks. This paper introduces each track’s goal, data and evaluation metrics, and reports the results of the received submissions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme společný příspěvek Edinburské univerzity a Univerzity Karlovy do soutěže v česko-anglickém strojovém překladu - WMT 2020 Shared Task on News Translation. Naše rychlé a kompaktní studentské modely destilují znalosti z většího, pomalejšího učitelského modelu. Jsou navrženy tak, aby nabízely dobrý kompromis mezi kvalitou překladu a efektivitou inference. Na česko-anglických testovacích sadách WMT 2020 dosahují rychlosti překladu přes 700 zdrojových slov za sekundu na jednom procesoru, což umožňuje neuronový strojový překlad na spotřebním hardwaru bez GPU.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe the joint submission of the University of Edinburgh and Charles University, Prague, to the Czech/English track in the WMT 2020 Shared Task on News Translation. Our fast and compact student models distill knowledge from a larger, slower teacher. They are designed to offer a good trade-off between translation quality and inference efficiency. On the WMT 2020 Czech-English test sets, they achieve translation speeds of over 700 whitespace-delimited source words per second on a single CPU thread, thus making neural translation feasible on consumer hardware without a GPU.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentuji způsob, jak využít stenografované záznamy jednání PSPČR pro účely trénování systémů rozpoznávání řeči. V článku je uvedena metoda pro získání dat, zarovnání na úrovni slov a výběr spolehlivých částí nepřesného přepisu. Konečně prezentuji systém rozpoznávání řeči natrénovaný na těchto i jiných datech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I present a way to leverage the stenographed recordings of the Czech parliament meetings for purposes of training a speech-to-text system. The article presents a method for scraping the data, acquiring word-level alignment and selecting reliable parts of the imprecise transcript. Finally, I present an ASR system trained on these and other data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Praxe tetování je součástí lidské kultury od počátku dějin. Navzdory kulturním posunům směrem k dobrovolné povaze a společenské přijatelnosti tetování se v tomto textu zaměřujeme na některé případy tetování jako nedobrovolného „značkování“, což je způsob vykonávání naprosté fyzické kontroly nad člověkem, jako v případě lidských otroků. Ve své empirické dimenzi vychází naše studie z Archivu vizuální historie nadace USC Shoah.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The practice of tattooing has been part of human culture since the dawn of history. In this paper, despite the mentioned cultural shifts toward the voluntary nature and social acceptability of tattooing, we focus on some cases of tattooing as an involuntary “branding” practice, which is a historical form of exercising total physical control over one’s person, such as in the case of human slaves. In terms of empirical material, our study is based on the USC Shoah Foundation's Visual History Archive.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme PyVallex, nástroj pro prezentaci, prohledávání/filtrování, editaci/rozšiřování a automatické zpracování strojově čitelných lexikografických dat v původním textovém formátu. Tento systém se skládá z několika komponent, parseru, nástrojů pro validaci dat, vyhledávání pomocí regulárních výrazů, frameworku typu map-reduce pro sestavování komplexnějších dotazů a analýz a webového rozhraní.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present PyVallex, a Python-based system for presenting, searching/filtering, editing/extending and automatic processing of machine readable lexicon data originally available in a text-based format. The system consists of several components: a parser for the specific lexicon format used in several valency lexicons, a data-validation framework, a regular expression based search engine, a map-reduce style framework for querying the lexicon data and a web-based interface integrating complex search and some basic editing capabilities. PyVallex provides most of the typical functionalities of a Dictionary Writing System (DWS), such as multiple presentation modes for the underlying lexical database, automatic evaluation of consistency tests, and a mechanism of merging updates coming from multiple sources. The editing functionality is currently limited to the client-side interface and edits of existing lexical entries, but additional script-based operations on the database are also possible. The code is published under the open source MIT license and is also available in the form of a Python module for integrating into other software.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozpoznávání řeči a strojový překlad učinily v posledních desetiletích velký pokrok a vedly ke vzniku praktických systémů, které dovedou mapovat jednu jazykovou posloupnost na druhou. Přestože jsou stále dostupnější data ve více modalitách jako je zvuk a video, nejmodernější systémy jsou ze své podstaty unimodální v tom smyslu, že jako vstup berou jedinou modalitu - ať už řeč nebo text. Zkušenosti z toho, jak se učí lidé učí jazyk, ukazují, že různé modality nesou navzájem se dolňující se signály, které jsou často klíčové pro řešení mnoha jazykových úkolů. V tomto článku popisujeme datovou sadu How2, rozsáhlou, kolekci videí s přepisy a jejich překlady. Ukazujeme, jak lze tuto datovou sadu využít k vývoji systémů pro různé jazykové úlohy a představujeme řadu modelů. V rámci řešení těchto úloh zjišťujeme, že budování multimodálních architektur, které by fungovaly lépe, než jejich unimodální protějšek, zůstává i nadále velkou výzvou. To ponechává velký prostor pro zkoumání pokročilejších řešení, která plně využívají multimodální povahu datového souboru How2, a také obecného směřování multimodálního učení s využitím jiných multimodálních datových sad.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Speech recognition and machine translation have made major progress over the past decades, providing practical systems to map one language sequence to another. Although multiple modalities such as sound and video are becoming increasingly available, the state-of-the-art systems are inherently unimodal, in the sense that they take a single modality⁠—either speech or text⁠—as input. Evidence from human learning suggests that additional modalities can provide disambiguating signals crucial for many language tasks. Here, we describe the How2 dataset, a large, open-domain collection of videos with transcriptions and their translations. We then show how this single dataset can be used to develop systems for a variety of language tasks and present a number of models meant as starting points. Across tasks, we find that building multi-modal architectures that perform better than their unimodal counterpart remains a challenge. This leaves plenty of room for the exploration of more advanced solutions that fully exploit the multi-modal nature of the How2 dataset, and the general direction of multimodal learning with other datasets as well.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>I když se při strojovém vyhodnocování překladů ve velké míře používají metriky se středem věty, výkonnost na úrovni dokumentů je pro profesionální použití přinejmenším stejně důležitá. V tomto dokumentu upozorňujeme na podrobné hodnocení na úrovni dokumentů zaměřené na markables (výrazy nesoucí většinu významu dokumentu) a negativní dopad různých markable error fenomenů na překlad.

Pro anotační experiment dvou fází jsme vybrali české a anglické dokumenty přeložené systémy, které byly předány do WMT20 News Translation Task. Tyto dokumenty jsou z domén News, Audit a Lease. Ukazujeme, že kvalita a také druh chyb se mezi doménami výrazně liší. Tento systematický rozptyl je v protikladu k automatickým výsledkům hodnocení.

Zkoumáme, které specifické značení je problematické pro systémy MT, a zakončíme analýzou vlivu značených chybových typů na výkonnost MT měřenou lidmi a automatickými hodnotícími nástroji.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Even though sentence-centric metrics are
used widely in machine translation evaluation,
document-level performance is at least equally
important for professional usage. In this paper,
we bring attention to detailed document-level
evaluation focused on markables (expressions
bearing most of the document meaning) and
the negative impact of various markable error
phenomena on the translation.
For an annotation experiment of two phases,
we chose Czech and English documents translated
by systems submitted to WMT20 News
Translation Task. These documents are from
the News, Audit and Lease domains. We show
that the quality and also the kind of errors
varies significantly among the domains. This
systematic variance is in contrast to the automatic
evaluation results.
We inspect which specific markables are problematic
for MT systems and conclude with an
analysis of the effect of markable error types
on the MT performance measured by humans
and automatic evaluation tools.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Často musí uživatelé internetu vyprodukovat text v cizím jazyce, který znají jen velmi málo a nejsou schopni ověřit kvalitu překladu. Úkol nazýváme "outbound translation" a zkoumáme ho zavedením open-source modulárního systému Ptakopět. Jeho hlavním účelem je kontrola interakce člověka se systémy MT posílenými o další subsystémy, jako je zpětný překlad a odhad kvality. Navazujeme na experiment s (českými) lidskými anotátory, kteří mají za úkol vytvářet otázky v jazyce, kterým nemluví (němčina), s pomocí Ptakopětu. Zaměřujeme se na tři případy využití v reálném světě (komunikace s IT podporou, popis administrativních záležitostí a kladení encyklopedických otázek), z nichž získáváme vhled do různých strategií, které uživatelé používají, když čelí outbound translation. Je známo, že zpětný překlad je pro hodnocení systémů MT nespolehlivý, ale naše experimentální hodnocení dokládá, že pro uživatele funguje velmi dobře, přinejmenším u systémů MT střední kvality.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>It  is  not  uncommon  for  Internet  users  to  have  to  produce  a  text  in  a  foreign  language  they  have  very  little  knowledge  of  and  areunable  to  verify  the  translation  quality.We  call  the  task  “outbound  translation”  and  explore  it  by  introducing  an  open-sourcemodular system Ptakopˇet.  Its main purpose is to inspect human interaction with MT systems enhanced with additional subsystems,such  as  backward  translation  and  quality  estimation.    We  follow  up  with  an  experiment  on  (Czech)  human  annotators  tasked  toproduce  questions  in  a  language  they  do  not  speak  (German),  with  the  help  of  Ptakopˇet.   We  focus  on  three  real-world  use  cases(communication with IT support, describing administrative issues and asking encyclopedic questions) from which we gain insight intodifferent strategies users take when faced with outbound translation tasks.  Round trip translation is known to be unreliable for evaluat-ing MT systems but our experimental evaluation documents that it works very well for users, at least on MT systems of mid-range quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje systém Ptakopět, který byl vyvinut pro zkoumání chování užívatelů při práci s interaktivním systémem pro tzv. odchozí překlad. Při něm se překládá do jazyka, kterému užívatel sice nerozumí, ale zodpovědnost za kvalitu překladu ostáva na něm.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The problems of outbound translation, machine translation user confidence and user interaction are not yet fully explored. The goal of the online modular system Ptakopět is to provide tools for studying these phenomena. Ptakopět is a proof-of-concept system for examining user interaction with enhanced machine translation. It can be used either for actual translation or running experiments on human annotators. In this article, we aim to describe its main components and to show how to use Ptakopět for further research. We also share tips for running experiments and setting up a similar online annotation environment.

Ptakopět was already used for outbound machine translation experiments, and we cover the results of the latest experiment in a demonstration to show the research potential of this tool. We show quantitatively that even though backward translation improves machine-translation user experience, it mainly increases users' confidence and not the translation quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Strojový překlad z angličtiny:

V tomto dokumentu předkládáme naše podání k úkolu překládat nenarozené projevy pro IWSLT 2020. Naším hlavním příspěvkem je navržený systém rozpoznávání řeči, který se skládá z akustického modelu a modelu foném-tographeme. Jako mezičlánek používáme telefony. Dokazujeme, že navrhovaný ropovod překonává komerčně využívané automatické rozpoznávání řeči (ASR) a zavádí jej na dráhu ASR. Doplňujeme toto ASR o běžně dostupné systémy MT, abychom se zapojili také do skladby pro překlad řeči.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present our submission to
the Non-Native Speech Translation Task for
IWSLT 2020. Our main contribution is a proposed speech recognition pipeline that consists of an acoustic model and a phoneme-to grapheme model. As an intermediate representation, we utilize phonemes. We demonstrate that the proposed pipeline surpasses
commercially used automatic speech recognition (ASR) and submit it into the ASR track.
We complement this ASR with off-the-shelf
MT systems to take part also in the speech
translation track.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška pojednává o vztahu implicitnosti diskurzních vztahů a dalších faktorů, jako je jejich frekvence, specifičnost jejich sémantiky, a o  signálech spoluvytvářejících význam implicitního diskurzního vztahu (konfrontace: slovosled, dlouhé a krátké tvary zájmen, přízvuk; přípustka: intonace).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The lecture deals with the relation of implicit discourse relations and other factors, such as their frequency, specificity of their semantics, and features signalling the meaning of implicit discourse relation (confrontation: word order, long and short forms of pronouns, accent; concession: intonation).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme velký soubor plenárních zasedání českého parlamentu. Korpus se skládá z přibližně 1200 hodin řečových dat a odpovídajících textových přepisů. Celý korpus byl segmentován na krátké zvukové segmenty, takže je vhodný jak pro trénink, tak pro hodnocení systémů automatického rozpoznávání řeči (ASR). Zdrojovým jazykem korpusu je čeština, což z něj činí cenný zdroj pro budoucí výzkum, protože v českém jazyce je k dispozici pouze několik veřejných datových souborů. Vydání dat doplňujeme experimenty dvou základních systémů ASR trénovaných na prezentovaných datech: tradičnější přístup implementovaný v Kaldi ASR toolkit, který kombinuje skryté Markovovy modely a hluboké neurální sítě (NN), a moderní ASR architekturu implementovanou v Jasper toolkit, který využívá NN v podobě end-to-end.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a large corpus of Czech parliament plenary sessions. The corpus consists of approximately 1200 hours of speech data and corresponding text transcriptions. The whole corpus has been segmented to short audio segments making it suitable for both training and evaluation of automatic speech recognition (ASR) systems. The source language of the corpus is Czech, which makes it a valuable resource for future research as only a few public datasets are available in the Czech language. We complement the data release with experiments of two baseline ASR systems trained on the presented data: the more traditional approach implemented in the Kaldi ASRtoolkit which combines hidden Markov models and deep neural networks (NN) and a modern ASR architecture implemented in Jaspertoolkit which uses deep NNs in an end-to-end fashion.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Žákovské korpusy, čili korpusy, které dokumentují jazyk tak, jak jej používají nerodilí mluvčí, poskytují důležité informace pro výzkum osvojování jazyka i pedagogickou praxi. Tato monografie představuje CzeSL – korpus češtiny nerodilých mluvčích, a to na pozadí teoretických a praktických otázek současného výzkumu v oboru žákovských korpusů.

Jazyky s bohatou morfologií a volným slovosledem, včetně češtiny, jsou pro analýzu osvojovaného jazyka obzvláště náročné. Autoři se zabývají složitostí chybové anotace a popisují tři vzájemně se doplňující anotační schémata. Věnují se také popisu nerodilé češtiny z hlediska standardních jazykových kategorií.

Kniha podrobně rozebírá praktické aspekty tvorby korpusu: proces sběru a anotace, potřebné nástroje, výsledná data, jejich formáty a vyhledávací rozhraní. Kapitola o aplikacích korpusu ilustruje jeho užitečnost pro výuku, výzkum akvizice i počítačovou lingvistiku. Každý, kdo se zabývá tvorbou žákovských korpusů, jistě ocení závěrečnou kapitolu, shrnující úskalí, kterým je třeba se vyhnout.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Learner corpora, linguistic collections documenting a language as used by learners, provide an important empirical foundation for language acquisition research and teaching practice. This book presents CzeSL, a corpus of non-native Czech, against the background of theoretical and practical issues in the current learner corpus research.

Languages with rich morphology and relatively free word order, including Czech, are particularly challenging for the analysis of learner language. The authors address both the complexity of learner error annotation, describing three complementary annotation schemes, and the complexity of description of non-native Czech in terms of standard linguistic categories.

The book discusses in detail practical aspects of the corpus creation: the process of collection and annotation itself, the supporting tools, the resulting data, their formats and search platforms.
The chapter on use cases exemplifies the usefulness of learner corpora for teaching, language acquisition research, and computational linguistics. Any researcher developing learner corpora will surely appreciate the concluding chapter listing lessons learned and pitfalls to avoid.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku se zabýváme možnostmi zpracování výrazů s výraznou pragmatickou složkou významu v jednojazyčném výkladovém slovníku. Nejprve obecněji nastiňujeme formální lexikografické nástroje pro zpracování takových slov, jako je využití kvalifikátorů, komentářů a usage notes. Dále se konkrétněji zaměřujeme na výrazy vyjadřující etnickou a rasovou příslušnost lidí, které reprezentují jeden z typů s výraznou pragmatickou složkou významu. Široce přitom analyzujeme problematiku těchto výrazů, která se projevuje např. v posunech vnímání významů výrazů z této oblasti, v komplikované či nejasné terminologii, v obtížné zachytitelnosti některých konotací apod. Rozbory dokládáme na příkladech z Českého národního korpusu a ze stávajících slovníků, především českých (včetně vznikajícího ASSČ), ale i některých slovenských a anglických. V příloze pak uvádíme příklady zpracování několika vybraných slovníkových hesel.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper deals with the possibilities of the dictionary treatment of words whose meanings include a significant pragmatic component. Firstly, the formal lexicographical tools for their treatment are outlined, including (style) labels, glosses, and usage notes. Furthermore, the paper concentrates in detail on words referring to the ethnic and racial identity of people, representing the type of words whose meanings include a significant pragmatic component. The issues connected with their use and their dictionary treatment are analysed, e.g. shifts in their meaning and perception, complicated and unclear terminology, and the difficulty of capturing particular connotations. The analyses are illustrated using examples from the Czech National Corpus and from dictionaries of contemporary Czech, and selected dictionaries of Slovak and English. The paper also introduces and explains some decisions made by the authors of Akademický slovník současné češtiny (Academic Dictionary of Con­temporary Czech) related to the treatment of the words under scrutiny. In addition, examples of sev­eral dictionary entries or proposals thereof, accompanied by the author’s comments, are attached.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku popisujeme MorfFlex, Morfologický slovník češtiny, jako cenný zdroj pro zkoumání formálního chování slov. Ukazujeme, že MorfFlex poskytuje bohatá na korpusu založená data umožňující podrobně zkoumat různé morfologické jevy. MorfFlex obsahuje slova z celé slovní zásoby, včetně nestandardních jednotek, vlastních jmen, zkratek atd. Navíc ve srovnání s typickými monolingválními slovníky češtiny MorfFlex zachycuje i nestandardní varianty, což je pro češtinu jako jazyk s bohatou flexí velmi důležité.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we describe MorfFlex, the Morphological Dictionary of Czech, as an invaluable resource for exploring the formal behavior of words. We demonstrate that MorfFlex provides valuable and rich data allowing to elaborate on various morphological issues in depth, which is also connected with the fact that the MorfFlex dictionary includes words throughout the whole vocabulary, including non-standard units, proper nouns, abbreviations, etc. Moreover, in comparison with typical monolingual dictionaries of Czech, MorfFlex also captures non-standard wordforms, which is very important for Czech as a language with a rich inflection. In the paper we also demonstrate how particular information on lemmas and wordforms (e.g. variants, homonymy, style information) is marked and structured. The dictionary is provided as a digital open access source available to all scholars via the LINDAT/CLARIAH-CZ language resource repository. It is available in an electronic format, and also in a more human-readable, browsable and partly searchable form.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>UDPipe 2 je nástroj pro tokenizaci, tagging, lemmatizaci a závislostní parsing CoNLL-U souborů.  Předtrénované jazykové modely jsou k dispozici pro téměř všechny UD korpusy a dosahují úspěšnosti srovnatelné s nejlepšími známými výsledky.

UDPipe 2 je svobodný software licencovaný pod Mozilla Public License 2.0 a jazykové modely jsou k dispozici pro nekomerční použití pod licencí CC BY-NC-SA, nicméně původní data použitá k vytvoření modelů mohou v některých případech ukládat další licenční omezení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>UDPipe 2 is an pipeline for tokenization, tagging, lemmatization and dependency parsing of CoNLL-U files. Trained models are provided for nearly all UD treebanks and achieve performance competitive with state-of-the-art.

UDPipe 2 is a free software under Mozilla Public License 2.0 and the linguistic models are free for non-commercial use and distributed under CC BY-NC-SA license, although for some models the original data used to create the model may impose additional licensing conditions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme náš příspěvek k shared tasku EvaLatin, což je první hodnotící kampaň věnovaná hodnocení nástrojů zpracování přirozeného textu pro latinu. Předložili jsme systém založený na UDPipe 2, jednom z vítězů soutěže CoNLL 2018 Shared Task, dále The 2018 Shared Task on Extrinsic Parser Evaluation a také SIGMORPHON 2019 Shared Task. Náš systém získal s velkým náskokem první místo jak v lemmatizaci tak v značkování v režimu otevřené modality, kde jsou povoleny další trénovací data, v kterémžto případě využíváme všechny latinské korpusy Universal Dependencies. V režimu uzavřené modality, kdy jsou povoleny pouze EvaLatin trénovací data, dosahuje náš systém nejlepších výsledků v lemmatizaci a značkování klasických textů a zároveň dosahuje druhého místa v nastavení napříč žánry a napříč časem. V ablačních experimentech hodnotíme vliv BERT a XLM-RoBERTa kontextualizovaných embeddingů a také kódování různých druhů latinských korpusů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our contribution to the EvaLatin shared task, which is the first evaluation campaign devoted to the evaluation of NLP tools for Latin. We submitted a system based on UDPipe 2.0, one of the winners of the CoNLL 2018 Shared Task, The 2018 Shared Task on Extrinsic Parser Evaluation and SIGMORPHON 2019 Shared Task. Our system places first by a wide margin both in lemmatization and POS tagging in the open modality, where additional supervised data is allowed, in which case we utilize all Universal Dependency Latin treebanks. In the closed modality, where only the EvaLatin training data is allowed, our system achieves the best performance in lemmatization and in classical subtask of POS tagging, while reaching second place in cross-genre and cross-time settings. In the ablation experiments, we also evaluate the influence of BERT and XLM-RoBERTa contextualized embeddings, and the treebank encodings of the different flavors of Latin treebanks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Derivations (UDer) je kolekce harmonizovaných lexikalních sítí zachycujících slovotvorné, zvl. derivační vztahy jednotlivých jazyků v témže anotačním schématu. Základní datovou strukturou tohoto anotačního schématu jsou zakořeněné stromy, ve kterých uzly odpovídají lexémům a hrany reprezentují derivační, příp. kompozitní vztahy. Stávající verze UDer v0.1 obsahuje 27 harmonizovaných zdrojů pokrývajících 20 různých jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Derivations (UDer) is a collection of harmonized lexical networks capturing word-formation, especially derivational relations, in a cross-linguistically consistent annotation scheme for many languages. The annotation scheme is based on a rooted tree data structure, in which nodes correspond to lexemes, while edges represent derivational relations or compounding. The current version of the UDer collection contains twenty-seven harmonized resources covering twenty different languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá harmonizací existujících datových zdrojů zachycujících slovotvorbu různých jazyků, konkrétně převodem originálně zachycených slovotvorných příznaků do stejného souborového formátu a zčásti též do téhož anotačního schématu. Shrnuty jsou rozdíly i podobnosti mezi harmonizovanými zdroji. Popsány jsou jednotlivé kroky prezentované harmonizační procedury, jež zahrnuje manuální anotace i aplikaci technik z oblasti strojového učení. Výsledká kolekce 'Universal Derivations 1.0' obsahuje 27 harmonizovaných datových zdrojů, které dohromady pokrývají 20 různých jazyků. Kolekce je volně dostupná v repozitáři LINDAT/CLARIAH CZ a data jednotlivých zdrojů lze též dotazovat pomocí nástroje DeriSearch.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper deals with harmonisation of existing data resources containing word-formation features by converting them into a common file format and partially aligning their annotation schemas. We summarise (dis)similarities between the resources and describe individual steps of the harmonisation procedure, including manual annotations and application of Machine Learning techniques. The resulting 'Universal Derivations 1.0' collection contains  27 harmonised resources covering 20 languages. It is publicly  available  in the LINDAT/CLARIAH CZ repository and can be queried via the DeriSearch tool.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zavádíme novou symetrickou míru (zvanou θpos), která využívá asymetrickou míru KLcpos3 (Rosa a Žabokrtský, 2015), abychom porovnali konzistenci anotace mezi různými anotovanými treebanky téhož jazyka, jestliže jsou anotované podle téhož anotačního schématu. Pro tuto míru můžeme nastavit práh a říct, že dva treebanky lze považovat za harmonické, pokud jde o jejich anotaci, jestliže θpos nepřekročí daný práh. Při stanovování prahové hodnoty posuzujeme vliv (i) různých velikostí dat a (ii) různého žánrového složení dat. Naše odhady vycházejí z dat z různých jazykových rodin, takže práh není tolik závislý na vlastnostech jednotlivých jazyků. Užití navržené míry demonstrujeme na treebancích z vydání 2.5 Universal Dependencies (Zeman et al., 2019): tam, kde je více než jeden treebank pro daný jazyk, uvádíme míru konzistence pro každý pár treebanků. Navržená míra může být nicméně využita pro vyhodnocení konzistence i v jiných anotačních schématech než Universal Dependencies.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a new symmetric metric (called θpos) that utilises the non-symmetric KLcpos3 metric (Rosa and Žabokrtský, 2015) to allow us to compare the annotation consistency between different annotated treebanks of a given language, when annotated under the same guidelines. We can set a threshold for this new metric so that a pair of treebanks can be considered harmonious in their annotation consistency if θpos surpasses the threshold. For the calculation of the threshold, we estimate the effects of (i) the size variation, and (ii) the genre variation in the considered pair of treebanks. The estimations are based on data from treebanks of distinct language families, making the threshold less dependent on the properties of individual languages. We demonstrate the utility of the proposed metric by listing the treebanks in Universal Dependencies version 2.5 (UDv2.5) (Zeman et al., 2019) data that are annotated consistently with other treebanks of the same language. However, the metric could be used to assess inter-treebank annotation consistency under other (non-UD) annotation guidelines as well.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rudolf Rosa z Matematicko-fyzikální fakulty Univerzity Karlovy už od střední školy vystupuje pod nickem R.U.R. a shodou okolností se mu do rukou dostal možná trochu bláznivý nápad zinscenovat ke stoletému výročí uvedení stejnojmenné hry od Karla Čapka, v níž poprvé zaznělo slovo „robot”, divadelní představení, jejímž autorem bude umělá inteligence. Přečtěte si rozhovor o tom, jak se nadšenci z Matfyzu spojili s herci ze Švandova divadla, aby vytvořili nový světový unikát.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Rudolf Rosa of Charles University's Faculty of Mathematics and Physics has been appearing under the nickname R.U.R. since high school, and as it happens, he has become involved in a perhaps slightly crazy idea of, at the occasion of the 100-year anniversary of the introduction of the play R.U.R. by Karel Čapek, in which the word "robot" was first mentioned, staging a theatrical performance written by an artificial intelligence. Read an interview about how Matfyz enthusiasts have teamed up with actors from the Švanda Theatre to create a new world-class unique.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Původně české slovo „robot“ oslaví v lednu příštího roku sto let od vstupu do našeho slovníku. Čapkovo drama R.U.R. bylo první hrou právě o tomto tématu. Jaké by to ale bylo, kdyby naopak roboti napsali hru o člověku, možná přímo o Karlu Čapkovi? Se současným stavem poznání si už lze troufnout prakticky na cokoliv. Umělá inteligence se bude čím dál víc stávat běžnou součástí našich životů a divadlo může být jedním z experimentálních prostorů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Originally, the Czech word "robot" will celebrate one hundred years since entering our dictionary in January next year. Chapek's R.U.R. drama was the first play on this very subject. But what would it be like if, on the other hand, the robots wrote a play about man, perhaps directly about Karel Čapek? With the current state of knowledge, practically anything can be ventured. Artificial intelligence will increasingly become a normal part of our lives, and theatre can be one of the experimental spaces.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Uživatelské rozhraní vytvářené pro potřeby Centra vizuální historie Malach zpřístupňuje několik velkých kolekcí a databází orálně historických pramenů k výzkumu dějin 20. století, se zvláštním zřetelem na problematiku holokaustu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Malach User Interface created for the needs of the Malach Center for Visual History makes several large collections and databases of oral history sources available for the research of 20th century history, with special attention to the issue of the Holocaust History.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Neurální sítě trénované na zpracování přirozeného jazyka zachycují syntaxi, i když není poskytována jako signál dohledu. To naznačuje, že syntaktická analýza je nezbytná pro podcenění jazyka v systémech umělé inteligence. Tento přehledný dokument se zabývá přístupy k hodnocení množství syntaktických informací obsažených v reprezentacích slov pro různé architektury neuronových sítí. Shrnujeme především výzkum anglických jednojazyčných dat o úkolech jazykového modelování a vícejazyčných dat pro systémy neurálního strojového překladu a vícejazyčné jazykové modely. Popisujeme, které předcvičené modely a znázornění jazyka jsou nejvhodnější pro přenos do syntaktických úloh.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Neural networks trained on natural language processing tasks capture syntax even though it is not provided as a supervision signal. This indicates that syntactic analysis is essential to the understating of language in artificial intelligence systems. This overview paper covers approaches of evaluating the amount of syntactic information included in the representations of words for different neural network architectures. We mainly summarize research on English monolingual data on language modeling tasks and multilingual data for neural machine translation systems and multilingual language models. We describe which pre-trained models and representations of language are best suited for transfer to syntactic tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce se zaměřuje na analýzu formy a rozsahu syntaktické abstrakce zachycené BERT extrahováním označených stromů závislosti ze sebepozornosti.
Předchozí práce ukázaly, že jednotlivé hlavy BERT mají tendenci kódovat konkrétní typy vztahů závislosti. Rozšiřujeme tato zjištění explicitním porovnáním vztahů BERT s anotacemi Universal Dependencies (UD), což ukazuje, že se často neshodují jedna ku jedné.
Navrhujeme metodu pro identifikaci vztahu a syntaktickou stavbu stromu. Náš přístup vytváří podstatně více konzistentních stromů závislosti než předchozí práce, což ukazuje, že lépe vysvětluje syntaktické abstrakce v BERT. Zároveň ji lze úspěšně aplikovat jen s minimální mírou dohledu a dobře zobecňuje napříč jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This work focuses on analyzing the form and extent of syntactic abstraction captured by BERT by extracting labeled dependency trees from self-attentions.
Previous work showed that individual BERT heads tend to encode particular dependency relation types. We extend these findings by explicitly comparing BERT relations to Universal Dependencies (UD) annotations, showing that they often do not match one-to-one.
We suggest a method for relation identification and syntactic tree construction. Our approach produces significantly more consistent dependency trees than previous work, showing that it better explains the syntactic abstractions in BERT. At the same time, it can be successfully applied with only a minimal amount of supervision and generalizes well across languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace je věnována implicitnosti diskurzních vztahů, jako je například podspecifikované užití konektivních prostředků nebo sémantická signalizace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presentation wis devoted to the current discourse-oriented projects/research directions at ÚFAL institute in connection with the implicitness of discourse relations, such as underspecified connectives or semantic signalling.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje první verzi lexikonu GeCzLex, online jazykového zdroje překladových ekvivalentů českých a německých anaforických konektorů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce the first version of GeCzLex, an online electronic resource for translation equivalents of Czech and German discourseconnectives.  The lexicon is one of the outcomes of the research on anaphoricity and long-distance relations in discourse, it containsat  present  anaphoric  connectives  (ACs)  for  Czech  and  German  connectives,  and  further  their  possible  translations  documented  inbilingual parallel corpora (not necessarily anaphoric). As a basis, we use two existing monolingual lexicons of connectives: the Lexiconof  Czech  Discourse  Connectives  (CzeDLex)  and  the  Lexicon  of  Discourse  Markers  (DiMLex)  for  German,  interlink  their  relevantentries via semantic annotation of the connectives (according to the PDTB 3 sense taxonomy) and statistical information of translationpossibilities from the Czech and German parallel data of the InterCorp project.  The lexicon is, as far as we know, the first bilingualinventory of connectives with linkage on the level of individual entries, and a first attempt to systematically describe devices engaged inlong-distance, non-local discourse coherence. The lexicon is freely available under the Creative Commons License.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Studie se věnuje analýze vzájemného uspořádání tzv. lokálních vztahů textové koherence a zkoumá využitelnost zjištěných výsledků při modelování globální koherence textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Descriptive approaches to discourse (text) structure and coherence typically proceed either in a bottom-up or a top-down analytic way. The former ones analyze how the smallest discourse units (clauses, sentences) are connected in their closest neighbourhood, locally, in a linear way. The latter ones postulate a hierarchical organization of smaller and larger units, sometimes also represent the whole text as a tree-like graph. In the present study, we mine a Czech corpus of 50k sentences
annotated in the local coherence fashion (Penn Discourse Treebank style) for indices signalling higher discourse structure. We analyze patterns of overlapping discourse relations and look into hierarchies they form. The
types and distributions of the detected patterns correspond to the results for English local annotation, with patterns not complying with the tree-like interpretation at very low numbers. We also detect hierarchical organization of local discourse relations of up to 5 levels in the Czech data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CzeDLex 0.7 je třetí vývojová verze slovníku českých diskurzních konektorů. Slovník obsahuje konektory částečně automaticky extrahované z Pražského diskurzního korpusu 2.0 (PDiT 2.0) a z dalších zdrojů. Nejfrekventovanější slovníková hesla (pokrývající více než 95% diskurzních vztahů anotovaných v PDiT 2.0) byla ručně zkontrolována, přeložena do angličtiny a doplněna dalšími lingvistickými informacemi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CzeDLex 0.7 is the third development version of a lexicon of Czech discourse connectives. The lexicon contains connectives partially automatically extracted from the Prague Discourse Treebank 2.0 (PDiT 2.0) and other resources. The most frequent entries in the lexicon (covering more than 95% of the discourse relations annotated in the PDiT 2.0) have been manually checked, translated to English and supplemented with additional linguistic information.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje první závislostní korpus indo-árijského jazyka bhódžpurštiny. Bhódžpurština je jedním z indických jazyků, pro které není k dispozici dostatek zdrojů v oblasti strojového zpracování jazyka. Účelem projektu Bhódžpurského závislostního korpusu (BHTB) je poskytnout velký, syntakticky anotovaný korpus bhódžpurštiny, který pomůže při tvorbě nástrojů pro automatické zpracování jazyka. Tento projekt také pomůže s mezijazykovým strojovým učením a s typologickým výzkumem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the first dependency treebank for Bhojpuri, an Indo-Aryan language. Bhojpuri is one of the resource-poor Indian languages. The objective of the Bhojpuri Treebank (BHTB) project is to provide a substantial, syntactically annotated treebank for Bhojpuri which helps in building language technological tools. This project will also help in cross-lingual learning and typological research. Currently, the treebank consists of 4,881 tokens using the annotation scheme of Universal Dependencies (UD). We develop a Bhojpuri tagger and parser using the machine learning approach. The accuracy of the model is 57.49% UAS, 45.50% LAS, 79.69% UPOS accuracy and 77.64% XPOS accuracy. Finally, we discuss linguistic analysis and annotation process of the Bhojpuri UD treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Existující modely vícejazyčných větných vektorových reprezentací (embeddingů) vyžadují rozsáhlé paralelní datové zdroje, které nejsou k dispozici pro všechny jazyky. Navrhujeme novou metodu neřízeného učení pro získání vícejazyčných větných embeddingů pouze z jednojazyčných dat. Nejprve pomocí neřízeného strojového překladu vytvoříme syntetický paralelní korpus a použijeme jej k doladění předtrénovaného cross-lingválního maskovaného jazykového modelu (XLM) a k odvození vícejazyčných větných reprezentací. Kvalita reprezentací je hodnocena na dvou úlohách dolování paralelních dat se zlepšením F1 skóre až o 22 bodů oproti standardnímu XLM. Dále pozorujeme, že jeden syntetický dvojjazyčný korpus je schopen vylepšit výsledky i pro jiné jazykové páry.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Existing models of multilingual sentence embeddings require large parallel data resources which are not available for low-resource languages. We propose a novel unsupervised method to derive multilingual sentence embeddings relying only on monolingual data. We first produce a synthetic parallel corpus using unsupervised machine translation, and use it to fine-tune a pretrained cross-lingual masked language model (XLM) to derive the multilingual sentence representations. The quality of the representations is evaluated on two parallel corpus mining tasks with improvements of up to 22 F1 points over vanilla XLM. In addition, we observe that a single synthetic bilingual corpus is able to improve results for other language pairs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje popis soutěžních systémů Univerzity Karlovy pro úlohu WMT20 ve strojovém překladu mezi němčinou a lužickou srbštinou při nedostatku dat. Provedli jsme experimenty s trénováním na syntetických datech a předtrénováním na příbuzných jazykových párech. V plně neřízeném režimu jsme dosáhli 25,5 a 23,7 BLEU při překladu z a do lužické srbštiny. Ve volnějším režimu jsme použili transfer learning z německo-českých paralelních dat a dosáhli 57,4 BLEU a 56,1 BLEU, což je zlepšení o 10 BLEU bodů oproti baseline natrénované pouze na malém množství dostupných německo-lužickosrbských paralelních vět.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a description of CUNI systems submitted to the WMT20 task on unsupervised and very low-resource supervised machine translation between German and Upper Sorbian. We experimented with training on synthetic data and pre-training on a related language pair. In the fully unsupervised scenario, we achieved 25.5 and 23.7 BLEU translating from and into Upper Sorbian, respectively. Our low-resource systems relied on transfer learning from German-Czech parallel data and achieved 57.4 BLEU and 56.1 BLEU, which is an improvement of 10 BLEU points over the baseline trained only on the available small German-Upper Sorbian parallel corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nový přístup pro generování textu z dat založený na postupných úpravách textu. Náš přístup maximalizuje úplnost a sémantickou přesnost výstupního textu a zároveň využívá současných předtrénovaných modelů pro editaci textu (LaserTagger) a modelování jazyka (GPT-2) pro zlepšení plynulosti textu. Za tímto účelem nejprve převádíme data na text pomocí triviální lexikalizace zvlášť pro každou položku a následně výsledný text postupně vylepšujeme neuronovým modelem natrénovaným na spojování vět. Náš přístup vyhodnocujeme na dvou používaných datových sadách (WebNLG, Cleaned E2E) a analyzujeme jeho přínosy a úskalí. Dále ukazujeme, že náš přístup umožňuje generování textu z dat bez dat z konkrétní domény za pomocí obecné datové sady pro spojování vět.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a novel approach to data-to-text generation based on iterative text editing. Our approach maximizes the completeness and semantic accuracy of the output text while leveraging the abilities of recent pretrained models for text editing (LaserTagger) and language modelling (GPT-2) to improve the text fluency. To this end, we first transform data to text using trivial per-item lexicalizations, iteratively improving the resulting text by a neural model trained for the sentence fusion task. The model output is filtered by a simple heuristic and reranked with an off-the-shelf pretrained language model. We evaluate our approach on two major data-to-text datasets (WebNLG, Cleaned E2E) and analyze its caveats and benefits. Furthermore, we show that our formulation of data-to-text generation opens up the possibility for zero-shot domain adaptation using a general-domain dataset for sentence fusion.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme náš systém pro generování textu z RDF pro soutěž WebNLG Challenge 2020. Svůj přístup zakládáme na modelu mBART, který je předtrénován pro vícejazyčný denoising. To nám umožňuje použít jednoduchý, identický end-to-end přístup pro angličtinu i ruštinu. S minimálními nároky specifickými pro konkrétní jazyk nebo úlohu se náš model umístil v první třetině žebříčku pro angličtinu a na prvním nebo druhém místě pro ruštinu v automatických metrikách. Podle lidského hodnocení se dostal do nejlepší nebo druhé nejlepším skupiny systémů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe our system for the RDF-to-text generation task of the WebNLG Challenge 2020. We base our approach on the mBART model, which is pre-trained for multilingual denoising. This allows us to use a simple, identical, end-to-end setup for both English and Russian. Requiring minimal task or language-specific effort, our model placed in the first third of the leaderboard for English and first or second for Russian on automatic metrics, and it made it into the best or second-best system cluster on human evaluation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Aplikace architektury Transformeru na úrovni znaků obvykle vyžaduje velmi hluboké architektury, které se obtížně a pomalu trénují. V článku ukazujeme, že předtrénováním podslovního modelu a jeho finetuningem na znaky můžeme získat kvalitní model pro neuronový strojový překlad, který funguje na úrovni znaků bez nutnosti tokenizace vstupu. Používáme pouze základní šestivrstvou architekturu Transformer Base. Naše modely na úrovni znaků lépe zachycují morfologické jevy a vykazují větší odolnost vůči šumu za cenu poněkud horší celkové kvality překladu. Naše studie je tak významným krokem ke kvalitním a snadno trénovatelným modelům, které modelují překlad na útrovni znaků a zároveň nejsou extrémně velké.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Applying the Transformer architecture on the character level usually requires very deep architectures that are difficult and slow to train. These problems can be partially overcome by incorporating a segmentation into tokens in the model. We show that by initially training a subword model and then finetuning it on characters, we can obtain a neural machine translation model that works at the character level without requiring token segmentation. We use only the vanilla 6-layer Transformer Base architecture. Our character-level models better capture morphological phenomena and show more robustness to noise at the expense of somewhat worse overall translation quality. Our study is a significant step towards high-performance and easy to train character-based models that are not extremely large.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Mnohojazyčné kontextové embedinky, jako vícejazyčný BERT (mBERT) a XLM-RoBERTa, se osvědčily pro mnoho vícejazyčných úloh. Předchozí práce zkoumala mnohojazyčnost reprezentací nepřímo s využitím nulového transferového učení na morfologických a syntaktických úkolech. Místo toho se zaměřujeme na jazykovou neutralitu mBERTu s ohledem na lexikální sémantiku. Naše výsledky ukazují, že kontextové embedinky jsou jazykově neutrálnější a obecně informativnější než zarovnané statické slovní embedinky, které jsou explicitně trénovány na jazykovou neutralitu. Kontextové embedinky jsou stále standardně pouze mírně jazykově neutrální, nicméně ukazujeme dvě jednoduché metody, jak dosáhnout silnější jazykové neutrality: zaprvé neřízeným vystředěním reprezentace pro jazyky a zadruhé explicitní projekcí na malých paralelních datech. Kromě toho ukazujeme, jak překonat nejlepší dosažené přesnosti při identifikaci jazyka a zarovnávání slov v paralelních větách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Multilingual contextual embeddings, such as multilingual BERT (mBERT) and XLM-RoBERTa, have proved useful for many multi-lingual tasks. Previous work probed the cross-linguality of the representations indirectly using zero-shot transfer learning on morphological and syntactic tasks. We instead focus on the language-neutrality of mBERT with respect to lexical semantics. Our results show that contextual embeddings are more language-neutral and in general more informative than aligned static word-type embeddings which are explicitly trained for language neutrality. Contextual embeddings are still by default only moderately language-neutral, however, we show two simple methods for achieving stronger language neutrality: first, by unsupervised centering of the representation for languages, and second by fitting an explicit projection on small parallel data. In addition, we show how to reach state-of-the-art accuracy on language identification and word alignment in parallel sentences.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme příspěvek do soutěže a kombinace automatického překladu a parafrázování ve výuce jazyků (STAPLE). Pro překlad jsme použili standardní model Transformer, doplněný kroslinguálním klasifikátorem pro filtrování překladových hypotéz. Abychom zvýšili rozmanitost výstupů, použili jsme další trénovací data a vyvinuli jsme parafrázovací model založený na architektuře Levenshtein Transformer, který generuje další synonymní překlady. Výsledky parafrázování byly opět filtrovány kroslinguálním klasifikátorem. Zatímco použití dalších dat a náš filtr zlepšily výsledky, parafrázování generovalo příliš mnoho neplatných výstupů, aby dále zlepšilo kvalitu výstupů. Náš model bez parafrázování skončil přibližně uprostřed soutěžního pořadí a přinesl zlepšení o 10-22% vážený F1 bodů oproti základnímu řešení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our submission to the Simultaneous Translation And Paraphrase for Language Education (STAPLE) challenge. We used a standard Transformer model for translation, with a crosslingual classifier predicting correct translations on the output n-best list. To increase the diversity of the outputs, we used additional data to train the translation model, and we trained a paraphrasing model based on the Levenshtein Transformer architecture to generate further synonymous translations. The paraphrasing results were again filtered using our classifier. While the use of additional data and our classifier filter were able to improve results, the paraphrasing model produced too many invalid outputs to further improve the output quality. Our model without the paraphrasing component finished in the middle of the field for the shared task, improving over the best baseline by a margin of 10-22 % weighted F1 absolute.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme naše systémy pro WMT20 Very Low Resource MT Task k překladu mezi němčinou a hornolužickou srbštinou. Pro trénink našich systémů generujeme syntetická data zpětným i dopředným překladem. Trénvací data navíc obohacujeme o německo-české překlady z češtiny do hornolužické srbštiny pomocí neřízeného statistického MT systému, který obsahuje ortograficky podobné slovní dvojice a transliterace slov mimo slovník. Náš nejlepší překladový systém mezi němčinou a srbštinou je založen na transferu modelu z česko-německého systému a má o 12 až 13 BLEU vyšší skóre než základní systém vytvořený pouze s využitím dostupných paralelních dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our systems for the WMT20 Very Low Resource MT Task for translation between German and Upper Sorbian. For training our systems, we generate synthetic data by both back- and forward-translation. Additionally, we enrich the training data with German-Czech translated from Czech to Upper Sorbian by an unsupervised statistical MT system incorporating orthographically similar word pairs and transliterations of OOV words. Our best translation system between German and Sorbian is based on transfer learning from a Czech-German system and scores 12 to 13 BLEU higher than a baseline system built using the available parallel data only.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme naše dva neuronové překladové systémy pro anglicko-český a anglicko-polský překlad, které se zúčastnily soutěže v překladu novinových článků WMT 2020. První systém překládá každou větu nezázvisle. Druhý systém je tzv. document-level, tedy překládá více vět naráz a je trénovaný na vícevětných sekvencích dlouhých až 3000 znaků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe our two NMT systems submitted to the WMT 2020 shared task in English-Czech and English-Polish news translation. One system is sentence level, translating each sentence independently. The second system is document level, translating multiple sentences, trained on multi-sentence sequences up to 3000 characters long.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška úvodem krátce představí některé úlohy, kterými se zabývá počítačová lingvistika, například automatickou opravu překlepů/gramatiky či automatický větný rozbor.

Hlavní pozornost bude věnována úloze strojového překladu, zejména vývoji různých typů překladačů z angličtiny do češtiny během posledního desetiletí. Dnešní nejlepší překladače jsou založeny na technologiích umělé inteligence, konkrétně hlubokých neuronových sítí, a kvalita výsledného překladu se blíží úrovni profesionální překladatelské agentury. Vyvstávají otázky, jak tento pokrok ve kvalitě strojového překladu dosažený během posledních let ovlivní výuku jazyků, ale též zda se na strojové překlady můžeme spolehnout.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The lecture will briefly introduce some of the tasks dealt with in computer linguistics, such as automatic correction of misspellings/grammar or automatic sentence analysis.

The main focus will be on the role of machine translation, in particular the development of different types of English-to-Czech translators over the last decade. Today's best translators are based on artificial intelligence technologies, namely deep neural networks, and the quality of the resulting translation is close to that of a professional translation agency. Questions are being asked about how this progress in machine translation quality made over recent years will affect language learning, but also whether we can rely on machine translations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kvalita lidského překladu byla dlouho považována za nedosažitelnou pro počítačové překladové systémy. V této studii představujeme systém hlubokého učení CUBBITT, který tento názor zpochybňuje. V zaslepeném lidském hodnocení překladu novinových článků z angličtiny do češtiny CUBBITT výrazně předčil lidský překlad od profesionální agentury v zachování významu textu (adequacy, přesnosti překladu). Zatímco lidský překlad je stále hodnocen jako plynulejší, ukázalo se, že CUBBITT je podstatně plynulejší než dosavadní překladače. Většina účastníků překladového Turingova testu navíc nedokázala rozlišit překlady CUBBITT od překladů lidských. Tato práce se blíží kvalitě lidského překladu a za určitých okolností jej dokonce v přiměřenosti překonává. To naznačuje, že hluboké učení může mít potenciál nahradit člověka v aplikacích, kde je hlavním cílem zachování významu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The quality of human translation was long thought to be unattainable for computer translation systems. In this study, we present a deep-learning system, CUBBITT, which challenges this view. In a context-aware blind evaluation by human judges, CUBBITT significantly outperformed professional-agency English-to-Czech news translation in preserving text meaning (translation adequacy). While human translation is still rated as more fluent, CUBBITT is shown to be substantially more fluent than previous state-of-the-art systems. Moreover, most participants of a Translation Turing test struggle to distinguish CUBBITT translations from human translations. This work approaches the quality of human translation and even surpasses it in adequacy in certain circumstances. This suggests that deep learning may have the potential to replace humans in applications where conservation of meaning is the primary aim.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento dokument představuje náš pokrok směrem k zavedení univerzální komunikační platformy v úkolu vysoce mnohojazyčného živého projevu
překlady pro konference a vzdálené schůzky živé titulkování. Platforma byla navržena se zaměřením na velmi nízkou latenci a
vysoká flexibilita při umožnění snadného propojení výzkumných prototypů nástrojů pro zpracování řeči a textu, bez ohledu na to, kde
fyzicky běhat. Nastíníme naše řešení architektury a také ho krátce porovnáme s platformou ELG. Technické podrobnosti jsou uvedeny
o nejdůležitějších součástech a shrnujeme události zkušebního nasazení, které jsme zatím provedli.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents our progress towards deploying a versatile communication platform in the task of highly multilingual live speech
translation for conferences and remote meetings live subtitling. The platform has been designed with a focus on very low latency and
high flexibility while allowing research prototypes of speech and text processing tools to be easily connected, regardless of where they
physically run. We outline our architecture solution and also briefly compare it with the ELG platform. Technical details are provided
on the most important components and we summarize the test deployment events we ran so far.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme naši práci zkoumající možnosti společného prostoru embedingů mezi textovou a vizuální modalitou.
Narozdíl od současného trendu zakotvení slov či vět v přidružených obrázcích, navrhujeme zakotvení visuálních reprezentací objektů v prostoru slovních embedingů systému generujícího popisků.
K tomu využíváme textové povahy popisků detekovaných objektů a předpokládanou expresivitu reprezentací těchto objektů.
Na základě předchozích poznatků aplikujeme dodatečné objektivní funkce k základní popiskovací objektivní funkci, jejichž cílem je tvorba heterogenních klastrů závislých na jejich třídě a napodobení sémantické struktury prostoru slovních embedingů.
Kromě toho také analyzujeme natrénované projekce prostoru vizualních objektů a jejich vliv na výkon popiskovacího systému.
I přes mírné zhoršení kvality generovaných popisků, modely se zakotvením konvergují výrazně rychleji během trénovaní vyžadujíce dvakrát až třikrát méně trénovacích iterací.
Zlepšení strukturální korelace mezi slovními embedingy a nejen původními objektovými reprezentacemi, ale i jejich projekcí naznačuje, že zakotvení je vzájemné.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our work in progress exploring the possibilities of a shared embedding space between textual and visual modality.
Leveraging the textual nature of object detection labels and the hypothetical expressiveness of extracted visual object representations, we propose an approach opposite to the current trend, grounding of the representations in the word embedding space of the captioning system instead of grounding words or sentences in their associated images.
Based on the previous work, we apply additional grounding losses to the image captioning training objective aiming to force visual object representations to create more heterogeneous clusters based on their class label and copy a semantic structure of the word embedding space.
In addition, we provide an analysis of the learned object vector space projection and its impact on the IC system performance.
With only slight change in performance, grounded models reach the stopping criterion during training faster than the unconstrained model, needing about two to three times less training updates.
Additionally, an improvement in structural correlation between the word embeddings and both original and projected object vectors suggests that the grounding is actually mutual.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje průběh jubilejního 45. ročníku Olympiády v českém jazyce. Představuje soutěž jako takovou, přibližuje konkrétní soutěžní úlohy včetně jejich řešení, komentuje řešení účastníků a přináší jména vítězů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article describes the course of the jubilee 45th year of the Olympiad in the Czech Language, presenting its general settings as well as some of the tasks, their solutions, the approaches of the participants and the names of the winners.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem příspěvku je představit česko-německou slovníkovou databázi obsahující frekventované jazykové výrazy podílející se na strukturaci textu – anaforické diskurzní konektory.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of the paper is to introduce a Czech-German dictionary database containing frequently used language expressions involved in the text structure – anaphoric discourse connectives.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vyhodnotili jsme strojový překlad na vztazích v rámci dokumentů. Ukázalo se, že systémy se na mezivětných vztazích příliš neliší.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>As the quality of machine translation rises and
neural machine translation (NMT) is moving
from sentence to document level translations,
it is becoming increasingly difficult to evaluate
the output of translation systems.
We provide a test suite for WMT19 aimed at
assessing discourse phenomena of MT systems participating in the News Translation
Task. We have manually checked the outputs
and identified types of translation errors that
are relevant to document-level translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme aplikace EVALD (Evaluator of Discourse) pro automatické vyhodnocování českých textů. Podrobně analyzujeme nově získaná jazyková data - texty psané cizími mluvčími dosahující první úrovně osvojování českého jazyka. Představujeme také nové pravopisné "featury" přidané do systému.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present EVALD applications (Evaluator of Discourse) for automated essay scoring. We analyze in detail newly acquired language data – texts written by non-native speakers reaching the threshold level of the Czech language acquisition. We also present new spelling features added in the system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>GeCzLex je databáze překladových ekvivalentů českých a německých textových konektorů. Je založena na značkovaných datech několika elekronických jazykových zdrojů: pro češtinu je jeho základem Pražský diskurzní korpus 2.0 a slovník českých konektorů CzeDLex 0.6, pro němčinu podobný slovník DiMLex, pro oba jazyky pak česko-německá část paralelního korpusu Intercorp 11. Současné, první vydání je pilotní verzí a zároveň výstupem výzkumného projektu o anaforičnosti českých a německých konektorů. Databáze tedy nyní obsahuje překladové ekvivalenty pro a) konektory utvořené zpravidla spojením předložky a anaforického prvku (jako např. "darum" v němčině a "proto" v češtině) a b) české konektory, u nichž byla demonstrována schopnost vázat se na nesousední, vzdálené textové segmenty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>GeCzLex - Lexicon of Czech and German Anaphoric Connectives  - is a translation equivalent database of Czech and German discourse connectives, based on the data of annotated corpora and lexicons: Prague Discourse Treebank 2.0 and CzeDLex 0.6 (for Czech), DiMLex 2.0 (for German) and Intercorp 11 (a large resource of parallel Czech - German texts). Its current, first release is a pilot version representing one of the outcomes of a research project on anaphoricity in Czech and German connectives. Thus, it contains translation equivalents for a) connectives originally formed from a preposition and an anaphoric element (e.g. "darum" in German, "proto" in Czech) and b) Czech connectives with the ability to relate "remotely" to non-adjacent text segments.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje podání Idiap pro WAT 2019 pro anglicko-hindský vícemodální překladatelský úkol. Použili jsme nejmodernější model Transformeru a jako dodatečný zdroj dat jsme použili anglicko-hindský paralelní korpus IITB. Z různých skladeb multimodálního úkolu jsme se zúčastnili skladby „Text-Only“ pro hodnocení a
testovací sady. Naše podání je mezi konkurenty špičkové jak z hlediska automatického, tak manuálního hodnocení. Na základě automatických skóre předčí naše pouze textové podání i systémy, které v úkolu „multimodální překlad“ zohledňují vizuální informace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the Idiap submission
to WAT 2019 for the English-Hindi MultiModal Translation Task. We have used
the state-of-the-art Transformer model and
utilized the IITB English-Hindi parallel corpus as an additional data source.
Among the different tracks of the multimodal task, we have participated in the
“Text-Only” track for the evaluation and
challenge test sets. Our submission tops in
its track among the competitors in terms
of both automatic and manual evaluation.
Based on automatic scores, our text-only
submission also outperforms systems that
consider visual information in the “multimodal translation” task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Visual Genome je dataset spojující strukturované obrazové informace s anglickým jazykem. Představujeme „Hindi Visual Genome“, multimodální datový soubor skládající se z textu a obrazů vhodný pro anglicko-hindský multimodální strojový překlad a multimodální výzkum.
Vybrali jsme krátké anglické segmenty (popisky) z Visual Genome spolu s přidruženými obrázky a automaticky je přeložili do hindštiny. Následovala pečlivá ruční kontrola, která vzala v úvahu související obrázky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Visual Genome is a dataset connecting
structured image information with English language. We present “Hindi Visual Genome”, a multi-modal dataset consisting of text and images suitable for English-Hindi multi-modal machine translation task and multi-modal research.
We have selected short English segments
(captions) from Visual Genome along with the associated images and automatically translated them to Hindi. A careful manual post-editing followed which took the associated images into account.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje sestavení dvou korpusů pro urijštinu (Oria): OdiEnCorp, paralelní korpus urijštiny a angličtiny, a OdiMonoCorp, jednojazyčný urijský korpus.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A multi-lingual country like India needs language corpora for
low resource languages not  only  to provide its citizens with technologies of natural language
processing (NLP) readily available in other countries, but also to support its people
in their education and cultural needs.

In this work, we focus on 
one of the low resource languages, Odia, and
build an Odia-English parallel (OdiEnCorp) and an Odia monolingual (OdiMonoCorp) corpus.
The parallel corpus is based on
Odia-English parallel texts extracted from online resources and formally
corrected by
volunteers. We also preprocess the parallel corpus for machine translation
research or training. The
monolingual corpus comes from a diverse set of 
online resources and we organize it into a collection of
segments and paragraphs, easy to handle by NLP tools.

OdiEnCorp parallel corpus contains 29346 sentence pairs and 756K English and 648K Odia
tokens. OdiMonoCorp contains 2.6 million tokens in  221K sentences in 71K
paragraphs.

Despite their small size, OdiEnCorp and OdiMonoCorp are still the largest Odia language resources, freely available for
non-commercial educational or research purposes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládáme testovací korpus audionahrávek a přepisů prezentací studentských firem spolu s jejich slidy a webovými stránkami. Korpus je určen k evaluaci automatického rozpoznávání řeči, primárně za podmínek, ve kterých je využitelná předchozí dostupnost terminologie a pojmenovaných entit z dané oblasti.
Korpus se zkládá z 39 prezentací v angličtině, každá trvá až 90 sekund.
Řečníci jsou studenti středních škol z evropských zemí. Angličtina je jejich druhý jazyk.
Na korpusu testujeme tři základní modely pro automatické rozpoznávání řeči a ukazujeme jejich nedostatky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a test corpus of audio recordings and transcriptions of presentations of students' enterprises together with their slides and web-pages. The corpus is intended for evaluation of automatic speech recognition (ASR) systems, especially in conditions where the prior availability of in-domain vocabulary and named entities is benefitable. 
The corpus consists of 39 presentations in English, each up to  90 seconds long. 
The speakers are high school students from European countries with English as their second language.
We benchmark three baseline ASR systems on the corpus and show their imperfection.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zkoumáme vícehlavou sebepozornost v enkodérech Transformer NMT pro tři zdrojové jazyky a hledáme vzory, které by mohly mít syntaktickou interpretaci. V mnoha hlavách pozornosti často nalézáme sekvence po sobě jdoucích stavů, které sledují stejnou pozici, které se podobají syntaktickým frázím. Navrhujeme transparentní deterministickou metodu kvantifikace množství syntaktické informace přítomné v sebepozornosti, založenou na automatickém vytváření a vyhodnocování frázových stromů z frázovitých sekvencí. Výsledné stromy porovnáváme se stávajícími syntaktickými korpusy, a to jak ručně, tak pomocí výpočtu přesnosti a úplnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We inspect the multi-head self-attention in Transformer NMT encoders for three source languages, looking for patterns that could have a syntactic interpretation. In many of the attention heads, we frequently find sequences of consecutive states attending to the same position, which resemble syntactic phrases. We propose a transparent deterministic method of quantifying the amount of syntactic information present in the self-attentions, based on automatically building and evaluating phrase-structure trees from the phrase-like sequences. We compare the resulting trees to existing constituency treebanks, both manually and by computing precision and recall.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slovní embedinky a hluboké neuronové sítě fungují skvěle. Nemají žádné explicitní znalosti jazykových abstrakcí. Jak fungují? Jaké emergentní abstrakce v nich můžeme pozorovat? Jak je můžeme interpretovat? Jsou emergentní struktury a abstrakce podobné klasickým lingvistickým strukturám a abstrakcím?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Word embeddings and Deep neural networks perform great. They do not have any explicit knowledge of linguistic abstractions. How do they work? What emergent abstractions can we observe in them? How can we interpret them? Are the emergent structures and abstractions similar to classical linguistic structures and abstractions?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci provádíme studii soustřeďující se na neuronový překlad (NMT) pro angličtinu-indonéštinu (EN-ID) a indonéštinu-angličtinu (ID-EN). Zaměřujeme se na doménu mluveného jazyka, jmenovitě na hovorový jazyk. Budujeme systémy NMT pomocí modelu Transformer pro oba směry překladu a implementujeme adaptaci domény, kde předtrénované systémy NMT trénujeme na datech mluveného jazyka (v doméně). Dále provádíme hodnocení toho, jak může metoda doménové adaptace v našem systému EN-ID vést k formálnějším výsledkům překladů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this  work, we conduct a study on Neural Machine Translation (NMT) for English-Indonesian (EN-ID) and Indonesian-English (ID-EN). We focus on spoken language domains, namely colloquial and speech languages. We build NMT systems using the Transformer model for both translation directions and implement domain adaptation, in which we train our pre-trained NMT systems on speech language (in-domain) data.  Moreover, we conduct an evaluation on how the domain-adaptation method in our EN-ID system can result in more formal translation out-puts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Neuronové systémy generování přirozeného jazyka jsou známy svými patologickými výstupy, tj. generováním textu, který nesouvisí se specifikovaným vstupem. V tomto článku ukazujeme vliv sémantického šumu na současné nejlepší neuronové generátory, které implementují různé mechanismy sémantické kontroly. Zjistili jsme, že vyčištění trénovacích dat může zlepšit sémantickou přesnost až o 97% při zachování plynnosti výstupů. Dále jsme zjistili, že nejčastějším typem chyby je vynechání informace, ne přidaná halucinovaná informace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Neural natural language generation (NNLG) systems are known for their pathological outputs, i.e. generating text which is unrelated to the input specification. In this paper, we show the impact of semantic noise on state-of-the-art NNLG models which implement different semantic control mechanisms. We find that cleaned data can improve semantic correctness by up to 97%, while maintaining fluency. We also find that the most common error is omitting information, rather than hallucination.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme systém pro automatický odhad kvality výstupů generování přirozeného jazyka založený na rekurentních neuronových sítích, který se učí zároveň přiřazovat numerická absolutní hodnocení jednotlivým výstupům a dodávat relativní hodnocení pro páry různých výstupů. Druhá úloha se trénuje pomocí párové hinge chyby nad skóre ze dvou kopií sítě pro absolutní hodnocení.
Pro zlepšení kvality absolutního hodnocení používáme i učení relativního hodnocení a syntetická trénovací data: syntetizujeme trénovací páry zašuměných výstupů generátorů a učíme systém preferovat ten méně zašuměný. Toto vedlo ke 12% zvýšení korelace s lidským hodnocením proti předchozí nejlepší dosažené hodnotě. Navíc ukazujeme první výsledky na datové sadě relativních hodnocení z E2E NLG Challenge (Dušek et al., 2019), kde syntetická data přinesla 4% zlepšení přesnosti oproti základnímu modelu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a recurrent neural network based system for automatic quality estimation of natural language generation (NLG) outputs, which jointly learns to assign numerical ratings to individual outputs and to provide pairwise rankings of two different outputs. The latter is trained using pairwise hinge loss over scores from two copies of the rating network.
We use learning to rank and synthetic data to improve the quality of ratings assigned by our system: we synthesise training pairs of distorted system outputs and train the system to rank the less distorted one higher. This leads to a 12% increase in correlation with human ratings over the previous benchmark. We also establish the state of the art on the dataset of relative rankings from the E2E NLG Challenge (Dušek et al., 2019), where synthetic data lead to a 4% accuracy increase over the base model.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme první datovou sadu zaměřenou na end-to-end generování jazyka v češtině v doméně restaurací, společně s několika silnými základními modely postavenými na architektuře sequence-to-sequence. Neanglické generování jazyka je obecně málo probádaný problém a čeština jakožto morfologicky bohatý jazyk představuje ještě těžší úkol: protože v češtině je třeba skloňovat jmenné entity, delexikalizace nebo jednoduché kopírovací mechanismy nefungují samy o sobě a lexikalizace výstupů generátoru je netriviální.

V našich experimentech představujeme dva různé přístupy k tomuto problému: (1) použití jazykového modelu pro výběr správné vyskloňované formy během lexikalizace, (2) dvoufázové generování: náš model sequence-to-sequence vygeneruje prokládanou sekvenci lemmat a morfologických značek, která je posléze zpracována morfologickým generátorem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the first dataset targeted at end-to-end NLG in Czech in the restaurant domain, along with several strong baseline models using the sequence-to-sequence approach. While non-English NLG is under-explored in general, Czech, as a morphologically rich language, makes the task even harder: Since Czech requires inflecting named entities, delexicalization or copy mechanisms do not work out-of-the-box and lexicalizing the generated outputs is non-trivial.

In our experiments, we present two different approaches to this problem: (1) using a neural language model to select the correct inflected form while lexicalizing, (2) a two-step generation setup: our sequence-to-sequence model generates an interleaved sequence of lemmas and morphological tags, which are then inflected by a morphological generator.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento dokument představuje výsledky premiérově sdíleného úkolu organizovaného společně s Konferencí o strojovém překladu (WMT) 2019.
Účastníci byli požádáni, aby sestavili systémy strojového překladu pro kterýkoli z 18 jazykových párů, které budou vyhodnoceny na základě testovací sady novinek. Hlavním metrem pro tento úkol je lidský úsudek o kvalitě překladu. Úkol byl také otevřen pro další testovací sady, které zkoumají specifické aspekty překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the premier
shared task organized alongside the Conference on Machine Translation (WMT) 2019.
Participants were asked to build machine
translation systems for any of 18 language
pairs, to be evaluated on a test set of news
stories. The main metric for this task is human judgment of translation quality. The task
was also opened up to additional test suites to
probe specific aspects of translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této kapitole chápeme vztah mezi budovaným prostředím a jeho obyvateli jako zpětnou vazbu a naším cílem je zachytit temporalitu této vazby v různých formách adaptace. Zaměřujeme se zejména na nově vznikající formy přizpůsobení, které jsou založeny na digitálně pořízených osobních datech, což vede k automatizaci či k různým opatřením ze strany stavebních subjektů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Recognizing the relation between inhabitants and their built environments as a feedback loop, our aim is to capture the temporality of this loop in various scenarios of adaptation. We specifically focus on the emerging types of adaptation that are motivated by digitally acquired personal data, leading to either automation or action taken by the building stakeholders. Between the microscopic daily mutations (e.g. automated adaptation to occupants’ presence or activity) and the macroscopic evolution of built environments, we identify a “mesoscopic” scale and argue for broadening its consideration in the research domain of adaptive built environments. In mesoscopic adaptations, inhabitants’ data undergo a process of thorough analysis and scrutiny, the results of which inform the re-envisioning of building design for its next cycles over the course of months-years. This contribution distinguishes and elaborates on four temporal scales of adaptation (minutes-hours, days-weeks, months-years, decades-centuries) and then exemplifies the meso-scale with a study conducted over three years within a living lab context. Through this example, we also aim to demonstrate the opportunity for living lab methodologies to contribute to the research on adaptive built environments at the mesoscopic scale.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento dokument představuje výsledky sdíleného úkolu WMT19 Metrics. Účastníci byli požádáni, aby pomocí automatických metrik ohodnotili výstupy překladatelských systémů soutěžících v WMT19 News Translation Task. 13 výzkumných skupin předložilo 24 metrik, z nichž 10 jsou „metriky“ bez odkazů a představují podání ke společnému úkolu s WMT19 Quality Estimation Task, „QE as a Metric“. Navíc jsme vypočítali 11 základních
metriky, s 8 běžně používanými výchozími hodnotami (BLEU, SentBLEU, NIST, WER, PER, TER, CDER a chrF) a 3 reimplementy (chrF+, sacreBLEU-BLEU a sacreBLEU-chrF). Metriky byly hodnoceny na systémové úrovni, jak dobře daná metrika koreluje s oficiálním manuálním řazením WMT19 a na úrovni segmentu, jak dobře metrika koreluje s lidskými úsudky o kvalitě segmentu. Letos používáme přímé hodnocení (DA) jako jedinou formu manuálního hodnocení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the
WMT19 Metrics Shared Task. Participants were asked to score the outputs of the translations systems competing in the WMT19 News Translation
Task with automatic metrics. 13 research
groups submitted 24 metrics, 10 of which
are reference-less “metrics” and constitute
submissions to the joint task with WMT19
Quality Estimation Task, “QE as a Metric”. In addition, we computed 11 baseline
metrics, with 8 commonly applied baselines (BLEU, SentBLEU, NIST, WER,
PER, TER, CDER, and chrF) and 3 reimplementations (chrF+, sacreBLEU-BLEU,
and sacreBLEU-chrF). Metrics were evaluated on the system level, how well a given
metric correlates with the WMT19 official manual ranking, and segment level,
how well the metric correlates with human
judgements of segment quality. This year,
we use direct assessment (DA) as our only
form of manual evaluation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku byla diskutována role reflexiv v popisu češtiny a představena zejména ta reflexiva, která jsou součástí lemmatu slovesa.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this contribution, a comprehensive theoretical account of the reflexives in Czech elaborated within the Functional Generative Description, with the main emphasis put on the role of the reflexives in valency behavior of Czech verbs, has been provided.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku se diskutují reflexiva v češtině, která tvoří konstrukce obdobné jako osobní zájmena, a jejich popis v rámci Funkčního generativního popisu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the paper, Czech reflexives and their description in the dependency-oriented theory,
Functional Generative Description, are addressed. The primary focus lies in the reflexives
that form analogous syntactic structures as personal pronouns.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Reflexiva v češtině plní nerůznější funkce. V tomto příspěvku jsme se zaměřily na změny ve valenci derivovaných reflexivních sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Reflexives in Czech are highly ambiguous. In this contribution, changes in valency of derived reflexive verbs have been discussed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zatímco reciproční slovesa jsou v současné jazykovědě často diskutována, popis reciprocity dalších slovních druhů je teprve na počátku. V tomto příspěvku využíváme poznatky o reciprocitě sloves k popisu jejich deverbálních protějšků. Ukazujeme, že mnoho rysů recipročních konstrukcí sloves reciproční struktury deverbálních jmen sdílejí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Reciprocal verbs are widely debated in the current linguistics. However, other parts of speech can be characterized by reciprocity as well – in
contrast to verbs, their analysis is underdeveloped so far. In this paper, we make
an attempt to fill this gap, applying results of the description of Czech
reciprocal verbs to nouns derived from these verbs. We show that many aspects
characteristic of reciprocal verbs hold for reciprocal nouns as well.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Soutěžní úkol pro rok 2019 na konferenci Computational Language Learning (CoNLL) byl věnován sémantickému parsingu (Meaning Representation Parsing, MRP) napříč různými přístupy. Soutěž zahrnuje pět formálně a lingvisticky rozdílných přístupů k reprezentaci významu (DM, PSD, EDS, UCCA a AMR). Do soutěže se přihlásilo osmnáct týmů, z nichž pět se neúčastnilo oficiálního hodnocení, protože jejich výsledky dorazily až po uzávěrce, nebo tým využil dodatečných trénovacích dat, popřípadě byl jeden ze spoluorganizátorů soutěže mezi zástupci týmu. Veškeré technické informace týkající se soutěže jsou k dispozici na webových stránkách úkolu: http://mrp.nlpl.eu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The 2019 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks. Five distinct approaches to the representation of sentence meaning in the form of directed graph were represented in the training and evaluation data for the task, packaged in a uniform abstract graph representation and serialization. The task received submissions from eighteen teams, of which five do not participate in the official ranking because they arrived after the closing deadline, made use of additional training data, or involved one of the task co-organizers. All technical information regarding the task, including system submissions, official results, and links to supporting resources and software are available from the task web site at: http://mrp.nlpl.eu.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku navrhujeme dvě neuronové architektury pro rozpoznávání vnořených pojmenovaných entit, což je úloha, ve které se pojmenované entity mohou překrývat a také být označeny více než jednou značkou. Vnořené značky zakódováváme pomocí linearizovaného schématu. V prvním navrženém přístupu jsou vnořené značky modelovány jako multiznačky náležející kartézkému součinu vnořených značek ve standardní LSTM-CRF architektuře. V druhém navrženém přístupu přistupujeme k úloze rozpoznávání vnořených pojmenovaných entit jako k sequence-to-sequence problému, ve kterém vstupní sekvence sestává z tokenů a výstupní sekvence ze značek, přičemž používáme vynucený mechanismus attention na slově, které právě značkujeme. Navržené metody překonávají současný stav poznání v úloze rozpoznávání vnořených pojmenovaných entit na čtyřech korpusech: ACE-2004, ACE-2005, GENIA a českém CNEC. Naše architektury jsme dále obohatili nedávno publikovanými kontextovými embeddingy: ELMo, BERT a Flair, čímž jsme dosáhli dalšího zlepšení na všech čtyřech korpusech. Navíc publikujeme nejlepší známé výsledky v ropoznávání pojmenovaných entit na korpusech CoNLL-2002 v nizozemštině a španělštině a korpusu CoNLL-2003 v angličtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We propose two neural network architectures for nested named entity recognition (NER), a setting in which named entities may overlap and also be labeled with more than one label. We encode the nested labels using a linearized scheme. In our first proposed approach, the nested labels are modeled as multilabels corresponding to the Cartesian product of the nested labels in a standard LSTM-CRF architecture. In the second one, the nested NER is viewed as a sequence-to-sequence problem, in which the input sequence consists of the tokens and output sequence of the labels, using hard attention on the word whose label is being predicted. The proposed methods outperform
the nested NER state of the art on four corpora: ACE-2004, ACE-2005, GENIA and Czech CNEC. We also enrich our architectures with the recently published contextual embeddings: ELMo, BERT and Flair, reaching further improvements for the four nested entity corpora. In addition, we report flat NER state-of-the-art results for CoNLL-2002 Dutch and Spanish and for CoNLL-2003 English.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Metody hlubokého učení (deep learning) umělých neuronových sítí zaznamenaly v posledních letech výrazný nástup a jejich pomocí se podařilo překonat a posunout úspěšnost automaticky řešených úloh v mnoha oborech. Výjimkou není ani oblast počítačové (či komputační) lingvistiky (Computational Linguistics) a její aplikační odnož, zpracování přirozeného jazyka (Natural Language Processing) s klasickými úlohami, jako jsou morfologické značkování (POS tagging), závislostní analýza (dependency parsing), rozpoznávání pojmenovaných entit (named entity recognition) a strojový překlad (machine translation). Tento příspěvek přináší přehled nedávných pokroků v uvedených úlohách se vztahem k českému jazyku a představuje zcela nové výsledky v oblastech morfologického značkování a rozpoznávání pojmenovaných entit v češtině, spolu s detailní chybovou analýzou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The deep learning methods of artificial neural networks have seen a significant uptake in recent years, and have succeeded in overcoming and advancing the success of auto-solving tasks in many fields. The field of computational linguistics and its application offshoot, natural language processing with classic tasks such as morphological tagging, dependency analysis, named entity recognition and machine translation are not exceptions. This post provides an overview of recent advances in these tasks related to the Czech language and presents completely new results in the areas of morphological marking and recognition of named entities in Czech, along with detailed error analysis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Je známo, že neuronový strojový překlad vyžaduje velké množství paralelních
trénovacích vět, které obecně brání tomu, aby vynikal na párech jazyků s nedostatečným množstvím zdrojů. Tato práe se zabývá využitím translingválního
učení na neuronových sítích jako způsobu řešení problému nedostatku zdrojů.
Navrhujeme několik přístupů k transferu znalostí za účelem opětovného využití
modelu předtrénovaného na jiné jazykové dvojici s velkým množstvím zdrojů.
Zvláštní pozornost věnujeme jednoduchosti technik. Studujeme dva scénáře:
a) když používáme předtrénovaný model bez jakýchkoli předchozích úprav
jeho trénovacího procesu a b) když můžeme předem připravit prvostupňový
model pro transfer znalostí pro potřeby dítěte. Pro první scénář představujeme
metodu opětovného využití modelu předtrénovaného jinými výzkumníky. V
druhém případě předkládáme metodu, která dosáhne ještě většího zlepšení.
Kromě navrhovaných technik se zaměřujeme na hloubkovou analýzu technik
transferu znalostí a snažíme se vnést trochu světla do pochopení transferového
učení. Ukazujeme, jak naše techniky řeší specifické problémy jazyků s málo
daty a že jsou vhodné i pro jazykové páry s velkým množstvím dat. Potenciální nevýhody a chování hodnotíme studiem transferového učení v různých
situacích, například pod uměle poškozeným trénovacím korpusem, nebo se
zafixovanými částmi modelu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Neural machine translation is known to require large numbers of parallel training sentences, which generally prevent it from excelling on low-resource language pairs. This thesis explores the use of cross-lingual transfer learning on
neural networks as a way of solving the problem with the lack of resources. We
propose several transfer learning approaches to reuse a model pretrained on a
high-resource language pair. We pay particular attention to the simplicity of
the techniques. We study two scenarios: (a) when we reuse the high-resource
model without any prior modifications to its training process and (b) when we
can prepare the first-stage high-resource model for transfer learning in advance.
For the former scenario, we present a proof-of-concept method by reusing a
model trained by other researchers. In the latter scenario, we present a method
which reaches even larger improvements in translation performance. Apart
from proposed techniques, we focus on an in-depth analysis of transfer learning
techniques and try to shed some light on transfer learning improvements. We
show how our techniques address specific problems of low-resource languages
and are suitable even in high-resource transfer learning. We evaluate the potential drawbacks and behavior by studying transfer learning in various situations,
for example, under artificially damaged training corpora, or with fixed various
model parts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto tutoriálu se naučíte používat tensor2tensor a jak aplikovat přenos znalostí na nízko-zdrojové jazykové páry. Tutorial je vhodný i pro účastníky, kteří nemají zkušenosti s trénováním neuronových MT modelů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This tutorial will show you how to use the Tensor2Tensor and how to apply Transfer Learning to low-resource languages. It should be easy to follow for everyone, even people that never trained Machine Translation models.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje systém CUNI do News WMT 2019 pro jazyky s nedostatečnými zdroji: Gujarati-Angličtina  a Kazakština-Angličtina. Zúčastnili jsme se na obou jazykových párech v obou směrech překladu. Náš systém kombinuje přenos znalostí z jiného dvojice jazyků s vysokým množstvím paralelních dat, po kterém následuje trénování na zpětně přeložených jednojazyčných dat. Díky simultánnímu tréninku v obou směrech můžeme iterovat proces zpětného překladu. Používáme Transformer model v constrained podmínkách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the CUNI submission
to the WMT 2019 News Translation Shared
Task for the low-resource languages: GujaratiEnglish and Kazakh-English. We participated
in both language pairs in both translation directions. Our system combines transfer learning from a different high-resource language
pair followed by training on backtranslated
monolingual data. Thanks to the simultaneous training in both directions, we can iterate
the backtranslation process. We are using the
Transformer model in a constrained submission.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku poskytujeme přehled zkušeností z první ruky a výhodných bodů pro osvědčené postupy z projektů v sedmi evropských zemích věnovaných výzkumu korpusu studentů (LCR) a vytváření korpusů studentů jazyků. Korpusy a nástroje zapojené do LCR jsou stále důležitější, stejně jako pečlivá příprava a snadné vyhledávání a opětovné použití korpusů a nástrojů. Nedostatek společně dohodnutých řešení pro mnoho aspektů LCR, interoperabilita mezi korpusy studentů a výměna dat z různých korpusových projektů studentů však zůstává výzvou. Ukážeme, jak mohou být koncepty jako metadata, anonymizace, taxonomie chyb a jazykové anotace, jakož i nástroje, řetězce nástrojů a datové formáty individuálně náročné a jak lze výzvy řešit.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this article we provide an overview of first-hand experiences and vantage points for best practices from projects in seven European countries dedicated to learner corpus research (LCR) and the creation of language learner corpora. The corpora and tools involved in LCR are becoming more and more important, as are careful preparation and easy retrieval and reusability of corpora and tools. But the lack of commonly agreed solutions for many aspects of LCR, interoperability between learner corpora and the exchange of data from different learner corpus projects remains a challenge. We show how concepts like metadata, anonymization, error taxonomies and linguistic annotations as well as tools, toolchains and data formats can be individually challenging and how the challenges can be solved.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento dokument poskytuje komplexní přehled datového souboru mezery pro ruštinu, který se skládá z 7,5k vět s mezerou (stejně jako 15k relevantních negativních vět) a obsahuje údaje z různých žánrů: zprávy, beletrie, sociální média a technické texty. Dataset byl připraven pro automatický sdílený úkol pro řešení rozdílů ruských dat (AGRR-2019) - soutěž zaměřená na stimulaci vývoje nástrojů a metod NLP pro zpracování elipsy. V tomto článku věnujeme zvláštní pozornost metodám rozlišování mezer, které byly zavedeny v rámci sdíleného úkolu, a také alternativní testovací sadě, která ukazuje, že náš korpus je různorodá a reprezentativní podmnožina mezery ruského jazyka dostatečná pro efektivní využití technik strojového učení. .</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper provides a comprehensive overview of the gapping dataset for Russian that consists of 7.5k sentences with gapping (as well as 15k relevant negative sentences) and comprises data from various genres: news, fiction, social media and technical texts. The dataset was prepared for the Automatic Gapping Resolution Shared Task for Russian (AGRR-2019) - a competition aimed at stimulating the development of NLP tools and methods for processing of ellipsis. In this paper, we pay special attention to the gapping resolution methods that were introduced within the shared task as well as an alternative test set that illustrates that our corpus is a diverse and representative subset of Russian language gapping sufficient for effective utilization of machine learning techniques.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje účastnický systém „ÚFAL–Oslo“ v soutěži Cross-Framework Meaning Representation Parsing (MRP, Oepen et al. 2019). Systém je postaven na několika existujících parserech třetích stran. V rámci oficiálních výsledků soutěže se umístil na 12. místě z celkem 14 účastníků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the “ÚFAL–Oslo” system submission to the shared task on Cross-Framework Meaning Representation Parsing (MRP, Oepen et al. 2019). The submission is based on several third-party parsers. Within the official shared task results, the submission ranked 12th out of 14 participating systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Mnohé lingvistické teorie a anotační schémata obsahují hloubkově-syntaktickou a/nebo sémantickou vrstvu. Ačkoli řada z nich byla aplikována na více než jeden jazyk, žádná se nepřibližuje množství jazyků, které jsou pokryty univerzálními závislostmi (Universal Dependencies, UD). V tomto článku představujeme prototyp hloubkových univerzálních závislostí (Deep Universal Dependencies): dvourychlostního konceptu, ve kterém určitá minimální hloubková anotace je automaticky odvozena z povrchových stromů UD, zatímco bohatší anotaci je možné přidat ručně u korpusů, kde jsou k dispozici potřebné zdroje. Data Deep UD zpřístupňujeme v repozitáři Lindat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Many linguistic theories and annotation frameworks contain a deep-syntactic and/or semantic layer. While many of these frameworks have been applied to more than one language, none of them is anywhere near the number of languages that are covered in Universal Dependencies (UD). In this paper, we present a prototype of Deep Universal Dependencies, a two-speed concept where minimal deep annotation can be derived automatically from surface UD trees, while richer annotation can be added for datasets where appropriate resources are available. We release the Deep UD data in Lindat.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme UDify, vícejazyčný víceúlohový model schopný přesně předpovědět univerzální slovní druhy, morfologické rysy, lemmata a závislostní stromy současně pro všech 124 treebanků Universal Dependencies napříč 75 jazyky. Využitím vícejazyčného modelu BERT předcvičeného na 104 jazycích jsme zjistili, že jeho dotrénování na všech zřetězených treebancích spolu s jednoduchými softmax klasifikátory pro každý úkol UD ústí v nejlepší známé výsledky pro UPOS, UFeats, lemmatizaci, UAS a LAS metriky, aniž by vyžadovalo jakékoli rekurentní nebo jazykově specifické komponenty. Hodnocení UDify ukazuje, že vícejazyčné učení nejvíce prospívá jazykům s málo daty. Vícejazykové trénovaní poskytuje kvalitní předpovědi i pro jazyky, které nebyly zastoupeny v trénovacích datech, naznačují, že i pro ně posky vícejazyčné školen. Zdrojový kód UDify je dostupný na https://github.com/hyperparticle/udify.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present UDify, a multilingual multi-task model capable of accurately predicting universal part-of-speech, morphological features, lemmas, and dependency trees simultaneously for all 124 Universal Dependencies treebanks across 75 languages. By leveraging a multilingual BERT self-attention model pretrained on 104 languages, we found that fine-tuning it on all datasets concatenated together with simple softmax classifiers for each UD task can result in state-of-the-art UPOS, UFeats, Lemmas, UAS, and LAS scores, without requiring any recurrent or language-specific components. We evaluate UDify for multilingual learning, showing that low-resource languages benefit the most from cross-linguistic annotations. We also evaluate for zero-shot learning, with results suggesting that multilingual training provides strong UD predictions even for languages that neither UDify nor BERT have ever been trained on. Code for UDify is available at https://github.com/hyperparticle/udify.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nedávný vývoj v oblasti strojového překladu experimentuje s myšlenkou, že model může zlepšit kvalitu překladu provedením více úloh, např. překládáním ze zdroje na cíl a také označováním každého zdrojového slova syntaktickými informacemi. Intuice je taková, že síť by zobecňovala znalosti na více úloh a zlepšila tak výkon překladu, zejména v podmínkách nízkých zdrojů. Vymysleli jsme experiment, který tuto intuici zpochybňuje. Podobné experimenty provádíme jak v multidekodérech, tak v prokládacích sestavách, které označují každé cílové slovo buď syntaktickou značkou, nebo úplně náhodnou značkou. Překvapivě ukazujeme, že model si vede skoro stejně dobře na nekorespondovaných náhodných značkách jako na skutečných syntaktických značkách. Naznačujeme některá možná vysvětlení tohoto chování. Hlavním poselstvím našeho článku je, že experimentální výsledky s hlubokými neuronovými sítěmi by měly být vždy doplněny triviálními výchozími hodnotami dokumentujícími, že pozorovaný přírůstek je
ne kvůli některým nesouvisejícím vlastnostem systému nebo tréninkovým efektům. Skutečná důvěra v to, odkud zisky pocházejí, bude pravděpodobně i nadále problematická.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Recent developments in machine translation experiment with the idea that a model can
improve the translation quality by performing multiple tasks, e.g., translating from source to
target and also labeling each source word with syntactic information. The intuition is that the
network would generalize knowledge over the multiple tasks, improving the translation performance, especially in low resource conditions. We devised an experiment that casts doubt on
this intuition. We perform similar experiments in both multi-decoder and interleaving setups
that label each target word either with a syntactic tag or a completely random tag. Surprisingly, we show that the model performs nearly as well on uncorrelated random tags as on true
syntactic tags. We hint some possible explanations of this behavior.
The main message from our article is that experimental results with deep neural networks
should always be complemented with trivial baselines to document that the observed gain is
not due to some unrelated properties of the system or training effects. True confidence in where
the gains come from will probably remain problematic anyway.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto textu chceme nastínit shody a rozdíly dvou tagsetů užívaných k automatické morfologické analýze češtiny. Ukážeme, nakolik původně nezáměrná a časem udržovaná dvojkolenost tzv. pražského a tzv. brněnského systému může býti v dohledné době překonána v rámci projektu NovaMorf.  Budeme se zabývat vztahy mezi značkováním morfologických kategorií a hodnot v návrhu NovaMorf v porovnání s oběma staršími systémy. Při posuzování brněnského systému vycházíme z článku Czech Morphological Tagset Revisited. (Jakubíček, Kovář, Šmerk, 2011). Poznatky o pražském systému zakládáme na popisu pražského pozičního tagsetu (viz http://ufal.mff.cuni.cz/pdt/Morphology_and_Tagging/Doc/hmptagqr.html) a na monografii Jana Hajiče (Hajič, 1994, 2004). Naším cílem bude ukázat, kterak zkušenosti s užíváním obou systémů vyústily ve snahu inspirovat se pozitivy a vyhnout se neúspěšným řešením na obou stranách (srv. Osolsobě et al., 2017). Ke vzájemné konverzi značek dosavadního pražského systému na brněnský viz Pořízka, Schäfer, 2009.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this text, we want to outline the coincidences and differences of the two tagsets used for automatic morphological analysis of Czech. We will show how much the originally unintentional and time-sustained double-knee of the so-called Prague and so-called Brno systems can be overcome in the foreseeable future under the NovaMorf project. We will look at the relationships between the branding of morphological categories and values in the NovaMorf proposal compared to the two older systems. We base our assessment of the Brno system on an article by the Czech Morphological Tagset Revisited. (Jakubíček, Kovář, Šmerk, 2011). We base our knowledge of the Prague system on a description of Prague's position tagset (see http://ufal.mff.cuni.cz/pdt/Morphology_and_Tagging/Doc/hmptagqr.html) and on a monograph by Jan Hajic (Hajic, 1994, 2004). Our goal will be to show how the experience of using both systems has resulted in an effort to inspire positives and avoid failed solutions on both sides (Srv. Osolsoba et al., 2017). For a mutual conversion of the marks of the Prague system to the Brno system, see Pořízka, Schäfer, 2009.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá praktikou pozorovanou v souboru videozáznamů z prostředí školní třídy. Studenti pracovali s on-line materiály ve skupinách po 2-3 osobách s jedním počítačem a vyplňovali papírový list s otázkami. Účastníci často ukazovali na obrazovku počítače, aby zdůraznili aspekty zobrazených textů, videoklipů, obrázků atd. V některých okamžicích ukazovalo na zobrazené objekty současně více osob. V jednom z takových případů se jedna účastsnice (P1) jemně dotkla natažené ruky své kolegyně (P2). Dotek nevyvolal žádnou pozorovatelnou reakci a po 2,5 sekundách se opakoval. Teprve po tomto druhém dotyku P2 ukončí ukazování a stáhne ruku z obrazovky.
Tato posloupnost interakcí je analyzována jako jediný případ, který zdůrazňuje etnometodologický význam „fenoménu opakování“. V žité realitě natočené situace pouze díky prvnímu dotyku (T1) získává druhý dotyk (T2) svůj význam jako „opakovaná akce“. Dvojitý dotyk je činností, jež je sama tvořena dvěma „samostatnými“ dotyky (T1 + T2) a není jen jedním úkonem, který se jen dvakrát opakuje.
Zdá se, že některé praktiky je třeba znásobit, aby se z nich staly smysluplně vytvořené aktivity (např. klepání na dveře kanceláře nebo klepání na něčí rameno), odlišené od pouhých nehod (např. jediný dotek něčího ramene nebo jediný náraz do dveří).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I will discuss a practice observed in a corpus of video-recordings of a classroom setting. Students worked with on-line materials in groups of 2-3 persons with one computer, filling a paper sheet with questions. Participants often pointed to the computer screen in order to topicalize relevant aspects of displayed texts, video clips, images etc. At times, more persons were concurrently pointing to the displayed objects. In one of such instances, one participant (P1) gently touched the pointing hand of her colleague (P2) – see Figure above. Inciting no observable reaction, after 2.5 seconds, the touch occurred again. Only after this second touch, P2 terminates the pointing and withdraws her hand from the screen.
This interactional sequence is analysed as a single case that highlights the ethnomethodological relevance of “the phenomenon of repetition”. In lived reality of the videotaped situation, it is only because of the first touch (T1), that the second touch (T2) gains its significance as a “repeated action”. In contrast to repeated utterances, which are usually accountable in CA as repair or registering, the double touch is a course of action that is itself constituted by the two “separate” touches (T1 + T2) rather than being a single action that is just repeated twice.
It seems that some acts must be multiplied in order to become meaningfully produced actions (e.g., knocking on the office door, or tapping someone’s shoulder), distinguished from mere accidents (e.g., a single touch of someone’s shoulder, or a single bump into the door). The topic of repeated “hand touching hand” also illustrates pertinent methodological issues in EM/CA research, such as the transcription of touch. Considering that T1 constitutes the phenomenal field of T2, I argue for the importance of profoundly ethnomethodological approach to studies of human interaction.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška poskytne teoretické základy pro navazující workshop zaměřený na jeden z největších digitálních archivů ústní historie: Archiv vizuální historie USC Shoah Foundation. Tato digitální databáze se skládá z více než 54 500 audiovizuálních záznamů orálněhistorických rozhovorů. Během workshopu, který kombinuje teorii a praxi, se zaměříme na sekundární analýzu archivních rozhovorů, diskusi o jejich úloze v současné společnosti a také na otázky interpretace při sekundárním využití archivované orální historie. V úvodní přednášce pojednám orální historii obecněji jako společenský fenomén a archivované rozhovory jako sociální sémiotické objekty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The lecture Sociological Aspects of Oral History in the Digital Age is going to provide theoretical foundations for the follow-up workshop focused on one of the largest digital archives of oral history: the USC Shoah Foundation’s Visual History Archive. This digital database consists of more than 54,500 audio-visual recordings of oral history interviews. The majority of the interviews were collected between 1994 and 2000, mainly in the USA and Europe. Most of the interviews (nearly 50%) are in English, but more than 35 other languages are also represented in the Visual History Archive. Over 150 interviews from the Archive are related to Luxembourg. During the workshop, combining theory and praxis, we will focus on secondary analysis of archival interviews, discussion of their role in contemporary society, as well as interpretive issues in secondary use of archived oral history. In the introductory lecture, we will consider oral history more generally as a social phenomenon, and archived oral history interviews as social semiotic objects.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V současné společnosti se otázky sociální spravedlnosti často týkají dokumentárních videoklipů a jejich interpretace. Pochopení způsobů práce s videem a jeho okolí v každodenním prostředí by mohlo přinést nové pohledy na tradiční témata. Tato práce vychází z etnometodologické studie videonahrávek kolektivní práce studentů s on-line materiály sestavenými z textů, obrázků a fragmentů orálněhistorických rozhovorů. Studenti pracovali s jedním počítačovým zařízením a jedním papírovým listem v malých skupinách po dvou nebo třech. Časová organizace práce v učebně s digitálními ústními dějinami je v tomto uspořádání rozdělena do tří fází: 1) příprava na sledování videoklipu, což vyžaduje vytvoření optimálního uspořádání hmotných artefaktů a tělesné orientace účastníků; 2) sledování videoklipu, během něhož je upřednostňováno nepřerušované sledování od začátku do konce, jakož i omezení hovoru účastníků na "průběžné komentáře"; 3) reflexe videoklipu, během níž se projevuje orientace na formulaci požadované odpovědi a její společné zapsání do papírového listu, včetně případného druhého sledování videoklipu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In contemporary society, social justice issues are often related to documentary video clips and their interpretation. Understanding the ways of working with and around video in everyday settings could provide novel perspectives on traditional themes. This paper is based on an ethnomethodological study of videotaped episodes of students’ collaborative work with on-line material constructed from texts, images and video clip fragments of oral history interviews. Students worked with a single computer device and one paper sheet in small groups of two or three. The temporal organization of classroom work with digital oral history in this classroom setting is divided to three phases: (1) preparation for watching the video clip, which requires the establishment of an optimal arrangement of material artifacts and participants’ bodily orientations; (2) watching the video clip, which points to a preference for uninterrupted watching from start to end, as well as limitation of participants’ talk to "running commentary"; (3) reflecting the video clip, which shows orientation to the formulation of required answer and its collaborative writing into the paper sheet, including occasional second watching.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pro efektivní a smysluplné používání audiovizuálních orálněhistorických materiálů (OH) je důležité pochopit, jak lidé chápou smysl takových videozáznamů v sociální interakci. Vyvstávají tak zásadní otázky týkající se sekundární analýzy a praktického využití archivovaného audiovizuálního materiálu OH, jako například: Jaké jsou rysy OH rozhovoru jako společenského objektu? Co jej činí smysluplným a interpretovatelným? Kolik toho potřebujeme vědět o sociálně situovaném charakteru rozhovoru, abychom ho správně pochopili? Existuje ve vztahu k OH „příliš málo“ nebo „příliš mnoho“ kontextu? Jaký smysl mají rozhovory na OH ve společenské praxi a jaký je jejich vztah k širším historickým znalostem? V rámci workshopu uchopíme tyto dalekosáhlé otázky z velice empirického a praktického hlediska. Oslovujeme akademické pracovníky, kteří se zajímají o mezioborové přístupy a pracují s interview, orální historií a digitalizovanými nebo digitálními kvalitativními daty obecně.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In order to use audiovisual oral history (OH) materials efficiently and meaningfully, it is important to understand how people make sense of such video recordings in social interaction. Fundamental questions thus emerge in regard to secondary analysis and practical utilization of archived audiovisual OH material, such as: What are the features of OH interview as a social object? What makes it meaningful and interpretable? How much do we need to know about the socially situated character of the interview in order to understand it properly? Is there “too little” or “too much” context in relation to OH? How do people make sense of OH interviews in social practice, and relate it to their broader historical knowledge? In the workshop, we will grasp such far-reaching questions from a very empirical and practical perspective. We are reaching out to scholars who are interested in cross-disciplinary approaches and work with interviews, oral history, and digitized or digital qualitative data in general.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce popisuje testovací sadu dokumentů z auditorské domény pro strojový překlad a její použití jako jedné ze „testovacích sad“ v WMT19 News Translation Task pro překladatelské směry zahrnující češtinu, angličtinu a němčinu.

Naše hodnocení naznačuje, že současné MT systémy optimalizované pro oblast všeobecného zpravodajství mohou docela dobře fungovat i v konkrétní oblasti auditních zpráv. Podrobné manuální hodnocení však ukazuje, že hluboká faktická znalost dané oblasti je nezbytná. Pouhým okem neodborníka se překlady mnoha systémů zdají téměř dokonalé a automatické hodnocení MT s jednou referencí je pro zvážení těchto detailů prakticky zbytečné.

Dále na ukázkovém dokumentu z oblasti smluv ukazujeme, že i ty nejlepší systémy zcela selhávají v zachování sémantiky smlouvy, specificky zachování identit stran.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes a machine translation test set of documents from the auditing domain and its use as one of the “test suites” in the WMT19 News Translation Task for translation directions involving Czech, English and German.

Our evaluation suggests that current MT systems optimized for the general news domain can perform quite well even in the particular domain of audit reports. The detailed manual evaluation however indicates that deep factual knowledge of the domain is necessary. For the naked eye of a non-expert, translations by many systems seem almost perfect and automatic MT evaluation with one reference is practically useless for considering these details.

Furthermore, we show on a sample document from the domain of agreements that even the best systems completely fail in preserving the semantics of the agreement, namely the identity of the parties.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CzeDLex 0.6 je druhá vývojová verze slovníku českých diskurzních konektorů. Slovník obsahuje konektory částečně automaticky extrahované z Pražského diskurzního korpusu 2.0 (PDiT 2.0), rozsáhlého korpusu s ručně anotovanými diskurzními vztahy. Nejfrekventovanější slovníková hesla (pokrývající více než 90% diskurzních vztahů anotovaných v PDiT 2.0) byla ručně zkontrolována, přeložena do angličtiny a doplněna dalšími lingvistickými informacemi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CzeDLex 0.6 is the second development version of a lexicon of Czech discourse connectives. The lexicon contains connectives partially automatically extracted from the Prague Discourse Treebank 2.0 (PDiT 2.0), a large corpus annotated manually with discourse relations. The most frequent entries in the lexicon (covering more than 90% of the discourse relations annotated in the PDiT 2.0) have been manually checked, translated to English and supplemented with additional linguistic information.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce se zabývá opomínejou otázkou valence příslovcí. Po stručném teoretickém úvodu představíme postup při extrahování seznamu potenciálních valenčních příslovcí ze dvou českých syntakticky značkovaných korpusů, SYN2015 a PDT. Dále zmíníme metodologické a teoretické problémy spojené s tímto problémem, zejména ty, které se týkají nejasných hranic slovních druhů, a charakterizujeme typy získaných valenčních příslovcí. Tam, kde je to vhodné, komentujeme lexikografické zpracování valenčních příslovcí a případně navrhujeme jeho úpravu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper deals with the neglected issue of the valency of adverbs. After providing a brief theoretical background, a procedure is presented of extracting the list of potentially valent adverbs from two syntactically parsed corpora of Czech, SYN2015 and PDT. Taking note of the methodological and theoretical problems surrounding this task, especially those relating to the fuzzy boundaries of word classes, we outline the types of adverbs identified as having valency properties. Where appropriate, we comment on – and occasionally suggest improvements in – the lexicographic treatment of valent adverbs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>DeriNet je rozsáhlý lingvistický zdroj obsahující více než 1 milion českých lexémů spojených téměř 810 tisíci derivačních vztahů. Jeho předchozí verze, DeriNet 1.7, kromě derivací neobsahovala další anotace – byly v ní uvedeny lemmata a slovnědruhové kategorie každého lexému, a od verze 1.5 binární příznak kompozitnosti.
Tento článek představuje rozšířenou verzi zdroje, nazvanou DeriNet 2.0, která přináší řadu nových anotací: všechny lexémy mají vyznačené základní morfologické kategorie (vid, rod a životnost), 250 tisíc lexémů má identifikované kořenové morfémy, 150 tisíc derivačních vztahů je označeno svou sémantickou kategorií (zdrobňování, přivlastňování, přechylování, opakovanost a změna vidu), některá kompozita jsou v rámci pilotního projektu přiřazena ke svým základovým slovům a přibylo několik tzv. fiktivních lexémů spojujících příbuzné derivační rodiny bez společného předka. Tyto nové anotace mohly být přidány díky novému souborovému formátu, který je obecný a rozšiřitelný a tedy potenciálně využitelný i v jiných podobných projektech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>DeriNet is a large linguistic resource containing over 1 million lexemes of Czech connected by
almost 810 thousand links that correspond to derivational relations. In the previous version,
DeriNet 1.7, it only contained very sparse annotations of features other than derivations – it listed
the lemma and part-of-speech category of each lexeme and since version 1.5, a true/false flag
with lexemes created by compounding.
The paper presents an extended version of this network, labelled DeriNet 2.0, which adds a number
of features, namely annotation of morphological categories (aspect, gender and animacy) with all
lexemes in the database, identification of root morphemes in 250 thousand lexemes, annotation
of five semantic labels (diminutive, possessive, female, iterative, and aspect) with 150 thousand
derivational relations, a pilot annotation of parents of compounds, and another pilot annotation
of so-called fictitious lexemes, which connect related derivational families without a common
synchronous parent. The new pieces of annotation could be added thanks to a new file format
for storing the network, which aims to be general and extensible, and therefore possibly usable to
other similar projects.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>DeriNet je lexikální síť zachycující derivační vztahy mezi českými slovy. Vrcholy sítě představují české lexémy a hrany reprezentují derivační nebo kompoziční vztahy mezi odvozeným a základovým/základovými slovem/slovy. Tato verze, DeriNet 2.0, obsahuje 1 027 665 lexémů převzatých ze slovníku MorfFlex spojených 808 682 derivačními a 600 kompozičními vztahy.
V porovnání s předchozími verzemi, verze 2.0 používá nový formát a obsahuje nové typy anotace: kompozice, anotace několika morfologických kategorií lexémů, identifikované kořenové morfy u 244 198 lexémů, sémantické označení 151 005 hran pomocí pěti značek a identifikaci několika fiktivních lexémů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>DeriNet is a lexical network which models derivational relations in the lexicon of Czech. Nodes of the network correspond to Czech lexemes, while edges represent derivational or compositional relations between a derived word and its base word / words. The present version, DeriNet 2.0, contains 1,027,665 lexemes (sampled from the MorfFlex dictionary) connected by 808,682 derivational and 600 compositional links.
Compared to previous versions, version 2.0 uses a new format and contains new types of annotations: compounding, annotation of several morphological categories of lexemes, identification of root morphs of 244,198 lexemes, semantic labelling of 151,005 relations using five labels and identification of several fictitious lexemes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozvoj a využívání jazykových zdrojů často zahrnuje zpracování osobních údajů. Obecné nařízení o ochraně osobních údajů (GDPR) stanoví celoevropský rámec pro zpracování osobních údajů pro výzkumné účely a zároveň umožňuje určitou flexibilitu na straně členských států. Dokument pojednává o právním rámci pro výzkum lan- guage po vstupu GDPR v platnost. V první části představíme některé základní pojmy ochrany údajů, které jsou důležité pro výzkum jazyků. Ve druhé části je diskutován obecný rámec zpracování osobních údajů pro výzkumné účely. V poslední části se zaměříme na modely, které některé členské státy EU používají k regulaci zpracování dat pro výzkumné účely.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The development and use of language resources often involve the processing of personal data. The General Data Protection Regulation (GDPR) establishes an EU-wide framework for the processing of personal data for research purposes while at the same time allowing for some flexibility on the part of the Member States. The paper discusses the legal framework for lan- guage research following the entry into force of the GDPR. In the first section, we present some fundamental concepts of data protection relevant to language research. In the second section, the general framework of processing personal data for research purposes is discussed. In the last section, we focus on the models that certain EU Member States use to regulate data processing for research purposes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek zachycuje funkce a překlady významově vyprázdněných dikurzních markerů AND, BUT a SO v angličtině, češtině, maďarštině, francouzštině a litevštině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Discourse markers are highly polyfunctional, particularly in spoken settings. Because of their syntactic optionality, they are often omitted in translations, especially in the restricted space of subtitles such as the parallel transcripts of TED Talks. In this study, we combine discourse annotation and translation spotting to investigate English discourse markers, focusing on their functions, omission and translation equivalents in Czech, French, Hungarian and Lithuanian. In particular, we study them through the lens of
underspecification, of which we distinguish one monolingual and two multilingual types. After making an inventory of all discourse markers in the dataset, we zoom in on the three most frequent and, but and so. Our small-scale yet fine-grained corpus study suggests that the processes of underspecification are based on the semantics of discourse markers and are therefore shared cross-linguistically. However, not all discourse marker types nor their functions are equally affected by underspecification. Moreover, monolingual and multilingual underspecification do not always map for a particular marker. Beyond the empirical
analysis of three highly frequent discourse markers in a sample of TED Talks, this study
illustrates how translation and annotation can be combined to explore the multiple facets of underspecification in a monolingual and multilingual perspective.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Diateze, a to jak ty tvořené pomocí pasivního participia (pasivum, prostý a posesivní rezultativ, recipientní diateze), tak i tzv. zvratné pasivum (deagentizace) byly v minulosti předmětem řady studií jak v bohemistické, tak i
v mezinárodní lingvistice, pro češtinu ale dosud chybělo jejich důkladné slovníkové zpracování. V této dizertační práci se zabývám zachycením diatezí tvořených pomocí pasivního participia a s nimi příbuzných verbonominálních konstrukcí
v gramatické komponentě valenčního slovníkuVALLEX.
Vlastnímu tématu práce předchází krátký historický úvod a podrobné shrnutí pojetí valence ve Funkčním generativním popisu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Diatheses have been the topic of a number of linguistic studies in Czech as well as international linguistics. Previous investigations have led to the delimitation of diatheses formed with the passive participle (the passive, objective (with auxiliary být ‘to be’) and possessive (with auxiliary mít ‘to have’)
resultative, and recipient diatheses) and the so-called reflexive passive (deagentive diathesis), but systematic dictionary treatment has not been carried out yet.
This dissertation is concerned with the diatheses that are formed with the passive participle and their treatment in the Functional Generative Description (FGD). After a thorough description of the valency theory of FGD, I discuss the design of the Grammatical Component of the valency lexicon VALLEX, concentrating on the rules for the formation of the diatheses that are built with the passive participle. Related verbonominal constructions and a new candidate for treatment as a diathesis (the subjective resultative) are also discussed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Automaticky vytvořené zápisy ze schůzek mohou výrazně zlepšit efektivitu práce. Abychom připravili rozumnou automatizaci potřebujeme mít přehled a pochopení, jaké typy schůzek a zápisů existují, jaké mají společné rysy.
V tomto článku shrnujeme jazykové vlastnosti zápisek a schůzek, analyzujeme existující metody sumarizace a možnost jejich aplikace k danému úkolu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Many meetings of different kinds will potentially benefit from technological support like automatic creation of meeting minutes. To prepare a reasonable automation, we need to have a detailed understanding of common types of meetings, of the linguistic properties and commonalities in the structure of meeting minutes, as well as of methods for their automation. 
In this paper, we summarize the quality criteria and linguistic properties of meeting minutes, describe the available meeting corpora and meeting datasets and propose a classification of meetings and minutes types. Furthermore, we  analyze the methods and tools for automatic minuting with respect to their use with existing types of datasets. We summarize the obtained knowledge with respect to our goal of designing automatic minuting and present our first steps in this direction.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Srovnáváme použítí korelativních konstrukcí v českých, ruských a německých překladech z angličtiny. Hledáme, kde je překlad s použitím korelativní konstrukce povinný, kde je možný a kde je vyloučen. Analyzujeme různé typy kontextů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Our research concerns correlative constructions in German, Czech and Russian translations and the corresponding structures in English that trigger these correlatives. In total, 100 parallel segments have been analysed manually for this study. For the study of optionality, we compare our results to original (not translated texts) in Czech and Russian National corpora. The occurrence of the constructions under analysis, as well as cross-linguistic differences in the transformation patterns, can be explained by the influence of a set of factors described in the presentation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nový ručně anotovaný morfematicky segmentovaný slovník pro perštinu, a dále algoritmus, který s využitím tohoto slovníku zkonstruuje morfologickou síť. Výsledná síť zachycuje jak derivační, tak flektivní vztahy mezi slovními formami. Algoritmus pro tvorbu sítě aproximuje rozdíl mezi kořenovými a afixovými morfémy na základě frekvenční informace o morfémech. Vyhodnocujeme kvalitu (ve smyslu lingvistické správnosti) výsledné sítě v konfiguraci s ručně označenými nekořenovými morfémy. V další části naší práce vyhodnocujeme různé strategie pro přidání nových (ve slovníku dosud nezachycených) do sítě s využitím systému MORFESSOR (v řízené i neřízené verzi). Experimenty potvrzují, že navržený postup lze použít pro přidávání dosud nepokrytých slov s přijatelnou úspěšností.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this work, we introduce a new large hand-annotated morpheme-segmentation lexicon of Persian words and present an algorithm that builds a morphological network using this segmented lexicon. The resulting network captures both derivational and inflectional relations. The algorithm for inducing the network approximates the distinction between root morphemes and affixes using the number of morpheme occurrences in the lexicon.
We evaluate the quality (in the sense of linguistic correctness) of the resulting network empirically and compare it to the quality of a network generated in a setup based on manually distinguished non-root morphemes. 
In the second phase of this work, we evaluated various strategies to add new words (unprocessed in the segmented lexicon) into an existing morphological network automatically. For this purpose, we created primary morphological networks based on two initial data: a manually segmented lexicon and an automatically segmented lexicon created by unsupervised MORFESSOR. Then new words are segmented using MORFESSOR and are added to the network. In our experiments, both supervised and unsupervised versions of MORFESSOR are evaluated and the results show that the procedure of network expansion could be performed automatically with reasonable accuracy.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>ITAT (Informačné Technológie - Aplikácie a Teória) je tradiční česko-slovenská konference sdružující vědce a odborníky pracující v širokém spektru informatiky. ITAT nabízí nejen možnost výměny nápadů a informací, ale klade také důraz na neformální komunikaci a diskuse mezi účastníky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>ITAT (Information Technologies—Applications and Theory) is a traditional Czecho-Slovak conference gathering scientists and experts working within a broad scope of computer science. ITAT not only offers a possibility to exchange ideas and information, it also emphasizes informal communication and discussions among participants.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje úvodní výzkum větné reprezentace ve spojitém prostoru. Získali jsme páry velmi podobných vět, které si liší pouze drobnou změnou (jako změna substantiva, přidání adjektiva) z datasetů na odvozování v přirozeném jazyce pomocí metody jednoduchých vzorců. Zkoumáme nakolik drobná změna ve větě ovlivní její reprezentaci ve spojitém prostoru a jak jsou tyto změny zobrazeny v některých populárních modelech větných embeddingů. Zjistili jsme, že některé embeddingy pěkně reflektují malé větné změny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an introductory investigation into continuous-space vector 
representations of sentences. We acquire pairs of very similar sentences
differing only by a small alterations (such as change of a noun, adding 
an adjective, noun or punctuation) from datasets for natural language 
inference using a simple pattern method. We look into how such a small 
change within the sentence text affects its representation in the continuous 
space and how such alterations are reflected by some of the popular sentence 
embedding models. We found that vector differences of some embeddings
actually reflect small changes within a sentence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme sbírku testů pro získávání informací napříč jazyky v medicínské doméně. Tato sbírka je postavena na zdrojích používaných laboratoří CLEF pro hodnocení elektronického zdravotnictví v letech 2013–2015 při úkolech vyhledávání informací zaměřených na pacienta a zlepšuje využitelnost a opakovanou použitelnost oficiálních údajů. Soubor dokumentů je totožný s oficiálním souborem, který byl pro tento úkol použit v roce 2015, a obsahuje asi milion anglických lékařských webových stránek. Sada dotazů obsahuje 166 položek použitých během tří let jako testovací dotazy, které jsou nyní dostupné v osmi jazycích. Rozšířená sbírka testů poskytuje další relevantní hodnocení, která téměř zdvojnásobily množství oficiálně posuzovaných párů dotaz-dokument. Tato práce popisuje obsah rozšířené sbírky, podrobnosti překladu dotazů a posouzení relevance a nejmodernější výsledky získané z této sbírky.
https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-2925</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a test collection for medical cross-lingual information retrieval. It is built on resources used by the CLEF eHealth Evaluation Lab 2013–2015 in the patient-centered information retrieval tasks and greatly improves applicability and reusability of the official data. The document set is identical to the official one used for the task in 2015 and contains about 1 million English medical webpages. The query set consists of 166 queries used during the three years of the campaign as test queries, now available in eight languages (the original English and human translations into Czech, French, German, Hungarian, Polish, Spanish and Swedish). The extended test collection provides extensively improved relevance judgements which almost doubled the amount of the officially assessed query-document pairs. This paper describes the content of the extended version of the test collection, details of query translation and relevance assessment, and state-of-the-art results on this collection.
The dataset can be obtained publicly in the following url: https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-2925</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozšířená testovací kolekce CLEF eHealth pro vyhledávání napříč jazyky v textech z medicíny</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This package contains an extended version of the test collection used in the CLEF eHealth Information Retrieval tasks in 2013--2015. Compared to the original version, it provides complete query translations into Czech, French, German, Hungarian, Polish, Spanish and Swedish and additional relevance assessment.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme metodu automatického rozšíření dotazů pro vyhledávání informací mezi jazyky v oblasti medicíny. Metoda využívá strojový překlad dotazů ze zdrojového jazyka do jazyka dokumentu a lineární regresi k předvídání výkonu vyhledávání pro každý přeložený dotaz, který je rozšířen o kandidátský termín. Kandidátské termíny (v jazyce dokumentu) pocházejí z více zdrojů: hypotézy pro překlad dotazů získané ze systému strojového překladu, články na Wikipedii a abstrakty PubMed. Rozšíření dotazu je použito pouze v případě, že model předpovídá skóre pro kandidátní termín přesahující vyladěnou hranici, která umožňuje rozšiřovat dotazy pouze se silně příbuznými termíny. Naše experimenty jsou prováděny s využitím kolekce testů elektronického zdravotnictví CLEF 2013–2015 a vykazují významná zlepšení jak v mnohojazyčném, tak jednojazyčném nastavení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a method for automatic query expansion for cross-lingual information retrieval in the medical domain. The method employs machine translation of source-language queries into a document language and linear regression to predict the retrieval performance for each translated query when expanded with a candidate term. Candidate terms (in the document language) come from multiple sources: query translation hypotheses obtained from the machine translation system, Wikipedia articles and PubMed abstracts. Query expansion is applied only when the model predicts a score for a candidate term that exceeds a tuned threshold which allows to expand queries with strongly related terms only. Our experiments are conducted using the CLEF eHealth 2013–2015 test collection and show significant improvements in both cross-lingual and monolingual settings.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku porovnáváme strukturu českých slovních embeddingů pro anglicko-český strojový překlad (NMT), word2vec a analýzu sentimentu. Ukazujeme, že i když je možné úspěšně předvídat část slovních druhů (POS) z embeddingů word2vec a různých překladových modelů, ne všechny prostory embeddingů vykazují stejnou strukturu. Informace o POS jsou přítomny v embeddingu word2vec, ale vysoký stupeň organizace POS v dekodéru NMT naznačuje, že tyto informace jsou důležitější pro strojový překlad, a proto je model NMT reprezentuje přímějším způsobem. Naše metoda je založena na korelaci dimenzí PCA s kategorickými lingvistickými údaji. Také ukazujeme, že další zkoumání histogramů tříd podél dimenzí PCA je důležité pro pochopení struktury znázornění informací v embeddinzích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we compare structure of Czech word embeddings for English-Czech neural machine translation (NMT), word2vec and sentiment analysis. We show that although it is possible to successfully predict part of speech (POS) tags from word embeddings of word2vec and various translation models, not all of the embedding spaces show the same structure. The information about POS is present in word2vec embeddings, but the high degree of organization by POS in the NMT decoder suggests that this information is more important for machine translation and therefore the NMT model represents it in more direct way. Our method is based on correlation of principal component analysis (PCA) dimensions with categorical linguistic data. We also show that further examining histograms of classes along the principal component is important to understand the structure of representation of information in embeddings.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Derivace je typ slovotvorného procesu, který odvozuje nová slova z existujících přidáváním, měněním či odebíráním afixů. V tomto článku zkoumáme potenciál slovních embeddingů pro identifikaci vlastností derivací v češtině. Extrahujeme derivační vztahy mezi páry slov z DeriNetu, sítě českých derivací, která sdružuje zhruba milion českých lemmat do derivačních stromů. Pro každý pár vypočteme rozdíl embeddingů obou forem a neřízeně clusterujeme výsledné vektory. Naše výsledky ukazují, že tyto clustery zhruba odpovídají manuálně označeným sémantickým kategoriím derivačních vztahů (čili vztah „péct-pekař“ patří do třídy „aktor“ a správné clusterování ho přiřadí do stejného clusteru jako „řídit-ředitel“).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Derivation is a type of a word-formation process which creates new words from existing ones by adding, changing or deleting affixes. In this paper, we explore the potential of word embeddings to identify properties of word derivations in the morphologically rich Czech language. We extract derivational relations between pairs of words from DeriNet, a Czech lexical network, which organizes almost one million Czech lemmas into derivational trees. For each such pair, we compute the difference of the embeddings of the two words, and perform unsupervised clustering of the resulting vectors. Our results show that these clusters largely match manually annotated semantic categories of the derivational relations (e.g. the relation ‘bake–baker’ belongs to category ‘actor’, and a correct clustering puts it into the same cluster as ‘govern–governor’).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje vážený konečný morfologický převodník pro krymskou tatarštinu, který je schopen analyzovat a generovat slova jak v latince, tak v cyrilici. Tento převodník byl vyvinut týmem, který sestává z člena komunity a jazykového experta, polního lingvisty, který pracuje s krymskotatarskou komunitou, turkologa se zkušeností s počítačovou lingvistikou a zkušeného počítačového lingvisty se zkušenostmi s turkickými jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes a weighted finite-state
morphological transducer for Crimean Tatar able to analyse and generate in both Latin and Cyrillic orthographies. This transducer was developed by a team including a community member and language expert, a field linguist who works with the community, a Turkologist with computational linguistics expertise, and an experienced computational linguist with Turkic expertise.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení oboru počítačové lingvistiky a zejména úlohy strojového překladu středoškolákům, účastníkům semináře 100vědců.cz: Strojové zpracování přirozeného jazyka v posledních několika málo letech zasáhla vlna hlubokého učení, "umělé inteligence", a podobně jako v jiných oblastech vidíme revoluční skoky v kvalitě výstupu. V příspěvku představím postupně základy techniky zpracování vět hlubokými neuronovými sítěmi, podrobněji se zaměřím na úlohu strojového překladu, kde stroje v posledním roce začaly dosahovat lidských kvalit, a nakonec se vrátím k obecnějším úvahám, zda nám stroje konečně začínají rozumět.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An introduction of computational linguistics and machine translation in particular to high school students, participants of the seminar 100vedcu.cz.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce představím technologii strojového překladu, její předpoklady a omezení, to jest zejména závislost na dostupnosti přeložených textů a obecně spolehlivost. Dále popíši zkušenosti s naším projektem, kdy překládáme SMS z vietnamštiny do češtiny pro účely Národní protidrogové centrály Policie České republiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the lecture, I will present machine translation technology, its assumptions and limitations, that is, in particular, the reliance on the availability of translated texts and also reliability. Next, I will describe the experience of our project, where we translate SMS from Vietnamese into Czech for the purposes of the National Drug Headquarters of the Police of the Czech Republic.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento dokument slouží jako stručný přehled zvláštního vydání JNLE o znázornění významu věty, který spojuje tradiční symbolické a moderní kontinuální přístupy. Uvedeme pozoruhodné aspekty významu věty a jejich slučitelnost s oběma proudy výzkumu a poté shrneme podklady vybrané pro tuto zvláštní otázku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper serves as a short overview of the JNLE special issue on representation of the meaning of the sentence, bringing together traditional symbolic and modern continuous approaches. We indicate notable aspects of sentence meaning and their compatibility with the two streams of research and then summarize the papers selected for this special issue.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládáme dvě případové studie, které prokazují, že soužití různých (pod)oborů komputační lingvistiky a nauky o přirozeném jazyce může vést k novým zjištěním a pomoci rozvoji oboru (oborů). Jeden příklad pokrývá studium synonymie vyhledáváním informací v různých lexikálních zdrojích s ohledem na mnohojazyčnost a druhý ukazuje na studium některých jevů týkajících se informační struktury a pořadí slov v angličtině a češtině, jak lze paralelní vícejazyčný a/nebo vícevrstevný korpus, pokud je správně opatřen anotací, použít pro studium některých aspektů hloubkové syntaktické struktury. Oba případy podporují naše přesvědčení, že tvorba jazykových zdrojů je základním krokem v hlavní úloze počítačové lingvistiky, že dobře definovaná anotace je důležitým krokem vpřed jak k testování původní jazykové teorie, tak k jejímu dalšímu rozvoji.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present two case studies that demonstrate that co-habitation of various (sub)disciplines of Computational Linguistics and Natural Language Pro-cessing can reach novel findings and help the advancement of the field(s). One example covers a study of synonymy by searching for information in different lexical resources with regard to multilinguality, and the other demonstrates on the study of some phenomena concerning information structure and word order in English and Czech how a parallel multilingual and/or multilayered corpus if properly annotated can be used for a study of some aspects of the deep syntactic structure. Both cases support our convic-tion that the creation of language resources is a fundamental step in the do-main of computational linguistics, that well-founded annotation is an im-portant step forward towards both testing the original linguistic theory and developing it further.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jako první v rámci oddílu I je zařazena stať Relativní čas napsanáu ve spoluautorství s Petrem Sgallem.P. Sgall je také autorem teoretického rámce pro závislostní a vícerovinný popis jazyka, Funkčního generativního popisu, na nějž  zde otištěné příspěvky navazují a dále ho rozvíjejí. Oddíl II o obecných otázkách syntaxe je zakončen statí o přínosu Vladimíra Šmilauera do novočeské syntaxe. Oddíl III obsahuje studie o valenci jakožto „novém paradigmatu“ v syntaxi. Valenci je věnováno hodně pozornosti, jí byly věnovány četné diskuse i polemiky týkající se např. kritérií pro určování sémantické obligatornosti a platnosti „dialogového testu“.Do oddílu IV jsou zařazena témata, v nichž se kumulují problémy morfologické se syntaktickými (zejména s valencí). Jejich zvládnutí a konsistentní popis se stává měřítkem úspěšnosti zvoleného teoretického rámce. Jde především otázky koreference a kontroly.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the present volume selected papers written during more than 50 years of the author´s linguistic career are presented. Twenty five articles are divided into four general Sections, namely (I) Studies on morphological meanings, (II) General issues on syntax, (III) Issues on valency, (IV) Reflexivity, reciprocity, coreference and control. The Functional Generative Description used as the theoretical framework provides the basis for the studies of form and function relations between the units of several language levels and for the dependency approach to the syntax.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tradiční třídění příslovečných určení na lokální, časová, způsobová a kauzální, s jakými se pracuje v našich syntaxích, a jejich aplikace pro školní výuku jsou pro tyto účely vyhovující. Pro empirické zpracování v rámci Funkčního generativního popis, který slouží mj. i pro úlohy automatického zpracování jazyka (zejména pro strojový překlad, ale v podstatě i pro translatologické problémy spojené s výukou češtiny jako cizího jazyka), se jeví jako potřebná jemnější klasifikace. V kapitole na analýze materiálu vybraných adverbiálních určení a z hlediska problémů, které jejich řešení doprovázejí, navrhujeme způsob jejich zachycení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The traditional classification of adverbials into local, temporal, modal, and causal, such as those used in our syntaxes, and their applications for schooling are suitable for these purposes. For empirical processing under the Functional Generational Description, which also serves, among other things, the tasks of automatic language processing (especially for machine translation, but essentially for the translatological problems associated with the teaching of Czech as a foreign language), a finer classification seems necessary. On the analysis of the material of selected adverbials and in terms of the problems that accompany their solution, we propose a way of capturing them.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku se předkládají argumenty pro začlenění reprezentace informační struktury věty do reprezentace větného významu. Tyto argumenty vycházejí ze skutečnosti, že informační struktura je sémanticky relevantní a je třeba ji brát v úvahu při významové interpretaci negace, presupozice i diskurzní koheze.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Arguments are presented why the information structure of the sentence should be represented in the representation of the meaning. This claim is supported i.a. by the fact that information structure is semantically relevant and is important for the interpretation of negation, presupposition and discourse connectedness.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve stati se rozebírá vliv kybernetiky na vědní obor počítačové lingvistiky, a to jak v historické perspektivě, tak i v současné metologii.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper analyses the impact of cybernetics on the scientific domain of computational linguistics both in the historical perspective as well as in the influence of modern methodology.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vzpomínka na Michaela Alexandra Kirkwooda Hallidaye se zvláštní pozorností věnovanou jeho spojení s pražskými učenci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Obituary of Michael Alexander Kirkwood Halliday with special attention on his connection with the Prague scholars.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nekrolog shrnuje zájmy a přínosy Petra Sgalla k oborům obecné lingvistiky, typologie, počítačové lingvistiky v oblasti vědecké, stejně jako jeho přínosy pedagogické a organizační.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the obituary the contributions of Petr Sgall to the general linguistics, language typology, computational linguistics as well as his participation in the pedagogical and organization activities are enumerated.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Uvádíme několik příkladů dokládajících možnosti využití dat z Pražského závislostního korpusu pro lingvistický výzkum, kde je možné na týchž textech sledovat vztah mezi povrchovou syntaktickou strukturou, hloubkovou syntaktickou strukturou včetně aktuálního členění i celkovou strukturou textu. Mohli jsme tak dojít k následujícím závěrům:(1) Nelze obecně tvrdit, že při charakteristice dichotomie základ – jádro (ať již v jakékoliv terminologii) je možné vycházet z rozdílu mezi starou (danou) a novou informací. (2) Vedle globálního členění věty na základ a jádro je pro popis informační struktury věty vhodné pracovat s členěním lokálním, uplatněným na všech úrovních větné struktury. (3) Pro návaznost vět v textu z hlediska jejich členění na základ a jádro se ukazuje, že je typičtější (pro češtinu, na rozdíl od angličtiny) návaznost základu dané věty na ohnisko věty předcházející (tzv. posloupnost lineární).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present examples how the data from the Prague Dependency Treebank can be used in linguistic research, allowing to study the relationship among surface syntactic structure, deep syntactic structure including the sentence information structure, and the overall text structure. Thus we could have come to the following conclusions: (1) In general, it cannot be claimed that the topic-focus dichotomy (in any terminology) can be based on the difference between the old (given) and the new information. (2) In addition to the global division of a sentence to topic and focus, it is appropriate to work with local segmentation applied at all layers of the sentence structure. (3) For linking the sentences in the text in terms of their division into topic and focus, it seems to be more typical (for Czech, unlike English) linking of the topic of the given sentence to the focus of the preceding sentence (so-called linear sequence).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Data paralelního anotovaného anglicko–českého korpusu byla použita ke zkoumání variability vzájemné pozice LOC a TWHEN v češtině a angličtině a k analýze vztahu mezi informační strukturou a daným pořadím v těchto dvou jazycích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Data from a parallel annotated English–Czech corpus serve for testing the variability of the mutual position of LOC and TWHEN in Czech and English and for the analysis of the relation between information structure and the given order in the two languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Na základě pražské teorie Topic/Focus Articulation a s využitím paralelního anglicko–českého korpusu jsme se zaměřili na dvě otázky: (i) Do jaké míry se shoduje Focus proper v angličtině a v češtině, (ii) pokud se Focus proper liší, platí alespoň, že Focus proper v angličtině je členem (globálního) Focusu v české větě?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Based on the Praguian theory of Topic/Focus Articulation and using a parallel English–Czech corpus, we have followed two research questions: (i) How far does the assignment of Focus proper agree in English and in Czech, (ii) If the assignment of Focus proper differs, is the Focus-proper element in English at least a member of the (global) Focus of the Czech sentence?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CLARIN-DSpace je rozšířená verze softwaru DSpace. Je uzpůsoben potřebám úložiště jazykových dat, ale byl využit i pro jiné účely a není omezen na konkrétní vědecký obor.

Poskytované prostředí umožňuje bez námahy integraci se službami třetích stran: automaticky doplňovat granty z OpenAIRE a výsledky naopak do OpenAIRE, Clarivate Data Citation Index a jinam.

Nejenže je prostředí pro uložení dat nakonfigurováno tak, aby vyžadovalo potřebná metadata, ale také poskytuje návod pro výběr vhodné licence v rámci otevřeného přístupu prostřednictvím integrace projektu Public license selector a komplexního frameworku pro licencování vč. podepisování licencí a kontrolz přístupu.

Pro dodatečnou ochranu dat lze systém snadno nakonfigurovat tak, aby automaticky zálohoval nové záznamy prostřednictvím služby EUDAT B2SAFE.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CLARIN-DSpace is an enhanced fork of the DSpace repository software. The out of the box it is tailored to suit the needs of a language data repository, but has been also deployed for contemporary history or film archives, and is not limited to specific scientific field.

The provided workflows make integrations with third party services auto-suggest grants from OpenAIRE, reporting to CLARIN Virtual Language Observatory, Clarivate Data Citation Index, or OpenAIRE, effortless.

Not only is the submission workflow preconfigured to require necessary metadata, but also provides a guide for Open Access licensing by integrating the Public License Selector. And because not all data can be open, there is also a support for submitters to assign custom licenses to their datasets, for users to sign the licenses, and for repository managers to manage and keep track of all of it.

When the software is configured to connect with Piwik (Matomo) analytics platform, the submitters of data are provided with concise periodic reports about popularity of their submissions.

To offer additional layer of protection, the system can be easily configured to automatically backup submissions via EUDAT B2SAFE service.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme softwarové řešení a zkušenosti s provozem repozitáře pro jazyková data a nástroje pro zpracování přirozených jazyků - LINDAT/CLARIN. Představíme unikátní podporu licencování s důrazem na Open Access a to, jak podporujeme všechny 4 klíčové principy FAIR. Ukážeme vytváření záznamů včetně volby licence, jejich schvalování a publikaci editory, i prostředí pro administraci repozitáře včetně definice licencí, jejich podepisování a kontroly přístupu. Ukážeme také integrace repozitáře s dalšími službami a provozní statistiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a software solution for and experience in running a digital repository for language data and natural language processing tools - LINDAT/CLARIN. We will present unique support for licensing with an emphasis on Open Access, and how we support all 4 key FAIR principles. We will show the submission workflow including license choice, approval and publishing or submissions by editors, as well as the repository administration environment including license definition, signing and access control. We will also present repository integration with other services, and statistics of operation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Overview of recent advances in cross-lingual information retrieval in the patient-centered web search.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Přehled posledních výsledků ve výkzumu multilingválního vyhledávání v oblasti zdraví.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přehledná přednáška na téma vyhledávání informací napříč jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An overview talk on the topic of cross-lingual information retrieval.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Podrobný morfologický popis slovních forem v jakémkoli jazyce je nezbytnou podmínkou úspěšného automatického zpracování jazykových údajů. Prezentujeme nový popis morfologických kategorií, zejména podkategorizaci slovních druhů v češtině v rámci projektu NovaMorf. NovaMorf se zaměřuje na popis morfologických vlastností českých slov kompaktnějším a konzistentnějším způsobem a s vyšší explikační silou než dosud používané přístupy. Cílem projektu je také sjednocení různých přístupů k morfologické anotaci češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A detailed morphological description of word forms in any language is a necessary condition for a successful automatic processing of linguistic data. The paper focuses on a new description of morphological categories, mainly on the subcategorization of parts of speech in Czech within the NovaMorf project. NovaMorf focuses on the description of morphological properties of Czech wordforms in a more compact and consistent way and with a higher explicative power than approaches used so far. It also aims at the unification of diverse approaches to morphological annotation of Czech. NovaMorf approach will be reflected in a new morphological dictionary to be exploited for a new automatic morphological analysis (and disambiguation) of corpora of contemporary Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato technická zpráva je anotačním manuálem k projektu zaměřenému na kontextovou synonymii a valenci sloves v dvojjazyčném
prostředí. Analýza sémantické „ekvivalence“  slovesných významů a jejich valenčního chování v paralelních česko-anglických jazykových zdrojích je jádrem probíhajícího výzkumu. Použití překladového kontextu podporuje více jazykově nezávislé určení vlastností slovesných tříd synonym a vede ke generalizace napříč jazyky. Hlavní přínos projektu
je hlubší vhled do tématu významu slovesa v
kontextu založeném na teorii funkčního generačního popisu, čímž se tento popis rozšiřuje směrem k popisu synonymie sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This technical report is the guideline connected with the project focuses on contextually-based synonymy and valency of verbs in a bilingual
setting. The analysis of semantic ‘equivalence’ (synonymy or near synonymy) of verb senses and their valency behavior in parallel Czech-English language resources is the core of the
proposed research. Using the translational context supports more language-independent
specification of properties of verb sense classes of synonyms and leads towards
generalization across languages. An initial sample bilingual verb lexicon of classes
representing synonym or near-synonym pairs of verbs (verb senses) based on richly annotated
corpora and existing lexical resources will be created. The main contribution of the project
will be a deeper insight from the bilingual perspective into the topic of verb meaning in
context based on the Functional Generative Description theory, thus extending it towards
the appropriate description of contextually-based verb synonymy.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce se zaměřuje na sémantické role, důležitou součást studia lexikální sémantiky, neboť jsou zachyceny jako součást dvojjazyčného (česko-anglického) slovníku slovesných synonym (CzEngClass). Tento slovník navazuje na
existující valenční lexikony zahrnuté v rámci anotace různých pražských závislostních korpusů. Současná analýza sémantických rolí
je nastíněna z pohledu FGP a doložena příklady z korpusu, z Pražského česko-anglického závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper focuses on Semantic Roles, an important component of
studies in lexical semantics, as they are captured as part of a bilingual (Czech-
English) synonym lexicon called CzEngClass. This lexicon builds upon the
existing valency lexicons included within the framework of the annotation of
the various Prague Dependency Treebanks. The present analysis of Semantic
Roles is being approached from the Functional Generative Description point of
view and supported by the textual evidence taken specifically from the Prague
Czech-English Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme probíhající projekt obohacující anotaci paralelního závislostního korpusu, konkrétně Pražského česko-anglického Treebanku, sémantickou anotací používající dvojjazyčný lexikon slovesných synonymních tříd, CzEngClass. Tento lexikon propojuje slovesné predikáty v korpusu s různými externími lexikony. V tomto článku popisujeme první milník dlouhodobého projektu; zatím je k dispozici 100 tříd CzEngClass, které obsahují asi 1800 různých sloves pro češtinu i angličtinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an ongoing project of enriching an annotation of a parallel dependency treebank, namely the Prague Czech-English Dependency Treebank, with verb-centered semantic annotation using a bilingual synonym verb class lexicon, CzEngClass. This lexicon, in turn, links the predicate occurrences in the corpus to various external lexicons. This paper describes a first milestone of a long-term project; so far, approx. 100 CzEngClass classes, containing about 1800 different verbs each for both Czech and English, are available for such annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>SynSemClass je slovník česko-anglických slovesných synonym. Základními hesly ve slovníku jsou dvojjazyčné česko-anglické slovesné synonymní třídy, v nichž jsou obsažena synonymní česká a anglická slovesa (členy třídy), reprezentovaná jako valenční rámce (tj. slovesné významy), jejichž pojetí vychází z teorie Funkčně generativního popisu jazyka. Sémantická ekvivalence jednotlivých členů třídy byla stanovena na základě jejich kontextového valenčního chování usouvztažněného k situačně-kognitivnímu obsahu (sémantickým rolím). Synonymické vztahy jsou ve slovníku chápány volně, jednotlivé členy třídy jsou ve vztahu nikoli striktní (úplné) synonymie, ale ve vztahu významové podobnosti, tj. částečné synonymie. Předností slovníku je použití paralelního česko-anglického korpusu, PCEDT(https://catalog.ldc.upenn.edu/LDC2012T08 ) jako hlavního zdroje jazykových dat, které umožňuje tzv. "bottom-up" přístup, tj. od praxe k teorii.  Předností slovníku je rovněž propojení všech členů jednotlivých synonymních tříd s dalšími lexikálními zdroji, a to s hesly valenčních slovníků (PDT-Vallex - http://hdl.handle.net/11858/00-097C-0000-0023-4338-F, EngVallex- http://hdl.handle.net/11858/00-097C-0000-0023-4337-2), CzEngVallex - http://hdl.handle.net/11234/1-1512 a Vallex -  http://ufal.mff.cuni.cz/vallex/3.5/), a s hesly sémantických  databází (FrameNet - https://framenet.icsi.berkeley.edu/fndrupal/, VerbNet  - http://verbs.colorado.edu/verbnet/index.html, PropBank - http://verbs.colorado.edu/%7Empalmer/projects/ace.html, Ontonotes - http://verbs.colorado.edu/html_groupings/ a Wordnet - https://wordnet.princeton.edu/).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The SynSemClass synonym verb lexicon is a result of a project investigating semantic ‘equivalence’ of verb senses and their valency behavior in parallel Czech-English language resources, i.e., relating verb meanings with respect to contextually-based verb synonymy. The lexicon entries are linked to PDT-Vallex (http://hdl.handle.net/11858/00-097C-0000-0023-4338-F), EngVallex (http://hdl.handle.net/11858/00-097C-0000-0023-4337-2), CzEngVallex (http://hdl.handle.net/11234/1-1512), FrameNet (https://framenet.icsi.berkeley.edu/fndrupal/), VerbNet (http://verbs.colorado.edu/verbnet/index.html), PropBank (http://verbs.colorado.edu/%7Empalmer/projects/ace.html), Ontonotes (http://verbs.colorado.edu/html_groupings/), and English Wordnet (https://wordnet.princeton.edu/). Part of the dataset are files reflecting interannotator agreement.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se věnuje popisu nesystémového valenčního chování českých deverbativních substantiv, jak se jeví na základě automatického srovnání valenčních rámců vzájemně propojených substantivních a slovesných lexikálních jednotek ve valenčních slovnících NomVallex a VALLEX. Nesystémové valenční chování se v NomVallexu nejčastěji projevuje nesystémovými formami aktantů, zatímco změny v počtu a typu aktantů jsou co do počtu případů zanedbatelné. Nesystémové formy významně přispívají k všeobecnému nárůstu počtu forem ve valenčních rámcích substantiv, ve srovnání s počtem forem ve valečních rámcích jejich základových sloves. Nesystémové formy jsou častější ve valenčních rámcích neproduktivně tvořených substantiv než ve valenčních rámcích substantiv tvořených produktivně.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In order to describe non-systemic valency behavior of Czech deverbal nouns, we present results of an automatic comparison of valency frames of interlinked noun and verbal lexical units included in valency lexicons NomVallex
and VALLEX. We show that the non-systemic valency behavior of the nouns is mostly manifested by non-systemic forms of their actants, while changes in the number or type of adnominal actants are negligible as for their frequency. Non-systemic forms considerably contribute to a general increase in the number of forms in valency frames of nouns compared to the number of forms in valency frames of their base verbs. The non-systemic forms are more frequent in valency frames of non-productively derived nouns than in valency frames of productively derived ones.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zkoumáme anotaci reflexiv v závislostních korpusech Universal Dependencies (UD) (Nevre et al., 2018), se silnějším zaměřením na slovanské jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We explore the annotation of reflexives in Universal Dependencies (UD) treebanks (Nivre et al., 2018), with a stronger focus on Slavic languages. We will show a number of examples where reflexive markers (in particular reflexive clitics) are used as true (argumental) reflexives, reciprocal arguments, passive/middle/impersonal markers etc.; we will confront the UD guidelines for the individual phenomena with the current state of the data. Improvements will be proposed, sometimes of the guidelines, but mostly of the annotation.

This is an extended version of our talk at TLT 2018 in Oslo.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Mnohé lingvistické teorie a anotační projekty zahrnují hloubkově-syntaktickou a/nebo sémantickou rovinu. I když mnohé z nich byly aplikovány na více než jeden jazyk, žádný syntakticko-sémantický projekt se ani zdaleka neblíží množství jazyků pokrytých projektem Universal Dependencies (UD).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Many linguistic theories and annotation frameworks contain a deep-syntactic and/or semantic layer. While many of these frameworks have been applied to more than one language, none of them is anywhere near the number of languages that are covered in Universal Dependencies (UD). I will present a prototype of Deep Universal Dependencies, a two-speed concept where minimal deep annotation can be derived automatically from surface UD trees, while richer annotation can be added for datasets where appropriate resources are available. The talk is based on joint work with Kira Droganova.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je jedenácté vydání treebanků UD, verze 2.5.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the eleventh release of UD Treebanks, Version 2.5.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek je příspěvkem k diskusi o hranici mezi kognitivním obsahem a jazykovým významem. V příspěvku se na příkladu určování repertoáru adverbiálních významů probírají praktické důsledky rozlišování mezi obsahem a významem. Je popsána metodologie práce při určování adverbiálních významů, zejména princip zaměnitelnosti synonym. Teroretickým východiskem je Funkční generativní popis.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The necessity to distinguish between cognitive content and linguistic meaning arose in European structural linguistics (Saussere, 1916) and was further elaborated in the Prague Linguistic Circle (Mathesius, 1942; Dokulil – Daneš, 1958; Daneš, 1974). In the contribution, we describe the practical aspects of applying the principle of distinguishing meaning and content to the task of delimitation of adverbial meanings, expressed by prepositional groups.  We present methodology and main principles we work with in completing the set of meanings of adverbials. We describe how we use the principle of substitutability of synonyms. All the examples in the contribution relate to spatial adverbials but the principles apply to adverbials in general.  Our theoretical framework is Functional Generative Description (Sgall et al., 1986).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Data Pražského česko-anglického závislostního korpusu (Prague Czech-English Dependency Treebank, PCEDT) posloužila jako materiál pro srovnávací studii věnovanou vydělování adverbiálních významů místního určení "v rámci daného místa". České předložkové skupiny obsahující předložky v, na a u vyjadřující výše uvedené místní určení byly srovnány s jejich anglickými ekvivalenty a byly dále rozděleny do tří sémantických podskupin, konkrétně "uvnitř", "na povrchu" a "v daném místě". Naše analýza potvrdila, že přestože oba jazyky strukturují realitu jiným způsobem, lze vypozorovat určité tendence ve vztahu forem a jejich funkcí, které vedou k jemnější klasifikaci daných adverbiálních významů. Jedná se o studii popisující aktuálně probíhající výzkum.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The data of the Prague Czech-English Dependency Treebank (a member of the family of Prague Dependency Treebanks) have served as a basis for the comparative study of delimiting adverbial meanings of the local relation “within the given place”. The Czech prepositional groups containing the prepositions v, na, and u with the above meaning are compared with their English equivalents, using a more subtle differentiation into three semantic subgroups of "inside", "on the surface" and "at the given place". Our analysis confirms that though every language structures the reality in a different way, certain tendencies may be observed in the relation of the forms and their functions that eventually result in a more detailed classification. The contribution presents results of an ongoing work.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Existuje několik morfologických slovníků pro češtinu. Liší se pouze řešením komplikovaných morfologických rysů. Byly učiněny různé pokusy o sjednocení jejich přístupů, ale jen některé z nich byly realizovány. Dokument se zabývá několika takovými rysy a porovnává jejich řešení přijatá ve dvou různých projektech, konkrétně příprava nového vydání PDT (Prague Dependency Treebank) a NovaMorf. Charakteristickými rysy prezentovanými v tomto dokumentu jsou: agregáty (slovní formy bez jasné části řeči, např. užs, oč, naň) a varianty - flektivní (více slovní formy pro konkrétní kombinaci lemmy a morfologické značky) i globální (zejména ortografické varianty vyjádřené ve všech slovních formách paradigmatu).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>There exist several morphological dictionaries
for Czech. They differ only in solutions of complicated morphological features. Various attempts have been made to unify their approaches, but only some of them were implemented. The paper deals with several such features and compares their solutions taken in two different projects,
namely preparation of the new edition of PDT (Prague Dependency Treebank) and NovaMorf. The features presented in this paper are: aggregates (the word-forms without a clear part of speech, e.g. užs, oč, naň), and variants – inflectional (more wordforms for a particular combination of lemma and morphological tag) as well as global ones (mainly orthographic variants expressed in all wordforms of a paradigm).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku jsou popsány systematické změny, které jsou realizovány v českém morfologickém slovníku v souvislosti s anotací nových dat v Pražských závislostních korpusech. Přináší řešení několika komplikovaných morfologických jevů, které se objevují v českých textech. Představeny jsou dva nové slovní druhy: cizí slovo a segment. Popisují se pravidla pro reprezentaci variantních a homonymních tvarů a slov (lemmat), pravidla pro zachycení zkratek a tzv. agregátů (např. naň). Změny ve slovníku jsou prováděny za účelem vyšší konzistence mezi daty a slovníkem a  v slovníku samotném.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe systematic changes that have been made to the Czech morphological dictionary related to annotating new data within the project of Prague Dependency Treebank (PDT). We bring new solutions to several complicated morphological features that occur in Czech texts. We introduced two new parts of speech, namely foreign word and segment. We adopted new principles for morphological analysis of global and inflectional variants, homonymous lemmas, abbreviations and aggregates. The changes were initiated by the need of consistency between the data and the dictionary and of the dictionary itself.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Připomenutí od Googlu ohledně ochrany soukromíPřečíst nyníPřečtu si je později

4027/5000
Metoda stylometrie nejčastějšími slovy neumožňuje přímé srovnání původních textů a jejich překladů, tj. Napříč jazyky. Pokusili jsme se odstranit tuto jazykovou bariéru pro přímé stylometrické srovnání literárního překladu tím, že jsme textové tokeny nahradili křížovými lingválními morfologickými značkami poskytovanými v rámci programu Universal Dependencies (http://universaldependencies.org). Připravili jsme sbírku přibližně 50 textů v češtině a němčině: obsahovala české originály a jejich německé překlady; Německé originály a jejich české překlady; a díla původně psaná v angličtině, francouzštině nebo ruštině, ale pouze v českých a německých překladech. Pomocí analyzátoru UD-Pipe jsme převedli texty na sekvence značek. Výstupem byla tabulka CONLL-u (jeden token na řádek, různé typy jazykových značek ve sloupcích: token, lemma, hrubý tag části řeči, jemnozrnné morfologické znaky, závislost na řídícím uzlu, ID řídící uzel). Hrubé tagy pro část řeči (POS) se nazývají UPOS a jemnozrnné morfologické znaky se nazývají FEATS. Závislostní vztahy se nazývají DEPREL. Zatímco UPOS jsou skutečně univerzální, soupis příslušných funkcí je závislý na jazyce. Například jazyk, který nepoužívá definitivitu substantiva, nepoužívá funkci `Definitivní 's hodnotami` Def` a `Indef`. Protože se české a německé soupisy FEATS liší, odstranili jsme atributy funkcí a hodnoty specifické pro oba jazyky, přičemž zachovali pouze funkce a hodnoty sdílené oběma jazyky.
V dalším kroku jsme původní lemma nahradili `` FLEMMA '', tj. "Falešné lemma". FLEMMA je mezikulturní lemma pro německo-český pár. Navrhli jsme to jako číselné ID. Získali jsme glosář FLEMMAS, který čerpal z česko-německého glosáře (Škrabal a Vavřín 2017), který byl automaticky generován z vícejazyčného paralelního korpusu InterCorp (Rosen 2016). Počítali jsme různé kombinace značek získaných pro čtyři úrovně parsingu podle obvyklého postupu Delta (Burrows 2002). Každý značkovací řetězec byl považován za jediný prvek (protějšek typů slov v tradiční stylometrické analýze podle nejčastějších slov) a jejich frekvence byly spočteny v textech v korpusu a porovnány v textových párech za účelem vytvoření matice měr vzdálenosti; v této studii byly vzdálenosti stanoveny pomocí modifikované Cosine Delta (Smith and Aldridge 2011), která je nyní považována za nejspolehlivější verzi (Evert et al. 2017). Funkce classify () v balíčku stylů (Eder et al. 2016) pro R (R Core Team 2016) byla použita k pokusu o posouzení úspěšnosti přiřazení autorství, když její referenční sada obsahovala texty v jednom jazyce a testovací sada obsahovala texty v jiný. Úspěch přiřazení byl počítán vždy, když byla hodnota Delta pro dvojici překladů stejného textu nejnižší. Nejúspěšnější kombinací pro přiřazování autorství byl FLEMMAS + UPOS (95,6%), zatímco samotný FLEMMAS dosáhl pouze (3,7%) a stejně tak UPOS sám. To ukazuje, že i velmi hrubý překlad ze slova na slovo (polysemy zanedbaný), spolu s hrubou částí řeči, pomáhá obejít jazykovou bariéru. Dalším zajímavým nálezem je zdánlivě skromná 20,3% úspěšnost pro přiřazení kombinací UPOS + FEATS + DEPREL. To je konec konců mnohem víc než jen hod pro mince pro takový počet textů. Ještě důležitější je, že hádání bylo pro stejné dvojice textů trvale úspěšné, což naznačuje, že tyto konkrétní dvojice by bylo snadnější uhodnout. To by zase mohlo naznačovat, že překlady v těchto případech aplikovaly strategie vedoucí k gramatickým a syntaktickým strukturám podobným původnímu, jevu často pozorovanému v překladu slovo za slovem, nebo obecněji v překladech zaměřených na tzv. nazývána „formální rovnocennost“ (Nida 1964). Tato práce byla financována z LTC18020.
Odeslat zpětnou vazbu
Historie
Uloženo
Komunita</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The method of stylometry by most frequent words does not allow direct comparison of original texts and their translations, i.e. across languages. We have tried to remove this language barrier for direct stylometric comparison of literary translation by replacing the text  tokens  with  the  cross-lingual  morphological  tags  provided  by  the  Universal Dependencies scheme (http://universaldependencies.org). We have prepared a collection of approximately 50 texts in Czech and German: it contained Czech originals and their German translations; German originals and their Czech translations; and works originally written  in  English,  French  or  Russian,  but  only  in  Czech  and  German  translations.  We converted  the  texts  into sequences  of  tags  using  the  UD-Pipe  parser.  The  output  was  a CONLL-u  table  (one  token  per  line,  different  types  of  linguistic  markup  in  columns: token,   lemma,   coarse   part-of-speech   tag,   fine-grained   morphological   features, dependency relation to the governing node, ID of the governing node).The   coarse   part-of-speech   (POS)   tags   are   called   UPOS   and   the   fine-grained morphological features are called FEATS. The dependency relations are called DEPREL. While  the  UPOS  are  truly  universal,  the  inventory  of  relevant  FEATS  is  language-dependent. For instance, a language that does not use noun definiteness does not use the `Definite` feature with its values `Def` and `Indef`. Since the FEATS inventories of Czech and  German  differ,  we  have  stripped  feature  attributes  and  values  specific  to  either language, keeping only features and values shared by both languages.
As a next step, we replaced the original lemmas with a ``FLEMMA'', i.e. a "fake lemma". FLEMMA is a cross-lingual lemma for the German-Czech pair. We designed it as a numerical ID. Weobtained a glossary of FLEMMAS drawing on a Czech-German glossary (Škrabal and Vavřín 2017) that was  automatically  generated  from the multilingal parallel  corpus  InterCorp (Rosen 2016). We counted various  combinations of tags obtained for the four parsinglevels according the the usual Delta procedure (Burrows 2002). Each tagging string was treated as a single element (the counterpart of word-types in traditional stylometric analysis by most frequent words), and their frequencies were counted in the texts in the corpus and compared in text pairs to produce a matrix of distance measures; in this study, the distances were established by means of the modified Cosine Delta (Smith and Aldridge 2011), which is now seen as the most reliable version (Evert et al. 2017). The function classify() in the stylo package (Eder et al. 2016) for R (R Core Team 2016) was used to try to assess authorship attribution success when its reference set contained texts in one language and the test set contained texts in the other. Attribution success was counted whenever the Delta value for the pair of the translations of the same text was lowest.The most successful combination for authorship attribution was FLEMMAS + UPOS (95.6%), while FLEMMAS alone achieved only (3.7%), and so did UPOS alone. This shows that even a very crude word-to-word translation (polysemy neglected), along with the coarse part of speech, helps bypass the language barrier. Another interesting finding is the seemingly modest 20.3% success rate for attribution by the combination UPOS + FEATS + DEPREL. This is, after all, much more than a coin toss for such a number of texts. More importantly, guessing was consistently successful for the same pairs of texts, suggesting that these particular pairs might be easier to guess. This, in turn, might indicate that the translations in these case applied strategies resulting in grammatical and syntactic structures somehow similar to the original, a phenomenon oftenobserved in word-for-word translation, or, more generally, in translations aiming for the so-called “formal equivalence” (Nida 1964).This work was funded by LTC18020.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vyhodnocujeme výběr aktuálně dostupných značkovačů na beletristických textech z 19. století. Texty patří do sbírky ELTeC vytvořené a udržované týmem WG1 projektu COST CA16204 „Vzdálené čtení pro evropskou literární historii“. Jedná se o mnohojazyčnou sbírku beletrie 19. století v následujících jazycích: čeština, angličtina, francouzština, němčina, maďarština, italština, norština (obě odrůdy), portugalština, srbština a slovinština (s novými jazyky). Každý jazyk je zastoupen nejméně 20 romány ve formátu TEI.
Hodnotili jsme značkovače pro následující jazyky:
Česky, anglicky, francouzsky, německy, maďarsky, norsky-nynorsk, portugalsky a slovinsky. Všechny jsme označili štítkem UD-Pipe. Kromě toho byla maďarština označena také e-magyar. UD-Pipe jsme vybrali kvůli přitažlivé křížové jazykové značce a jednotné tvorbě modelů pro všechny jazyky. Již dříve bylo známo, že maďarský model je velmi malý a že maďarské značkování je špatné. Proto jsme také vyhodnotili e-magyar, nejlepší značkovač v současnosti dostupný pro maďarštinu
Schéma anotací UD-Pipe
Značkovač a analyzátor UD-Pipe čerpá z Universal Dependencies (universaldependencies.org), dále od UD. UD je schéma vícejazyčných anotací pro morfologii a syntaxi. V současné době má více než 70 jazyků vlastní UD stromovou banku, na které lze trénovat syntaktický analyzátor. Kategorie, které anotují UD-Pipe, jsou: lemma, hrubá část řeči (např. VERB), univerzální rysy (např. Množné číslo, minulé napětí) a vztahy syntaktické závislosti (např. Předmět, predikát).
Postup
Vybrali jsme náhodný vzorek pro ruční opravu značkování v každém jazyce. Každý vzorek obsahoval přibližně 5 000 žetonů v celých větách. Věty byly extrahovány z ELTeC napříč všemi dokumenty v každém jazyce a označeny ve formátu CONLL-u: jeden token na řádek, sloupce: token, lemma, univerzální POS, univerzální funkce. Anotace provedla anotaci v tabulce ve čtyřech dalších sloupcích pro tokenizaci, lemma, POS a funkce. V prvních třech sloupcích měly označovat odpovídající typ chyby s písmenem „F“. Ve čtvrtém sloupci měli anotátoři zadat počet nesprávně rozpoznaných jazykových prvků.
Proto anotace zachycuje čtyři různé typy chyb pro každý token. Počet chyb funkcí měří pouze přesnost, nikoli stažení, značkovače. Je to proto, že morfologická anotace je poněkud složitá a bylo by příliš náročné na čas a zdroje na to, aby bylo možné vyškolit plně kompetentního morfologického anotátora pro každý jazyk, který by byl schopen korigovat výstup automatického značkovače na nový zlatý standard.
Pro každý jazyk jsme vytvořili popisné a explorativní statistiky a vizualizace, abychom porovnali výkon UD-Pipe mezi jazyky a analyzovali chyby. Jako druhý krok jsme porovnali slovní zásobu jednotlivých vzorků se slovní zásobou referenčních stromů příslušných jazyků, abychom zjistili, do jaké míry se doména beletrie 19. století liší od domény referenční stromové struktury. Ručně jsme rozdělili tokeny specifické pro vzorky z 19. století do následujících kategorií: Cizí slovo / těžký dialekt, archaické hláskování / morfologie, morfologická dvojznačnost, archaické tokenizace, typo, vlastní jméno. Když anotátor nechal žádnou buňku prázdnou, když se žádný ze štítků neshodoval. Štítky kategorií lze také úmyslně kombinovat.
Na základě obou anotací jsme schopni pro každý jazyk říct, kde má tagger největší problémy (POS, lemmatizace? Tokenizace? Funkce?) As jakými slovy (kolik archaického pravopisu souvisí s výkonem lemmatizace?). Schopnost značkovače „hádat“ neznámá slova se v každém jazyce liší. Díky této dvojité anotaci jsme schopni zahájit spolupráci s vývojáři UD-Pipe na možné adaptaci domény pro vybrané jazyky a vytvářet spolehlivé modely pro označování fikce 19. století.
Tato práce byla financována z LTC18020.
Odeslat zpětnou vazbu</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We  evaluate  a  selection  of  currently  available  taggers  on  fiction  texts  from  the  19th century. The texts belong to the ELTeC collection created and maintained by the WG1 team of the COST project CA16204 "Distant Reading for European Literary History". It is  a  multilingual  collection  of  19th-century  fiction  in  the  following  languages:  Czech, English,  French,  German,  Hungarian,  Italian,  Norwegian  (both  varieties),  Portuguese, Serbian, and Slovene (with new languages being added). Each language is represented by at least 20 novels in the TEI format.We have evaluated the taggers for the following languages:Czech,  English,  French,  We evaluate a selection of currently available taggers on fiction texts from the 19th century. The texts belong to the ELTeC collection created and maintained by the WG1 team of the COST project CA16204 "Distant Reading for European Literary History". It is a multilingual collection of 19th-century fiction in the following languages: Czech, English, French, German, Hungarian, Italian, Norwegian (both varieties), Portuguese, Serbian, and Slovene (with new languages being added). Each language is represented by at least 20 novels in the TEI format.
We have evaluated the taggers for the following languages:
Czech, English, French, German, Hungarian, Norwegian-Nynorsk, Portuguese, and Slovene. We have tagged them all with the UD-Pipe tagger. On top of that, Hungarian was also tagged with e-magyar. We have selected UD-Pipe because of the appealing cross-lingual markup and uniform model building for all languages. It was known before that the Hungarian model is very small and that the Hungarian tagging is poor. Therefore we have also evaluated e-magyar, the best tagger currently available for Hungarian
The annotation scheme of UD-Pipe
The UD-Pipe tagger and parser draws on Universal Dependencies (universaldependencies.org), henceforth UD. UD is a cross-lingual annotation scheme for morphology and syntax. At the moment, more than 70 languages have their own UD treebank to train a parser on. The categories that UD-Pipe annotates are: lemma, coarse part of speech (e.g. VERB), universal features (e.g. Plural, Past Tense), and syntactic dependency relations (e.g. Subject, Predicate).
The procedure
We have selected a random sample for a manual tagging correction in each language. Each sample comprised approximately 5,000 tokens in entire sentences. The sentences had been extracted from ELTeC across all documents in each language and tagged in the CONLL-u format: one token per line, columns: token, lemma, universal POS, universal features. The annotators performed the annotation in a spreadsheet, in four additional columns for tokenization, lemma, POS, and features, respectively. In the first three columns, they were to indicate the corresponding error type(s) with an "F". In the fourth column, the annotators were to type the number of incorrectly recognized linguistic features.
Hence, the annotation captures four different error types for each token. The number of feature errors measures only the precision, not the recall, of the tagger. This is because the morphological annotation is rather complex and it would have been too time- and resource-consuming to train a fully competent morphological annotator for each language, who would have been able to correct the automatical tagger output to a new gold standard.
We have produced descriptive and explorative statistics and visualizations for each language to compare the performance of UD-Pipe among languages and to analyze the errors. As a second step, we have compared the vocabulary of the individual samples to the vocabulary of the referential treebanks of the respective languages to find out how much the domain of the 19th-century fiction differs from the domain of the referential treebank. We have manually classified the tokens specific to the 19-century samples into the following categories: Foreign word/heavy dialect, archaic spelling/morphology, morphological ambiguity, archaic tokenization, typo, proper noun. The annotator was allowed to leave a cell blank when none of the labels matched. The category labels could also be deliberately combined.
Based on both annotations, we are able to tell, for each language, where the tagger has most problems (POS, lemmatization? Tokenization? Features?) and with which words (how much is archaic spelling associated with lemmatization performance?). The ability of the tagger to "guess" unknown words differs in each language. With this double annotation, we are able to start a collaboration with UD-Pipe developers on a possible domain adaptation for selected languages and build reliable models to tag 19-th century fiction.
This work was funded by LTC18020.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci prezentujeme analýzu hypotetických preferencí žánru specifických pro žánry u spontánních příběhů v korpusu 286 přepsaných záznamů mladších školních dětí (ve věku 6–11). Vycházíme z Nicolopoulouvé pojetí vyprávění jako symbolické aktivity s tendencemi danými pohlavím: dívky dávají přednost „rodinnému“ žánru a chlapci preferují „hrdinně agonistický“. Operacionalizovali jsme žánrovou klasifikaci podle Nikolopoulou do 10 tematických vyprávěcích rysů a zaznamenali jsme jejich příslušnou přítomnost nebo nepřítomnost v každém vyprávění. Kombinace statistických metod na náš vzorek odhalila, že narativní preference spojené s pohlavím jsou tak slabé, že nemají žádné důsledky, např. pro školní praxi. Nejvýznamnější výsledek ve skutečnosti dokonce odporuje apriornímu předpokladu pohlavních narativních preferencí: bez ohledu na věk a pohlaví, děti projevují silnou preferenci „vzájemné pomoci a spolupráce“.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present an analysis of hypothesized sex-specific genre preferences in spontaneous narratives in a corpus of 286 transcribed recordings of younger school children (aged 6–11). We draw upon Nicolopoulou’s conception of narratives as a symbolic activity with sex-specific tendencies: girls prefer the “family” genre and boys prefer the “heroic-agonistic” one. We operationalized Nikolopoulou’s genre classification into 10 thematic narrative features and an-notated their respective presence or absence in each narrative. The application of a combination of statistical methods to our sample revealed that the sex-correlated narrative preferences are so weak that they have no implications, e.g. for school practice. The most prominent result, in fact, even coun-ters the a priori assumption of sex-based narrative preferences: no matter what sex or age, children show a strong preference for “mutual aid and cooperation”.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá tvořením vidového protějšku u přejatých slovesných neologismů v češtině. To, zda je vidový protějšek tvořen sufixací či prefixací, závisí - podle mé hypotézy diskutované v příspěvku - na tom, zda je báze neologismu v češtině interpretovaná jako slovesná nebo nominální.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper focuses on how loan verbal neologisms form their aspectual counterpart in Czech, either by changing the thematic suffix in a simplex verb, or by deriving a prefixed verb. The main hypothesis is that the formation of aspectual pairs is determined by whether the neologism is interpreted as having either a verbal base, or a noun base.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje poloautomatickou proceduru, při níž byly vztahy mezi základovými slovy a jejich deriváty v lexikální síti DeriNet označkovány sémantickými značkami. V prezentovaném pilotním experimentu, který byl omezen na pět sémantických značek (diminutiva, posesiva, ženské protějšky maskulin, iterativa a vidové významy), byla data značkována metodami strojového učení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes a semi-automatic procedure introducing semantic labels into the DeriNet network, which is a large, freely available resource modelling derivational relations in the lexicon of Czech. The data
were assigned labels corresponding to five semantic categories (diminutives, possessives, female nouns, iteratives, and aspectual meanings) by a machine learning model, which achieved excellent results in terms of both precision and recall.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je desáté vydání treebanků UD, verze 2.4.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the tenth release of UD Treebanks, Version 2.4.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme návrh projektu pro automatické generování scénářů divadelních her. Navrhujeme hierarchický postup generování založený na expanzi textu (inverze sumarizace textu), která iterativně rozgenerovává název hry, dokud není vygenerován celý scénář. Diskutujeme výzvy projektu a navrhujeme pro ně řešení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a proposal for a project for automatic generation of theatre play scripts. We suggest a hierarchical generation scenario based on text expansion (an inverse of text summarization), iteratively expanding the title of the play until the whole script is generated. We review the challenges of the project and solutions that we envision for them.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Podobně jako v jiných oborech, zpracování přirozeného jazyka prošlo nedávno revolucí zavedením hlubokého učení. V mé přednášce se zaměřím na dvě specifické vlastnosti přírozeného textu jako vstupu pro systém strojového učení a na současné způsoby, jakými jsou v hlubokých neuronových sítích řešeny:
- Reprezentace masivně vícehodnotových diskrétních dat (slov) kontinuálními nízkodimenzionálními vektory (slovní embedinky).
- Zpracování vstupních sekvencí s proměnnou délkou pomocí vztahů na dlouhou vzdálenost mezi elementy (větami) neurálními jednotkami pevné velikosti (mechanismy pozornosti).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Similarly to other fields, Natural language processing has recently been
revolutionized by the introduction of Deep learning. In my talk, I will
focus on two specific properties of natural text as input to a machine
learning system, and the current ways in which these are addressed in
Deep neural networks:
- Representing massively multi-valued discrete data (words) by
continuous low-dimensional vectors (word embeddings).
- Handling variable-length input sequences with long-distance relations
between elements (sentences) by fixed-sized neural units (attention
mechanisms).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CoNLL 2018 Shared Task, tým CUNI-x-ling -- zdrojové kódy pro soutěž ve vícejazyčném syntaktickém parsingu</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CoNLL 2018 Shared Task Team CUNI-x-ling -- source codes for a competition in multilingual parsing</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Používáme anglický model BERT a zkoumáme, jak vypuštění jednoho slova ve větě mění reprezentace jiných slov. Naše hypotéza je taková, že odstranění redukovatelného slova (např. přídavného jména) neovlivní reprezentaci jiných slov natolik, jako odstranění např. hlavního slovesa, které činí větu pro jazykový model negramatickou a „velmi překvapivou“. Odhadujeme reducibility jednotlivých slov a také delších souvislých frází (slovních n-gramů), studujeme jejich syntaktické vlastnosti a poté je také použijeme k odvození plných závislostních stromů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We use the English model of BERT and explore how a deletion of one word in a sentence changes representations of other words. Our hypothesis is that removing a reducible word (e.g. an adjective) does not affect the representation of other words so much as removing e.g. the main verb, which makes the sentence ungrammatical and of "high surprise" for the language model. We estimate reducibilities of individual words and also of longer continuous phrases (word n-grams), study their syntax-related properties, and then also use them to induce full dependency trees.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Reprezentace textu v počítači.
Reprezentace slov (slovní embedinky).
Generování vět I. (n-gramové jazykové modely).
Základní principy strojového učení.
Generování vět II. (umělé neuronové sítě).
Úvahy nad silněji řízeným generováním.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Representing text on a computer.
Representation of words (word embeddings).
Generating Sentences I. (n-gram language models).
Basic principles of machine learning.
Generating Sentences II (artificial neural networks).
Reflections on more powerfully managed generation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zkoumáme, do jaké míry lze flexi automaticky oddělit od derivace, jen na základě slovních forem. Očekáváme, že při použití vhodné míry vzdálenosti budou páry vyskloňovaných tvarů stejného lemmatu k sobě blíže než páry vyskloňovaných forem dvou různých lemmat (stále odvozených od stejného kořene). Vzdálenosti slovních tvarů odhadujeme pomocí editační vzdálenosti, která představuje podobnost založenou na znacích, a pomocí podobnosti slovních embedinků, která slouží jako proxy k významové podobnosti. Konkrétně zkoumáme Levenshteinovu a Jarovu-Winklerovu editační vzdálenost a kosinovou podobnost FastTextových slovních embedinků. Vyhodnocujeme oddělitelnost flexe a derivace na vzorku z databáze DeriNet, což je databáze slovotvorných vztahů v češtině. Zkoumáme míry vzdálenosti slov jednak přímo a jednak a jako složku shlukovacího postupu. Nejlepších výsledků je dosaženo kombinací Jarovy-Winklerovy editační vzdálenosti a kosionové podobnosti slovních embedinků, která překonává míry použité samostatně. Další analýza ukazuje, že metoda funguje lépe pro některé třídy flexí a derivací než pro jiné, což ukazuje některá omezení metody, ale také podporuje myšlenku nahrazení binární dichotomie flexe-derivace kontinuální škálou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We investigate to what extent inflection can be automatically separated from derivation, just based
on the word forms. We expect pairs of inflected forms of the same lemma to be closer to each other
than pairs of inflected forms of two different lemmas (still derived from a same root, though),
given a proper distance measure. We estimate distances of word forms using edit distance, which
represents character-based similarity, and word embedding similarity, which serves as a proxy
to meaning similarity. Specifically, we explore Levenshtein and Jaro-Winkler edit distances, and
cosine similarity of FastText word embeddings. We evaluate the separability of inflection and
derivation on a sample from DeriNet, a database of word formation relations in Czech. We
investigate the word distance measures directly, as well as embedded in a clustering setup. Best
results are achieved by using a combination of Jaro-Winkler edit distance and word embedding
cosine similarity, outperforming each of the individual measures. Further analysis shows that the
method works better for some classes of inflections and derivations than for others, revealing some
limitations of the method, but also supporting the idea of replacing a binary inflection-derivation
dichotomy with a continuous scale.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zaměřujeme se na úlohu neřízené lemmatizace, tj. seskupení vyskloňovaných forem jednoho slova pod jeden štítek (lemma) bez použití anotovaných trénovacích dat. Navrhujeme provádět aglomerativní shlukování slovních forem s novou mírou vzdálenosti. Naše míra vzdálenosti je založena na pozorování, že flexe jednoho slova mají tendenci být podobné řetězcově i významově. Proto kombinujeme kosinovou podobnost slovních embedinků, která slouží jako proxy k významové podobnosti, s editační vzdáleností Jaro-Winklera. Naše experimenty na 23 jazycích ukazují, že náš přístup je slibný a překonal baseline pro 23 z 28 testovacích sad.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We focus on the task of unsupervised lemmatization, i.e. grouping together inflected forms of one word under one label (a lemma) without the use of annotated training data. We propose to perform agglomerative clustering of word forms with a novel distance measure. Our distance measure is based on the observation that inflections of the same word tend to be similar both string-wise and in meaning. We therefore combine word embedding cosine similarity, serving as a proxy to the meaning similarity, with Jaro-Winkler edit distance. Our experiments on 23 languages show our approach to be promising, surpassing the baseline on 23 of the 28 evaluation datasets.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení výzkumu automatického vyhodnocování koherence v češtině s pomocí naanotovaných velkých dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Introduction of the research of automatic coherence evaluation in Czech using unlabeled large data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Využíváme velkých neanotovaných dat (n-gramový model, odhady hustoty featur) ke zlepšení kvality automatického hodnocení koherence textů v češtině. Spolu s novými featurami z různých jazykových rovin přispělo využití neanotovaných dat k signifikantnímu zlepšení výsledků systému.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We use large unlabeled data (n-gram model, density estimates of features) to improve quality of surface coherence evaluation in Czech texts. Along with new features across layers of language description, the additions significantly improve the performance of the system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Počítačový nástroj Evaluátor diskurzu pro začátečníky (EVALD 4.0 pro začátečníky) slouží k automatickému hodnocení povrchové koherence (koheze) textů v češtině psaných začínajícími nerodilými mluvčími češtiny. Software hodnotí úroveň předloženého textu z hlediska jeho povrchové výstavby (zohledňuje např. frekvenci a rozmanitost užitých spojovacích prostředků, bohatost slovní zásoby, koreferenční vztahy, aktuální členění, obtížnost čtení apod.) a vyhodnocuje, zda text dostahuje alespoň základní úrovně A1.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Evaluator of Discourse for Beginners (EVALD 4.0 for Beginners) is a software that serves for automatic evaluation of surface coherence (cohesion) in Czech texts written by non-native speakers of Czech. Software evaluates the level of the given text from the perspective of its surface coherence (i.e. it takes into consideration, e.g., the frequency and diversity of connectives, richness of vocabulary, coreference realations, topic-focus articulation, readability etc.), and reports whether the text reaches the basic A1 level.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Počítačový nástroj Evaluátor diskurzu pro cizince (EVALD 4.0 pro cizince) slouží k automatickému hodnocení povrchové koherence (koheze) textů v češtině psaných nerodilými mluvčími češtiny. Software hodnotí úroveň předloženého textu z hlediska jeho povrchové výstavby (zohledňuje např. frekvenci a rozmanitost užitých spojovacích prostředků, bohatost slovní zásoby, koreferenční vztahy, aktuální členění apod.). Nová verze (4.0) přidává množinu rysů zaměřených především na obtížnost čtení textu a nově také využívá velká neanotovaná data.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Evaluator of Discourse for Foreigners (EVALD 4.0 for Foreigners) is a software that serves for automatic evaluation of surface coherence (cohesion) in Czech texts written by non-native speakers of Czech. Software evaluates the level of the given text from the perspective of its surface coherence (i.e. it takes into consideration, e.g., the frequency and diversity of connectives, richness of vocabulary, coreference relations, topic-focus articulation etc.). The new version (4.0) adds a set of features related to readability of the text and also newly uses large unlabeled data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Počítačový nástroj Evaluátor diskurzu (EVALD 4.0) slouží k automatickému hodnocení povrchové koherence (koheze) textů v češtině psaných rodilými mluvčími češtiny. Software tedy hodnotí (známkuje) úroveň předloženého textu z hlediska jeho povrchové výstavby (zohledňuje např. frekvenci a rozmanitost spojovacích prostředků, bohatost slovní zásoby, koreferenční vztahy, aktuální členění apod.). Nová verze (4.0) přidává množinu rysů zaměřených především na obtížnost čtení textu a nově také využívá velká neanotovaná data.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Evaluator of Discourse (EVALD 4.0) is a software that serves for automatic evaluation of surface coherence (cohesion) in Czech texts written by native speakers of Czech. Software evaluates the level of the given text from the perspective of its surface coherence (i.e. it takes into consideration, e.g., the frequency and diversity of connectives, richness of vocabulary, coreference relations, topic-focus articulation etc.). The new version (4.0) adds a set of features related to readability of the text and also newly uses large unlabeled data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku popisujeme naše systémy předložené v rámci soutěže Building Educational Applications (BEA) 2019 Shared Task (Bryant a kol., 2019). Zúčastnili jsme se všech tří variant. Naše modely jsou systémy NMT založené na architektuře Transformer, který vylepšujeme začleněním několika vylepšení: dropout celých zdrojových a cílových slov, vážení cílových podslov, průměrování modelu a použití trénovaného modelu iterativním způsobem. Systém v Restricted Track je trénován na poskytnutých korpusech s nadměrně zesílenými "čistšími" větami a na testovací sadě dosahuje skóre 59,39 F0,5. Systém v režimu nízkých zdrojů je trénován z historie revizí Wikipedie a dosahuje skóre 44,13 F0,5. V neomezeném režimu jsme dotrénováním systému z režimu nízkých zdrojů dosáhli 64.55 F0.5 skóre a obsadili tak třetí místo.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we describe our systems submitted to the Building Educational Applications (BEA) 2019 Shared Task (Bryant et al., 2019). We participated in all three tracks. Our models are NMT systems based on the Transformer model, which we improve by incorporating several enhancements: applying dropout to whole source and target words, weighting target subwords, averaging model checkpoints, and using the trained model iteratively for correcting the intermediate translations. The system in the Restricted Track is trained on the provided corpora with oversampled "cleaner" sentences and reaches 59.39 F0.5 score on the test set. The system in the Low-Resource Track is trained from Wikipedia revision histories and reaches 44.13 F0.5 score. Finally, we finetune the system from the Low-Resource Track on restricted data and achieve 64.55 F0.5 score, placing third in the Unrestricted Track.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Automatická oprava gramatiky v angličtině je dlouho studovaný problém s mnoha existujícími systémy a datovými zdroji. Výzkum oprav chyb v jiných jazycích je však pouze omezený. V této práci představujeme nový dataset AKCES-GEC pro gramatickou korekci chyb pro češtinu. Dále provádíme experimenty na češtině, němčině a ruštině a ukazujeme, že při využití syntetického paralelního korpusu může model neuronového strojového překladu Transformer dosáhnout na těchto datasetech nejlepších známých výsledků. AKCES-GEC vychází pod licencí CC BY-NC-SA 4.0 na adrese https://hdl.handle.net/11234/1-3057 a zdrojový kód modelu GEC je k dispozici na adrese https://github.com/ufal/low-resource-gec-wnut2019.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Grammatical error correction in English is a long studied problem with many existing systems and datasets. However, there has been only a limited research on error correction of other languages. In this paper, we present a new dataset AKCES-GEC on grammatical error correction for Czech. We then make experiments on Czech, German and Russian and show that when utilizing synthetic parallel corpus, Transformer neural machine translation model can reach new state-of-the-art results on these datasets. AKCES-GEC is published under CC BY-NC-SA 4.0 license at https://hdl.handle.net/11234/1-3057 and the source code of the GEC model is available at https://github.com/ufal/low-resource-gec-wnut2019.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje výsledky sdílených úkolů z 6. workshopu o asijském překladu (WAT2019), včetně dílčích úkolů Ja↔En, Ja↔Zh pro překlad vědeckých článků, Ja↔En, Ja↔Ko, Ja↔En pro překlad patentů, Hi↔En, My↔En, Km↔En, Ta↔En pro překlad smíšených doménových dílčích úkolů, Ru↔Ja komentář pro překlad zpráv a En pro multimodální překlad. V rámci programu WAT2019 se sdílených úkolů účastnilo 25 týmů.
Obdrželi jsme také 10 písemných podání k výzkumu, z nichž 71 bylo přijato. Automatickému hodnotícímu serveru bylo předloženo asi 400 výsledků překladu a vybraná podání byla ručně vyhodnocena.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the
shared tasks from the 6th workshop on
Asian translation (WAT2019) including
Ja↔En, Ja↔Zh scientific paper translation subtasks, Ja↔En, Ja↔Ko, Ja↔En
patent translation subtasks, Hi↔En,
My↔En, Km↔En, Ta↔En mixed domain
subtasks, Ru↔Ja news commentary
translation task, and En→Hi multi-modal
translation task. For the WAT2019, 25
teams participated in the shared tasks.
We also received 10 research paper submissions out of which 71 were accepted.
About 400 translation results were submitted to the automatic evaluation server,
and selected submissions were manually
evaluated.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento výzkum se zaměřuje na různé funkce diskurzního konektivu "a" v anotovaných zapsaných mluvených textech TED Talks v angličtině a litevštině. Anotace litevských textů bylo zahájena teprve nedávno, proto tvoří litevské texty ve srovnání prozatím menší vzorek. Výsledky výzkumu ukazují, že výraz "and" a jeho ekvivalenty v litevštině vyjadřují řadu vágních významů, včetně prostého přidávání, řízení a strukturace diskurzu</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present research focuses on the multiple functions performed by the discourse marker "and" in annotated spoken-like texts of TED Talks in English and Lithuanian. The annotation of TED Talks in Lithuanian has started only recently, which results in the limitation regarding the quantity of annotated texts. The research findings show that and and its Lithuanian counterparts perform multiple fuzzy functions, including the function of addition, discourse management and structuring discourse.
It was also established that the most frequent variants of translation of the discourse marker and are those provided by bilingual English–Lithuanian dictionaries and that translators choose paraphrases to convey the pragmatics of the spoken-like texts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Využití modelů řízených daty pro sumarizaci textu nebo podobné úlohy se v posledních letech stává velmi běžným. Zatímco většina studií hlásí pouze základní přesnost, není nic známo o schopnosti zmíněných modelů se zlepšit, jsou-li trénovány na větších datech. V tomto příspěvku definujeme a navrhujeme tři metriky efektivity dat: efektivita úspěšnosti dat, časové nedostatečnosti dat a celkové účinnosti dat. Navrhujeme také jednoduché schema využívající těchto metod a využívající je pro ucelenější hodnocení populárních metod sumarizace textů a generování nadpisů. Pro druhou z úloh zpracováváme a uvol%nujeme rozsáhlou kolekci 35 miliónů párů abstrakt-název vědeckých článků. Naše výsledky odhalují, že mezi tetovanými metodami je Transformer nejúčinnější pro obě úlohy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Using data-driven models for solving text summarization or similar tasks has become very common in the last years. Yet most of the studies report basic accuracy scores only, and nothing is known about the ability of the proposed models to improve when trained on more data. In this paper, we define and propose three data efficiency metrics: data score efficiency, data time deficiency and overall data efficiency. We also propose a simple scheme that uses those metrics and apply it for a more comprehensive evaluation of popular methods on text summarization and title generation tasks. For the latter task, we process and release a huge collection of 35 million abstract-title pairs from scientific articles. Our results reveal that among the tested models, the Transformer is the most efficient on both tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Výzkum získávání klíčových slov probíhá od devadesátých let, ale pokročilé přístupy jako kodér-dekodér a učení z posloupností byly objeveny až v poslední době. Více než tucet abstrakčních metod poskytujících smysluplná klíčová slova a dosahujících aktuálních výsledků bylo navrženo v posledních třech letech. Testujeme zde různé aspekty metod generování klíčových slov.
Soustředíme se zejména na metody z poslední doby založené na neuronových sítích. Zvláštní důraz klademe na mechanismy řízení přesnosti těch posledně jmenovaných. Byla vytvořena velmi rozsáhlá kolekce metadat o vědeckých článcích a uvolněna vědecké komunitě. Také jsme prezentovali různé vzory výzkumu generování klíčových slov a sumarizace textu a trendy posledních dvou dekád.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Extractive keyphrase generation research has been around since the nineties, but the more advanced abstractive approach based on the encoder-decoder framework and sequence-to-sequence learning has been explored only recently. In fact, more than a dozen of abstractive methods have been proposed in the last three years, producing meaningful keyphrases and achieving state-of-the-art scores. In this survey, we examine various aspects of the extractive keyphrase generation methods and focus mostly on the more recent abstractive methods that are based on neural networks. We pay particular attention to the mechanisms that have driven the perfection of the later. A huge collection of scientific article metadata and the corresponding keyphrases is created and released for the research community. We also present various keyphrase generation and text summarization research patterns and trends of the last two decades.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Klíčová slova, která svým vědeckým článkům přiřadili jejich autoři jsou nepostradatelná pro rozpoznání obsahu a témat dané článku. Většina
řízených i neřízených metod generování klíčových slov není schopna přiřazovat termíny, které to dobře vystihují, ale nevyskytují se v textu. V tomto příspěvku zkoumáme možnost klíčových slov coby shrnutím názvu práce a abstraktu. Nejdříve sesbíráme, zpracujme a vydáme velkou sadu metadat vědeckých článků čítajících 2,2 milionu záznamů. Pak vyzkoušíme populární neurální architektury pro sumarizaci textů. Na rozdíl od pokročilých metod hlubokého učení, velkých objemů dat a mnoha
dní výpočtů naše systematické vyhodnocování na čtyřech testovacích sadách dat ukázalo, že zkoumané metody sumarizace textu nemohou vytvořit
lepší klíčová slova než jednoduché neřízené či řízené metody.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Authors' keyphrases assigned to scientific articles are essential for recognizing content and topic aspects. Most of the proposed supervised and unsupervised methods for keyphrase generation are unable to produce terms that are valuable but do not appear in the text. In this paper, we explore the possibility of considering the keyphrase string as an abstractive summary of the title and the abstract. First, we collect, process and release a large dataset of scientific paper metadata that contains 2.2 million records. Then we experiment with popular text summarization neural architectures. Despite using advanced deep learning models, large quantities of training data and many days of computation, our systematic evaluation on four test datasets reveals that the explored text summarization methods could not produce better keyphrases than the much simpler unsupervised methods or the existing supervised ones.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V oblasti online komunikace, komerce a překladů, se analýza polarity sentimentu textů napsaných v různých přirozených jazycích stává zásadní. Zatímco pro angličtinu je k dispozici mnoho příspěvků a zdrojů, "menší" jazyky, jako je čeština, se zatím netěší větší pozornosti. V tomto
přehledu zkoumáme efektivitu mnoha algoritmů strojového učení pro analýzu sentimentu příspěvků na českém Facebooku a recenzí různých produktů. Sepíšeme sady optimálních hodnot parametrů pro každý algoritmus a ohodnocení v obou datasetech. Nakonec zaznamenáme, že metoda podpůrných vektorů je nejlepším klasifikátorem a snahy dále zlepšit výkon pomocí baggingu, boostingu, či hlasovacích schemat selhaly.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the area of online communication, commerce and transactions, analyzing sentiment polarity of texts written in various natural languages has become crucial. While there have been a lot of contributions in resources and studies for the English language, "smaller" languages like Czech have not received much attention. In this survey, we explore the effectiveness of many existing machine learning algorithms for sentiment analysis of Czech Facebook posts and product reviews. We report the sets of optimal parameter values for each algorithm and the scores in both datasets. We finally observe that support vector machines are the best classifier and efforts to increase performance even more with bagging, boosting or voting ensemble schemes fail to do so.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Morfologická segmentace slov je proces rozdělování slova na menší jednotky nazývané morfémy, což je úloha, která je obtížná zejména v případě morfologicky bohatých nebo polysyntetických jazyků. V této práci navrhujeme pro řešení této úlohy několik rekurzivních neuronových sítí a dalších přístupů založených na strojovém učení. Jako trénovací data používáme ručně segmentované slovníky. K vyhodnocení vlivu velikosti slovníku na kvalitu segmentace používáme rozsáhlý, ručně anotovaný segmentační slovník perštiny. Dále používáme menší segmentační slovníky pro češtinu a finštinu. Na těchto jazycích zkoumáme rovněž vliv nastavení hyperparametrů a zvolených rekurentních architektur.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Morphological segmentation of words is the process of dividing a word into smaller units called morphemes; it is tricky especially when a morphologically rich or polysynthetic language is under question. In this work, we designed and evaluated several Recurrent Neural Network (RNN) based models as well as various other machine learning based approaches for the morphological segmentation task. We trained our models using annotated segmentation lexicons. To evaluate the effect of the training data size on our models, we decided to create a large hand-annotated morphologically segmented corpus of Persian words, which is, to the best of our knowledge, the first and the only segmentation lexicon for the Persian language. In the experimental phase, using the hand-annotated Persian lexicon and two smaller similar lexicons for Czech and Finnish languages, we evaluated the effect of the training data size, different hyper-parameters settings as well as different RNN-based models.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku studujeme abstraktiví sumarizaci videí bez doménového omezení. Na rozdíl od tradiční sumarizace zpravodajských textů není cílem "komprimovat" textové informace, ale spíše poskytnout plynulé textové shrnutí informací, které byly shromážděny z různých zdrojových modalit, v našem případě videozáznamů a audio přepisů (nebo textu). Ukazujeme, jak vícezdrojový model sekvenčního učení s hierarchickým mechanismem pozorností dokáže integrovat informace z různých modalit do uceleného výstupu, porovnáváme různé modely trénované s různými modalitami a prezentuje pilotní experimenty na How2 korpusu instruktážních videí. Navrhujeme také novou hodnotící metriku (Conent F1) pro abstraktivní sumarizaci, která měří spíše sémantickou adekvátnost než plynulost, kterou naopak zachcují tradiční metriky jako jako ROUGE a BLEU.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we study abstractive summarization for open-domain videos. Unlike the traditional text news summarization, the goal is less to "compress" text information but rather to provide a fluent textual summary of information that has been collected and fused from different source modalities, in our case video and audio transcripts (or text). We show how a multi-source sequence-to-sequence model with hierarchical attention can integrate information from different modalities into a coherent output, compare various models trained with different modalities and present pilot experiments on the How2 corpus of instructional videos. We also propose a new evaluation metric (Content F1) for abstractive summarization task that measures semantic adequacy rather than fluency of the summaries, which is covered by metrics like ROUGE and BLEU.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje experimentální výzkum srovnávající, jak jsou texty psané nerodilými mluvčími češtiny hodnoceny softwarovou aplikací EVALD a učiteli češtiny jako cizího jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce an experimental probe comparing how texts written by non-native speakers of Czech are evaluated by a software application EVALD and by teachers of Czech as a foreign language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme první kompletní hlasový dialogový systém řízený multidimenzionálním statistickým dialogovým manažerem. Tento framework prokazatelně významně snižuje potřebu dat využitím doménově nezávislých dimenzí, jako jsou společenské konvence nebo zpětná vazba, které (jak ukazujeme) lze přenášet mezi doménami. V tomto článku provádíme uživatelskou sudii a ukazujeme, že výkon multidimenzionálního systému, který lze adaptovat ze zdrojové domény, je ekvivalentní výkonu jednodimenzionální baseline, kterou je třeba natrénovat od nuly.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the first complete spoken dialogue system driven by a multiimensional statistical dialogue manager. This framework has been shown to substantially reduce data needs by leveraging domain-independent dimensions, such as social obligations or feedback, which (as we show) can be transferred between domains. In this paper, we conduct a user study and show that the performance of a multi-dimensional system, which can be adapted from a source domain, is equivalent to that of a one-dimensional baseline, which can only be trained from scratch.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška představuje výsledky anotace implicitních vztahů v češtině na základě korpusu PDiT-EDA 1.0. Zaměřuje se na distribuci implicitních a explicitních vztahů, jejich sémantiku, přítomnost větné negace a vztah implicitnosti k textovému žánru.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The lecture presents the results of the annotation of implicit relations in Czech based on the PDiT-EDA 1.0 corpus. It focuses on the distribution of implicit and explicit relations, their semantics, the presence of sentence negation and the relation of implicitness to the text genre.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek podává základní přehled o implicitních diskurzních vztazích v korpusu PDiT-EDA 1.0. Zabývá se vztahem implicitnosti k následujícím jazykovým faktorům: sémantika diskurzního vztahu, přítomnost větné negace, textový žánr.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper gives a basic overview of implicit discourse relations in the PDiT-EDA 1.0 corpus. It deals with the relation of implicitness to the following language factors: semantics of discourse relation, presence of sentence negation, text genre.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Analyzujeme některé faktory ovlivňující explicitnost/implicitnost diskurzních vztahů, např. žánr textu, sémantický typ diskurzního vztahu a přítomnost negace v diskurzních argumentech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We analyze some of the factors influencing the explicitness/implicitness of discourse relations, such as the text genre, semantic type of the discourse relation and the presence of negation in discourse arguments.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Recenze knihy Syntax mluvené češtiny, jejímiž editory jsou Jana Hoffmannová, Jiří Homoláč a Kamila Mrázková.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Review of Syntax of Spoken Czech, a book edited by Jana Hoffmannová, Jiří Homoláč and Kamila Mrázková.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Koherence textů může být zajištěna řadou jazykových prostředků, např. diskurzními konektory, anaforickým odkazováním, lexikálním opakováním apod. Představujeme výzkum nejčastějších chyb v koherenci žákovských textů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Coherence may be manifested through various language means, e.g. by discourse connectives, anaphoric devices, lexical repetition etc. We present research of the most common errors in coherence occurring in learners’ essays.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládáme popis našeho příspěvku do soutěže CoNLL 2019, Cross-Framework Meaning Representation Parsing (MRP 2019). Navržená architektura je naším prvním pokusem o sémantický parsing v rámci UDPipe 2.0, nástroje pro lemmatizaci, POS tagging a závislostní parsing.

Pro MRP 2019, který zahrnuje pět formálně a lingvisticky rozdílných přístupů k reprezentaci významu (DM, PSD, EDS, UCCA a AMR), navrhujeme uniformní, jazykově a formálně agnostickou architekturu založenou na transformaci grafů pomocí umělých neuronových sítí. Bez jakékoli znalosti grafové struktury, a specificky bez jakýchkoli lingvisticky nebo formálně motivovaných klasifikačních rysů náš systém implicitně modeluje reprezentaci významu v grafu.

Po opravě člověkem způsobené chyby (použili jsme nesprávnou verzi poskytnutých analýz testovacích dat) se náš příspěvek umístil na třetím místě v soutěžním hodnocení. Zdrojový ḱód našeho systému je dostupný na adrese https://github.
com/ufal/mrpipe-conll2019.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a system description of our contribution to the CoNLL 2019 shared task, Cross-
Framework Meaning Representation Parsing (MRP 2019). The proposed architecture is our first attempt towards a semantic parsing extension of the UDPipe 2.0, a lemmatization, POS tagging and dependency parsing pipeline.

For the MRP 2019, which features five formally and linguistically different approaches to meaning representation (DM, PSD, EDS, UCCA and AMR), we propose a uniform, language and framework agnostic graph-to-graph neural network architecture. Without any knowledge about the graph structure, and specifically without any linguistically or framework motivated features, our system implicitly models the meaning representation graphs.

After fixing a human error (we used earlier incorrect version of provided test set analyses),
our submission would score third in the competition evaluation. The source code of our
system is available at https://github.
com/ufal/mrpipe-conll2019.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nedávno byly navrženy kontextové embeddingy, které vhodně zachycují význam slova v závislosti na kontextu. V tomto příspěvku vyhodnocujeme dvě metody pro výpočet takových embeddingů, BERT a Flair, na čtyřech úlohách zpracování přirozeného jazyka v češtině: značkování slovních druhů (POS tagging), lemmetizace, závislostní parsing a rozpoznávání pojmenovaných entit. První tři úlohy jsou vyhodnoceny na dvou korpusech: Pražský závislostní korpus 3.5 a Universal Dependencies 2.3. Rozpoznávání pojmenovaných entit je vyhodnoceno na Českém korpusu pojmenovaných entit (Czech Named Entity Corpus) 1.1 a 2.0. Publikujeme state-of-the-art výsledky ve všech výše zmíněných úlohách na všech korpusech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Contextualized embeddings, which capture appropriate word meaning depending on context, have recently been proposed. We evaluate two methods for precomputing such embeddings, BERT and Flair, on four Czech text processing tasks: part-of-speech (POS) tagging, lemmatization, dependency parsing and named entity recognition (NER). The first three tasks, POS tagging, lemmatization and dependency parsing, are evaluated on two corpora: the Prague Dependency Treebank 3.5 and the Universal Dependencies 2.3. The named entity
recognition (NER) is evaluated on the Czech Named Entity Corpus 1.1 and 2.0. We report state-of-the-art results for the above mentioned tasks and corpora.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme rozsáhlé hodnocení tří nedávno navržených metod pro kontextualizované embeddiny na 89 korpusech v 54 jazycích projektu Universal Dependencies 2.3 ve třech úkolech: POS tagging, lemmatizace a závislostní analýza. Využitím BERT, Flair a ELMo jako předtrénovaných embeddingů do systému UDPipe 2.0, jednoho z vítězů CoNLL 2018 Shared Task a celkového vítěze EPE 2018, představujeme porovnání těchto tří kontextualizovaných metod, word2vec předtrénovaných embedingů a trénovatelných embedingů založených na znacích slov. Popsané metody dosahují nejlepších známých výsledků na UD 2.2 v porovnání s výsledky na CoNLL 2018 Shared Task.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an extensive evaluation of three recently proposed methods for contextualized embeddings on 89 corpora in 54 languages of the Universal Dependencies 2.3 in three tasks: POS tagging, lemmatization, and dependency parsing. Employing the BERT, Flair and ELMo as pretrained embedding inputs in a strong baseline of UDPipe 2.0, one of the best-performing systems of the CoNLL 2018 Shared Task and an overall winner of the EPE 2018, we present a one-to-one comparison of the three contextualized word embedding methods, as well as a comparison with word2vec-like pretrained embeddings and with end-to-end character-level word embeddings. We report state-of-the-art results in all three tasks as compared to results on UD 2.2 in the CoNLL 2018 Shared Task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme náš příspěvek k shared tasku SIGMORPHON 2019: Crosslingualita a kontext v morfologii, úkol 2: kontextová morfologická analýza a lemmatizace.

Odevzdali jsme modifikaci UDPipe 2.0, jednoho z výherního systému CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies a celkového vítěze The 2018 Shared Task on Extrinsic Parser Evaluation. Jako první vylepšení používáme předtrénované kontextualizované embeddingy (BERT) jako další vstupy do sítě, za druhé používáme jednotlivé morfologické vlastnosti jako regularizaci a nakonec slučujeme vybrané korpusy stejného jazyka.

V lemmatizačním úkolu náš systém výrazně převyšuje všechny ostatní systémy s přesností lemmatizace 95,78 (druhý nejlepší byl 95,00, třetí 94,46). V morfologické analýze se náš systém umístil těsně druhý: přesnost naší morfologické analýzy byla 93,19, vítězný systém měl 93,23.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our contribution to the SIGMORPHON 2019 Shared Task: Crosslinguality and Context in Morphology, Task 2: contextual morphological analysis and lemmatization.

We submitted a modification of the UDPipe 2.0, one of best-performing systems of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies and an overall winner of the The 2018 Shared Task on Extrinsic Parser Evaluation. As our first improvement, we use the pretrained contextualized embeddings (BERT) as additional inputs to the network; secondly, we use individual morphological features as regularization; and finally, we merge the selected corpora of the same language.

In the lemmatization task, our system exceeds all the submitted systems by a wide margin with lemmatization accuracy 95.78 (second best was 95.00, third 94.46). In the morphological analysis, our system placed tightly second: our morphological analysis accuracy was 93.19, the winning system’s 93.23.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Derivations (UDer) je sada lexikalních sítí zachycujících slovotvorné, zvl. derivační vztahy jednotlivých jazyků, tyto sítě byly harmonizovány do jednotného formátu. Stávající verze UDer v0.5 obsahuje 11 harmonizovaných zdrojů pokrývajících 11 různých jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Derivations (UDer) is a collection of harmonized lexical networks capturing word-formation, especially derivational relations, in a cross-linguistically consistent annotation scheme for many languages. The annotation scheme is based on a rooted tree data structure, in which nodes correspond to lexemes, while edges represent derivational relations or compounding.
The current version of the UDer collection contains eleven harmonized resources covering eleven different languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem tohoto článku je otevřít diskusi o harmonizaci existujících datových zdrojů zabývajících se derivační morfologií. Představujeme nově vytvořený soubor jedenácti harmonizovaných zdrojů pojmenovaný „Universal Derivations“ (zjevně inspirovaný úspěchem iniciativy Universal Dependencies mezi syntakticky anotovanými korpusy) a harmonizační proces, kterým jsme tyto zdroje sjednotili do stejného anotačního schématu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of this paper is to open a discussion on harmonization of existing data resources related
to derivational morphology. We present a newly assembled collection of eleven harmonized
resources named “Universal Derivations” (clearly being inspired by the success story of the
Universal Dependencies initiative in treebanking), as well as the harmonization process that
brings the individual resources under a unified annotation scheme.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje příspěvek týmu Univerzity Karlovy a Maltské univerzity do soutěže CoNLL-SIGMORPHON 2019 v morfologické analýze a lematizaci v kontextu. Předkládáme lematizační model postavený na dříve publikované metodě neuronových převodníků (Makarov 2018; Aharoni a Goldberg 2017). Klíčovým rozdílem je, že náš model transformuje celý slovní tvar každého kmene, místo aby ho konzumoval znak po znaku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the submission by the Charles University-University of Malta team to
the CoNLL--SIGMORPHON 2019 Shared Task on Morphological Analysis and Lemmatization in context. We present a lemmatization model based on previous work on neural transducers \cite{makarov2018neural,aharoni-goldberg-2017-morphological}. The key difference is that our model transform the whole word form in every stem, instead of consuming it character by character. We propose a merging strategy inspired by Byte-Pair-Encoding that reduces the space of valid operations by merging frequent adjacent operations. The resulting operations not only encode the action/s to be performed but the relative position in the word token and
how characters need to be transformed.

Our morphological tagger is a vanilla biLSTM tagger that operates over operation representations, encoding operations and words in a hierarchical manner.

Even though relative performance according to metrics is below the baseline, experiments show that our models capture important associations between interpretable operation labels and fine-grained morpho-syntax labels.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Digitální sbírky audiovizuálních orálněhistorických rozhovorů (OHI) mohou být pojaty jako zvláštní kategorie velkých textových korpusů, které vyžadují implementaci metod vzdáleného čtení. Po digitální revoluci ve výzkumu orální historie (např. Thomson, 2007; Boyd &amp; Larson, 2014) dnes mnoho institucí poskytuje přístup k mnoha digitálním kolekcím najednou, což uživatelům a výzkumníkům poskytuje možnost zkombinovat a porovnat například několik OHI provedených se stejnou osobou v různorodém situačním a sociohistorickém rámci jako součást různých projektů. To klade před takové přístupové instituce řadu otázek: Jak můžeme uživatelům co nejvíce usnadnit práci s několika samostatnými digitálními archivy OHI? Které výpočetní metody mohou usnadnit vzdálené čtení a efektivní využití rozsáhlého sběru audiovizuálních materiálů OHI? A jaké jsou přetrvávající problémy - technické, metodologické, etické atd. -, které musí instituce vyřešit?
Při nastínění odpovědí na tyto otázky budeme v naší prezentaci diskutovat o našich zkušenostech a technologických řešeních z Centra vizuální historie Malach (CVHM) na Univerzitě Karlově. 
Současná situace, kdy je zpřístupněno několik nesourodých sbírek najednou, představuje několik výzev na úrovni zlepšení účinnosti metod vyhledávání a textové analýzy korpusů. Na jedné straně uživatelé-výzkumníci potřebují úplný přehled metadat, aby mohli vytvářet příslušné soubory dat, objevovat duplicitní případy a analyzovat profily sbírek OHI. Zároveň vyvstává otázka prohledávání obsahu napříč celým korpusem. Pro tyto účely vyvinulo CVHM interní rozhraní, které integruje několik kolekcí najednou a poskytuje řešení obou těchto otázek. Díky tomuto řešení mohou uživatelé snadno získávat relevantní výsledky na několika úrovních bez nutnosti samostatného přístupu k jednotlivým sbírkám nebo k OHI.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Digital collections of audiovisual oral history interviews (OHIs) can be conceived as a specific category of large text corpora that require implementation of distant reading methods. Following the digital revolution in oral history research (e.g. Thomson, 2007; Boyd &amp; Larson, 2014), many institutions now provide access to divergent digital collections at once, which provides the users and researchers with an opportunity to combine and compare, for example, several OHIs conducted with the same person in varied situational and socio-historical framework as part of various "process-generated" (Freund, 2009) oral history projects. This constitutes a pertinent issue for such access institutions: How can we make it as easy as possible for users to work with several separate digital archives of OHIs? Which computational methods can facilitate distant reading and efficient use of large collection of audiovisual OHI materials? And what are the persistent problems -- technical, methodological, ethical, etc. -- that have to be solved by the institutions of multiple access?
In outlining answers to these questions, in our presentation, we will discuss our experience and technological solution from the Malach Center for Visual History (CVHM) at the Charles University in Prague (see Mlynář, 2015). Over the last decade, CVHM has been providing access for students, researchers and general public to several established collections of OHIs. Since 2009, CVHM is an access point to the USC Shoah Foundation's Visual History Archive (VHA), which is an ever-growing collection of interviews with witnesses and survivors of genocides, especially the Holocaust. At the present moment, the VHA contains nearly 56,000 audiovisual recordings of OHIs in more than 40 languages (see Gustman et al., 2002). Since 2018, the Fortunoff Video Archive for Holocaust Testimonies of the Yale University Library with more than 4,400 audiovisual recordings of OHIs is also available at CVHM. In addition, users in CVHM can work with smaller collections lacking an integrated user interface such as the Refugee Voices archive (150 English interviews), and a small portion of interviews from the Jewish Holocaust Center in Melbourne (15 interviews with people of Czechoslovak origin).
The present situation of hosting several disparate collections at once poses several challenges on the level of improving effectivity of search and textual corpora analysis methods. On one hand, users-researchers are in need of a complete metadata overview, in order to generate relevant datasets, discover duplicate cases and analyze OHI collections’ profiles. On the other hand, the question of performing content-based queries across the whole corpus is imminent. For these purposes, CVHM developed an in-house interface integrating several collection at once providing solutions to both of these issues. Besides providing general access to metadata, automated speech recognition based transcripts (generated by AMALACH algorithm, post-edited) serve simultaneously as textual data for the multilingual cross-corpus search in English, Czech and Slovak and searchable automatically generated keywords dataset (KER - Keyword Extractor provided by LINDAT/CLARIN). Owing to this approach, users are able to easily acquire relevant results on several levels without the necessity of separately accessing the individual collections or OHIs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Užitečnost jazykových anotací v překladu do neurálních strojů byla zřejmě prokázána v minulých pracích. Pokusy se však omezovaly na opakující se sekvenční architektury a relativně malé nastavení dat.

Zaměřujeme se na nejmodernější model Transformeru a používáme srovnatelně větší korporáty. Konkrétně se snažíme podporovat znalosti syntaxe na zdrojové straně pomocí víceúkolového učení buď pomocí jednoduchých technik manipulace s daty, nebo pomocí speciální modelové komponenty. Konkrétně jednoho cvičíme
Transformer se soustředí na vytvoření stromu závislosti na straně zdroje.

Celkově naše výsledky zpochybňují užitečnost víceúkolových sestav s jazykovými informacemi. Techniky manipulace s daty, doporučované v předchozích dílech, se v nastavení velkých dat ukazují jako neúčinné.

Zacházení se sebepozorností jako se závislostmi se zdá mnohem slibnější: pomáhá při překladu a odhaluje, že model Transformer dokáže velmi snadno uchopit syntaktickou strukturu.
Důležitým, ale kuriózním výsledkem však je, že identického zisku se dosáhne použitím triviálních ,,lineárních stromů`` namísto skutečných závislostí. Přínos tedy nemusí vyplývat z přidaných jazykových znalostí, ale z nějakého jednoduššího regularizačního efektu, který jsme navodili u matricí, které se věnují samy sobě.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The utility of linguistic annotation in neural machine translation seemed to had been established in past papers. The experiments were however limited to recurrent sequence-to-sequence architectures and relatively small data settings.

We focus on the state-of-the-art Transformer model and use comparably larger corpora. Specifically, we try to promote the knowledge of source-side syntax using multi-task learning either through simple data manipulation techniques or through a dedicated model component. In particular, we train one
of Transformer attention heads to produce source-side dependency tree.

Overall, our results cast some doubt on the utility of multi-task setups with linguistic information. The data manipulation techniques, recommended in previous works, prove ineffective in large data settings.

The treatment of self-attention as dependencies seems much more promising: it helps in translation and reveals that Transformer model can very easily grasp the syntactic structure.
An important but curious result is, however, that identical gains are obtained by using trivial ``linear trees'' instead of true dependencies. The reason for the gain thus may not be coming from the added linguistic knowledge but from some simpler regularizing effect we induced on self-attention matrices.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek přináší lingvistický i technický podklad pro vývoj diskurzního parseru pro češtinu. Zaměřuje se na diskurzní vztahy  mezi nesousedními segmenty textu signalizované (většinou) anaforickými konektory.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper is a linguistic as well as technical survey for the development of a shallow discourse parser for Czech. It focuses on long-distance discourse relations signalled by (mostly) anaphoric discourse connectives. Proceeding from the division of connectives on “structural” and “anaphoric” according to their (in)ability to accept distant (non-adjacent) text segments as their left-sided arguments, and taking into account results of related analyses on English data in the framework of the Penn Discourse Treebank, we analyze a large amount of language data in Czech. We benefit from the multilayer manual annotation of various language aspects from morphology to discourse, coreference and bridging relations in the Prague Dependency Treebank 3.0. We describe the linguistic parameters of long-distance discourse relations in Czechin connection with their anchoring connective, and suggest possible ways of their detection. Our empirical research also outlines some theoretical consequences for the underlying assumptions in discourse analysis and parsing, e.g. the risk of relying too much on different (language-specific?) part-of-speech categorizations of connectives or the different perspectives in shallow and global discourse analyses (the minimality principle vs. higher text structure).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku popisujeme překladový systém CUNI pro úlohu neřízeného strojového překladu novinových textů na ACL 2019 Fourth Conference on Machine Translation (WMT19).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we describe the CUNI translation system used for the unsupervised news shared task of the ACL 2019 Fourth Conference on Machine Translation (WMT19). We follow the strategy of Artetxe ae at. (2018b), creating a seed phrase-based system where the phrase table is initialized from cross-lingual embedding mappings trained on monolingual data, followed by a neural machine translation system trained on synthetic parallel data. The synthetic corpus was produced from a monolingual corpus by a tuned PBMT model refined through iterative back-translation. We further focus on the handling of named entities, i.e. the part of vocabulary where the cross-lingual embedding mapping suffers most. Our system reaches a BLEU score of 15.3 on the German-Czech WMT19 shared task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku přestavuje náš přísvek do soutěže v robustním strojovém překaladu na konferenci WMT19. Náš základní systém je CUNI Transformer systém trénovaný překlad novinových textů pro WMT18. Kvantitativní výsledky ukazují, že systém CUNI Transformer je již mnohem robustnější základní model založený na LSTM, který poskytli organizátoři soutěže. Kvalitu překladu našeho modelu jsme dále vylepšili vyladěním na zašuměných datech, která byla poskytnuta k soutěži.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our submission to the WMT19 Robustness Task. Our baseline system is the CUNI Transformer system trained for the WMT18 shared task on News Translation. Quantitative results show that the CUNI Transformer system is already far more robust to noisy input than the LSTM-based baseline provided by the task organizers. We further improved the performance of our model by fine-tuning on the in-domain noisy data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tradičně se většina úloh zpracování přirozeného jazyka řeší výhradně uvnitř jazyka, kdy modely spoléhají na distribuční vlastnosti slov. Hluboké učení se svojí schopností učit se vhodné reprezentace vstupních dat umožňuje využití více informací tím, že trénovací signál nepochází pouze z jazyka, ale o i z obrazové modality. Jednou z úloh, které se pokoušejí využít vizuální informace, je multimodální strojový překlad: překlad popisků obrázků, kdy je stále k dispozici původní obrázek, který lze využít jako vstup pro překladač. Tato práce shrnuje metody společného zpracovávání jazykových dat a fotografií s využitím hlubokého učení. Uvádíme přehled metod, které se využívají k řešení multimodálního strojového překladu a popisujeme náš původní příspěvek k řešení této úlohy. Představujeme metody kombinování více vstupů z potenciálně různých modalit v modelech sekvenčního učení založených na rekurentních neuronových sítích a neuronových sítí s mechanismem sebepozornosti. Uvádíme výsledky, kterých jsme dosáhli při řešení multimodálního strojového překladu a dalších úloh souvisejících se strojovým překladem. Na závěr analyzujeme, jak multimodalita ovlivňuje sémantické vlastnosti větných reprezentací, které v sítích vznikají, a jak sémantické vlastnosti reprezentací souvisí s kvalitou překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Traditionally, most natural language processing tasks are solved within the language, relying on distributional properties of words. Representation learning abilities of deep learning recently allowed using additional information source by grounding the representations in the visual modality. One of the tasks that attempt to exploit the visual information is multimodal machine translation: translation of image captions when having access to the original image. The thesis summarizes joint processing of language and real-world images using deep learning. It gives an overview of the state of the art in multimodal machine translation and describes our original contribution to solving this task. We introduce methods of combining multiple inputs of possibly different modalities in recurrent and self-attentive sequence-to-sequence models and show results on multimodal machine
translation and other tasks related to machine translation. Finally, we analyze how the multimodality influences the semantic properties of the sentence representation learned by the networks and how that relates to translation quality</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Většina úloh zpracování přirozeného jazyka se tradičně řeší v rámci jazyka a spoléhá se na distribuční vlastnosti slov. Schopnost modelů hlubokého učení učit se vhodnou reprezentaci vstupních dat nedávno umožnila využít také obrazovou informaci. Jedna z úloh, které se pokoušejí využít vizuální informaci, je multimodální strojový překlad, tj. překlad popisků obrázků, který má k původnímu obrázku.

Přednáška shrnuje metody kombinování více vstupů s možnou různou modalitou v model využívajích rekurentní neuronové sítě a modelu Tranformer a ukáže výsledky multimodálního a vícezdrojového strojového překladu. Nakonec probereme, jak multimodalita ovlivňuje sémantické vlastnosti reprezentace, které se sítě naučily, a jak to souvisí s kvalitou překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Traditionally, most NLP tasks are solved within the language, relying on distributional properties of words. Representation learning abilities of deep learning recently allowed using additional information source by grounding the representations in the visual modality. One of the tasks that attempt to exploit the visual information is multimodal machine translation, translation of image captions when having access to the original image.

The talk will summarize methods of combining multiple inputs of possibly different modality in recurrent and Transformer sequence-to-sequence models and show results on multimodal and multi-source machine translation. Finally, we will discuss how the multimodality influences the semantic properties of the representation learned by the networks and how does that relate to the translation quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Filtry konvolučních neuronových sítí používaných v počítačovém vidění se často vizualizjí jako malé čtevrcové obrázky, které maximalizují odezvu filtru. V tomto abstraktu používáme stejný postup při interpretaci váhových matic v jednoduchých architekturách pro úkoly zpracování přirozeného jazyka. Intepretujeme konvoluční neuronovou síť pro klasifikaci sentimentu jako slovní pravidla. Pomocí těchto pravidel jsme schopni rokonstruovat fungování původního modelu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Filters of convolutional networks used in computer vision are often visualized as image patches that maximize the response of the filter. We use the same approach to interpret weight matrices in simple architectures for natural language processing tasks. We interpret a convolutional network for sentiment classification as word-based rules. Using the rule, we recover the performance of the original model.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek populárně vysvětluje základy fungování umělých neuronových sítí a popisuje, jak fungují modely pro neuronový strojový překlad. Dále ukazuje problémy strojového překladu jako třeba genderové stereotypy, které modely vykazují.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article explains the basics of artificial neural networks and describes model for neural machine translation. Further, it discusses problem of the current models such as handling gender stereotypes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vícejazyčný BERT (mBERT) poskytuje větné reprezentace pro 104 jazyků, které jsou užitečné pro mnoho vícejazyčných úloh. Předchozí práce zkoumala mnohojazyčnost mBERTu s využitím nulového transferového učení na morfologických a syntaktických úkolech. Místo toho se soustředíme na sémantické vlastnosti mBERTu. Ukazujeme, že reprezentace mBERT mohou být rozděleny na jazykově specifickou složku a jazykově neutrální složku a že jazykově neutrální složka je dostatečně obecná, pokud jde o modelování sémantiky, aby umožnila vysoce přesné zarovnání slov a vyhledávání vět, ale zatím není dostatečně dobrá pro obtížnější úkol odhadu kvality MT. Naše práce přináší zajímavé výzvy, které musí být vyřešeny, aby bylo možné sestavit lepší jazykově neutrální reprezentace, zejména u úkolů vyžadujících jazykový přenos sémantiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Multilingual BERT (mBERT) provides sentence representations for 104 languages, which are useful for many multi-lingual tasks. Previous work probed the cross-linguality of mBERT using zero-shot transfer learning on morphological and syntactic tasks. We instead focus on the semantic properties of mBERT. We show that mBERT representations can be split into a language-specific component and a language-neutral component, and that the language-neutral component is sufficiently general in terms of modeling semantics to allow high-accuracy word-alignment and sentence retrieval but is not yet good enough for the more difficult task of MT quality estimation. Our work presents interesting challenges which must be solved to build better language-neutral representations, particularly for tasks requiring linguistic transfer of semantics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Z nedávné literatury vyplývá, že jazykové modelování na velkých datech poskytuje vynikající opakovaně použitelné větné reprezentace. Bylo rovněž prokázáno, že vizuální informace sloužit jako jeden ze způsobů ukotvení větných reprezentací. V tomto článku představujeme metastudii hodnotící kvalitu reprezentace modelů, kde trénovací signál pochází z různých modalit: jazykového modelování, předvídání reprezentace obrázku a textového a multimodálního strojového překladu. Hodnotíme textové a vizuální vlastnosti větných reprezentací na úlohách vyhledávání obrázků a sématické podobnosti textů. Naše experimenty odhalují, že na středně velkých datasetech poskytuje větný protějšek v cílovém jazyce nebo vizuální modalitě mnohem silnější trénovací signál pro reprezentaci vět než jazykové modelování. Důležité je, že zatímco modely Transformer dosahují vyšší kvality strojového překladu, reprezentace z modelů založených na rekurentních neuronové sítí dosahují výrazně lepších výsledků při hodnocení sématické relevance.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Recent literature shows that large-scale language modeling provides excellent reusable sentence representations with both recurrent and self-attentive architectures. However, there has been less clarity on the commonalities and differences in the representational properties induced by the two architectures. It also has been shown that visual information serves as one of the means for grounding sentence representations. In this paper, we present a meta-study assessing the representational quality of models where the training signal is obtained from different modalities, in particular, language modeling, image features prediction, and both textual and multimodal machine translation. We evaluate textual and visual features of sentence representations obtained using predominant approaches on image retrieval and semantic textual similarity. Our experiments reveal that on moderate-sized datasets, a sentence counterpart in a target language or visual modality provides much stronger training signal for sentence representation than language modeling. Importantly, we observe that while the Transformer models achieve superior machine translation quality, representations from the recurrent neural network based models perform significantly better over tasks focused on semantic relevance.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Lexikální desambiguace je úlohu, při které má být zvolen ten význam slova, který je v daném kontextu relevantní. V posledních letech můžeme pozorovat úspěšnou aplikaci vektorových reprezentací slov napříč různými úlohami v oblasti zpracování přirozeného jazyka. Vzhledem ke schopnosti těchto vektorových reprezentací odrážet distribuční sémantiku byla v nedávné době věnovaná pozornost i možnosti využití pro lexikální desambiguaci. V tomto článku navrhujeme novou neřízenou metodu pro lexikální desambiguace v jednom jazyce s využitím vektorových reprezentací natrénovaných pro jiný jazyk a překladového slovníku. V našich experimentech byly pro lexikální desambiguaci perských slov využity vektorové reprezentace anglických překladů slov z blízkého kontextu. Každý možný překlad polysémního slova je porovnán s vektorovými reprezentacemi okolních slov, z toho je vygenerováno podobnostní skóre, přičemž překladový ekvivalent s nejvyšším skóre reprezentuje zvolený význam. Tato metoda vyžaduje pouze neznačkovaný korpus a překladový slovník. Úspěšnost našeho přístupu je ilustrovaná na malém vzorku ručně desambiguovaných dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Word sense disambiguation is the task of assigning the correct sense of a polysemous word in the context in which it appears. In recent years, word embeddings have been applied successfully to many NLP tasks. Thanks to their ability to capture distributional semantics, more recent attention have been focused on utilizing word embeddings to disambiguate words.
In this paper, a novel unsupervised method is proposed to disambiguate words from the first language by deploying a trained word embeddings model of the second language using only a bilingual dictionary. While the translated words are useful clues for the disambiguation process, the main idea of this work is to use the information provided by English-translated surrounding words to disambiguate Persian words using trained English word2vec; well-known word embeddings model. Each translation of the polysemous word is compared against word embeddings of translated surrounding words to calculate word similarity scores and the most similar word to vectors of translated surrounding words is selected as the correct translation. This method only requires a raw corpus and a bilingual dictionary to disambiguate the word under question. The experimental results on a manually-created test dataset demonstrate the accuracy of the proposed method.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>ílem sdíleného úkolu o automatickém rozlišení mezer v ruštině (AGRR2019) v roce 2019 je boj proti netriviálnímu lingvistickému jevu, který se vyskytuje v koordinovaných strukturách, a eliminuje opakovaný predikát, obvykle
z druhé věty.
V tomto článku definujeme metriku úkolů a hodnocení, poskytujeme podrobné informace
informace o přípravě údajů, schématech anotace a metodice,
analyzovat výsledky a popsat různé přístupy účastníka
řešení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The 2019 Shared Task on Automatic Gapping Resolution for Russian (AGRR2019) aims to tackle non-trivial linguistic phenomenon, gapping, that occurs in coordinated structures and elides a repeated predicate, typically
from the second clause.
In this paper, we define the task and evaluation metrics, provide detailed
information on data preparation, annotation schemes and methodology,
analyze the results and describe different approaches of the participating
solutions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prakticky všechny metriky a techniky hodnocení strojového překladu, automatické i manuální, mají svá vlastní nebezpečí. Některá nebezpečí lze považovat za podjatá, pokud existují systémy MT, které mohou (nespravedlivě) těžit z daných aspektů hodnocení. Ve své prezentaci se zaměřuji na následující aspekty: hodnocení na úrovni vět vs. dokumentu (document-level vs. document-aware evaluation), source-based vs. reference-based, přímé hodnocení vs. hodnocení založené na srovnávání, plynulost vs. přesnost. Zabývám se také aspektem překladštiny (translationese) a rodným jazykem překladatelů a hodnotitelů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Virtually all MT evaluation metrics and techniques, both automatic and manual, have their own perils.
Some of the perils can be considered biases if there are MT systems which can (unfairly) benefit from a given evaluation aspects. In my presentation, I focus on the following aspects of evaluations: sentence-level vs. document-level vs.
document-aware, source-based vs. reference-based, direct assessment vs. comparison-based, fluency-biased vs. adequacy-biased. I also discuss the aspect of translationese and native target/source-language translators and evaluators.</seg>
            </tuv>
        </tu>
        
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme naše čtyři systémy neuronového strojového překladu (NMT), které jsme odeslali do shared tasku IWSLT19 pro anglicko-český překlad  TED Talks. Cílem této studie je porozumět interakcím mezi NMT na úrovni dokumentů a doménovou adaptací. Všechny naše systémy jsou založeny na modelu Transformer implementovaném ve frameworku Tensor2Tensor. Dva ze systémů slouží jako baseline a nejsou přizpůsobeny doméně TED Talks: SENTBASE je trénován na jednotlivých větách, DOCBASE na vícevětných (document-level) sekvencích. Další dva předložené systémy jsou přizpůsobeny doméně TED Talks: SENTFINE je adaptován na jednotlivých větách, DOCFINE na vícevětných sekvencích. Představujeme jak automatické metrické hodnocení, tak manuální analýzu kvality překladu se zaměřením na rozdíly mezi těmito čtyřmi systémy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe our four NMT systems submitted to the IWSLT19 shared task in English→Czech text-to-text translation of TED talks. The goal of this study is to understand the interactions between document-level NMT and domain adaptation. All our systems are based on the Transformer model implemented in the Tensor2Tensor framework. Two of the systems serve as baselines, which are not adapted to the TED talks domain: SENTBASE is trained on single sentences, DOCBASE on multi-sentence (document-level) sequences. The other two submitted systems are adapted to TED talks: SENTFINE is fine-tuned on single sentences, DOCFINE is fine-tuned on multi-sentence sequences. We present both automatic-metrics evaluation and manual analysis of the translation quality, focusing on the differences between the four systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce představuje náš probíhající výzkum předtrénování bez dohledu v oblasti neuronového strojového překladu (NMT). Naše metoda inicializuje váhy enkodéru a dekodéru pomocí dvou jazykových modelů, které jsou trénovány na jednojazyčných datech. Celý model pak dolaďujeme na paralelních datech s pomocí elastické konsolidace vah (EWC), abychom zabránili zapomenutí původní úlohy jazykového modelování. Srovnáváme regularizaci EWC s předchozí prací, která se zaměřuje na regularizaci s pomocí optimalizačních cílů jazykového modelování.

Pozitivním výsledkem je, že použitím EWC s dekodérem dosáhneme podobných hodnot BLEU jako předchozí práce. Model však konverguje 2-3krát rychleji a nevyžaduje původní jednojazyčná data během dolaďování.

Oproti tomu je EWC regularizace méně účinná, pokud spolu původní a navazující úloha úzce nesouvisí. Ukazujeme, že inicializace obousměrného NMT enkodéru pomocí jednosměrného jazykového modelu a nucení modelu zapamatovat si původní úlohu modelování jazyka zleva doprava omezuje schopnosti enkodéru naučit se oboustranný kontext.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This work presents our ongoing research of unsupervised pretraining in neural machine translation (NMT). In our method, we initialize the weights of the encoder and decoder with two language models that are trained with monolingual data and then fine-tune the model on parallel data using Elastic Weight Consolidation (EWC) to avoid forgetting of the original language modeling tasks. We compare the regularization by EWC with the previous work that focuses on regularization by language modeling objectives.

The positive result is that using EWC with the decoder achieves BLEU scores similar to the previous work. However, the model converges 2-3 times faster and does not require the original unlabeled training data during the finetuning stage.

In contrast, the regularization using EWC is less effective if the original and new tasks are not closely related. We show that initializing the bidirectional NMT encoder with a left-toright language model and forcing the model to remember the original left-to-right language modeling task limits the learning capacity of the encoder for the whole bidirectional context.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Hluboké učení přináší průlomy v mnoha podoborech strojového vidění, včetně rozpoznávání notopisu (Optical Music Recognition, OMR), kde bylo zaznamenáno množství pokroků v detekci notopisných symbolů pomocí obecných modelů hlubokého učení. Zatím však byly všechny tyto pokroky měřené pouze na některém z dostupných datasetů a využívaly různá evaluační kritéria, takže lze jen obtížně kvantifikovat nový stav poznání dosažený těmito metodami a porovnat jejich výhody a nevýhody v doméně hudební notace. V tomto článku prezentujeme  základní výsledky detekce symbolů hudební notace pomocí tří různých obecných detekčních modelů hlubokého učení, a to napříč třemi typologicky různými datasety, změřené standardizovaným postupem. Experimentální výsledky potvrzují, že přímá detekce notačních objektů pomocí hlubokého učení má velmi slibné výsledky, nicméně zároveň ilustruje omezení generických detektorů na této doméně. Kvalitativní porovnání poté naznačuje, jak detekci zlepšit: jak na základě vlastností detekčních modelů, tak na základě vlastností datasetů. Dle našich poznatků je toto poprvé, kdy je vícero špičkových metod detekce notačních objektů přímo porovnáváno. Doufáme, že tato práce bude sloužit jako reference pro měření dalších pokroků v OMR.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Deep learning is bringing breakthroughs to many computer vision subfields including Optical Music Recognition (OMR), which has seen a series of improvements to musical symbol detection achieved by using generic deep learning models. However, so far, each such proposal has been based on a specific dataset and different evaluation criteria, which made it difficult to quantify the new deep learning-based state-of-the-art and assess the relative merits of these detection models on music scores. In this paper, a baseline for general detection of musical symbols with deep learning is presented. We consider three datasets of heterogeneous typology but with the same annotation format, three neural models of different nature, and establish their performance in terms of a common evaluation standard. The experimental results confirm that the direct music object detection with deep learning is indeed promising, but at the same time illustrates some of the domain-specific shortcomings of the general detectors. A qualitative comparison then suggests avenues for OMR improvement, based both on properties of the detection model and how the datasets are defined. To the best of our knowledge, this is the first time that competing music object detection systems from the machine learning paradigm are directly compared to each other. We hope that this work will serve as a reference to measure the progress of future developments of OMR in music object detection.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Naším cílem je anotace korpusu CzeSL podle gramatiky jazyka, se kterou pracuje nerodilý mluvčí, a ne podle standartní gramatiky. Tento přístup přináší několik problémů. Za prvé nemáme dostatek dat na to, abychom analyzovali gramatiky jednotlivých autorů. Za druhé v jazyce nerodilých mluvčí je podstatně více složitějších jevů  než v jazyce rodilých mluvčí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Our goal has been to annotate the CzeSL corpus according to the non-native grammar in the mind of the author, not according to the standard grammar. However, this brings many challenges. First, we do not have enough data to get reliable insights into the grammar of each author. Second, many phenomena are far more complicated than they are in native languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CzeSL je žákovský korpus textů nerodilých mluvčích češtiny, který je cenným zdrojem unikátních znalostí o jazyce nerodilých mluvčích. Pomáhá pedagogům a výzkumníkům v oblasti osvojování druhého jazyka. V našem projektu se zaměřujeme na syntaktickou anotaci textů z CzeSL v rámci formalismu Universal Dependencies. Pokud je nám známo, jedná se o první pokus takové anotace pro jazyk s bohatou flexí. Naší ambicí je anotovat dle gramatiky, kterou používá nerodilý mluvčí, namísto standardní gramatiky. Relativné malé množství dat přináší celou řadu otázek.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CzeSL is a learner corpus of texts produced by non-native speakers of Czech. Such corpora area great source of information about specific features of learners’ language, helping language teachers and researchers in the area of second language acquisition. In our project, we have focused on syntactic annotation of the non-native text within the framework of Universal Dependencies. As far as we know, this is a first project annotating a richly inflectional non-native language. Our ideal goal has been to annotate according to the non-native grammar in the mind of the author, not according to the standard grammar. However, this brings many challenges. First, we do not have enough data to get reliable insights into the grammar of each author. Second, many phenomena are far more complicated than they are in native languages.  We believe that the most important result of this project is not the actual annotation, but the guidelines and principles that can be used as a basis for other non-native languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje průběh 44. ročníku Olympiády v českém jazyce. Představuje celkový průběh soutěže, soutěžní úkoly a jejich řešení, komentuje řešení účastníků soutěže a přináší jména vítězů celostátního kola.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article describes the course of the 44th year of the Olympiad in the Czech Language, presenting its general settings as well as the tasks, their solutions, the approaches of the participants and the names of the winners.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Diskurzní konektory představujeme z pohledu reference (tj. přítomnosti anaforického nebo kataforického prvku). Diskurzní konektory dělíme na: 1) konektory bez interní reference (např. "a", "ale", "nebo", "jestli", "však", "pak") a 2) konektory s interní referencí, která může být fakultativní (např. "výsledkem je" vs. "výsledkem toho je") nebo obligatorní – srov. již gramatikalizované konektory (tj. primární konektory typu "potom", "proto", "tímto") i dosud ne zcela gramatikalizované konektory (tj. sekundární konektory typu "kvůli tomu", "z tohoto důvodu").</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We examine discourse connectives from the perspective of reference (i.e. a presence of an anaphoric/cataphoric element). We introduce a division of connectives into: i) connectives without an inherent (internal) reference (e.g. "and", "but", "or", "if", "however", "then"), and ii) connectives with an inherent (internal) reference that is either optional (e.g. "as a result" vs. "as a result of this"), or obligatory – cf. already grammaticalized connectives (i.e. primary connectives like "thereafter", "therefore" or "thereby") vs. not yet grammaticalized connectives (i.e. secondary connectives like "because of this", "for this reason").</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá analýzou koherence textu v češtině a v němčině. Zaměřuje se na tzv. anaforické diskurzní konektory, které se významně podílejí na stavbě textu. Příspěvek analyzuje roli těchto konektorů v celkové komunikační kompetenci rodilých i nerodilých mluvčích češtiny a němčiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a contrastive study of text coherence in Czech and German. Specifically, we focus on the discourse-anaphoric devices (called anaphoric connectives) contributing to text coherence and we analyze their role in the overall communicative competence of both native and non-native speakers of Czech and German.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Koherentní text (diskurz) se skládá ze vzájemně propojených vztahů, které mohou být vyjádřeny explicitními jazykovými prostředky. V našem článku se zabýváme jedním typem těchto prostředků, diskurzními konektory. Přebíráme přitom dělení konektivních prostředků na primární konektory, sekundární konektory a volné konektivní fráze. Na základě dokladů z češtiny, angličtiny, francouzštiny a němčiny rozvíjíme definice těchto skupin, především s ohledem na mezijazykové rozdíly. U primárních a sekundárních konektorů navrhujeme způsob jejich zachycení ve slovnících. Představujeme konkrétní návrh vícejazyčného slovníku konektorů, do kterého je v současné době zapojeno pět různých jazyků. Další jazyky budou postupně přidávány.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Starting from the perspective that discourse structure arises from the presence of coherence relations, we provide a map of linguistic discourse structuring devices (DRDs), and then focus on those found  in  written  text: connectives. To subdivide  this  class  further, we  follow  the  recent idea of structuring the set of connectives by differentiating between primary and secondary connectives, on the one hand, and free connecting phrases, on the other. Considering examples from Czech, English, French and German, we develop definitions of these groups, with attention to certain cross-linguistic differences. For primary and secondary connectives, we propose that their behavior can be described to a large extent by declarative lexicons, and we demonstrate a concrete proposal which has been applied to five languages, with others currently being added in ongoing work. The lexical representations can be useful both for humans (theoretical investigations, transfer to other languages) and for machines (automatic discourse parsing and generation).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>This paper presents a case study in translating short image captions of the Visual Genome dataset from English into Hindi using out-of-domain data sets of varying size. We experiment with three NMT models: the shallow and deep sequence-to-sequence and the Transformer model as implemented in Marian toolkit. Phrase-based Moses serves as the baseline. The results indicate that the Transformer model outperforms others in the large data setting in a number of automatic metrics and manual evaluation, and it also produces the fewest truncated sentences. Transformer training is however very sensitive to the hyperparameters, so it requires more experimenting.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a case study in translating short image captions of the Visual Genome dataset from English into Hindi using out-of-domain data sets of varying size. We experiment with three NMT models: the shallow and deep sequence-to-sequence and the Transformer model as implemented in Marian toolkit. Phrase-based Moses serves as the baseline. The results indicate that the Transformer model outperforms others in the large data setting in a number of automatic metrics and manual evaluation, and it also produces the fewest truncated sentences. Transformer training is however very sensitive to the hyperparameters, so it requires more experimenting. The deep sequence-to-sequence model produced more flawless outputs in the small data setting and it was generally more stable, at the cost of more training iterations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V současných neuronových systémech pro strojový překlad textů přirozeného jazyka (NMT) se morfologicky příbuzná slova zpracovávají jejich rozdělením na podslovní jednotky takovým způsobem, aby se slovník jednotek vešel do limitů daných zvoleným NMT modelem a do paměti grafické karty. V tomto článku srovnáváme dva nejobvyklejší, nelingvistické, způsoby vytváření podslovních jednotek (BPE a STE, metody implementované v nástroji Tensor2Tensor) se dvěma lingvisticky motivovanými způsoby: Nástrojem Morfessor a námi vyvinutou metodou založenou na derivačních vztazích. Naše experimenty s překladem z němčiny do češtiny, morfologicky bohatých jazyků, ukazují, že prozatím mají lepší výsledky nelingvistické metody. K tomu identifikujeme důležitý rozdíl mezi BPE a STE a ukazujeme, že jednoduché předzpracování textu před BPE výrazně zvyšuje kvalitu překladu vyhodnocovanou automatickými metrikami.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The state of the art of handling rich morphology in neural machine translation (NMT) is to break word forms into subword units, so that the overall vocabulary size of these units fits the practical limits given by the NMT model and GPU memory capacity. In this paper, we compare two common but linguistically uninformed methods of subword construction (BPE and STE, the method implemented in Tensor2Tensor toolkit) and two linguistically-motivated methods: Morfessor and one novel method, based on a derivational dictionary. Our experiments with German-to-Czech translation, both morphologically rich, document that so far, the non-motivated methods perform better. Furthermore, we identify a critical difference between BPE and STE and show a simple pre-processing step for BPE that considerably increases translation quality as evaluated by automatic measures.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce porovnává kvalitu a rychlost systémů pro neuronový strojový překlad (Tensor2Tensor, Marian, Nematus, Neural Monkey, OpenNMT) na dvou srovnatelných překladových úlohách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This work is a comparison of a quality and speed of  NMT systems Tensor2Tensor, Marian, Nematus, Neural Monkey and OpenNMT on two comparable translation tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Extrakce syntaktických struktur z self-attentions NMT enkodéru.
Extrakce syntaktických struktur z self-attentions NMT enkodéru.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Extracting Syntactic Trees from NMT Encoder Self-Attentions.
Extracting Syntactic Trees from NMT Encoder Self-Attentions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Extrakce syntaktických struktur z self-attentions enkodéru Transformeru. Extrakce syntaktických struktur z self-attentions enkodéru Transformeru</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Extracting Syntactic Trees from Transformer Encoder Self-Attentions. Extracting Syntactic Trees from Transformer Encoder Self-Attentions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato prezentace shrnuje závěry týmu Grounded Sequence-to-Sequence Transduction na pátém Fred Jelinek Memorial Summer Workshop. V prezentovaných modelech kombinujeme psaný text, mluvené slovo a objekty a akce v pozorované . Když vidíme, že člověk krájí červené předměty a umístí je na hnědý povrch, je pravděpodobné, že spíše vysvětluje, jak vyrobit sendvič, než jak změnit pneumatiku. A- mohli bychom se dozvědět, že červené předměty se nazývají "rajčata". Náš tým pracoval na vývoji metod, které využívají multimodální informace pro zpracování a analýzu videoklipů, ve třech hlavních lohách: rozpoznávání řeči, sumarizace videa a textu a překlad do jiného jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This presentation summarizes output of the Grounded Sequence-to-Sequence Transduction team at the Fifth Fred Jelinek Memorial Summer Workshop. We combine written, spoken, and seen objects and actions in how-to videos: if we see a person slicing round, red objects and putting them on a brown surface, it is more likely that she or he is explaining how to make a sandwich than how to change a car tire. And we might learn that the red objects are called “tomatoes”. Our team  develop methods that exploit multimodality to process and analyze videos to accomplish three main tasks: speech captioning, video-to-text summarization and translation into a different language. These tasks are diverse but not unrelated. Therefore, we model them using a multi-task sequence-to-sequence learning framework where these (and other, auxiliary) tasks can benefit from shared representations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Modely strojového učení přinášejí slibné výsledky v mnoha oborech včetně zpracování přirozeného jazyka. Tyto modely jsou nicméně náchylné k adversálním příkladům. Jedná se o uměle vytvořené příklady s dvěma důležitými vlastnostmi: podobají se skutečným tréninkovým příkladům, ale matou již natrénovaný model. Tento článek zkoumá účinek používání adversálních příkladů při tréninku rekurentních neuronových sítí, jejichž vstup je ve formě slovních či znakových embeddingů. Účinky jsou studovány na kompilaci osmi datasetů. Na základě experimentů a charakteristik datasetů dospíváme k závěru, že použití adversálních příkladů pro úkoly zpracování přirozeného jazyka, které jsou modelovány pomocí rekurentních neuronových sítí, přináší efekt regularizace a umožňuje trénovat modely s větším počtem parametrů bez overfittingu. Na závěr popisujeme, které kombinace datasetů a nastavení modelů by mohly mít z adversálního tréninku největší prospěch.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Machine learning models have been providing promising results in many fields including natural language processing. These models are, nevertheless, prone to adversarial examples. These are artificially constructed examples which evince two main features: they resemble the real training data but they deceive already trained model. This paper investigates the effect of using adversarial examples during the training of recurrent neural networks whose text input is in the form of a sequence of word/character embeddings. The effects are studied on a compilation of eight NLP datasets whose interface was unified for quick experimenting. Based on the experiments and the dataset characteristics, we conclude that using the adversarial examples for NLP tasks that are modeled by recurrent neural networks provides a regularization effect and enables the training of models with greater number of parameters without overfitting. In addition, we discuss which combinations of datasets and model settings might benefit from the adversarial training the most.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek přináší výsledky soutěže ve vyhodnocování strojového překladu WMT18 Metrics Shared Task. Vyhodnocujeme 10 metrik získaných od 8 týmů a navíc 8 standardních metrik.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the
WMT18
Metrics Shared Task.  We
asked participants of this task to score
the outputs of the MT systems in-
volved in the
WMT18
News Transla-
tion Task with automatic metrics. We
collected scores of 10 metrics and 8 re-
search groups. In addition to that, we
computed scores of 8 standard met-
rics (BLEU, SentBLEU, chrF, NIST,
WER, PER, TER and CDER) as base-
lines. The collected scores were eval-
uated in terms of system-level corre-
lation (how well each metric’s scores
correlate with
WMT18
official man-
ual ranking of systems) and in terms
of segment-level correlation (how often
a metric agrees with humans in judging
the quality of a particular sentence rel-
ative to alternate outputs). This year,
we employ a single kind of manual eval-
uation: direct assessment (DA).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvěk se zabývá otázkou, jak může syntaktická vlastnost sloves, jako je reciprocita, být promítnuta do slovníka a přispět k dalšímu popisu a klasifikaci sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we deal with reciprocity, its representation in a lexicon and its possible contribution to the description of verbs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Na pozadí dvou skupin sloves, lexikálních a syntaktických reciprok v češtině, diskutujeme otázky homonymie a kombinovatelnosti reflexivity a reciprocity. Ukazujeme, že zdrojem homonymie i možnosti kombinace reflexivity a reciprocity je reflexivní zájmeno a že klíčovou úlohu sehrává jeho povrchověsyntaktická pozice.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this article, we focus on two groups of Czech verbs, lexical and syntactic reciprocals. We provide an analysis of their syntactic properties with respect to reciprocity and reflexivity, their possible ambiguity and combination. We demonstrate that it is the reflexive pronoun and its surface expression that play a key role in the studied phenomena.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku shrnujeme teoretickou analýzu syntaktického chování českých komplexních predikátů s kategoriálním slovesem, která byla ověřena v anotaci 1500 komplexních predikátů. Tato anotace je součátí slovníku VALLEX, verze 3.5.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we summarize a theoretical analysis of syntactic behavior of Czech light verb constructions and their verification in the data annotation of 1,500 Czech light verbs constructions, which have been integrated in VALLEX, version 3.5.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška se zabývala tzv. komplementovým systémem v češtině, a to zejm. vztahem mezi závislými obsahovými klauzemi a infinitivy sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk dealt with the so-called complement system in Czech as it manifests in the data from the substitutability of dependent content clauses with infinitives.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme první volně dostupný závislostní korpus sanskrtu. Je založen na textech z Paňčatantry, starověké indické sbírky bajek. Zvolili jsme formalismus Universal Dependencies, který v současnosti představuje faktickou normu mezijazykově srovnatelné morfologické a syntaktické anotace. V článku probíráme obtíže se segmentací textu na slova, představujeme inventář morfologických kategorií, jakož i některé syntaktické konstrukce, které jsou zajímavé ve světle pravidel Universal Dependencies. Dále popisujeme experiment s automatickou syntaktickou analýzou (parsingem).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the first freely available dependency treebank of Sanskrit. It is based on text from Panchatantra, an ancient Indian collection of fables. The annotation scheme we chose is that of Universal Dependencies, a current de-facto standard for cross-linguistically comparable morphological and syntactic annotation. In the present paper,
we discuss word segmentation issues, morphological inventory and certain interesting
syntactic constructions in the light of the Universal Dependencies guidelines. We also present an initial parsing experiment.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje mezinárodní projekt, který připravil univerzální terminologii a anotační postup pro slovesné víceslovné výrazy. Výstupem je korpus zveřejněný pod otevřenou licencí (18 jazyků, 5,4 milionů slov, 62 tisíc víceslovných výrazů) a univerzální anotační instrukce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Multiword expressions (MWEs) are known as a “pain in the neck” due to their idiosyncratic behaviour. While some categories of MWEs have been largely studied, verbal MWEs (VMWEs) such as to take a walk, to break one’s heart or to turn off have been relatively rarely modelled. We describe an initiative meant to bring about substantial progress in understanding, modelling and processing VMWEs. In this joint effort carried out within a European research network we elaborated a universal terminology and annotation methodology for VMWEs. Its main outcomes, available under open licenses, are unified annotation guidelines, and a corpus of over 5.4 million words and 62 thousand annotated VMWEs in 18 languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje CUNI system na WAT 2018 pro překlad mezi angličtinou a Hindštinou. Náš systém využívá transfer learningu z anglicko-českého modelu. Využíváme technologii neuronového transformeru.
Náš systém začíná trénováním na jazykovém páru s mnoho paralelními daty, česko-anglickém, na který naváže jazykový pár s nedostatkem dat. Náš systém se umístil první podle lidského hodnocení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the CUNI submission to WAT 2018 for the English-Hindi translation task using a transfer 
learning techniques which has proven effective under low resource conditions. We have used the Transformer model and utilized an English-Czech parallel corpus as additional data source. Our simple transfer learning approach first trains a “parent” model for a high-resource 
language pair (English-Czech) and then continues the training on the low-resource (English-Hindi) pair by replacing
the training corpus. This setup improves the performance compared with the baseline and in combination with back-translation of Hindi monolingual data, it allowed us to win the English-Hindi task. The automatic scoring by BLEU did not correlate well with human judgments.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Bylo dokázáno, že přetrénování natrénovaných modelů vede k zlepšení strojového překladu. Existující metody potřebují, aby zkoumané jazykové páry byly lingvisticky podobné. My představujeme přístup, který nepotřebuje lingvistickou podobnost jazyků ani specifické trénovací kroky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Transfer learning has been proven as an effective
technique for neural machine translation
under low-resource conditions. Existing
methods require a common target language,
language relatedness, or specific training
tricks and regimes. We present a simple
transfer learning method, where we first train
a “parent” model for a high-resource language
pair and then continue the training on a lowresource
pair only by replacing the training
corpus. This “child” model performs significantly
better than the baseline trained for lowresource
pair only. We are the first to show
this for targeting different languages, and we
observe the improvements even for unrelated
languages with different alphabets.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zúčastnili jsme se překladové soutěže WMT 2018 v překladu novinových článků. Zúčastnili jsme se v jazykových párech Angličtina-Estonština, Angličtina-Finština a Angličtina-Čeština. Náš hlavní cíl byly jazyky s nedostatkem trénovacích dat, tedy překlad mezi angličtinou a estonštinou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We participated in the WMT 2018 shared
news translation task in three language
pairs: English-Estonian, English-Finnish, and
English-Czech. Our main focus was the lowresource
language pair of Estonian and English
for which we utilized Finnish parallel
data in a simple method. We first train a
“parent model” for the high-resource language
pair followed by adaptation on the related lowresource
language pair. This approach brings
a substantial performance boost over the baseline
system trained only on Estonian-English
parallel data. Our systems are based on the
Transformer architecture. For the English
to Czech translation, we have evaluated our
last year models of hybrid phrase-based approach
and neural machine translation mainly
for comparison purposes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme náš překladový systém pro překlad z Baskičtiny do Angličtiny. K jeho natrénování jsme využili metodu přetrénování jiného překladového modelu, v našem případě anglicko-českého.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our submission to the IWSLT18 Low Resource task focused on the translation from
Basque-to-English. Our submission is based on the current state-of-the-art self-attentive
neural network architecture, Transformer. We further improve this strong baseline by
exploiting available monolingual data using the back-translation technique. We also
present further improvements gained by a transfer learning, a technique that trains a model using a high-resource language pair (Czech-English) and then fine-tunes the model using the target low-resource language pair (Basque-English).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje dvoustupňovou metodu přepisu historických rukopisů. V této metodě používá první krok reprezentaci na stránce, což usnadňuje přepis dokumentu po stránce a řádku po řádku, zatímco druhý krok to převádí do textového formátu TEI / XML, aby zajistit, aby byl dokument plně prohledávatelný.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article describes a two-step method for transcribing historic manuscripts. In this method, the first step uses a page-based representation making it easy to transcribe the document page-by-page and line-by-line, while the second step converts this to the TEI/XML text-based format, in order to make sure the document becomes fully searchable.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kapitola prezentuje kvantitativní analýzu vybraných závislostních vztahů v češtině. Zavádíme pojem valenčního rámec jako lingvistické jednotky a zkoumáme jeho základní charakteristiky, zejména frekvenční charakteristiky a dále vztah mezi typem syntaktické funkce a množstvím různých valenčních rámců, jež tato funkce nese. Analýzu jsou priváděny na korpusu Czech Universal Dependency Treebank.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article presents a quantitative analysis of some syntactic dependency properties in Czech. A dependency frame is introduced as a linguistic unit and its characteristics are investigated. In particular, a ranked frequencies of dependency frames are observed and modelled and a relationship between particular syntactic functions and the number of dependency frames is examined. For the analysis, the Czech Universal Dependency Treebank is used.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme uživatelské rozhraní mezi českým valenčním lexikonem, PDT-Vallex [1] a KonText1 -
webová aplikace pro dotazování na korpusy dostupné v rámci projektu LINDAT / CLARIN. KonText
umožňuje vyhodnocování jednoduchých a komplexních dotazů, zobrazování výsledků jako shodných linek,
výpočet distribuce frekvence, výpočet asociačních opatření pro kolokace a dále
práce s jazykovými daty. Pro každé sloveso ve shodné lince umožňuje naše rozhraní zobrazit
informace o jeho valenčním rámci v samostatném okně, pokud existují odpovídající položky
PDT-Vallex, stejně jako seznam možných valenčních rámců pro toto konkrétní sloveso. Informace
týkající se slovesného rámu obsahuje lemma slovesa, prvky rámce se sémantickými rolami, slovník slovníku
popis a příklady z PDT-Vallex, Praha Dependency Treebank [2] a Praha
Česko-anglická závislostní stromová banka [3]. Informace jsou požadovány z REST-API z
valenční lexikon PDT-Vallex, který obsahuje přes 11000 valenčních rámců pro více než 7000 sloves
která se objevila v pražské Dependency Treebank, Praha Czech-English Dependency Treebank nebo
Praha závislost Treebank mluveného češtiny. Používáme vidlici aplikace KonText (vyvinutá
Ústavem českého národního korpusu), který byl dále rozšířen Ústavem
formální a aplikované lingvistiky, aby vyhovovaly potřebám projektu LINDAT / CLARIN. Plugin jsme
present poskytuje unikátní řešení pro český jazyk, které integruje valenční informace z
Český valenční lexikon s prostředky dotazování na pražskou Závislostní bankou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a user interface between the Czech valency lexicon, PDT-Vallex [1], and KonText1 –
a web application for querying corpora available within the LINDAT/CLARIN project. KonText
allows evaluation of simple and complex queries, displaying their results as concordance lines,
computing frequency distribution, calculating association measures for collocations and further
work with language data. For every verb in a concordance line, our interface allows to display
information concerning its valency frame in a separate window if corresponding entries exist in
PDT-Vallex, as well as a list of possible valency frames for that particular verb. Information
concerning verb frame comprises verb lemma, frame elements with semantic roles, vocabularystyle
description and examples from PDT-Vallex, Prague Dependency Treebank [2] and Prague
Czech-English Dependency Treebank [3]. The information is requested by REST-API from the
valency lexicon PDT-Vallex that contains over 11000 valency frames for more than 7000 verbs
which occurred in Prague Dependency Treebank, Prague Czech-English Dependency Treebank or
Prague Dependency Treebank of Spoken Czech. We use a fork of KonText application (developed
by the Institute of the Czech National Corpus) that has been further extended by the Institute
of Formal and Applied Linguistics to suit the needs of LINDAT/CLARIN project. The plugin we
present provides a unique solution for Czech language that integrates valency information from the
Czech valency lexicon with the means of querying Prague Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozbalení pomocí křížových značek je založeno na nahrazení jedné vrstvy anotací za jinou při zpracování dat v jednom jazyce. Nejčastěji není k dispozici ani rodný značkovač nebo syntaktický analyzátor závislostí používaný v (před) anotaci Gold stromové banky. Přístup přes křížové štítky umožňuje anotovat nové texty pomocí volně dostupných nástrojů nebo nástrojů optimalizovaných podle potřeb uživatele. Vyhodnocujeme robustnost analyzování ruské závislostí s použitím různých morfologických a syntaktických tagů ve vstupu a výstupu. Kvalitativní analýza chyb ukazuje, že křížová substituce tří morfologických značek a dvou syntaktických značek způsobuje pouze mírný pokles výkonu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Cross-tagset parsing is based on the substitution of one annotation layer for another while processing data within one language. As often as not, either the native tagger or the dependency parser used in (pre-)annotation of the Gold treebank is not available. The cross-tagset approach allows one to annotate new texts using freely available tools or tools optimized to user's needs. We evaluate the robustness of Russian dependency parsing using different morphological and syntactic tagsets in input and output. A qualitative analysis of errors shows that the cross-substitution of three morphological tagsets and two syntactic tagsets causes only a mild drop in performance.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku se zaměřujeme na syntaktickou anotaci a její konzistenci v korpusech Universal Dependencies (UD) pro ruštinu: SynTagRus, GSD, Taiga a PUD. Popisujeme tyto čtyři korpusy, jejich distinktivní rysy a vývoj.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we focus on syntactic annotation consistency within Universal Dependencies (UD) treebanks for Russian: UD_Russian-SynTagRus, UD_Russian-GSD, UD_Russian-Taiga, and UD_Russian-PUD. We describe the four treebanks, their distinctive features and development. In order to test and improve consistency within the treebanks, we reconsidered the experiments by Martínez Alonso and Zeman; our parsing experiments were conducted using a state-of-the-art parser that took part in the CoNLL 2017 Shared Task. We analyze error classes in functional and content relations and discuss a method to separate the errors induced by annotation inconsistency and those caused by syntactic complexity and other factors.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Umělé závislostní stromy v anotačním stylu Universal Dependencies v2, zaměřené na druh elipsy zvaný anglicky gapping (v UD odpovídá vztahu 'orphan'). Motivace a popis těchto dat je obsažen v Droganova et al., 2018 (LREC, Miyazaki, Japonsko).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Artificial dependency trees in the Universal Dependencies v2 style, focused on gapping (the 'orphan' relation in UD). For motivation and description of the data, see Droganova et al., 2018 (LREC, Miyazaki, Japan).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci se zaměřujeme na konkrétní jazykový jev, elipsu, a zkoumáme výstupy současných parserů, abychom zjistili jejich úspěšnost a typické chyby s ohledem na elipsy. K tomuto účelu jsme sebrali a zpracovali výstupy několika nejlepších parserů, které se zúčastnily společné úlohy CoNLL 2017. Oficiální vyhodnocovací software jsme rozšířili, aby bylo možné zjistit chyby v analýze elips. Protože studované struktury jsou poměrně vzácné, a tudíž není k dispozici dostatečné množství dat pro experimenty, popisujeme dále tvorbu nového datového zdroje, polouměle vytvořeného závislostního korpusu elips.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this work we focus on a particular linguistic phenomenon, ellipsis, and explore the latest parsers in order to learn about parsing accuracy and typical errors from the perspective of elliptical constructions. For this purpose we collected and processed outputs of several state-of-the art parsers that took part in the CoNLL 2017 Shared Task. We extended the official shared task evaluation software to obtain focused evaluation of elliptical constructions. Since the studied structures are comparatively rare, and consequently there is not enough data for experimentation, we further describe the creation of a new resource, a semi-artificially constructed treebank of ellipsis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme pokusy s několika přístupy k automatickému rozšíření trénovacích dat pro závislostní syntaktické analyzátory s využitím velkých webových korpusů. Jedna sada metod je obecná, inspiruje se samotrénováním a trojtrénováním a přidává nový algoritmus, který napodobuje strukturální složitost původního treebanku. Metody ve druhé sadě se více zaměřují na eliptické konstrukce. Pokusy vyhodnocujeme na 5 jazycích: češtině, angličtině, finštině, ruštině a slovenštině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We report on experiments with several approaches to automatically extending training data for dependency parsers, using large crawled web corpora. One set of methods is general, draws upon self-training and tri-training and adds a novel algorithm of mimicking the structural complexity of the original treebank. Methods from the other set are more focused on elliptical constructions. We provide evaluation on 5 languages: Czech, English, Finnish, Russian and Slovak.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek shrnuje diskusi skupiny aktivních výzkumníků rozpoznávání notopisu (Optical Music Recognition, OMR), která se konala v rámci 12th IAPR International Workshop on Graphics Recognition, a prezentuje její výstupy: OMR by mělo zpřesnit svou terminologickou a taxonomickou základnu, a komunita výzkumníků v oboru by měla intenzivněji spolupracovat jak mezi sebou, tak se zájemci o OMR.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This document summarizes the discussion of the interest group on Optical Music Recognition (OMR) that took place in the 12th IAPR International Workshop on Graphics Recognition, and presents the main conclusions drawn during the session: OMR should revisit how it describes itself, and the OMR community should intensify its collaboration both internally and with other stakeholders.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme LemmaTag, architekturu neuronové sítě, která společně generuje morfologické značky a lemmata pomocí obousměrných rekurentních neuronových sítí pomocí slovních a znakových embeddingů. Demonstrujeme, že oběma úkolům pomáhá sdílet enkodér, předvídat podtypy značek a používat předpovězené značky na vstup lemmatizátoru. Vyhodnocujeme náš model na několika jazycích se složitou morfologií, a překonáváme nejlepší známé výsledky jak morfologického značkování tak lemmatizace v češtině, němčině a arabštině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present LemmaTag, a featureless neural network architecture that jointly generates part-of-speech tags and lemmas for sentences by using bidirectional RNNs with character-level and word-level embeddings. We demonstrate that both tasks benefit from sharing the encoding part of the network, predicting tag subcategories, and using the tagger output as an input to the lemmatizer. We evaluate our model across several languages with complex morphology, which surpasses state-of-the-art accuracy in both part-of-speech tagging and lemmatization in Czech, German, and Arabic.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace výzkumného projektu a jeho výsledků je v kontextu workshopu příkladem využití sbírek orálněhistorických interview jako příkladů narativního vyjádření uprchlické zkušenosti během druhé světové války. Důraz je kladen na vzdělávací uplatnění úryvků z rozhovorů prostřednictvím informačních technologií ve výuce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In 2017/2018, I was working on a project „Collective memory as an interactional practice: The case of the Czech-Jewish experience in Switzerland during the World War II period”, carried out at the University of Fribourg in Switzerland. My research consisted of three successive stages with mutually interrelated aims:
(1) Analysis of oral history interviews with Holocaust survivors born in the former Czechoslovakia, who have spent time in Switzerland as refugees, asylum seekers or displaced persons during the World War II period. The oral history interviews were selected from the USC Shoah Foundation’s Visual History Archive. 
(2) Short clips of interviews were then incorporated into on-line educational material, accompanied by text and images. The resulting on-line lesson had the following structure: (I) The context (Why did people leave Czechoslovakia?); (II) The decision (How do the narrators reflect the decision to leave Czechoslovakia?); (III) The journey (How was the emigration practically accomplished?); (IV) The memory: (How is the migration retrospectively reflected by the narrators?).
(3) The on-line lesson was tested at five schools in Switzerland and in the Czech Republic between January and March 2018. The sessions were videotaped and analysed in detail: not only to evaluate and improve the developed activities, but also to explore the interactional specifics of the educational setting.
The research project provides an example of using oral history interviews as cases of narrative expression of the refugee experience during the World War II period. It explored one of the possible ways of utilizing large archives of oral histories for the transmission of historical knowledge on refugeedom, focusing on personal aspects. Moreover, it highlights several issues in educational use of archival interviews, such as the necessity of sufficient context for the students’ comprehension, and also the more general topics of incorporating digital technologies into classroom interaction.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zaměřil na analýzu situovaného jednání v rámci testovacích vyučovacích hodin, jež byly realizovány ve školním roce 2017/18 na třech gymnáziích v ČR. Během těchto hodin studenti pracovali v menších skupinách 2–3 osob s on-line materiálem složeným z textů, obrázků a klipů vyňatých z audiovizuálních orálněhistorických rozhovorů. Videonahrávky pořízené v průběhu hodin byly následně podrobeny detailní analýze vycházející z tradice etnometodologie a konverzační analýzy. V prezentaci vybraných výsledků analýzy se soustředím především na specifika začlenění videoklipu do sociální interakce. Samotná práce s videoklipem je účastníky metodicky zasazena do širšího kontextu dalších aktivit, zvláště četby úvodních textů na monitoru a následného zapisování odpovědí do pracovního listu. Z hlediska sekvenčnosti práce s videoklipem lze rozlišit tři fáze, a to (i) přípravu na spuštění videa, během níž je třeba dosáhnout optimálního uspořádání materiálních artefaktů a tělesné orientace účastníků; (ii) sledování videa, kde je zřejmá preference nepřerušeného přehrávání od začátku do konce, stejně jako omezení řečových projevů účastníků na „průběžný komentář“; (iii) reflexe videa, která nejčastěji směřuje k rychlé orientaci na formulaci odpovědi a její zápis do pracovního listu. Ukážu, že videoklip je v analyzovaných interakcích přítomen jako zdroj (např. dočasné pozastavení klipu či jeho opakované sledování za účelem identifikace klíčových pasáží) i jako téma (např. komentáře týkající se délky klipu či jazyka mluvčího, jenž na nahrávce hovoří). Příspěvek tak skrze rozbor konkrétních událostí vypovídá i o roli moderních digitálních technologií v současné společnosti a ve vzdělávání.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper is based on analysis of situated action in testing lessons, which were implemented in the school year 2017/18 at three high schools in the Czech Republic. Video recordings taken over the course of the lessons were then subjected to a detailed analysis grounded in the tradition of ethnomethodology and conversation analysis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V kapitole je předložen základní rámec pro uchopení tématu připomínání holocaustu, založený na analýze vybraných interview z Archivu vizuální historie USC Shoah Foundation.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This chapter presents a conceptual framework for approaching the topic of Holocaust commemoration, based on the analysis of selected interviews from the USC Shoah Foundation's Visual History Archive.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Text pojednává o možných přístupech ke zkoumání umělé inteligence jako sociálního fenoménu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Social sciences have been always formed and influenced by the development of society, adjusting the conceptual, methodological, and theoretical frameworks to emerging social phenomena. In recent years, with the leap in the advancement of Artificial Intelligence (AI) and the proliferation of its everyday applications, “non-human intelligent actors” are increasingly becoming part of the society. This is manifested in the evolving realms of smart home systems, autonomous vehicles, chatbots, intelligent public displays, etc. In this paper, we present a prospective research project that takes one of the pioneering steps towards establishing a “distinctively sociological” conception of AI. Its first objective is to extract the existing conceptions of AI as perceived by its technological developers and (possibly differently) by its users. In the second part, capitalizing on a set of interviews with experts from social science domains, we will explore the new imaginable conceptions of AI that do not originate from its technological possibilities but rather from societal necessities. The current formal ways of defining AI are grounded in the technological possibilities, namely machine learning methods and neural network models. But what exactly is AI as a social phenomenon, which may act on its own, can be blamed responsible for ethically problematic behavior, or even endanger people’s employment? We argue that such conceptual investigation is a crucial step for further empirical studies of phenomena related to AI’s position in current societies, but also will open up ways for critiques of new technological advancements with social consequences in mind from the outset.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku popisujeme metody a výsledky výzkumu zaměřeného na vícejazyčné korpusové srovnání spojovacího výrazu "and" v angličtině a jeho ekvivalentů v češtině, francouzštině, maďarštině a litevštině u vybraných příspěvků z TED talks.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we report on the methods and findings of a multilingual corpus study focusing on the functions of and in English and its translations into Czech, French, Hungarian and Lithuanian, in a selection of TedTalks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Analýza funkcí spojovacího výrazu AND a jeho ekvivalentů v angličtině, češtině, litevštině, francouzštině a maďarštině podle doménové klasifikace Crible - Degand (2017).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Analysis of functions of the discourse connective AND and its counterparts in English, Czech, Lithuanian, French and Hungarian according to the taxonomy with cross-domain functions (Crible, Degand, 2017).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Mezijazykové srovnání funkcí spojky "a" a možností jejího překladu, založené na paralelních překladech titulků v TED talks. Výchozí studie o nespecifičnosti významu některých spojek v různých jazycích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Cross-linguistic comparison of the functions of the conjunction "and" and the possibilities of its translation, based on the parallel translations of subtitles in TED talks. Pilot study about the underspecification of semantics of some conjunctions in different languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V lingvistice se obvykle slova považují za složená z morfémů, což jsou dále nedělitelné jazykové jednotky nesoucí význam. Zadáním této práce je nalézt automatickou metodu dělení českých slov na morfémy, které by bylo možné přidat do DeriNetu, sítě derivačních vztahů mezi českými slovy.

Vytvořili jsme dvě různé takové metody. První nalézá hranice morfémů na základě hledání rozdílů mezi slovem a jeho derivačním předkem, a tranzitivně mezi všemi slovy v derivačním hnízdě. Tato metoda explicitně modeluje hláskové a morfologické alternace a nalézá nejvhodnější hranice morfémů pomocí metody maximální věrohodnosti. Ve srovnání s moderním systémem Morfessor FlatCat naše metoda přinejhorším mírně zaostává, ovšem v některých testech naopak dosahuje výsledků výrazně lepších.

Druhou metodou je neuronová síť pro současné předpovídání morfologické segmentace a derivačních předků, trénovaná na datech získaných první metodou a na derivačních vztazích ze sítě DeriNet. S naší hypotézou, že tento způsob trénování dvou úloh naráz pomůže k dosažení lepších výsledků oproti trénování samotné segmentace, jsou však ve shodě pouze některé provedené pokusy. Celkově dosahuje neuronová síť horších výsledků než první metoda, pravděpodobně kvůli trénování na datech obsahujících chyby, které se tím přidávají k chybám metody samotné.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In linguistics, words are usually considered to be composed of morphemes: units that carry meaning and are not further subdivisible. The task of this thesis is to create an automatic method for segmenting Czech words into morphemes, usable within the network of Czech derivational relations DeriNet.

We created two different methods. The first one finds morpheme boundaries by differentiating words against their derivational parents, and transitively against their whole derivational family. It explicitly models morphophonological alternations and finds the best boundaries using maximum likelihood estimation. At worst, the results are slightly worse than the state of the art method Morfessor FlatCat, and they are significantly better in some settings.

The second method is a neural network made to jointly predict segmentation and derivational parents, trained using the output of the first method and the derivational pairs from DeriNet. Our hypothesis that such joint training would increase the quality of the segmentation over training purely on the segmentation task seems to hold in some cases, but not in other. The neural model performs worse than the first one, possibly due to being trained on data which already contains some errors, multiplying them.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>DeriNet je lexikální síť modelující derivační vztahy v češtině. Uzly odpovídají lexémům, hrany odpovídají slovotvorným derivacím. Současná verze, DeriNet 1.6, obsahuje 1 027 832 lexémů převzatých ze slovníku MorfFlex, spojených 803 404 derivačními vztahy. Kromě toho obsahuje DeriNet, počínaje verzí 1.5, anotace kompozit (kompozita jsou označena vyhrazeným znakem ve svém záznamu o slovním druhu).
Oproti verzi 1.5 byla verze 1.6 rozšířena o vztahy extrahované ze slovníků dostupných pod svobodnými licencemi, například Wiktionary, a o množství kompozitních značek.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>DeriNet is a lexical network which models derivational relations in the lexicon of Czech. Nodes of the network correspond to Czech lexemes, while edges represent derivational relations between a derived word and its base word. The present version, DeriNet 1.6, contains 1,027,832 lexemes (sampled from the MorfFlex dictionary) connected by 803,404 derivational links. Furthermore, starting with version 1.5, DeriNet contains annotations related to compounding (compound words are distinguished by a special mark in their part-of-speech labels).
Compared to version 1.5, version 1.6 was expanded by extracting potential links from dictionaries available under suitable licences, such as Wiktionary, and by enlarging the number of marked compounds.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek analyzuje kompatibilitu stávající kategorizace licencí v CLARINu s paradigmatem Open Science. V první části se prezentují základní koncepty a teoretický rámec, druhá část řeší kategorizaci licencí v projektu CLARIN do tříd PUB, ACA a RES a možnosti do budoucna tento systém změnit. Varianty možných změn jsou analyzovány jako podklad k další diskusi o změně přístupu k licencím v CLARINu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article investigates the compatibility of the current CLARIN license categorization scheme with the open science paradigm. The first part presents the main concepts and theoretical framework required for the analysis, while the second part discusses the use of the CLARIN categorization system, divided into PUB (public), ACA (academic), and RES (restricted), and potential ways to change it. This paper serves to explore various suggestions for change and to begin discussion of a reformed CLARIN license category scheme.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zkoumáme anotaci zvratných zájmen v korpusech Universal Dependencies (UD) 2.2 (Nivre et al., 2018), přičemž se zaměřujeme zejména na slovanské jazyky. Snažíme se zjistit, zda jsou současná anotační pravidla dostatečně jasná, aby se jimi anotátoři dokázali řídit. Ukazujeme celou řadu nesrovnalostí napříč jazyky v současných datech a navrhujeme vylepšení–někdy anotačních pravidel, většinou však přímo anotace dat. Cílem článku je přispět ke konzistentnější a mezijazykově paralelnější anotaci reflexiv v budoucích vydáních UD.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We explore the annotation of reflexives in Universal Dependencies (UD) 2.2 treebanks (Nivre et al., 2018), with a stronger focus on Slavic languages. We have tried to find out if the current guidelines are transparent and clear enough for the annotators to follow them successfully. We point out a number of inconsistencies in the current annotation across languages, and propose improvements—sometimes of the guidelines, but mostly of the annotation. The goal of the paper is to contribute to more consistent and cross-linguistically parallel annotation of reflexives in the future releases of UD.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku popisujeme anotaci koreferenčních vztahů v multijazykovém paralelním korpusu PAWS. Zaměřujeme se na mezijazykové rozdíly ve vejadřování koreference, jako např. osobní a neosobní slovesné tvary v polypredikativních konstrukcích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we decribe the coreference annotation on a multi-lingual parallel treebank (PAWS), a portion of Wall Street Journal translated into Czech, Russian and Polish which continues the tradition of multilingual treebanks with coreference annotation. The paper focuses on language-specific differences. We analyse syntactic structures concerning anaphoric relations in the languages under analysis, such as personal and impersonal constructions in polypredicative constructions and pro-drop qualities.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>PAWS je anglicko-česko-rusko-polský korpus s anotací koreference.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>PAWS (Parallel Anaphoric Wall Street Journal) is a multi-lingual parallel treebank with coreference annotation. It consists of English texts from the Wall Street Journal translated into Czech, Russian and Polish. In addition, the texts are syntactically parsed and word-aligned. PAWS is based on PCEDT 2.0 and continues the tradition of multilingual treebanks with coreference annotation. PAWS offers linguistic material that can be further leveraged in cross-lingual studies, especially on coreference.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku představujeme PAWS, vícejazykový paralelní treebank s anotací koreference.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present PAWS, a multi-lingual parallel treebank with coreference annotation. It consists of English texts from the Wall Street Journal translated into Czech, Russian and Polish. In addition, the texts are syntactically parsed and word-aligned. PAWS is based on PCEDT 2.0 and continues the tradition of multilingual treebanks with coreference annotation. The paper focuses on the coreference annotation in PAWS and its language-specific differences. PAWS offers linguistic material that can be further leveraged in cross-lingual studies, especially on coreference.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Provádíme analýzu textových prostředků (na angličtině, češtině a němčitě), která je založená na textech, přeložených z angličtiny. Zvláštní důraz této publikace klademe na vlivu překladového faktoru na použití různých typů koheze.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Since translations are influenced by various factors of translation process, it is often difficult to explain the real reasons of a certain construction used in translation data. 
The present contribution describes a cross-linguistic analysis of the interplay between discourse-relational devices (DRDs) and other discourse-related phenomena, such as coreference and bridging relations. The difference between the phenomena under analysis lies in the type of relations, which is expressed by a corresponding device. DRDs express logico-semantic relations between propositions, such as contrast, time, addition and others). Coreference serves the task of linking identical objects or events (i.e. complex anaphors) and bridging anaphora expresses non-identical or near-identical relations between referents, linking them with semantic interconnection.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem výzkumu je srovnání souhry konektorů, koreference a asociační anafory ve němčině, češtině a angličtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The purpose of our study is to analyse the interplay between conjunctions and other discourse-related
phenomena, such as coreference and bridging relations in German, English and Czech. The difference between the phenomena under analysis lies in the type of relations, which is expressed by a
corresponding device.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku představíme kontrastivní analýzu zájmenných adverbií v němčině (dabei, daauf, damit aj.) a jejich ekvivalentů v angličtině, češtině a rušině. Analýza je založena na empirickém výzkumu paralelních textů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents a contrastive analysis of pronominal adverbs in German (dabei, darauf, damit etc.) and their equivalents in English, Czech and Russian. The analysis is based on an empirical study of parallel news texts. Our main focus is to show the interplay between cohesive devices expressed through German pronominal adverbs in text and explore their equivalents in English, Czech and Russian. As the dataset at hand contains translations, we also focus on the influence of the translation factor in parallel texts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek se věnuje přiřazování audia přímo k notopisu reprezentovanému jako obraz, bez jakýchkoliv abstratktních reprezentací. Navrhujeme metodu, která se naučí společný prostor pro reprezentaci krátkých útržků audia a jejich protějšků v obrázkách not pomocí multimodálních konvolučních neuronových sítí. Následně ukazujeme, jak s těmito naučenými reprezentacemi (1) identifikovat příslušnou skladbu podle nahrávky, (2) vyhledávat nahrávky pomocí obrázků not. Všechny vyhledávací modely jsou natrénované na novém velkém multimodálním datasetu audia a notopisu, který je spolu s tímto článkem dán veřejně k dispozici. Dataset obsahuje 479 detailně anotovaných klavírních skladeb od 53 skladatelů, celkem 1129 stran not a více než 15 hodin k nim zarovnaného audia, které bylo z příslušných not syntetizováno. Nad modelem natrénovaným těmito syntetickými daty však provádíme pokusy, které vyhledávají v databázi komplexních not (např. téměř celé dílo pro sólový klavír F. Chopina) a komerčních nahrávek špičkových klavíristů. Naše výsledky naznačují, že navržená metoda spolu s velkým datasetem vede k vyhledávacím modelům, které úspěšně zobecňují ze syntetických trénovacích dat na skutečná data.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This work addresses the problem of matching musical audio directly to sheet music, without any higher-level abstract representation. We propose a method that learns joint embedding spaces for short excerpts of audio and their respective counterparts in sheet music images, using multimodal convolutional neural networks. Given the learned representations, we show how to utilize them for two sheet-music-related tasks: (1) piece/score identification from audio queries and (2) retrieving relevant performances given a score as a search query. All retrieval models are trained and evaluated on a new, large scale multimodal audio–sheet music dataset which is made publicly available along with this article. The dataset comprises 479 precisely annotated solo piano pieces by 53 composers, for a total of 1,129 pages of music and about 15 hours of aligned audio, which was synthesized from these scores. Going beyond this synthetic training data, we carry out first retrieval experiments using scans of real sheet music of high complexity (e.g., nearly the complete solo piano works by Frederic Chopin) and commercial recordings by famous concert pianists. Our results suggest that the proposed method, in combination with the large-scale dataset, yields retrieval models that successfully generalize to data way beyond the synthetic training data used for model building.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Současné modely pro křížové vyhledávání mezi nahrávkami a notovými zápisy přes natrénované multimodální reprezentace používají konvoluční neuronové sítě, jenž očekávají na vstupu pro audio pevně daný časový úsek. Podle tempa nahrávky však toto okno zachycuje různá množství hudebních událostí (not), zatímco fixně dlouhé okno do not jich zachycuje množství, které na tempu téměř nezávisí. V této práci se snažíme tento problém obejít pomocí mechanismu měkké pozornosti, která umožňuje modelu zakódovat pouze ty části útržku audia, které jsou nejvíce relevantní pro vybudování efektivního vyhledávacího klíče. Experimentální výsledky na klasické klavírní hudbě ukazují, že pozornostní mechanismus vyhledávání zlepšuje, a má intuitivně jasné výhody.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Current models for audio–sheet music retrieval
via multimodal embedding space learning use con-
volutional neural networks with a fixed-size win-
dow for the input audio. Depending on the tempo
of a query performance, this window captures
more or less musical content, while notehead den-
sity in the score is largely tempo-independent. In
this work we address this disparity with a soft
attention mechanism, which allows the model to
encode only those parts of an audio excerpt that
are most relevant with respect to efficient query
codes. Empirical results on classical piano music
indicate that attention is beneficial for retrieval performance, and exhibits intuitively appealing
behavior.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek se zabývá parafrázovatelností českých slovesných víceslovných výrazů (kategoriálních slovesa a idiomů). V příspěvku je navržena lexikografická reprezentace parafrází a demonstrováno jejich praktické využití ve strojovém překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this chapter, we explore paraphrasability of Czech verbal MWEs (light verbs constructions and idioms) by single verbs in a semiautomatic experiment
using word embeddings. Further, we propose a lexicographic representation and demonstrate one of its practical application in a machine translation
experiment.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato zpráva popisuje účast týmu Univerzity Karlovy v Praze na CLEF eHealth Consumer Health
Search Task 2018</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present our participation in CLEF Consumer Health Search Task 2018, mainly, its monolingual and multilingual subtasks: IRTask1 and IRTask4. In IRTask1, we use language-model
based retrieval model, vector-space model and Kullback-Leiber divergence query expansion mechanism to build our runs. In IRTask4, we submitted 4 runs for each language of Czech, French and German. We
follow query-translation approach in which we employ a Statistical Machine Translation (SMT) system to get a ranked list of translation hypotheses in English. We use this list for two systems: the first one uses 1-best-list translation to construct queries, and the second one uses a hypotheses reranker to select the best translation (in terms of retrieval performance) to construct queries. We also present our term reranking model for query expansion, in which we deploy feature set from different
resources (the document collection, Wikipedia articles, translation hypotheses). These features are used to train a logistic regression model that can predict the performance when a candidate term is added to a base query.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Hegelovo prázdninové boogie aneb Mondrian, Magritte a avantgardní dialektika

Avantgarda, jako umění, které často tematizuje svoji vlastní formu, je z hlediska Hegelovy filosofie zajímavá jako příklad rozvoje sebe-vědomí. V tomto přípěvku se však naopak podíváme na to, proč avantgardu zajímala Hegelova filosofie. Na příkladech Mondrianovy abstrakce a Magrittova surrealismu ukážeme dva diametrálně odlišné přístupy k inspiraci Hegelem ve výtvarném umění.
Piet Mondrian se na Hegela odkazuje ve svých textech o teorii malířství. Pod vlivem hegelovské filosofie dospívá ke koncepci zobrazování, které označuje jako „abstraktně-reálné“, výraz „oživené skutečnosti abstraktního“.
René Magritte se k Hegelově filosofii vyjadřuje přímo názvy a obsahem svých obrazů. V některých z nich explicitně odkazuje k dialektice, která je však implicitně přítomná ve většině jeho díla.
Na závěr se zamyslíme nad tím, jak můžeme vývoj avantgardního malířství interpretovat jako dialektiku rozumného a skutečného.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Hegel's Holiday Boogie or Mondrian, Magritte and  Avantgarde Dialectics

In this talk we expore hegelian inspiration in Mondrian's theory of art and Magritte's paintings and propose to interpret the development of avantgarde painting as dialectics of the real and the reasonable.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Budou někdy počítače schopny pochopit psaný text? Přeložit ho bezchybně třeba z angličtiny do češtiny? Popsat co je na obrázku? Co jsou umělé neuronové sítě a jak mohou pomoci vyřešit tyto otázky? To se dozvíte v této přednášce. Když bude čas, zmíníme i jak neuronky hrají go nebo řídí auta.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Will computers ever be able to understand written text? Will they be able to translate it from English to Czech without mistakes? Will they be able to describe a picture? What are artificial neural networks and how are they connected to these questions?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace word embeddings a souvisejícího výzkumu probíhajícího na ÚFALu v rámci Jednoho dne s informatikou a matematikou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presentation of word embeddings and related research at ÚFALu within Jeden den s informatikou a matematikou.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Stručný úvod k panelu o umělé inteligenci a humanitních oborech představuje možnosti neuronového strojového překladu a hlubokého učení obecně pro lingvistiku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A brief opening presentation for a panel on AI and the Humanities</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Strojový překlad je královskou disciplínou umělé inteligence a počítačové lingvistiky. Jednu revoluci zažil v 90. letech minulého století (zavedení statistických metod), druhou zažívá právě teď, když neuronové sítě skokově zlepšily kvalitu překladu. Jak se nyní překlad dělá? Podívejme se na ten zázrak technicky, bez silných marketingových slov. Je snad konečně naděje, že počítače začnou textu rozumět?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Machine translation is the one of the key application of artificial inteligence and computational linguistics. Two revolutions happened recently: in the 1990's and these days. What's the current state of the art? Finally a chance that machines will understand the text?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve svém příspěvku podrobně vysvětlím, jakou hlavní výhodu má neuronový překlad oproti předešlým statistickým přístupům k MT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In my talk, I will highlight the benefit that neural machine translation (NMT) has over previous statistical approaches to MT. I will then present the current state of the art in neural machine translation, briefly describing the current best architectures and their performance and limitations. In the second part of the talk, I will outline my planned search for correspondence between sentence meaning as traditionally studied by linguistics (or even semantics and semiotics) and the continuous representations learned by neural networks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zvaná přednáška na seminář pořádaný na FJFI, představující zajímavá vědecká témata studentům magisterského programu. Ve své přednášce jsem představil počítačovou lingvistiku a zejména neuronový strojový překlad.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An invited talk for Master students of FJFI at a seminar introducing various interesting research topics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V prezentaci jsem představil současnou úroveň kvality dosahovanou neuronovým strojovým překladem a podrobně se v diskusi věnoval očekávaným výhodám a rizikům, která do překladatelské profese přináší projekt ELITR.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In my talk, I presented the current translation quality achieved by neural machine translation and thoroughly discussed the expected benefits and risks for interpreters arising from the EU project ELITR.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Hluboké strojové učení v roce 2016 zásadním způsobem změnilo techniky používané ve strojovém překladu. Kvalita strojových překladů se skokově zlepšila a strmý růst stále ještě pozorujeme. Neuronové sítě slibují možnost "učit se reprezentacím". Daří se je tedy učit reprezentacím, které by ladily s něčím jako je význam vět?

V krátkém příspěvku představím překotný vývoj na poli strojového překladu a otázky pro tento obor na příští týden i příštích deset let.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Deep machine learning has crucially changed the techniques used in machine translation in 2016. In my short talk, I will summarize the quick development and questions for the field for the next week and the next ten years.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Seznámení zájemců o obor informatiky a počítačové lingvistiky s nejnovějším vývojem na poli strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A quick introduction to prospective students of Computer Science and Computational Linguistics with recent advances in machine translation</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Automatizovaný systém pro dávkový překlad pro IBM. Prvním jazykovým párem byl překlad z angličtiny do češtiny, další jazyky (maďarština, arabština ap.) jsou přidávány dle potřeby.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Automated system for batch translation for IBM. The first supported translation direction was from English into Czech, other languages (e.g. Hungarian, Arabic) are added as needed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Užíváme systém EVALD k automatickému ohodnocení výstupů několika systémů strojového překladu; výsledky ukazují, že automatické ohodnocení diskurzu v textech umožňuje rozlišit různé systémy strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present results of automatic evaluation of discourse in machine translation (MT) outputs using the EVALD tool, showing that automatic evaluation of discourse in translated texts allows for distinguishing individual MT systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek přináší výsledky hlavní společné úlohy organizované při konferenci o strojovém překladu (WMT) v roce 2018.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the premier
shared  task  organized  alongside  the  Confer-
ence  on  Machine  Translation  (WMT)  2018.
Participants   were   asked   to   build   machine
translation systems for any of 7 language pairs
in both directions, to be evaluated on a test set
of news stories.  The main metric for this task
is human judgment of translation quality. This
year, we also opened up the task to additional
test suites to probe specific aspects of transla-
tion.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Závislostní korpus anotovaný na rovině morfologické (2 miliony slov), syntaktické (1,5 milionu slov) a sémantické (přes 0,8 milionu slov, tedy 49,5 tis. vět). Obsahuje všechny PDT anotace originálních textů, které vznikly v rámci různých projektů na UFALu mezi roky 1996 a 2018 (PDT 1.0, PDT 2.0, PDT 2.5, PDT 3.0, PDiT 1.0 and PDiT 2.0) a jejich opravy; seznam autorů pokrývá autory všech publikovaných korpusů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Prague Dependency Treebank 3.5 is the 2018 edition of the core Prague Dependency Treebank (PDT). It contains all PDT annotation made at the Institute of Formal and Applied Linguistics under various projects between 1996 and 2018 on the original texts, i.e., all annotation from PDT 1.0, PDT 2.0, PDT 2.5, PDT 3.0, PDiT 1.0 and PDiT 2.0, plus corrections, new structure of basic documentation and new list of authors covering all previous editions. The Prague Dependency Treebank 3.5 (PDT 3.5) contains the same texts as the previous versions since 2.0; there are 49,431 annotated sentences (over 800 thousand nodes) on all layers, from tectogrammatical to words, and additional sentences on the analytical (surface dependency syntax) and morphological layers of annotation (approx. 2 million words in total). Closely linked to the tectogarammtical layer is the annotation of sentence information structure, multiword expressions, coreference, bridging relations and discourse relations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve stati se předládají vedle přehledu konstrukcí pokládaných za gramatickou diatezi argumenty pro případy, kdy deagentní diateze spojená s reciproční diatezí může vyjádřit konatele pomocí předložkového výrazu mezi + Instrumentál.Jsou zde uvedena jistá omezení na tvorbu těchto konstrukcí. Ilustruje se také jejich častá homonymie s konstrukcemi s významem lokace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The short survey of the constructions understood as grammatical diathesis is given. The discussion of the generally accepted hypothesis that the surface expression of the actor is excluded in deagentive constructions. The counterexamples where the combination of the deagentive and reciprocal diathesis are submitted. Some restrictions for such constructions are studied as well.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku se podrobují diskusi distinkce uvnitř lokálního určení spojené s distribucí výrazů v předložkových konstrukcích v + Loc, na + Loc, u + Gen, po + 6 a s jejich sémantickými specifiky. Stať je stavěna na materiálu získaného pomocí nástroje ForFun 1.0.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The distribution of the Czech prepositional constructions connected with the semantics of space is studied and the proposal how to present their semantics within the grammatical description is submitted. The prepositional constructions v+6, 
na+6, u+2, and po+6 are included, the rich data for this analysis were obtained from PDT 3.0 using the tool ForFun 1.0</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek je věnován několika problémům, které vznikají při aplikaci valenční teorie FGP na valenci deverbativních substantiv a které demonstrují fakt, že aplikace valence slovesné na substantivní nemůže být přímočará a že substantivní valenční rámce nejsou vždy analogií rámců  jejich základových sloves. U substantiva přibývají formy typické pro volná doplnění uvozené zpravidla sekundárními předložkami, které se svou lexikální náplní obsahově blíží obligatorním aktantům (oznámení ze strany úřadu ohledně návštěvních hodin). Pokládáme je za stylové varianty, které se nezapisují do valenčních rámců, ale jsou ošetřeny speciálními pravidly. Jiné, uvozené primárními předložkami, mají marginální povahu; vyžadují však zjištění kontextových podmínek pro jejich užití (otázka nad stavem společnosti / po stavu společnosti), včetně podmínek jejich vzájemné zaměnitelnosti, popř. též úvah nad odstraněním jejich homonymie (otázka na psychologa.ADDR/PAT).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we demonstrate how to treat some difficult phenomena related to non-standard expression of valency patterns of Czech deverbal nouns. We introduce some new attributes to indicate (i) alternative, stylistically marked forms of actants expressed by secondary prepositions, and (ii) marginal forms of actants expressed by primary prepositions. We show that the number of forms of adnominal actants is often increased in comparison with the verbal ones, differentiating regular forms and alternative forms. We also discuss cases of ambiguous forms of two different actants and give criteria for their interpretation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kritický rozbor díla předního francouzského lingvisty J.-M. Zemba, především jeho prací o informační struktuře věty a negaci z pohledu autorčina vlastního přístupu k těmto základním otázkám syntaktické struktury. věty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A critical analysis of the writings of a prominent French linguist  J.-M. Zemb, especially those concerning the information structure and negation, from the point of view of the author's own approach to these basic issues.of the syntactic structure of the sentence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Korpusová studie lokální koherence textu založené na anaforických vztazích mezi elementy v tématické (Topic) a rématické (Focus) části věty v různých žánrech textů Pražského závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A corpus-based study of local coherence as established by anaphoric links between the elements in the thematic (Topic) and the rhematic (Focus) parts of sentences in different genres of texts in the Prague Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Analyzujeme data Pražského diskurzního korpusu 2.0 a na jejich základě studujeme tzv. tématické posloupnosti, tj. řetězce anaforických vztahů mezi větami vzhledem k jejich informační struktuře (topic–focus articulation).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We analyze the data of the Prague Discourse Treebank 2.0 as for the text coherence based on the so-called thematic progressions, that is chains of anaphoric links between sentences with regard to their topic–focus articulation (information structure).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zkoumáme dvě dichotomie, kterými se zabývá literatura o informační struktuře věty, konkrétně dichotomii topic/focus založenou na vztahu "býti o čem", a dichotomii mezi danou a novou informací. Zaměřujeme se obzvláště na otázku, zda dichotomie topic/focus může být založena na rozlišení mezi danou a novou informací, či zda vztah "býti o čem" tvoří přesnější obraz situace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We put under scrutiny two dichotomies discussed in the information structure literature, namely the dichotomy of topic and focus based on the relation of aboutness, and the dichotomy between given and new information. In particular, we examine whether the topic/focus dichotomy can be based on the distinction between given and new information, or whether the ‘aboutness’ relation is a more appropriate basis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek o slovníku anglicko-českých slovesných synonym na základě PCEDT. Synonyma jsou řazena do synonymních tříd, obsahujících anglické a české členy, které jsou zároveň propojeny s externími a interními lexikálními zdroji.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the present paper, we focus on synonymy of verbs in a bilingual, Czech-English setting. We introduce a new lexical resource called CzEngClass. Our research of semantic equivalence of verbs is based on the FGD theory on the syntactic side, and gets main inspiration from FrameNet on the semantic side. As the main evidence, we use a parallel dependency corpus (the Prague Czech-English Dependency Treebank 2.0). We consider this “bottom-up” approach a novel and appropriate approach to study verbal synonymy. Synonymous Czech and English verbs are being grouped into cross-lingual synonym classes and captured in the CzEngClass lexicon. This lexicon contains not only mappings of valency arguments to semantic roles for each member of the synonym group, but also links them to individual verb entries as captured in selected existing lexical resources, such as FrameNet, VerbNet, Vallex(es) and Czech and English WordNets. We believe such explicit description of verbal synonymy relations, collected in a richly interconnected lexicon,  contributes valuable knowledge to both traditional linguistics and various NLP tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Synonymní slovník CzEngClass je výsledkem projektu zkoumajícího sémantickou „ekvivalenci“ slovesných významů a jejich valenčního chování v paralelních zdrojích česko-anglického jazyka, tj. vztahujících se k slovním významům s ohledem na kontextuálně založená slovesná synonyma. Položky lexikonu jsou propojeny s PDT-Vallexem (http://hdl.handle.net/11858/00-097C-0000-0023-4338-F), EngVallexem (http://hdl.handle.net/11858/00-097C-0000-0023-4337-2), CzEngVallexem (http://hdl.handle.html.net/11234/1-1512), FrameNetem (https://framenet.icsi.berkeley.edu/verbverbands/), VerbNetem (http://coloredu.html.html Součástí datového souboru je soubor odrážející volby anotátorů pro přiřazení sloves do tříd.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The CzEngClass synonym verb lexicon is a result of a project investigating semantic ‘equivalence’ of verb senses and their valency behavior in parallel Czech-English language resources, i.e., relating verb meanings with respect to contextually-based verb synonymy. The lexicon entries are linked to PDT-Vallex (http://hdl.handle.net/11858/00-097C-0000-0023-4338-F), EngVallex (http://hdl.handle.net/11858/00-097C-0000-0023-4337-2), CzEngVallex (http://hdl.handle.net/11234/1-1512), FrameNet (https://framenet.icsi.berkeley.edu/fndrupal/), VerbNet (http://verbs.colorado.edu/verbnet/index.html), PropBank (http://verbs.colorado.edu/%7Empalmer/projects/ace.html), Ontonotes (http://verbs.colorado.edu/html_groupings/), and Czech (http://hdl.handle.net/11858/00-097C-0000-0001-4880-3) and English Wordnets (https://wordnet.princeton.edu/). Part of the dataset is a file reflecting annotators choices for assignment of verbs to classes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Synonymní slovník CzEngClass je výsledkem projektu zkoumajícího sémantickou „ekvivalenci“ slovesných významů a jejich valenčního chování v paralelních zdrojích česko-anglického jazyka, tj. vztahujících se k slovním významům s ohledem na kontextuálně založená slovesná synonyma. Položky lexikonu jsou propojeny s PDT-Vallexem (http://hdl.handle.net/11858/00-097C-0000-0023-4338-F), EngVallexem (http://hdl.handle.net/11858/00-097C-0000-0023-4337-2), CzEngVallexem (http://hdl.handle.html.net/11234/1-1512), FrameNetem (https://framenet.icsi.berkeley.edu/verbverbands/), VerbNetem (http://coloredu.html.html Součástí datového souboru je soubor odrážející volby anotátorů pro přiřazení sloves do tříd.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The CzEngClass synonym verb lexicon is a result of a project investigating semantic ‘equivalence’ of verb senses and their valency behavior in parallel Czech-English language resources, i.e., relating verb meanings with respect to contextually-based verb synonymy. The lexicon entries are linked to PDT-Vallex (http://hdl.handle.net/11858/00-097C-0000-0023-4338-F), EngVallex (http://hdl.handle.net/11858/00-097C-0000-0023-4337-2), CzEngVallex (http://hdl.handle.net/11234/1-1512), FrameNet (https://framenet.icsi.berkeley.edu/fndrupal/), VerbNet (http://verbs.colorado.edu/verbnet/index.html), PropBank (http://verbs.colorado.edu/%7Empalmer/projects/ace.html), Ontonotes (http://verbs.colorado.edu/html_groupings/), and Czech (http://hdl.handle.net/11858/00-097C-0000-0001-4880-3) and English Wordnets (https://wordnet.princeton.edu/). Part of the dataset are files reflecting annotators choices and agreement for assignment of verbs to classes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku se zaměřujeme na valency a synonymii sloves v bilingvním česko-anglickém kontextu. Náš výzkum sémantické ekvivalence sloves je z pohledu syntaktického (včetně valence) založen na FGP a z pohledu sémantického je inspirován především FrameNetem a VerbNetem. Jako hlavní zdroj korpusových dokladů používáme Pražský česko-anglický závislostní korpus 2.0. Postup výzkumu “od spoda nahoru” považujeme za nový a adekvátní přístup pro stadium slovesné synonymie. Synonymní česká a anglická slovesa jsou uskupeny do mezijazykových synonymních tříd v novém slovníku CzEngClass. Tento slovník pro každé synonymní sloveso dané třídy obsahuje jak mapování valenčních argumentů na sémantické role, tak propojení s jednotlivými slovesnými významy ve FrameNetu, VerbNetu, ve Vallexech a v anglickém WordNetu, což ze slovníku CzEngClass zároveň vytváří bohatě propojený slovník.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the present article, we focus on valency and synonymy of verbs in a bilingual, Czech-English setting. Our research of semantic equivalence of verbs is based on the FGD theory on the syntactic side (including valency), and gets main inspiration from FrameNet and VerbNet on the semantic side. As the main source of evidence, we use the Prague Czech-English Dependency Treebank 2.0. We consider this “bottom-up” approach a novel and appropriate approach to study verbal synonymy. Synonymous Czech and English verbs are being grouped into cross-lingual synonym classes and captured in the new CzEngClass lexicon. This lexicon contains not only mappings of valency arguments to semantic roles for each member of the synonym group, but also links them to individual verb entries in FrameNet, VerbNet, Vallex(es) and Czech and English WordNets, making CzEngClass also a richly interconnected lexicon.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme první výsledky našeho projektu o slovesné synonymii na základě paralelního česko-anglického korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the first findings of our recently started project of building a new lexical resource called CzEngClass, which consists
of bilingual verbal synonym groups. In order to create such a resource, we explore semantic ‘equivalence’ of verb senses of generally
different verbs in a bilingual (Czech-English) setting by using translational context of real-world texts in a parallel, richly annotated
dependency corpus. When grouping semantically equivalent verb senses into classes of synonyms, we focus on valency (arguments
as deep dependents with morphosyntactic features relevant for surface dependencies) and its mapping to a set of semantic “roles” for
verb arguments, common within one class. We argue that the existence of core argument mappings and certain adjunct mappings to a
common set of semantic roles is a suitable criterion for a reasonable verb synonymy definition, possibly accompanied with additional
contextual restrictions. By mid-2018, the first version of the lexicon called CzEngClass will be publicly available.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Při studiu synonymických vztahů jsme vzali v potaz vztah syntaxe a sémantiky, abychom mohli lépe definovat slovesnou valenci. Za pomocí anotačního  experimentu na datech UD a PUDs jsme zjišťovali koreleaci syntaxe a sémantiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>While studying verbal synonymy, we have investigated the relation between syntax and semantics
in hope that the exploration of this relationship will help us to get more insight into the question
of synonymy as the relationship relating (similar) meanings between different lexemes. Most
synonym lexicons (Wordnets and similar thesauri) are based on an intuition about the similarity
of word meanings, or on notions like “semantic roles.” In some cases, syntax is also taken into
account, but we have found no annotation and/or evaluation experiment to see how strongly
can syntax contribute to synonym specification. We have prepared an annotation experiment
for which we have used two treebanks (Czech and English) from the Universal Dependencies
(UD) set of parallel corpora (PUDs) in order to see how strong correlation exists between syntax
and the assignment of verbs in context to pre-determined (bilingual) classes of synonyms. The
resulting statistics confirmed that while syntax does support decisions about synonymy, such
support is not strong enough and that more semantic criteria are indeed necessary. The results of
the annotation will also help to further improve rules and specifications for creating synonymous
classes. Moreover, we have collected evidence that the annotation setup that we have used
can identify synonym classes to be merged, and the resulting data (which we plan to publish
openly) can possibly serve for the evaluation of automatic methods used in this area.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku popisujeme slovník bilingvní slovník anglických a českých slovesných synonym, CzEngClass. Ta jsou zachycena v synonymních třídách na základě kontextu. Zároveň je jejich valenční struktura vztažena k sémantickým rolím přiřazeným dané synonymní třídě.  Navíc jsou jednotlivé členy synonymní třídy propojeny s externími (FrameNet, VerbNet, PropbBank, WordNet) a interními zdroji (PDT-valency lexicons, Vallex and Czech WordNet). V čánku představujeme první verzi slovníku, obsahující 200 tříd (cca 1800 sloves) a evaluaci mezianotátorské shody.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes CzEngClass, a bilingual lexical resource being built to investigate verbal synonymy in bilingual context and to relate semantic roles common to one synonym class to verb arguments (verb valency). In addition, the resource is linked to existing resources with the same or a similar aim: English and Czech WordNet, FrameNet, PropBank, VerbNet (SemLink), and valency lexicons for Czech and English (PDT-Vallex, Vallex and EngVallex). There are several goals of this work and resource: (a) to provide gold standard data for automatic experiments in the future (such as automatic discovery of synonym classes, word sense disambiguation, assignment
of classes to occurrences of verbs in text, coreferential linking of verb and event arguments in text, etc.), (b) to build a core (bilingual) lexicon linked to existing resources, for comparative studies and possibly for training automatic tools, and (c) to enrich the annotation of a parallel treebank, the Prague Czech English Dependency Treebank, which so far contained valency annotation but has not linked synonymous senses of verbs together. The method used for extracting the synonym classes is a semi-automatic process with a substantial amount of manual work during filtering, role assignment to classes and individual members’ arguments, and linking to the external lexical resources. We present the first version with 200 classes (about 1800 verbs) and evaluate interannotator agreement using several metrics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek představuje editor pro tvorbu bilingvního slovesného synonymického slovníku, který propojuje jednotlivé slovesné třídy a jejich členy s dalšími lexikálními zdroji.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper (poster) presents a tool used for building a new cross-lingual verbal synonym lexical resource called CzEngClass, as well
as the structure of the lexicon. This lexicon captures interlingual synonyms, using valency behavior of synonymous verbs in relation
to semantic roles as one of the criteria for defining such interlingual synonymy. The tool, called Synonym Class Editor - SynEd, is a
user-friendly tool specifically customized to build and edit individual entries in the lexicon. It helps to keep the cross-lingual synonym
classes consistent and linked to internal as well as well-known external lexical resources. The structure of SynEd also allows to keep and
edit the appropriate syntactic and semantic information for each Synonym Class member. The editor makes it also possible to display
examples of class members usage in translational (parallel corpus) context. SynEd runs platform independently and may be used for
multiple languages. SynEd, CzEngClass and services based on them will be openly available.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předložkové skupiny představují druhou nejčastější formu valenčních doplnění českých substantiv. V této studii byla zkoumána na datech valenčního slovníku NomVallex, a to u tří sémantických tříd, Communication, Mental action a Psychological state. Tyto tři třídy adnominální předložkové skupiny ve velké míře sdílejí, liší se však míra jejich zastoupení. Srovnání valenčních vlastností substantiv z NomVallexu a jejich základových sloves z valenčního slovníku Vallex nám umožnilo upřesnit popis protějšků adnominálních i adverbálních předložkových skupin a ukázat, že tyto předložkové skupiny podléhají nejrůznějším posunům ve formách participantů. Díky tomuto srovnání byl také zjištěn vysoký podíl nově získaných adnominálních předložkových skupin v celkovém počtu předložkových skupin (minimálně 39%).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Prepositional groups represent the second most frequent form of valency complementations of Czech nouns (preceded only by prepositionless genitive). In this paper, we exploit the NomVallex lexicon which covers nouns belonging to three semantic classes, i.e. Communication, Mental action and Psychological state. We show that adnominal prepositional groups are shared among these semantic classes to a large extent but their frequency differs. A comparison between valency of nouns from NomVallex and valency of their base verbs from Vallex lexicon reveals that various non-systemic shifts in surface forms of participants exist between the verbal and adnominal prepositional groups. It also enables us to state a high ratio of non-systemic adnominal prepositional groups to all adnominal prepositional groups in the lexicon (not less than 39%).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>České valenční slovníky doposud pokrývaly především valenci slovesnou, slovníkové zpracování valence substantiv se tedy potýká s řadou teoretických i praktických otázek. V příspěvku představujeme východiska a cíle při budování valenčního slovníku českých deverbativních substantiv zvaného NomVallex, který vzniká v rámci projektu Valenční slovník substantiv založený na korpusu, s počátkem řešení v lednu 2016. Po teoretické stránce vycházíme z valenční teorie funkčního generativního popisu, při vlastním lexikografickém zpracování se opíráme o korpusový materiál (data Pražského závislostního korpusu a Českého národního korpusu – subkorpusů řady SYN). Při tvorbě slovníku budou částečně využita data valenčního slovníku PDT-Vallex, zpracování hesel však bude zdokonaleno v několika aspektech (například v pokrytí všech významů substantiva). Kritériem pro zařazení do slovníku bude především sémantická třída substantiva a složitost jeho valenčního chování, zejména specifické formy participantů. Valence substantiv bude zachycena pomocí valenčních rámců, výčtu všech možných kombinací adnominálních participantů a korpusových příkladů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The corpus-based valency lexicon of Czech nouns NomVallex is a starting project building upon the theory of valency developed within Functional Generative Description and extending the two lexicons developed within this tradition, PDT-Vallex and Vallex. The linguistic material is extracted from Czech linear and syntactically annotated corpora. In comparison with PDT-Vallex, the treatment of entries will be more exhaustive, for example, in the coverage of senses and in the semantic classification added to selected lexical units. The main criteria for including nouns in the lexicon will be semantic class membership and the complexity of valency patterns. Valency of nouns will be captured in the form of valency frames, enumeration of combinations of adnominal participants, and corpus examples.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Počítačový nástroj Evaluátor diskurzu pro cizince (EVALD 3.0 pro cizince) slouží k automatickému hodnocení povrchové koherence (koheze) textů v češtině psaných nerodilými mluvčími češtiny. Software hodnotí úroveň předloženého textu z hlediska jeho povrchové výstavby (zohledňuje např. frekvenci a rozmanitost užitých spojovacích prostředků, bohatost slovní zásoby, koreferenční vztahy apod.). Nová verze (3.0) přidává množinu rysů zaměřených na informační strukturu věty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Evaluator of Discourse for Foreigners (EVALD 3.0 for Foreigners) is a software that serves for automatic evaluation of surface coherence (cohesion) in Czech texts written by non-native speakers of Czech. Software evaluates the level of the given text from the perspective of its surface coherence (i.e. it takes into consideration, e.g., the frequency and diversity of connectives, richness of vocabulary, coreference relations etc.). The new version (3.0) adds a set of features related to the topic-focus articulation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Počítačový nástroj Evaluátor diskurzu (EVALD 3.0) slouží k automatickému hodnocení povrchové koherence (koheze) textů v češtině psaných rodilými mluvčími češtiny. Software tedy hodnotí (známkuje) úroveň předloženého textu z hlediska jeho povrchové výstavby (zohledňuje např. frekvenci a rozmanitost spojovacích prostředků, bohatost slovní zásoby, koreferenční vztahy apod.). Nová verze (3.0) přidává množinu rysů zaměřených na informační strukturu věty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Evaluator of Discourse (EVALD 3.0) is a software that serves for automatic evaluation of surface coherence (cohesion) in Czech texts written by native speakers of Czech. Software evaluates the level of the given text from the perspective of its surface coherence (i.e. it takes into consideration, e.g., the frequency and diversity of connectives, richness of vocabulary, coreference relations etc.). The new version (3.0) adds a set of features related to the topic-focus articulation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Podíváme se na jeden ze základních kamenů taxonomie vztahů v Universal Dependencies, na tzv. core arguments (základní aktanty) a na způsoby, jak je lze odlišit od tzv. oblique dependents (vedlejší aktanty a volná doplnění).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We will look at one of the cornerstones of Universal Dependencies relation taxonomy, the core arguments and their difference from oblique dependents.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato monografie představuje srovnávací studii přístupů k anotaci morfologie a syntaxe přirozených jazyků s důrazem na použitelnost v mnohojazyčném prostředí. Anotací se rozumí přidání lingvistických kategorií a vztahů do digitalizovaného textu v přirozeném jazyce. Výsledkem je anotovaný korpus; vzhledem k tomu, že syntaktické vztahy jsou často reprezentované jako závislostní stromy, tato monografie se soustředí na závislostní korpusy (treebanky).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This monograph presents a comparative study of annotation approaches to morphology and syntax of natural languages, with emphasis on applicability in a multilingual environment. Annotation is understood as adding linguistic categories and relations to digitally encoded natural language text, resulting in annotated corpus; as syntactic relations are often represented in the form of dependency trees, the annotated corpora covered by the monograph are dependency treebanks. Many treebanks exist and their annotation styles vary significantly, which hampers their usefulness for linguists and language engineers. We survey several harmonization efforts that tried to come up with cross-linguistically applicable annotation guidelines, including the most recent and broadest effort to date, Universal Dependencies. We examine language description on three levels: 1. tokenization and word segmentation, 2. morphology, and 3. surface dependency syntax. For each language phenomenon we provide a comparison of its analysis and annotation in various existing treebanks (or other corpora, for tokenization and morphology), pointing out advantages and disadvantages of the competing approaches. On the morphological layer, we go even beyond the currently available corpora and provide a typological survey of features that will be needed when less-resourced languages are covered by an annotation project. We conclude that no single approach is suitable for all purposes, but a good approach must not lose information, so that annotation can be converted to another style when necessary.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představím Universal Dependencies, komunitní projekt v počítačové lingvistice, který poskytuje morfologicky a syntakticky anotované korpusy v mnoha jazycích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I will present Universal Dependencies, a community project in computational linguistics that develops morphologically and syntactically annotated corpora for a large number of languages. UD started as an attempt to harmonize language resources used in various software tools for natural language processing. However, it quickly became a valuable resource also for corpus-based research. Three years after its first release, UD contains over 100 dependency treebanks in more than 60 languages. Despite the inevitable bias towards “big” languages, treebanks from less resourced language families are gradually added. In my talk, I will discuss some challenges of building a universal annotation scheme for all languages. In particular, how to make sure that comparable phenomena are annotated in a comparable fashion, without making all the languages look the same.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představím Universal Dependencies, celosvětový komunitní projekt zacílený na tvorbu mnohojazyčných korpusů anotovaných podle jednotných pravidel na morfologické a syntaktické rovině. Proberu koncept základních aktantů (core arguments), jednoho z pilířů, na kterých stojí anotační schéma UD. Ve druhé části přednášky se zaměřím na některé zajímavé problémy spojené s aplikací Universal Dependencies na slovanské jazyky. Na příkladech z 12 slovanských jazyků, které jsou v současné době reprezentované v UD, ukážu, že na mezijazykové konzistenci je ještě stále co zlepšovat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I will present Universal Dependencies, a worldwide community effort aimed at providing multilingual corpora, annotated at the morphological and syntactic levels following unified annotation guidelines. I will discuss the concept of core arguments, one of the cornerstones of the UD framework. In the second part of the talk I will focus on some interesting problems and challenges of applying Universal Dependencies to the Slavic languages. I will discuss examples from 12 Slavic languages that are currently represented in UD and show that cross-linguistic consistency can still be improved.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Testovací data rozebraná systémy, které se účastnily soutěže v syntaktické analýze Universal Dependencies při konferenci CoNLL 2018.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Test data parsed by systems submitted to the CoNLL 2018 shared task in parsing Universal Dependencies.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Konference o počítačovém učení přirozeného jazyka (CoNLL) každoročně zahrnuje tzv. společnou úlohu, tedy soutěž, jejíž účastníci trénují a testují své systémy strojového učení na jednotné sadě dat. Jedna z úloh roku 2018 byla věnována učení závislostních syntaktických analyzátorů (parserů) pro velké množství jazyků, v reálné situaci bez jakýchkoli ručních anotací na vstupu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Every year, the Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their learning systems on the same data sets. In 2018, one of two tasks was devoted to learning dependency parsers for a large number of languages, in a real-world setting without any gold-standard annotation on test input.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku představujeme první verzi Pražské databáze forem a funkcí, ForFun, jakožto neocenitelný zdroj jazykových dat pro lingvistický výzkum, zejména při popisu syntaktických funkcí a jejich formálních realizací v češtině. ForFun je vybudován na několika bohatě syntakticky anotovaných korpusecg z rodiny Pražských závislostních korpusů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>NOT FINAL VERSION.
In this paper, we introduce first version of ForFun, Prague Database of Syntactic Forms and Functions, as an invaluable resource for
profound linguistic research, particularly in describing syntactic functions and their formal realizations. ForFun takes advantage of sev-
eral richly syntactically annotated corpora, collectively called Prague Dependency Treebanks. The ForFun brings this rich and complex
annotation of Czech sentences closer to common researchers. We demonstrate that the ForFun 1.0 provides valuable and rich material
allowing to elaborate various syntactic issues in depth. We believe that nowadays when corpus linguistics differs from traditional linguis-
tics in its insistence on a systematic study of authentic examples of language in use, our database will contribute to the comprehensive
syntactic description.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládáme popis času a prostoru v českých větách na základě databáze ForFun. ForFun je nový lingvistický zdroj postavený na Pražskými závislostními korpusy češtiny. Umožňuje prohledávání tisíců skutečných příkladů tříděných podle užitých forem a také hloubkově syntaktických funkcí. Na základě databáze provedeme podrobný popis významů časových a prostorových určení včetně seznamu formálních prostředků. Příklady pocházejí z psaných i tištěných textů. Práce vychází z dat a neklade si za cíl nový teoretický popis.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a description of time and space modifications in Czech sentences based on the ForFun database. ForFun is a new resource built on annotated corpora of Czech—Prague Dependency Treebanks—for inspecting thousands of real examples categorized by their form as well as by their deep syntactic function. Based on the database, we perform a detailed description of meanings of time and space modifications including a list of formal means with real examples coming from both written and spoken texts. It should be emphasized that the study is data-oriented rather than theory-oriented.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme databázi forem a funkcí vybudovanou na podkladě ručně anotovaných víceúrovňových korpusů češtiny zvaných Pražské závislostní korpusy. Pražská databáze forem a funkcí (ForFun) byla vytvořena, aby usnadnila lingvistické badání o vztahu funkce a formy, což je jeden z hlavních úkolů, jak v teoretické lingvistice, tak v oblasti počítačového zpracování jazyka. V článku jsou představeny možnosti, jakým způsobem lze v databázi vztah funkce a formy zkoumat. 
Článek je z větší části založen na již dříve prezentovaném příspěvku na 16th International Workshop on Treebanks and Linguistic Theories in Prague (Bejček et al., 2017).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of the contribution is to introduce a database of linguistic forms and their functions
built with the use of the multi-layer annotated corpora of Czech, the Prague Dependency
Treebanks. The purpose of the Prague Database of Forms and Functions (ForFun) is to help the
linguists to study the form-function relation, which we assume to be one of the principal tasks
of both theoretical linguistics and natural language processing. We demonstrate possibilities
of the exploitation of the ForFun database.
This article is largely based on a paper presented at the 16th International Workshop on
Treebanks and Linguistic Theories in Prague (Bejček et al., 2017).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Na pozadí dvou skupin sloves, lexikálních a syntaktických reciprok v češtině, diskutujeme otázky homonymie a kombinovatelnosti reflexivity a reciprocity. Ukazujeme, že zdrojem homonymie i možnosti kombinace reflexivity a reciprocity je reflexivní zájmeno a že klíčovou úlohu sehrává jeho povrchověsyntaktická pozice. V případě homonymie je při reciprocitě a reflexivitě zasažena stejná dvojice situačních participantů / valenčních doplnění, reflexivní zájmeno tedy obsazuje totožnou povrchověsyntaktickou pozici a koreferuje s plurálovým subjektem. V případě kombinací těchto významů jde o rozdílné pozice a reflexivita a reciprocita zasahují různé dvojice participantů / doplnění. Uvedené dvě skupiny sloves se přitom vzhledem k homonymii a kombinovatelnosti těchto významů chovají do značné míry odlišně.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this article, we focus on two groups of Czech verbs, lexical and syntactic reciprocals. We provide an analysis of their syntactic properties with respect to reciprocity and reflexivity, their possible ambiguity and combination. We demonstrate that it is the reflexive pronoun and its surface expression that play a key role in the studied phenomena.  In case of ambiguity, both reciprocity and reflexivity affect the same pair of situational participants / valency complementations; the reflexive pronoun occupies the same syntactic position and it corefers with the plural subject. In contrast, when these phenomena combine, different pairs of participants / complementations are involved. We also show that lexical and syntactic reciprocals differ with respect to the studied phenomena.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zaměřil na polyfunkčnost reflexiv v češtině, vymezení jejich hlavní funckí, které v jazyce plní a důsledky pro jejich popis ve slovníku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk focused on multiple functions which the reflexives have in the Czech language and their representation in a lexicon.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valenční slovník českých sloves (též VALLEX 3.5) poskytuje informace o valenční struktuře českých sloves v jejich jednotlivých významech, které charakterizuje pomocí glos a příkladů; tyto základní údaje jsou doplněny o některé další syntaktické a sémantické charakteristiky. VALLEX 3.5 zachycuje téměř 4 600 českých sloves, která odpovídají více než 10 700 lexikálním jednotkám, tedy vždy danému slovesu v daném významu; tato verze je rozšířena o anotaci komplexních predikátů s kategoriálním slovesem (zpracováno celkem téměř 1 500 komplexních predikátů).
VALLEX je budován jako odpověď na potřebu rozsáhlého a kvalitního veřejně dostupného slovníku, který by obsahoval syntaktické a sémantické charakteristiky českých sloves a byl přitom široce uplatnitelný v aplikacích pro zpracování přirozeného jazyka. Při navrhování koncepce slovníku byl proto kladen důraz na všestranné využití jak pro lidského uživatele, tak pro automatické zpracování češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>VALLEX 3.5 provides information on the valency structure (combinatorial potential) of verbs in their particular senses, which are characterized by glosses and examples. VALLEX 3.5 describes almost 4 600 Czech verbs in more than 10 700 lexical units, i.e., given verbs in the given senses; this version is enriched with an annotation of light verb constructions -- almost 1 500 complex predicates have been added.
VALLEX is a is a collection of linguistically annotated data and documentation, resulting from an attempt at formal description of valency frames of Czech verbs. In order to satisfy different needs of different potential users, the lexicon is distributed (i) in a HTML version (the data allows for an easy and fast navigation through the lexicon) and (ii) in a machine-tractable form as a single XML file, so that the VALLEX data can be used in NLP applications.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Licenční smlouva a smlouva o poskytnutí služeb mezi Univerzitou Karlovou a Datlowe, s.r.o. (1. 1. 2018 -- 1. 1. 2020) obsahuje právo na užití dat PDT 2.5 a software pro textovou analytiku natrénovaného na PDT 2.5, a to pro komerční účely. Jedná se o upravené verze nekomerčního software a dat jinak dostupných z repozitáře LINDAT/CLARIN. Jmenovitě: software pro analýzu tvarosloví a lematizaci, syntaktickou analýzu a analýzu názvů pojmenovanýchch entit.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Licence and service agreement between Charles University and Datlowe Ltd.(January 1, 2018 -- January 1, 2020) covers the right to use the data of PDT 2.5 and the software for text analytics trained on PDT 2.5, for commercial purposes. The data and software (non-adapted) is otherwise available for free through the LINDAT/CLARIN repository for non-commercial use under the appropriate variant of the Creative Commons license. Namely, software for morphological analysis, lemmatization, syntactic analysis and named-entity recognition.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ačkoli treebanky anotované podle pravidel univerzálních závislostí (UD) dnes existují pro mnoho jazyků, cíl anotace stejných jevů křížově jazykově konzistentním způsobem není vždy splněn. V této studii zkoumáme jeden jev, u něhož se domníváme, že takový soulad chybí, a to expletiva. Takové prvky zaujímají pozici, která je strukturálně spojena s hlavním argumentem (nebo někdy s volnou závislostí), a přesto jsou nesouvisející a sémanticky prázdné. Mnoho UD stromů označuje alespoň některé prvky za expletiva, ale škála jevů se liší mezi stromy, a to i u úzce příbuzných jazyků, a někdy dokonce u různých stromů pro stejný jazyk. V tomto článku uvádíme kritéria pro určení výrazů, které jsou použitelná napříč jazyky a kompatibilní s cíli UD, poskytujeme přehled výrazů, které se nacházejí v současných UD stromech, a předkládáme doporučení pro anotaci expletiv, aby bylo možné v budoucích vydáních dosáhnout konzistentnější anotace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Although treebanks annotated according to the guidelines of Universal Dependencies (UD) now exist for many languages, the goal of annotating the same phenomena in a crosslinguistically consistent fashion is not always met. In this paper, we investigate one phenomenon where we believe such consistency is lacking, namely expletive elements. Such elements occupy a position that is structurally associated with a core argument (or sometimes an oblique dependent), yet are non-referential and semantically void. Many UD treebanks identify at least some elements as expletive, but the range of phenomena differs between treebanks, even for closely related languages, and sometimes even for different treebanks for the same language. In this paper, we present criteria for identifying expletives that are applicable across languages and compatible with the goals of UD, give an overview of expletives as found in current UD treebanks, and
present recommendations for the annotation of expletives so that more consistent annotation can be achieved in future releases.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje analýzu českých slovesných předpon, což je první krok projektu, který má konečný cíl automatickou morfematickou analýzu češtiny. Studovali jsme předpony, které se mohou vyskytnout v českých slovesech, zejména jejich možné a nemožné kombinace. Popisujeme postup rozpoznávání předpon a odvozujeme obecná pravidla pro výběr správného výsledku. Analýza "dvojitých" prefixů umožňuje vyvodit závěry o univerzálnosti první předpony. Připojili jsme také lingvistické komentáře k několika typům předpon.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents the analysis of Czech verbal prefixes, which is the first step of a project that has the ultimate goal an automatic
morphemic analysis of Czech. We studied prefixes that may occur in Czech verbs, especially their possible and impossible combinations. We
describe a procedure of prefix recognition and derive several general rules for selection of a correct result. The analysis of “double” prefixes enables to make conclusions about universality of the first prefix. We also added linguistic comments to several types of prefixes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prefixace je pro češtinu jedním ze základních slovotvorných prostředků. Zejména pro slovesa představuje velmi produktivní způsob, jakým lze modifikovat jejich význam a vid. V současném příspěvku ukážeme, jak lze korpusová data použít k analýze vícenásobných předpon vyskytujících se u českých sloves. Ve druhé části se zabýváme předponami, které se používají k modifikaci sloves přejatých z cizích jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Prefixation is one of the basic word formation means in Czech. Especially for verbs, it is a very productive way, which can modify their meaning and aspect. In the present contribution we show how the corpus data can be used to analyze multiple
prefixes occurring in Czech verbs. In the second part, we deal with the prefixes used to modify the loan verbs coming from foreign languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přinášíme pilotní studii strojového překladu vybraných gramatických konstrukcí, které se neshodují při překladu mezi češtinou a angličtinou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We  present  a  pilot  study  of  machine  transla-
tion of selected grammatical contrasts between
Czech and English in WMT18 News Transla-
tion  Task.   For  each  phenomenon,  we  run  a
dedicated  test  which  checks  if  the  candidate
translation  expresses  the  phenomenon  as  ex-
pected or not. The proposed type of analysis is
not an evaluation in the strict sense because the
phenomenon can be correctly translated in var-
ious ways and we anticipate only one. What is
nevertheless interesting are the differences be-
tween various MT systems and the single ref-
erence translation in their general tendency in
handling the given phenomenon.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek seznamuje s výsledky automatické analýzy pořádku slov ve 23 závislostních korpusech projektu HamleDT. Analýza se zaměřuje na základní vlasnosti pořádku slov, pořadí tří hlavních konstituentů - predikátu, subjektu a objektu. Kvantitativní analýza je provedena zvlášť pro hlavní a vedlejší věty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper gives an overview of results of automatic analysis of word order in 23 dependency treebanks of the HamleDT project. The analysis concentrates on basic characteristics of word order, the order of three main constituents, a predicate, a subject and an object. A quantitative analysis is performed separately for main clauses and subordinated clauses.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zaměřuje na deverbální substantiva tvořená nulovým sufixem a probírá jejich vlastnosti (hlásková skladba, lexikální význam, korpusová frekvence, valence) s cílem najít rysy, kterými se liší od nemotivovaných substantiv s nulovým sufixem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper focuses on action nouns with a zero suffix in Czech, which are described as derivation from verbs since the action meaning is primarily expressed by verbs. However, such nouns have a simpler morphemic structure than the corresponding verbs (cf. běh ‘run’ – běh-a-t ‘to run’) and thus differ from the majority of word-formation types in Czech in which the base word is formally simpler and the derivative more complex. In this paper, 100 top-frequent action nouns extracted from a representative corpus of Czech are analysed for features which could support the semantic evidence when determining the direction of motivation between a formally simpler noun and a more complex verb. First, we test whether action nouns derived from verbs do not undergo morphophonemic alternations in inflection (cf. běh.nom – běhu.gen) while in pairs of a base noun and a derived verb the noun is sensitive to alternations (cf. sníh‘snow’, which is the base word for sněžit ‘to snow’, has genitive sněhu; Millet 1958). Second, we verify the assumption that a derivative is less frequent than its base word. Third, the valency potential of both groups of nouns is compared.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce bude představena lexikální síť DeriNet, a to zvláště teoretické pozadí tohoto specializovaného zdroje jazykových dat, výběr lexémů a postup vytváření derivačních vztahů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the talk, the lexical network DeriNet will be presented. I focus on the theoretical background of this specialized language data resource, selection of lexemes and procedures used for identification of derivational links.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se věnuje grafémovým alternacím při odvozování slov v češtině z pohledu budování lexikální sítě DeriNet, která se specializuje na zachycování derivačních vztahů v českém lexikonu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper deals with morphographemic alternations in Czech derivation with regard to the build-up of a large-coverage lexical resource specialized in derivational morphology of contemporary Czech (DeriNet database). After a summary of available descriptions in the Czech linguistic literature and Natural Language Processing, an extensive list of alternations is provided in the first part of the paper with a focus on their manifestation in writing. Due to the significant frequency and limited predictability of alternations in Czech derivation, several bottom-up methods were used in order to adequately model the alternations in DeriNet. Suffix-substitution rules proved to be efficient for alternations in the final position of the stem, whereas a specialized approach of extracting alternations from inflectional paradigms was used for modelling alternations within the roots. Alternations connected with derivation of verbs were handled as a separate task. DeriNet data are expected to be helpful in developing a tool for morphemic segmentation and, once the segmentation is available, to become a reliable resource for data-based description of word formation including alternations in Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá změnami v kategorii slovesného vidu, k nimž dochází během odvozování sloves od sloves v češtině. Po stručném shrnutí základních bodů aspektologických diskuzí nad videm českého slovesa je tvoření vidových protějšků prezentováno jako integrální součást derivace českých sloves. Ve shodě s tímto pohledem je kategorie vidu využita jako důležitý rys při modelování slovesné derivace v databázi zachycující derivační morfologii češtiny. V příspěvku představujeme sadu kritérií, na jejichž základě byla slovesa v databázi organizována.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper deals with the changes of the category of grammatical aspect during derivation of verbs from verbs in Czech. After summarizing the main issues of the long-standing debate over aspect in Czech, it is argued that formation of aspectual pairs can be seen as an integral part of derivation of Czech verbs. In a language data resource capturing the derivational morphology of Czech, the category of aspect was employed as an important feature in modelling verb-to-verb derivation. A set of criteria used for organization of verbs in the database is presented in the paper.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek probírá postupně tři funkce, které jsou v odborné literatuře připisovány předponě po- u českých sloves. Na pozadí rozsáhlých korpusových dat se předponou po- zabýváme nejprve jako formantem odvozujícím předponová slovesa od sloves bezpředponových, dále jako morfémem podílejícím se na tvoření vidového protějšku a naposledy jako morfémem tvořícím syntetické nedokonavé futurum.
Cílem naší analýzy v jednotlivých oddílech je dosavadní poznatky jednak shrnout a syntetizovat, jednak je konfrontovat se zásadní otázkou, jak autentické korpusové doklady k jednotlivým funkcím přiřadit. Na závěr se na předponu po- u sloves díváme z hlediska konkurence s dalšími předponami a z hlediska, jaké místo předponová slovesa zaujímají v příslušných slovotvorných hnízdech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present case study deals with the functions of the Czech verbal prefix po-. Three functions of the prefix are analysed by contrasting the existing theoretical descriptions with corpus data. In its primary, word-formational function, po- modifies the meaning of the base verb (expressing one of the semantic features described as Aktionsart or other meanings; e.g. kreslit ‘to draw’ > pokreslit ‘to cover with drawings’). In its second function, po- derives perfective counterparts from the imperfective verb; here, the prefix is considered to be a grammatical means used for the formation of aspectual pairs of verbs (cf. kárat ‘to admonish.impf’ > pokárat ‘to admonish.pf’). The third function of po- is manifested in the class of determinate verbs; it is a part of the morphological form of these verbs in their (imperfective) future meaning (e.g. běžet ‘to run’ – poběží ‘(he) will run’). A group of verbs suspected of exhibiting similar behaviour as the pure determinate verbs is analysed and attested using the corpus data. Finally, the competition between the prefix po- and
several tens of prefixes in Czech verbs is commented upon and the position of the prefixed verbs within word-formation nests is sketched.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek informuje o novinkách ve vývoji lexikální databáze DeriNet, která je budována jako zdroj jazykových data specializovaný na derivaci v češtině a aktuálně modifikována tak, aby zde bylo možné zachycovat i slovotvornou kompozici. Možnosti využití těchto dat jsou ilustrovány nejnovějšími experimenty z oblasti zpracování přirozeného jazyka i z lingvistického výzkumu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper reports on recent progress in development of the lexical database DeriNet, which has been designed as a resource specialized in derivation of Czech but, recently, the structure of the database has been modified in order to allow  for  capturing  compounding  and  combined  word-formation  processes, too. The ambition is to cover a major part of the word-formation system of Czech in all its complexity.  The potential of the resource for both linguistic research and experiments  in  Natural  Language  Processing  are exemplified  by  recent  case studies based on the data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je deváté vydání treebanků UD, verze 2.3.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the ninth release of UD Treebanks, Version 2.3.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je osmé vydání treebanků UD, verze 2.2.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the eighth release of UD Treebanks, Version 2.2.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme naši stále probíhající práci na mezijazyčném přenosu syntaktických parserů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our ever ongoing work on cross-lingual syntactic parsing.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentuji, co jsem se dozvěděl o PhD studiu na University of Genova, a navrhuji několik změn pro PhD studium na ÚFALu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I present what I learned about the PhD studies at University of Genova, and suggest some changes to PhD studies at ÚFAL.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této dizertaci se zaměřujeme na problém automatického syntaktického rozboru jazyků, pro něž nejsou k dispozici žádná syntakticky anotovaná trénovací data. Zkoumáme několik metod mezijazyčného přenosu syntaktické i morfologické anotace, a nakonec docházíme k metodám založeným na využití dvojjazyčných či vícejazyčných korpů zarovnaných na úrovni vět, a strojového překladu. Zvláštní pozornost věnujeme automatickému odhadování vhodnosti zdrojového jazyka pro analýzu daného cílového jazyka, a navrhujeme novou míru založenou na podobnostech častých sledů slovních druhů. Účinnost představených postupů byla ověřena jak v našich pokusech, tak nezávisle v pracech uznávaných světových vědců.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this thesis, we focus on the problem of automatically syntactically analyzing a language for which there is no syntactically annotated training data. We explore several methods for cross-lingual transfer of syntactic as well as morphological annotation, ultimately based on utilization of bilingual or multilingual sentence-aligned corpora and machine translation approaches. We pay particular attention to automatic estimation of the appropriateness of a source language for the analysis of a given target language, devising a novel measure based on the similarity of part-of-speech sequences frequent in the languages. The effectiveness of the presented methods has been confirmed by experiments conducted both by us as well as independently by other respectable researchers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení projektu GA UK, zejména metody měření podobnosti jazyků, a osobní pohled na výhody a nevýhody GAUKu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presentation of the GAUK project, especially the language similarity measure, and a personal view of the advantages and disadvantages of GAUKs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Data v podobě prostého textu získaná z dumpů Wikipedie v únoru 2018.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Wikipedia plain text data obtained from Wikipedia dumps with WikiExtractor in February 2018.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje systém CUNI x-ling zaslaný do soutěže CoNLL 2018 UD Shared Task. Zaměřili jsme se na syntaktickou analýzu jazyků s nedostatkem zdrojů, které mají k dipozici malá nebo žádná trénovací data. Použili jsme široké spektrum přístupů, včetně jednoduchého překladu závislostních korpusů slovo po slově, kombinace delexikalizovaných parserů, a využití dostupných tvaroslovných slovníků. Náš příspěvek byl v oficiálním vyhodnocení označen za jasného vítěze v kategorii analýzy jazyků s nedostatkem zdrojů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This is a system description paper for the CUNI x-ling submission to the CoNLL 2018 UD Shared Task. We focused on parsing under-resourced languages, with no or little training data available. We employed a wide range of approaches, including simple word-based treebank translation, combination of delexicalized parsers, and exploitation of available morphological dictionaries, with a dedicated setup tailored to each of the languages. In the official evaluation, our submission was identified as the clear winner of the Low-resource languages category.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>SloNLP je workshop speciálně zaměřený na zpracování přirozeného jazyka a počítačovou lingvistiku. Jeho hlavním cílem je podpořit spolupráci mezi výzkumníky v oblasti NLP v Česku a na Slovensku.

Mezi témata workshopu patří automatické rozpoznávání mluvené řeči (ASR), automatická analýza a generování přirozeného jazyka (morfologie, syntax, sémantika...), dialogové systémy (DS), strojový překlad (MT), vyhledávání informací (IR), praktické aplikace NLP technologií, a další témata počítačové lingvistiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>SloNLP is a workshop focused on Natural Language Processing (NLP) and Computational Linguistics. Its primary aim is to promote cooperation among NLP researchers in Slovakia and Czech Republic.

The topics of the workshop include automatic speech recognition, automatic natural language analysis and generation (morphology, syntax, semantics, etc.), dialogue systems, machine translation, information retrieval, practical applications of NLP technologies, and other topics of computational linguistics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme svůj projekt LSD, ve kterém se snažíme zjistit, zda neuronové sítě pracují s reprezentacemi podobnými klasickým lingvistickým strukturám.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In recent years, deep neural networks have achieved and surpassed
state-of-the-art results in many tasks, including many natural language
processing problems, such as machine translation, sentiment analysis, image
captioning, etc.

Traditionally, solving these tasks relied on many intermediary
steps, internally using explicit linguistic annotations and representations,
such as part-of-speech tags, syntactic structures, semantic labels, etc. These
smaller substeps were thought of as useful or even necessary to solve the
larger and more complex tasks. 

However, deep neural networks have made it possible to use end-to-end
learning, where the network directly learns to produce the desired outputs
from the inputs, without any explicit internal intermediary representations.

Nevertheless, the networks are structured in such a way that we can still
think of them as using some intermediary representations of the inputs,
although these are learned only implicitly. Some of the representations can be
directly linked to certain parts of the input -- such as word embeddings
corresponding to individual words -- others are linked to the inputs more
vaguely, due to using recurrent units, attention, etc.

In our project, we are interested in investigating these internal representations,
trying to see what information they capture, how they are structured, and what
meaning we can assign to them. More specifically, we are currently trying to
reliably determine to what extent neural networks seem to capture some basic
linguistic notions, such as part of speech, in their various components --
encoder word embeddings, decoder word embeddings, encoder hidden states... We
are also interested in how this depends on the task for which the network is
trained -- language modelling (word2vec), machine translation, sentiment
analysis... Ultimately, we are interested in the somewhat philosophical
question of whether neural networks seem to understand language, or at least
capture the meanings of sentences.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme dialogový systém Politik, který imituje virtuálního politika. Systém zodpovídá otázky na politická témata. Otázky jsou analyzovány pomocí nástrojů počítačového zpracování jazyka pomocí systému Treex bez znalostní báze. Systém je navržen pro češtinu a angličtinu. Po zpracování morfologickém a syntaktickém zpracování otázky se vybírá vhodná šablona odpovědi z ručně vytvořených šablon. Následně je šablona transformována do gramiticky správné věty. Také jsme pilotně zkoumali rozdíly v doméně rozhovoru.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the question-answering system Politician, which
is a chatbot designed to imitate a fictional politician. The chatbot accepts questions on political issues and answers them accordingly. The questions are analyzed using natural language processing techniques, mainly using a custom scenario built on Treex, and no complex knowledge base is involved. There are two working versions so far, in Czech and in English.

Once morphological and syntactic annotations are available for
the question, an appropriate answer template is selected from the manually created set of answer templates based on the nouns, verbs and named entities present in the question. After that, the answer template is transformed into a grammatically correct reply.

We also briefly investigated the differences between the two languages and potential generalization of the approach to other topics. Apparently, morphological and syntactic information provides enough data for a very basic understanding of questions on a specific topic.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku provádíme experimenty s projekcí koreference na velkých datech a analyzujeme je napříč jemně zvolenými kategoriemi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We perform a fine-grained large-scale analysis
of coreference projection.  By projecting gold
coreference  from  Czech  to  English  and  vice
versa  on  Prague  Czech-English  Dependency
Treebank 2.0 Coref, we set an upper bound of
a proposed projection approach for these two
languages.  We undertake a detailed thorough
analysis that combines the analysis of projec-
tion’s  subtasks  with  analysis  of  performance
on individual mention types.  The findings are
accompanied with examples from the corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje experimenty s bilinguálně informovaným rozpoznáváním koreference na česko-anglických datech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Coreference is a basic means to retain coherence of a text that likely exists in every language. However, languages may differ in how a coreference relation is manifested on the surface. A possible way how to measure the extent and nature of such differences is to build a coreference resolution system that operates on a parallel corpus and extracts information from both language sides of the corpus. In this work, we build such a bilingually informed coreference resolution system and apply it on Czech-English data. We compare its performance with the system that learns only from a single language. Our results show that the cross-lingual approach outperforms the monolingual one. They also suggest that a system for Czech can exploit the additional English information more effectively than the other way round. The work concludes with a detailed analysis that tries to reveal the reasons behind these results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tématem této knihy je studium vlastností koreference s použitím mezijazykových přístupů. Navrhujeme dvě mezijazykové metody: rozpoznávání koreference s informací z druhého jazyka a projekci koreference. Výsledky našich experimentů s těmito metodami na česko-anglických datech naznačují, že s ohledem na koreferenci přináší angličtina více informací do češtiny než naopak.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The subject of this monograph is to study properties of coreference using cross-lingual approaches. We designed two cross-lingual methods: the bilingually informed coreference resolution and the coreference projection. The results of our experiments suggest that with respect to coreference English is more informative for cyech than vice versa.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tématem této práce je studium vlastností koreference s použitím mezijazykových přístupů. Motivací práce je výzkum lingvistické typologie založené na koreferenci. Další motivací je prozkoumání, jestli rozdíly ve způsobech, jak jazyky vyjadřují koreferenci, mohou být využity k natrénování lepších modelů pro rozpoznávání koreference. Navrhujeme dvě mezijazykové metody: rozpoznávání koreference s informací z druhého jazyka a projekci koreference. Výsledky našich experimentů s těmito metodami na česko-anglických datech naznačují, že s ohledem na koreferenci přináší angličtina více informací do češtiny než naopak. Rozpoznávání koreference s informací z druhého jazyka navíc dokázalo při aplikaci na paralelních datech překonat na obou jazycích výsledky jednojazykového systému na rozpoznávání. Při experimentech používáme jednojazykový rozpoznávač koreference a vylepšenou metodu na zarovnání koreferenčních výrazů, které jsme rovněž navrhli v rámci této práce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The subject of this thesis is to study properties of coreference using cross-lingual approaches. The work is motivated by the research on coreference-related linguistic typology. Another motivation is to explore whether differences in the ways how languages express coreference can be exploited to build better models for coreference resolution. We design two cross-lingual methods: the bilingually informed coreference resolution and the coreference projection. The results of our experiments with the methods carried out on Czech-English data suggest that with respect to coreference English is more informative for Czech than vice versa. Furthermore, the bilingually informed resolution applied on parallel texts has managed to outperform the monolingual resolver on both languages. In the experiments, we employ the monolingual coreference resolver and an improved method for alignment of coreferential expressions, both of which we also designed within the thesis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme systém pro automatické hodnocení textové koherence v českých esejích psaných nerodilými mluvčími. Systém EVALD pracuje s množinou rysů z oblasti pravopisu, slovní zásoby, morfologie, syntaxe, diskurzních vzahů a koreference. Nově nyní přidáváme rysy z oblasti informační struktury věty (aktuálního členění větného).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a system for automatic evaluation of surface text coherence in Czech essays written by native and non-native speakers. The features of the EVALD system cover spelling, vocabulary, morphology, syntax, discourse relations and coreference. Newly we add features targeting topic–focus articulation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje inovativní kombinaci recurrent neural-network based modelu na úrovni znaků a jazykového modelu aplikovanou na úlohu doplnění diakritiky do textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we describe a novel combination of a character-level recurrent neural-network based model and a language model applied to diacritics restoration. In many cases in the past and still at present, people often replace characters with diacritics with their ASCII counterparts. Despite the fact that the resulting text is usually easy to understand for humans, it is much harder for further computational processing. This paper opens with a discussion of applicability of restoration of diacritics in selected languages. Next, we present a neural network-based approach to diacritics generation. The core component of our model is a bidirectional recurrent neural network operating at a character level. We evaluate the model on two existing datasets consisting of four European languages. When combined with a language model, our model reduces the error of current best systems by 20% to 64%. Finally, we propose a pipeline for obtaining consistent diacritics restoration datasets for twelve languages and evaluate our model on it. All the code is available under open source license on https://github.com/arahusky/diacritics_restoration.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pokrok v kvalitě strojového překladu si žádá nové metody a metriky automatického vyhodnocování. V tomto příspěvku rozšiřujeme testovací sadu pro morfologické rysy (Burlot a Zvon, 2017) o další jazykové páry.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Progress in the quality of machine translation
output calls for new automatic evaluation pro-
cedures  and  metrics.    In  this  paper,  we  ex-
tend  the  Morpheval  protocol  introduced  by
Burlot  and  Yvon  (2017)  for  the  English-to-
Czech  and  English-to-Latvian  translation  di-
rections to three additional language pairs, and
report its use to analyze the results of WMT
2018’s  participants  for  these  language  pairs.
Considering  additional,  typologically  varied
source  and  target  languages  also  enables  us
to  draw  some  generalizations  regarding  this
morphology-oriented evaluation procedure.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje změny v proceduře, která se používá při převodu závislostního korpusu Index Thomisticus do anotačního stylu Universal Dependencies. Index Thomisticus je korpus středověkých latinských textů od Tomáše Akvinského.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the changes applied to the original process used to convert the Index Thomisticus Treebank, a corpus including texts in Medieval Latin by Thomas Aquinas, into the annotation style of Universal Dependencies. The changes are made both to harmonise the Universal Dependencies version of the Index Thomisticus Treebank with the two other available Latin treebanks and to fix errors and inconsistencies resulting from the original process. The paper details the treatment of different issues in PoS tagging, lemmatisation and assignment of dependency relations. Finally, it assesses the quality of the new conversion process by providing an evaluation against a gold standard.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Evaluace rozpoznávání notopisu (Optical Music Recognition, OMR) je dlouholetým trnem v patě oboru. Tento krátký position paper se pokouší vyjasnit, co přesně jsou překážky ve vyhodnocování OMR: detailnější pohled ukazuje, že hlavním problémem je nalézt způsob, jak vypočítat editační vzdálenost mezi prakticky použitelnými reprezentacemi notopisu.  Odhadovat tyto "ceny úprav" pro aplikaci OMR pro přímočarou digitalizaci not je obtížné, tvrdím však, že problémy s modelováním dalších faktorů ovlivňující náročnost lidského post-editování výstupu OMR je možné separovat od vyhodnocování sytémů OMR během jejich vývoje, pokud se použije intrinsní evaluace místo evaluace motivované aplikací, a načrtávám způsob, jak takovou evaluaci udělat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Evaluating Optical Music Recognition (OMR) has
long been an acknowledged sore spot of the field. This short
position paper attempts to bring some clarity to what are actually
open problems in OMR evaluation: a closer look reveals that the
main problem is finding an edit distance between some practical
representations of music scores. While estimating these editing
costs in the transcription use-case of OMR is difficult, I argue
that the problems with modeling the subsequent editing workflow
can be de-coupled from general OMR system development using
an intrinsic evaluation approach, and sketch out how to do this.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozpoznávání notopisu (Optical Music Recognition, OMR) je oblast výzkumu zaměřená na automatické dekódování hudební notace z dokumentů. Vzhledem k tomu, že většina hudebních skladeb v západní tradici existuje pouze v psané podobě, zapojení této modality do digitální domény může podstatně diversifikovat zdroje pro digitální muzikologii, hudební informatiku, aj. Doposud bylo OMR považováno za převážně nevyřešený problém, avšak tato situace se v posledních letech začla měnit: byly vydány rozsáhlé datové sady, metody založené na strojovém učení překonávají dříve nepřekonatelné překážky, a aplikace OMR migrují z motivačních úvodů jednotlivých článků spíše do sekcí Výsledky. Náš tutoriál prezentuje tento nový stav poznání: metody, výsledky, nástroje, a datasety. Po tutoriálu budou účastníci obeznámeni se stavem rozpoznávání notopisu, a měli by být schopní pomocí existujících nástrojů začít OMR integrovat do své práce, ať už hudebně-informatické či muzikologické; pro účastníky, kteří mají zájem na OMR přímo pracovat, pak tutoriál představuje efektivní úvod do problematiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Optical Music Recognition (OMR) is a field of research that investigates how to computationally decode music notation in documents. As most musical compositions in the Western tradition have been written rather than recorded, bringing this music into the digital domain can significantly diversify the sources for MIR, digital musicology, and more broadly lower the costs of introducing previously unheard works to audiences worldwide. While OMR has been regarded as a largely unsolved problem, this situation has recently shifted: new large-scale datasets and tools have been released, methods based on deep learning are successfully dealing with musical symbol detection and partial end-to-end recognition, and applications of OMR such as retrieval have started migrating from article introductions to the Results sections. Our tutorial will present this new and rather exciting state of the art in OMR. We will demonstrate recent methods and results, introduce the audience to the tools and datasets used to achieve them, and showcase the opportunities for using OMR. Finally, we will introduce the current challenges in OMR. After the tutorial, the participants should be familiar with state-of-the-art OMR research, and should be able to start using existing tools to integrate OMR into their own work, whether in MIR or (digital) musicology. For those interested in working on OMR themselves, the tutorial should provide a head start.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozpoznávání notopisu (optical music recognition, OMR) slibuje, že díky němu půjde prohledávat hudební "full-text" v rozsáhlých notových archivech. Otevřel by se tak nový způsob přístupu k obrovskému množství hudby, která byla napsána, avšak ne nahrána. OMR však toto už slibuje dlouho a povětšinou bezvýsledně: jeho výsledky nejsou dostatečně dobré, především pro hudební rukopisy či pro nedokonalé scany. V poslední době se však OMR zlepšilo, především díky pokrokům ve strojovém učení. V tomto příspěvku vezmeme OMR systém založený na tradičním několikakrokovém postupu a druhý systém založený na učení end-to-end, a ilustrujeme jejich možnosti v jednoduchých, prototypických vyhledávacích aplikacích. Na příkladu také ukazujeme, jak použití OMR může výrazně snížit náklady na studie kvantitativní muzikologie. Dohromady tyto výsledky interpretujeme tak, že v určitých úlohách již lze současné technologie OMR nasadit jako obecný nástroj pro obohacování digitálních knihoven.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Optical Music Recognition (OMR) promises to make large collections of sheet music searchable by their musical content. It would open up novel ways of accessing the vast amount of written music that has never been recorded before. For a long time, OMR was not living up to that promise, as its performance was simply not good enough, especially on handwritten music or under non-ideal image conditions. However, OMR has recently seen a number of improvements, mainly due to the advances in machine learning. In this work, we take an OMR system based on the traditional pipeline and an end-to-end system, which represent the current state of the art, and illustrate in proof-of-concept experiments their applicability in retrieval settings. We also provide an example of a musicological study that can be replicated with OMR outputs at much lower costs. Taken together, this indicates that in some settings, current OMR can be used as a general tool for enriching digital libraries.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Detekce notačních symbolů je nejpalčivější otevřený podproblém v rozpoznávání notopisu (Optical Music Recognition, OMR). Ukazujeme, že architektura U-Net pro sémantickou segmentaci spolu s triviálním detektorem představuje silnou baseline, a navrhujeme několik triků, které výsledky ještě zlepšují: trénování proti konvexním obalům notačních objektů, a vícekanálové výstupy které umožňují sdílet parametry sítě pro několik sémanticky příbuzných tříd objektů. Oba triky přináší výrazné zlepšení v detekci klíčů, což má zásadní následky pro výsledky OMR. Následně začleníme U-Nety do kompletního rozpoznávacího systému: přidáme model doplňující vztahy mezi rozpoznanými symboly, a dosáhneme tak výsledného f-score 0.81 pro extrakci výšek zapsaných tónů. Nad takto automaticky extrahovanými tóny provedeme pokusy pro vyhledávání rukopisných kopií stejné hudby, které přináší první empirické indikace, že využívání mocných modelů hlubokého učení pro OMR skutečně dle očekávání přibližuje full-textové vyhledávání ve velkých sbírkách hudebních rukopisů na dosah.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Detecting music notation symbols is the most immediate unsolved  subproblem  in  Optical  Music  Recognition  for musical manuscripts.  We show that a U-Net architecture for semantic segmentation combined with a trivial detector  already  establishes  a  high  baseline  for  this  task,  and we  propose  tricks  that  further  improve  detection  performance: training against convex hulls of symbol masks, and multichannel output models that enable feature sharing for semantically related symbols. The latter is helpful especially for clefs, which have severe impacts on the overall OMR result. We then integrate the networks into an OMR pipeline by applying a subsequent notation assembly stage, establishing a new baseline result for pitch inference in handwritten music at an f-score of 0.81.  Given the automatically inferred pitches we run retrieval experiments on handwritten scores, providing first empirical evidence that utilizing the powerful image processing models brings content-based search in large musical manuscript archives within reach.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Neuronový strojový překlad lze použít jako jedno z metod pro získání spojité větné reprezentace. V příspěvku ukazujeme, že pro takto získané reprezentace kvalita překladu negativně koreluje s kvalitou zachycení významu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>One of possible ways of obtaining continuous-space
sentence representations is by
training neural machine translation (NMT)
systems. The recent attention mechanism
however removes the single point in the
neural network from which the source sentence
representation can be extracted. We
propose several variations of the attentive
NMT architecture bringing this meeting
point back. Empirical evaluation suggests
that the better the translation quality, the
worse the learned sentence representations
serve in a wide range of classification and
similarity tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Český korpus právních textů 2.0 (CLTT 2.0) obsahuje texty z legislativní domény, které jsou manuálně syntakticky anotovány. Nová verze korpusu obsahuje vylepšenou syntaktickou anotaci a dvě nové vrstvy anotací - anotaci jmenných entit a sémantických vztahů. Celkově korpus obsahuje 2 právní dokumenty, 1121 vět a 40950 tokenů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Czech Legal Text Treebank 2.0 (CLTT 2.0) contains texts that come from the legal domain and are manually syntactically annotated. The syntactic annotation in CLTT 2.0 is more elaborate than in CLTT 1.0.  In addition, CLTT 2.0 contains two new annotation layers, namely the layer of entities and the layer of semantic entity relations. In total, CLTT 2.0 consists of two legal documents, 1,121 sentences and 40,950 tokens.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku se zabýváme jedním typem rozdílu v zachycení valenčních struktur v českém a anglickém valenčním slovníku a paralelním česko-anglickém syntakticky anotovaném korpusu. Jde o konstrukce tzv. alternace subjektu a instrumentu (Instrument-Subject Alternation) a jí podobné alternace subjektu a abstraktní příčiny (Abstract Cause-Subject Alternation), příp. alternace subjektu a materiálu (Locatum-Subject Alternation) (Levin, 1993), odpovídající zhruba třem sémantickým typům sloves. Tyto konstrukce představují specifický problém v rámci vzájemného mapování valenčních struktur v paralelním korpusu a jejich zachycení ve valenčních slovnících, neboť u nich dochází ke konkurenčnímu naplnění pozice povrchověsyntaktického subjektu a hloubkověsyntaktického aktoru dvěma sémanticky různými typy doplnění. Výsledkem jsou dvě interpretace, agentní a neagentní, které vstupují do konfliktu při zachycení v syntakticky anotovaných korpusových datech. Konflikt těchto dvou interpretací významně ovlivňuje sémantickou interpretaci „cíle evaluace“ v konstrukcích s evaluativními slovesy v rámci analýzy sentimentu.  Problém analyzujeme na základě vzájemného vztažení syntaktických, sémantických a situačních participantů, s ohledem na již publikovanou literaturu k tématu. Zaměřujeme se na společné rysy všech tří dotčených sémantických slovesných typů i na rozdíly mezi nimi. Zmíněné alternující konstrukce vztahujeme k dalším analogickým konstrukcím a studujeme z hlediska morfosyntaktického, lexikálněsémantického i propozičně obsahového.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper analyzes one type of valency structure difference in a bilingual, Czech-English valency lexicon and a parallel syntactically annotated treebank, namely the instances of Instrument-Subject Alternation, Abstract Cause-Subject Alternation and Locatum-Subject Alternation (Levin, 1993), roughly corresponding to three specific verb semantic classes. These alternations represent a problematic point of mutual alignment of valency structures in a parallel corpus and a bilingual valency lexicon because of the dual possible assignment of a semantic role to the position of a syntactic subject, and consequently, the dual interpretation of the deep-syntactic role of an actor.  As a result, two different interpretations arise, an agentive one and a non-agentive one, which collide in the syntactically annotated data. The conflict of the two intepretations influences substantially the semantic interpretation of the “target of evaluation” in constructions involving evaluative verbs in Sentiment Analysis tasks. We discuss the problem through linking individual syntactic, semantic and situational participants, focusing both on the similarities and differences between the three alternation types in question, providing analogies to other known constructions, and studying them from the morphosyntactic, semantic and propositional content point of view.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek prezentuje aplikaci pro laické, netrénované uživatele, která slouži pro generování kvalitních zarovnaných fonetických přepisů mluveného slova. Aplikace se již několik let používá a přepsalo se jí přes 600 tisíc slovních forem napříč dvěma verzemi webového rozhraní. Představujeme opatření pro kompenzaci nedostatku odborného tréninku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents an application for lay, untrained users to generate high-quality, aligned phonetic transcription of speech. The application has been in use for several years and has served to transcribe over 600 thousand word forms over two versions of a web interface. We present measures for compensating the lack of expert training.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek prezentuje webovou aplikaci nové generace, která umožňuje uživatelům přispívat opravami automaticky získaného přepisu dlouhých záznamů mluveného slova. Popisujeme rozdíly od podobných případů, porovnáváme svoje řešení s ostatními a pozastavujeme se nad vývojem oproti nyní 6 let staré aplikaci, z níž vycházíme, ve světle učiněného pokroku, získaného poučení a v prohlížeči nově dostupných technologií.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a next-generation web application that enables users to contribute corrections to automatically acquired transcription of long speech recordings. We describe differences from similar settings, compare our solution with others and reflect on the development from the now 6 years old work we build upon in the light of the progress made, lessons learned and the new technologies available in the browser.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozšířená anotace diskurzu v části Pražského diskurzního korpusu, přidávající implicitní vztahy, vztahy založené na koreferenci, vztahy mezi otázkou a odpovědí a další typy vztahů strukturujících výpověď.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Enriched discourse annotation of a subset of the Prague Discourse Treebank, adding implicit relations, entity based relations, question-answer relations and other types of discourse structuring relations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška představuje hlavní výsledky srovnávací analýzy věnované vágním diskurzním konektorům v angličtině, češtině, maďarštině, francouzštině a litevštině. Analýza je založena na textech z TED talks.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presentation describes general results of comparison how underspecified discourse connectives are used in English, Czech, Hungarian, French and Lithuanian, based on the texts of TED talks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V knize představujeme výzkum primárních a sekundárních diskurzních konektorů v češtině, provedený na datech Pražského diskurzního korpusu 2.0 (PDiT). Výsledky výzkumu jsou rozděleny do čtyř částí. První část přináší vymezení a charakteristiku diskurzních konektorů v češtině, druhá popisuje anotaci konektorů v PDiT. Ve třetí části jsou představeny výsledky diskurzní anotace PDiT a ve čtvrté obecné principy a tendence v užívání diskurzní konektorů v komunikaci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the monograph, we examine primary and secondary discourse connectives in Czech. The analysis is carried out on the data of the Prague Discourse Treebank 2.0 (PDiT). The results of the research are presented in four parts. The first one focuses on delimitation and characteristics of discourse connectives in Czech, the second one describes annotation of connectives in the PDiT and demonstrates its evaluation, the third one presents results of this annotation, and the fourth one discusses linguistic principles and tendencies in using discourse connectives.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá jazykovými faktory, které ovlivňují výběr diskurzních konektorů při tvorbě koherentního textu. Zaměřuje se na konkurenci tzv. primárních konektorů (gramatikalizovaných a většinou jednoslovných výrazů typu „protože“) a sekundárních konektorů (dosud ne plně gramatikalizovaných výrazů typu „z tohoto důvodu“). Analýza vychází z dat Pražského diskurzního korpusu 2.0 (Prague Discourse Treebank, PDiT), který obsahuje téměř 50 000 vět z českých novinových textů. Ukazuje se, že užívání diskurzních konektorů ovlivňuje tzv. jazyková ekonomie, tj. autoři textu usilují o dosažení maximálního výsledku s vynaložením minimálního úsilí. Nejčastěji proto volí krátké a sémanticky obecnější primární konektory. Naopak v případech, kdy by diskurzní vztahy mohly být interpretovány nesprávně, upřednostňují autoři složitější a specifické konektivní struktury.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we explore the linguistic factors that influence an author's choice of discourse connectives in the production of a coherent text. We focus on the competition between so-called primary connectives (grammaticalized and mostly one-word expressions such as therefore) and secondary connectives (not yet fully grammaticalized compositional discourse phrases such as for this reason). We attempt to describe the linguistic constraints on and preferences in connective selection. The analysis is based on manually annotated data from the Prague Discourse Treebank 2.0 (PDiT), which contains almost 50000 sentences from Czech newspaper texts. We demonstrate that discourse connectives are used in accordance with the economy principle in language, i.e. authors aim to achieve the maximal result with minimal effort. They most frequently choose short and semantically more generalized primary connectives. However, in cases where the discourse relations can be misunderstood, authors prefer more complex and specific structures.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje možnosti, jak zlepšovat dovednosti studentů psát koherentní text pomocí e-learningových metod. Zaměřuje se na téma automatického hodnocení povrchové koherence textu a představuje zlepšení softwaru, který toto hodnocení provádí pro texty v češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents possibilities of improving students’ writing skills through e-learning. It addresses the topic of automated evaluation of surface text coherence and introduces improvements of a system solving this task for Czech. With the implementation of new morpho-syntactic features, the system works with the reliability of 0.63 macro-average F-score. Except for the overall mark for coherence, the system newly provides also a detailed feedback for users. The user (student) learns stronger and weaker aspects of his or her text in the following language fields: spelling, vocabulary, morphology, syntax, and discourse (in terms of coreference and discourse relations). The student (a learner of Czech) is thus given a detailed evaluation of his or her text, which is available online and gives thus space for practicing writing skills easily through e-learning.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme pilotní verzi CzeDLexu, slovníku českých diskurzních konektorů. Ve své aktuální verzi CzeDLex obsahuje 205 lemmat konektorů, získaných z anotací Pražského diskurzního korpusu 2.0 (PDiT). 19 lemmat bylo plně ručně zpracováno, což zahrnuje více než dvě třetiny všech diskurzních vztahů anotovaných v PDiT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a pilot version of CzeDLex, a Lexicon of Czech Discourse Connectives. Currently, CzeDLex contains 205 lemmas of connectives coming from the annotation of the Prague Discourse Treebank 2.0 (PDiT). At this stage, 19 lemmas have been fully manually processed, which covers more than two thirds of all discourse relations annotated in the PDiT.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>UDPipe je trénovatelný nástroj, který provádí segmentaci vět, tokenizaci, morfologické značkování, lemmatizaci a syntaktickou analýzu. Představujeme prototyp UDPipe 2.0 a jeho vyhodnocení v Soutěži CoNLL 2018 UD: Multilingual Parsing from Raw Text to Universal Dependencies, která využívá tři míry pro hodnocení. Z 26 účastníků obsadil prototyp první místo dle míry MLAS, třetí dle míry LAS a třetí dle míry BLEX. V extrinsic hodnocení EPE 2018 se systém umístil na prvním místě v celkovém hodnocení.

Prototyp je založen na neuronovou síťi s jediným společným modelem pro současné morfologické značkování, lemmatizaci a syntaktickou analýzu a je trénován pouze pomocí trénovacích dat CoNLL-U a předtrénovaných slovních embeddingů, na rozdíl od obou systémů, které překonaly tento prototyp v LAS a BLEX mírách.

Open-source zdrojový kód prototypu je k dispozici na adrese http://github.com/CoNLL-UD-2018/UDPipe-Future.

Po soutěží CoNLL 2018 jsme mírně vylepšili modelovou architekturu, což vedlo k lepšímu výkonu jak v intrinsic hodnocení (odpovídající prvnímu, druhému a druhému místu dle metrik MLAS, LAS a BLEX), tak i v extrinsic hodnocení. Vylepšené modely budou brzy k dispozici v UDPipe na adrese http://ufal.mff.cuni.cz/udpipe.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>UDPipe is a trainable pipeline which performs sentence segmentation, tokenization, POS tagging, lemmatization and dependency parsing. We present a prototype for UDPipe 2.0 and evaluate it in the CoNLL 2018 UD Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, which employs three metrics for submission ranking. Out of 26 participants, the prototype placed first in the MLAS ranking, third in the LAS ranking and third in the BLEX ranking. In extrinsic parser evaluation EPE 2018, the system ranked first in the overall score.

The prototype utilizes an artificial neural network with a single joint model for POS tagging, lemmatization and dependency parsing, and is trained only using the CoNLL-U training data and pretrained word embeddings, contrary to both systems surpassing the prototype in the LAS and BLEX ranking in the shared task.

The open-source code of the prototype is available at http://github.com/CoNLL-UD-2018/UDPipe-Future.

After the shared task, we slightly refined the model architecture, resulting in better performance both in the intrinsic evaluation (corresponding to first, second and second rank in MLAS, LAS and BLEX shared task metrics) and the extrinsic evaluation. The improved models will be available shortly in UDPipe at http://ufal.mff.cuni.cz/udpipe.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Shrnutí dokumentu je dobře studovaným NLP úkolem. Se vznikem modelů umělé neuronové sítě se zvyšuje souhrnná výkonnost, stejně jako požadavky na výcvikové údaje. Pro Čechy je však k dispozici pouze několik datových souborů, z nichž žádný není zvlášť velký. Kromě toho bylo shrnutí vyhodnoceno převážně na angličtině, přičemž běžně používaná metrika ROUGE je specifická pro angličtinu. V tomto příspěvku se snažíme řešit obě otázky. Představujeme SumeCzech, český datový soubor pro sumarizaci zpráv. Obsahuje více než milion dokumentů, z nichž každá obsahuje nadpis, několik věty dlouhý abstrakt a úplný text. Sadu dat lze stáhnout pomocí dodaných skriptů, které jsou k dispozici na adrese http://hdl.handle.net/11234/1-2615. Vyhodnocujeme několik souhrnných základních dat na množině dat, včetně silného abstrakčního přístupu založeného na architektuře neuronových sítí Transformeru. Hodnocení se provádí jazykově-agnostickou variantou ROUGE.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Document summarization is a well-studied NLP task. With the emergence of artificial neural network models, the summarization performance is increasing, as are the requirements on training data. However, only a few datasets are available for Czech, none of them particularly large. Additionally, summarization has been evaluated predominantly on English, with the commonly used ROUGE metric being English-specific. In this paper, we try to address both issues. We present SumeCzech, a Czech news-based summarization dataset. It contains more than a million documents, each consisting of a headline, a several sentences long abstract and a full text. The dataset can be downloaded using the provided scripts available at http://hdl.handle.net/11234/1-2615. We evaluate several summarization baselines on the dataset, including a strong abstractive approach based on Transformer neural network architecture. The evaluation is performed using a language-agnostic variant of ROUGE.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje projekt, který sjednocuje jazykové infrastruktury v USA (LAPPSGrid) a Evropě (Clarin) po stránce workflow a z hlediska přístupu uživatelů z jiné oblasti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Andrew K. Mellon Foundation has funded a project to create a “trust network” between the Language Applications (LAPPS) Grid, a major framework for composing pipelines of natural language processing (NLP) tools, and the WebLicht workflow engine hosted by the CLARIN-D Center in Tuebingen. The project also includes integration of NLP services available from the LINDAT/CLARIN Center in Prague. The goal is to allow users on one side of the bridge to gain appropriately authenticated access to the other and enable seamless communication among tools and resources in both frameworks. The resulting “meta-framework” provides users across the globe with access to an unprecedented array of language processing facilities that cover multiple languages, tasks, and applications, all of which are fully interoperable.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zpráva se zaměřuje na existující morfologické zdroje obsahující derivační slovotvorné vztahy. Pro každý zdroj je ve zprávě popsána jeho historie, licence, formát, struktura dat a některé základní statistiky pro porovnání zdrojů. Zpráva představuje první krok v přezkoumání a harmonizaci těchto zdrojů podobným způsobem, jako tomu bylo již učiněno se zdroji syntaktických stromů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The report focuses on existing morphological resources containing derivational wordformation relations. For each resource, the report describes history, licence, format, data structure and some other basic statistics to compare. Therefore, it means the first step to review and harmonize these resources in a similar way as it has already been done with resources of syntactic trees.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme morfologický analyzátor shipiba-koniba, domorodého jazyka z oblasti peruánské Amazonie. Díky robustnosti konečněstavových systémů můžeme vytvořit model komplexní morfosyntaxe tohoto jazyka. Vyhodnocení na textových korpusech ukazuje slibné pokrytí gramatických jevů, omezením je pouze zatím malý slovník. Nástroj je volně k dispozici pro kohokoliv, čímž chceme podpořit další výzkum peruánských domorodých jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a morphological analyzer for Shipibo-Konibo, a low-resourced native language spoken in the Amazonian region of Peru. We resort to the robustness of finite-state systems in order to model the complex morpho-syntax of the language. Evaluation over raw corpora shows promising coverage of grammatical phenomena, limited only by the scarce lexicon. We make this tool freely available so as to aid the production of annotated corpora and impulse further research in native languages of Peru.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento nástroj je prvním morfologickým analyzátorem jazyka šipibo-konibo. Je to</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This tool is the first morphological analyzer ever for this language.
The analyzer is a FST that produces all possible segmentations and tagging sequences in a word-by-word fashion.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Paralelní dvojjazyčné korpusy v souladu s větou jsou hlavním a někdy jediným požadovaným zdrojem pro výuku systémů pro překlad statistických a neurálních strojů (SMT, NMT). Navrhujeme koncovou hlubokou neuronovou architekturu pro jazykové nezávislé zarovnání vět. Kromě zarovnání typu "one-to-one" může náš zarovnávač také provádět cross-a many-to-many alignment. Předkládáme také případovou studii, která ukazuje, jak může výrazná jazyková analýza výrazně zlepšit výkon čisté neuronové sítě. V souboru Europarl korpus (Koehn, 2005) a anglicko-perského korpusu (Pilevar et al., 2011) jsme použili tři páry jazyků pro vytvoření souhrnu dat. Pomocí této datové sady jsme testovali náš systém jednotlivě a v systému SMT. V obou nastaveních jsme dosáhli výrazně lepších výsledků ve srovnání s výchozími zdroji open source.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Sentence-aligned parallel bilingual corpora are the main and sometimes the only required resource for training Statistical and Neural Machine Translation systems (SMT, NMT). We propose an end-to-end deep neural architecture for language independent sentence alignment. In addition to one-to-one alignment, our aligner can perform cross- and many-to-many alignment as well. We also present a case study which shows how simple linguistic analysis can improve the performance of a pure neural network significantly. We used three language pairs from Europarl corpus (Koehn, 2005) and an English-Persian corpus (Pilevar et al., 2011) to generate an alignment dataset. Using this dataset, we tested our system individually and in an SMT system. In both settings, we obtained significantly better results compared to an open source baseline.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Wikipedia poskytuje neocenitelný zdroj paralelních vícejazyčných dat, které jsou ve vysoké poptávce pro různé druhy jazykových šetření, včetně teoretických i praktických studií. Zavedeme vede nový end-to-end neuronový model pro rozsáhlé paralelní sběr dat z Wikipedie. Náš model je nezávislý na jazyku, robustní a vysoce škálovatelný. Používáme náš systém pro shromažďování, francouzsko-anglické a perzština-anglické věty. Hodnocení člověka na konci ukazují silný výkon tohoto modelu při shromažďování vysoce kvalitních paralelních dat. My navrhnout také statistický rámec, který rozšiřuje výsledky našeho lidského hodnocení na jiné jazykové páry. Náš model také získal nejmodernější výsledek německo-anglické datové sady ze společného úkolu BUCC 2017 na paralelní extrakci vět z srovnatelných korpusů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Wikipedia provides an invaluable source of parallel multilingual data, which are in high demand
for various sorts of linguistic inquiry, including both theoretical and practical studies. We intro-
duce a novel end-to-end neural model for large-scale parallel data harvesting from Wikipedia.
Our model is language-independent, robust, and highly scalable. We use our system for collect-
ing parallel German-English, French-English and Persian-English sentences. Human evaluations
at the end show the strong performance of this model in collecting high-quality parallel data. We
also propose a statistical framework which extends the results of our human evaluation to other
language pairs. Our model also obtained a state-of-the-art result on the German-English dataset
of BUCC 2017 shared task on parallel sentence extraction from comparable corpora.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku navrhujeme nový jazykově založený přístup k odpovědi na otázky non-factoid otevřené domény z nestrukturovaných dat. Nejprve vypracujeme architekturu pro textové kódování, na jejímž základě zavedeme hluboký neuronový model. Tato architektura využívá mechanismus dvoustranné pozornosti, který pomáhá modelu soustředit se na otázku a větu odpovědi současně na extrakci frázové odpovědi. Za druhé, do modelu předáváme výstup analyzátoru volebních obvodů a integrujeme do sítě síť jazykových složek, abychom se mohli soustředit na kusy odpovědi spíše než na jeho jednotlivé slova pro vytvoření přírodnějšího výstupu. Díky optimalizaci této architektury se nám podařilo získat výsledky z hlediska výkonnosti, které jsou téměř shodné s lidskými parametry, a konkurenceschopné na nejmodernějších systémech datových souborů SQuAD a MS-MARCO.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we propose a new linguistically-based approach to answering non-factoid open-domain questions from unstructured data. First, we elaborate on an architecture for textual encoding based on which we introduce a deep end-to-end neural model. This architecture benefits from a bilateral attention mechanism which helps the model to focus on a question and the answer sentence at the same time for phrasal answer extraction. Second, we feed the output of a constituency parser into the model directly and integrate linguistic constituents into the network to help it concentrate on chunks of an answer rather than on its single words for generating more natural output. By optimizing this architecture, we managed to obtain near-to-human-performance results and competitive to a state-of-the-art system on SQuAD and MS-MARCO datasets respectively.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška přináší přehled vývoje slovníků textově-spojovacích prostředků napříč jazyky, od prvních z 90. let až po ty nejnovější, včetně projektu CzeDLex, slovníku českých textových konektorů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk is devoted to the development of dictionaries and lexicons of discourse-relational devices across languages, from the very first ones from 1990ies to the recent ones, including the CzeDLex, the Lexicon of Czech Discourse Connectives.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek představuje poloautomatickou metodu budování derivačních sítí. Metoda byla aplikována na polská a španělská jazyková data.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents a semi-automatic method for the construction of derivational networks. The proposed approach applies a sequential pattern mining technique in order to construct useful morphological features in an unsupervised manner.  The features take the form of regular expressions and later are used to feed a machine-learned ranking model.  The network is constructed by applying resulting model to sort the lists of possible base words and selecting the most probable ones.  This approach, besides relatively small training set and a lexicon, does not require any additional language resources such as a list of alternations groups, POS tags etc.  The proposed approach is applied to the lexeme sets of two languages, namely Polish and Spanish, which results in the establishment of two novel word-formation networks.  Finally, the network constructed for Polish is merged with the derivational connections extracted from the Polish WordNet and those resulting from the derivational rules developed by a linguist, resulting in the biggest word-formation network for that language. The presented approach is general enough to be adopted for other languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Neural Monkey je open-source toolkit pro sequence-to-sequence učení. Článek je zaměřen na prezentaci současného stavu toolkitu cílovým skupinám, mezi které patří studenti a výzkumníci, ať už aktivní v komunitě hlubokého učení či nováčci. Pro každou skupinu popisujeme nejvíce relevantní vlastnosti toolkitu spolu s jednoduchým schématem konfigurace, metodami analýzy modelů, které podporují užitečnou intuici, nebo modulární design umožňující snadné prototypování. Shrnujeme relevantní příspěvky vědecké komunity, které vznikly s využitím tohoto toolkitu a rozebíráme charakteristiky našeho toolkitu s ohledem na ostatní existující systémy. Článek uzavírá nástin budoucího vývoje toolkitu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Neural Monkey is an open-source toolkit for sequence-to-sequence learning. The focus of this paper is to present the current state of the toolkit to the intended audience, which includes students and researchers, both active in the deep learning community and newcomers. For each of these target groups, we describe the most relevant features of the toolkit, including the simple configuration scheme, methods of model inspection that promote useful intuitions, or a modular design for easy prototyping. We summarize relevant contributions to the research community which were made using this toolkit and discuss the characteristics of our toolkit with respect to other existing systems. We conclude with a set of proposals for future development.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Toto je česká verze datasetu Multi30k, který se používá při soutěžích v Multimodálním strojovém překladu. Dataset je založený ja datové sadě Flickr30k, která obsahuje přes 30 tisíc fotografií opatřených anglickými popisky. Pro soutěž na WMT16 a WMT17 byly tyto věty přeloženy do Němčiny a Francozštiny. Pro soutež v roce 2018 jsme obahatili tento dataset také o překlady do českého jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This is the Czech version of the Multi30k dataset that is used for WMT competitions in Multimodal Machine Translation. The dataset is based on the Flickr30k dataset with more 30,000 images accompanied by English captions. For the WTM16 and WMT17 German and French translation were added to these captions. For the WTM18 competition, we added also the translation into the Czech language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku představuje příspěvek do soutěže v multimodálním strjovém překladu na WMT18. V našem systému požíváme self-attentive neuronové sítě místo rekurentních. Evaluujeme dvě metody, jak lze zahrnout vizuální rysy do modelu: v prvním používáme vizuální informaci jako další vstup do dekodéruů v druhé metodě trénujeme enkodér tak, aby predikoval vizuální reprezentaci. Pro náš příspěvek jsem vytěžili dodatečná data. Obě navrhované metody přináší výrazné zlepšení oproti obdobným modelům využívajícím neuronové sítě.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our submission to the WMT18 Multimodal Translation Task. The main feature of our submission is applying a self-attentive network instead of a recurrent neural network. We evaluate two methods of incorporating the visual features in the model: first, we include the image representation as another input to the network; second, we train the model to predict the visual features and use it as an auxiliary objective. For our submission, we acquired both textual and multimodal additional data. Both of the proposed methods yield significant improvements over recurrent networks and self-attentive textual baselines.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V novinách poslední dobou můžeme číst, co zase kde udělala umělá inteligence, co dokázala, co umí, co bude umět, kolik lidí připraví o práci, kdy ovládne lidstvo a tak podobně. Za většinou těchto zpráv jsou úspěchy, kterých informatika dosáhla v posledních letech pomocí umělých neuronových sítí. Nejedná se však, jak by mohl název napovídat, o simulaci lidského mozku, ale o celkem jednoduché matematické modely, které se kdysi biologickými neurony vzdáleně inspirovaly. Na příkladu automatického překladu z jednoho jazyka do druhého, který právě díky neuronovým sítím dosáhl v poslední době velkého pokroku, si vysvětlíme, jak neuronové sítě fungují. Ukážeme si, jak lze na internetu sbírat příklady překladů a jak se s jejich pomocí neuronová síť dokáže naučit řešit tak komplexní úlohu jako je právě překlad.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Nowadays, newspapers often write what the artificial intelligence achieved, what other tasks it will master, and when it will conquer the wold. On the example of automatic translation from one language to another, which has recently made great progress due to neural networks, we will explain how neural networks work. We will show how to collect examples of translations on the Internet and how to use them with a neural network can learn how to solve a complex task like translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Automatické hodnocení kvality strojního překladu bylo zásadní pro rychlý vývoj systémů strojního překladu v posledních dvou desetiletích. Zatím se věnuje největší pozornost metrikám, které pracují s textem na úrovni vět, protože stejně tak fungují i překladové systémy. Kvalita překladu ale závisí i na diskurzních jevech, které se nemusí vůbec projevit, pokud se nacházejí uvnitř věty (např. koreference, diskurzní konektory, časová souslednost apod.). Navrhneme tedy několik metrik hodnocení strojového překladu na úrovni dokumentů: zobecnění exsitujících metrik a jazykově nezávislé metody měření lexikální soudržnosti a zachování koreference a morfologie zachování v cílovém jazyce. U těchto také měříme shodu s lidským úsudkem na nově vytvořené datové sadě, které obsahuje lidské hodnocení překladu pro čtyři jazykové páry.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Automatic machine translation evaluation was crucial for the rapid development of machine translation systems over the last two decades. So far, most attention has been paid to the evaluation metrics that work with text on the sentence level and so did the translation systems. Across-sentence translation quality depends on discourse phenomena that may not manifest at all when staying within sentence boundaries  (e.g. coreference, discourse connectives, verb tense sequence etc.). To tackle this, we propose several document-level MT evaluation metrics: generalizations of sentence-level metrics, language-(pair)-independent versions of lexical cohesion scores and coreference and morphology preservation in the target texts. We measure their agreement with human judgment on a newly created dataset of pairwise paragraph comparisons for four language pairs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vydali jsme natrénované modely pro nástroj Neural Monkey, které řeší 3 úlohy: strojový překlad, popisování obrázků, a analýzu sentimentu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This submission contains trained end-to-end models for the Neural Monkey toolkit for Czech and English, solving three NLP tasks: machine translation, image captioning, and sentiment analysis.
The models are trained on standard datasets and achieve state-of-the-art or near state-of-the-art performance in the tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci se zaměřujeme na tři různé úlohy NLP: popis obrázků, strojový překlad, a rozbor sentimentu. Reimplementujeme úspěšné postupy jiných autorů a přizpůsobujeme je českému jazyku. Nabízíme end-to-end architektury, které dosahují nejlepších či téměř nejlepších výsledků známých pro tyto úlohy, přičemž všechny jsou implementovány ve stejném nástroji pro sekvenční učení. Natrénované modely jsou k dispozici jak ke stažení, tak v podobě online dema.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this work, we focus on three different NLP tasks: image captioning, machine translation, and sentiment analysis. We reimplement successful approaches of other authors and adapt them to the Czech language. We provide end-to-end architectures that achieve state-of-the-art or nearly state-of-the-art results on all of the tasks within a single sequence learning toolkit. The trained models are available both for download as well as in an online demo.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Multimodální a abstraktní sumarizace videí bez doménového omezení vyžaduje shrnutí obsahu celého videa v několika krátkých větách a zároveň sloučení informací z více modalit, v našem případě videa a zvuku (nebo textu). Na rozdíl od obvyklé sumarizace žurnalistických textů není cílem pouze „komprimovat“ textové informace, ale poskytnout plynulé textové shrnutí informací, které byly shromážděny z různých vstupních modalit. V tomto příspěvku představujeme úlohu abstraktní sumarizace pro videa bez doménového omezení, ukazujeme, jak může model sekvenčního učení s hierarchickým mechanismem pozorností integrovat informace z různých modalit do uceleného výstupu. Dále prezentujeme pilotní experimenty na How2 korpusu instruktážních videí. Představujeme také novou evaluační metriku pro sumarizaci nazvanou Content F1, která měří spíše sémantickou přiměřenost než plynulost abstraktů, narozdíl od metrik jako jsou ROUGE a BLEU.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Multimodal and abstractive summarization of open-domain videos requires summarizing the contents of an entire video in a few short sentences, while fusing information from multiple modalities, in our case video and audio (or text). Different from traditional news summarization, the goal is less to “compress” text information only, but to provide a fluent textual summary of information that has been collected and fused from different source modalities. In this paper, we introduce the task of abstractive summarization for open-domain videos, we show how a sequence-to-sequence model with hierarchical attention can integrate information from different modalities into a coherent output, and present pilot experiments on the How2 corpus of instructional videos. We also present a new evaluation metric for this task called Content F1 that measures semantic adequacy rather than fluency of the summaries, which is covered by ROUGE and BLEU like metrics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Autoregresivní dekódování je jedinou součástí převádějících sekvence na sekvence, která bráňí masivní paralelizaci při inferenci. Neautoregresivní modely umožňují dekodéru generovat všechny výstupní symboly nezávisle a tedy paralelně. V článku představujeme novou neautoregresivní architekturu založenou na konekcionistické temporální klasifikaci (CTC). Na rozdíl od jiných neautoregresivních metod, které je nutné trénovat v několika krocích, představovaný systém se trénuje monoliticky. Experimentuje se strojovým překladem mezi angličinou a rumunštionou a angličtinou němčinou na standardních testovacích datech z WMT. Naše modely dosahují výrazného zrychlení oproti autoregresivním modelům, přičemž kvalita překladu je srovnatelná s jinými neautoregresivními modely.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Autoregressive decoding is the only part of sequence-to-sequence models that prevents them from massive parallelization at inference time. Non-autoregressive models enable the decoder to generate all output symbols independently in parallel. We present a novel non-autoregressive architecture based on connectionist temporal classification and evaluate it on the task of neural machine translation. Unlike other non-autoregressive methods which operate in several steps, our model can be trained end-to-end. We conduct experiments on the WMT English-Romanian and English-German datasets. Our models achieve a significant speedup over the autoregressive models, keeping the  translation quality comparable to other non-autoregressive models.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Při sekvenčním učením s více zrdoje informace, může být mechanismus pozornosti (attention) modelován různými způsoby. Toto téma bylo důkladně studováno na rekurentních neurnovoých sítích. V tomto článku se zabýváme tímto problém v architektuře Transormer. Navrhujeme čtyři různé strategie kombinace vstupů: sériové, paralelní, ploché a hierarchické. Navrhované metody vyhodnocujeme na úloze multimodálního překladu a překladu z více zdrojových jazyků současně. Z  výsledků experimentů vyplývá, že modely jsou schopny využívat více zdrojů a fungovat lépe než modely s pouze jedním zdrojem informace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In multi-source sequence-to-sequence tasks, the attention mechanism can be modeled in several ways. This topic has been thoroughly studied on recurrent architectures. In this paper, we extend the previous work to the encoder-decoder attention in the Transformer architecture. We propose four different input combination strategies for the encoder-decoder
attention: serial, parallel, flat, and hierarchical. We evaluate our methods on tasks of multimodal translation and translation with
multiple source languages. The experiments show that the models are able to use multiple sources and improve over single source baselines.</seg>
            </tuv>
        </tu>
        

        <tu>
            <tuv xml:lang="cs">
                <seg>Popularizační přednáška pro studenty gymnázia o tom, čím se zabývá ÚFAL: zpracování přirozeného jazyka, strojový překlad, umělá inteligence.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A popular lecture for high-school students about the research done at ÚFAL: natural language processing, machine translation, artificial intelligence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje naše experimenty s neuronovým strojovým překladem pomocí frameworku Tensor2Tensoru a modelu Transformer (Vaswani a kol., 2017).
Zkoumáme některé kritické parametry, které ovlivňují kvalitu překladu, paměťovou náročnost, stabilitu trénování a délku trénování. Každý experiment uzavíráme souborem doporučení. Krom jiného zkoumáme škálování na více GPU a poskytujeme praktické tipy pro vylepšené trénování týkající se velikosti dávky, rychlosti učení, počtu zahřívacích kroků, maximální délky věty a průměrování modelů. Doufáme, že naše pozorování umožní ostatním výzkumníkům dosáhnout lepších výsledků vzhledem k jejich specifickým hardwarovým a datovým omezením.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article describes our experiments in neural machine translation using the recent Tensor2Tensor
framework and the Transformer sequence-to-sequence model (Vaswani et al., 2017).
We examine some of the critical parameters that affect the final translation quality, memory
usage, training stability and training time, concluding each experiment with a set of recommendations
for fellow researchers. In addition to confirming the general mantra “more data
and larger models”, we address scaling to multiple GPUs and provide practical tips for improved
training regarding batch size, learning rate, warmup steps, maximum sentence length
and checkpoint averaging. We hope that our observations will allow others to get better results
given their particular hardware and data constraints.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku prezentujeme sadu vylepšení taggeru pro automatickou detekci slovesných víceslovných výrazů, MUMULS. Náš tagger se zúčastnil PARSEME shared tasku a jako jediný byl založen na neuronových sítích. Ukazujeme, že embeddingy slov na základě jejich znaků vedou k zlepšením, především díky redukci množství out-of-vocabulary slov. Dále nahrazením softmaxové vrstvy v dekodéru klasifikátorem založeným na conditional random fields dosahujeme dalšího zlepšení. Na závěr porovnáváme různé druhy reprezentací příznaků zohledňující okolní kontext slova za pomocí různých architektur enkodérů. Experimenty s češtinou ukazují, že kombinace embeddingů založených na konvoluci jednotlivých znaků, self-attentive architektura enkodéru a conditional random filed klasifikátor dosahují nejlepších empirických výsledků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present a set of improvements introduced to MUMULS, a tagger for the automatic detection of verbal multiword expressions. Our tagger participated in the PARSEME shared task and it was the only one based on neural networks. We show that character-level embeddings can improve the performance, mainly by reducing the out-of-vocabulary rate. Furthermore, replacing the softmax layer in the decoder by a conditional random field classifier brings additional improvements. Finally, we compare different context-aware feature representations of input tokens using various encoder architectures. The experiments on Czech show that the combination of character-level embeddings using a convolutional network, self-attentive encoding layer over the word representations and an output conditional random field classifier yields the best empirical results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvěk se zabýval popisem kohezních prostředků vyskytujících se v textech psaných nerodilými mluvčími češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presentation described cohesive means appearing in texts written by learners of Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem příspěvku je výzkum provázanosti textové koreference a aktuálního členění větného a jejich podíl na koherenci textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of the paper is to examine the interplay of text coreference and sentence
information structure and its role in text coherence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek shrnuje průběh 43. ročníku Olympiády v českém jazyce a stručně představuje zadané jazykové úkoly, jejich řešení, přístupy účastníků k úkolům (společně s hodnocením) a jména vítězů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article summarizes the course of the 43rd year of the Olympiad in the Czech language, presenting in brief its general settings as well as the tasks, their solutions, the approaches of the participants (jointly with their evaluation) and the names of winners.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku představujeme dvě softwarové aplikace na automatické hodnocení koherence textu v češtině s názvem EVALD – Evaluátor diskurzu. První aplikace "EVALD 1.0" hodnotí texty psané rodilými mluvčími češtiny (hodnotící škálou užívanou v českých školách: 1–5). Druhá aplikace "EVALD 1.0 pro cizince" hodnotí texty nerodilých mluvčích češtiny (na škále A1–C2 podle "Společného evropského referenčního rámce"). Obě aplikace jsou dostupné online na https://lindat.mff.cuni.cz/services/evald-foreign/.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In  the  paper, we  introduce  two  software applications  for  automatic  evaluation  of coherence in Czech texts called EVALD – Evaluator  of  Discourse.  The  first  one – EVALD 1.0 – evaluates texts  written by native  speakers  of  Czech on a  five - step scale  commonly  used  at  Czech  schools (grade 1 is the best, grade 5 is the worst). The second application is EVALD 
1.0 for Foreigners  assessing  texts  by  non-native speakers  of  Czech  using  six - step  scale (A1–C2) according to CEFR. Both applications are available online at https://lindat.mff.cuni.cz/services/evald-foreign/.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slovník afixů zpracovává předpony (prefixy) a přípony (sufixy), představuje však i četné části složených slov, jimž se někdy přiznává blízkost afixům. Je to tedy kompendium popisující všechny důležité odvozovací prvky slov užívaných v současných českých textech – včetně slov přejatých. Netradiční pohled na tvoření slov od afixů dává trochu odlišný obraz než běžný směr od derivátů: do ohniska pozornosti se leckdy dostanou doklady z různých příčin opomíjené. Výhodou je i korpus jako zdroj dat: materiál je dostatečně reprezentativní a poskytuje vesměs spolehlivý základ pro odborné závěry. Pomocí slovníku je možné studovat systém afixů v závislosti na jejich frekvenci či produktivitě. Lze z něho vyjít při řešení otázek synonymie afixů, návaznosti na povahu základu a mnoha dalších jevů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The dictionary of affixes presents not only prefixes and sufixes but also many word-parts of compound words.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kniha přináší nejnovější poznatky z oblasti strojového zpracování českých emocionálně laděných textů. Za prvé přináší analýzu jazykových prostředků, které společně formují emocionální význam psaných výpovědí v češtině. Za druhé využívá zjištění týkající se emocionálního jazyka v komputačních aplikacích.

Autorka podává systematický přehled lexikálních, morfosyntaktických, sémantických a pragmatických aspektů emocionálního významu v českých výpovědích a navrhuje formální reprezentaci emocionálních struktur v rámci Pražského závislostního korpusu a konstrukční gramatiky.

V oblasti komputačních aplikací se zaměřuje na témata sentiment analysis, tedy automatické extrakce emocí z textu. Popisuje tvorbu ručně anotovaných emocionálních zdrojů dat a řeší základní úlohy postojové analýzy, jako je např. klasifikace polarity a identifikace cíle hodnocení, a to za využití nejnovějších metod z oblasti automatického zpracování přirozeného jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The study introduces latest research findings from the area of machine processing of Czech emotional language data. First, it provides an analysis of language means which together form an emotional meaning of written utterances in Czech. Second, it employs the findings concerning emotional language in computational applications.

The author provides a systematic overview of lexical, morphosyntactic, semantic and pragmatic aspects of emotional meaning in Czech utterances. Also, she proposes two formal representations of emotional structures within the framework of the Prague Dependency Treebank and Construction Grammar.

Regarding the computational applications, the study focuses on sentiment analysis, i.e. automatic extraction of emotions from text. It describes a creation of manually annotated emotional data resources in Czech and performs main sentiment analysis tasks, such as e.g. polarity classification and opinion target identification on Czech data, employing the up-to-date methods of natural language processing.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku navrhujeme tři různé metody auotmatického vyhodnocování kvality strojového překladu. Dvě z těchto metrik jsou trénovatelné na skóre přímého vyhodnocení a dvě z nich používají závislostní struktury. Trénovatelná metrika AutoDA, která používá hloubkově syntaktické rysy, dosáhla lepší korelace s lidmi ve srovnání např. s metrikou chrF3.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this report paper we propose three different methods for automatic evaluation of the machine translation (MT) quality. Two of the metrics are trainable on direct-assessment scores and two of them use dependency structures. The trainable metric AutoDA, which uses deep-syntactic features, achieved better correlation with humans compared e.g. to the chrF3 metric.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento balíček obsahuje datové sady pro vývoj a testování modelů strojového překladu pro krátké vyhledávací dotazy v oblasti medicíny, a to pro čestinu, angličtinu, francouzštinu, němčinu, španělštinu, maďarštinu, polištinu a švédštinu. Dotazy obsažené v datech pochází jak od zdravotnických profesionálů, tak od laické veřejnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This package contains data sets for development and testing of machine translation of medical search short queries between Czech, English, French, German, Hungarian, Polish, Spanish and Swedish. The queries come from general public and medical experts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento balíček obsahuje sady paralelních vět pro vývoj a testování strojového překladu souhrnů vědeckých článků z oboru medicíny mezi češtinou, angličtinou, francouzštinou, němčinou, maďarštinou, polištinou, španělštinou a švédštinou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This package contains data sets for development (Section dev) and testing (Section test) of machine translation of sentences from summaries of medical articles between Czech, English, French, German, Hungarian, Polish, Spanish
and Swedish. Version 2.0 extends the previous version by adding Hungarian, Polish, Spanish, and Swedish translations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek popisuje utváření syntaktické struktury komplexních predikátů v češtině s ohledem jak na hloubkovou, tak povrchovou syntax.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the syntactic structure formation of Czech complex predicates with light verbs with respect to both deep and surface syntax.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku popisujeme česká deverbativní substantiva v substantivních i slovesných konstrukcích (tj. v komplexních predikátech s funkčním slovesem) a složité propojení slovníku a gramatiky při jejich tvoření. Naznačujeme, že komplexní predikáty s funkčním slovesem jsou výsledkem pravidelné sytaktické operace. Představujeme dva propojené valenční slovníky, NomVallex a VALLEX, s cílem ukázat jak minimalizovat počet slovníkových hesel při současném zachování možnosti generovat správně utvořené substantivní i slovesné konstrukce s deverbativními substantivy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we provide a well-founded description of Czech deverbal nouns in both nominal and verbal structures (light verb constructions), based on a complex interaction between the lexicon and the grammar. We show that light verb constructions result from a regular syntactic operation. We introduce two interlinked valency lexicons, NomVallex and VALLEX , demonstrating how to minimize the size of lexicon entries while allowing for the generation of well-formed nominal and verbal structures of deverbal nouns.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento příspěvek shrnuje poznatky zjištěné při vytváření lexikografického modelu pro popis komplexních predikátů s kategoriálním slovesem v češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper summarizes theoretical findings on the lexicographic description of Czech complex predicates with light verbs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku se zabýváme koreferencí u českých komplexních predikátů s kategoriálním slovesem. Popisujeme jednotlivé typy koreference mezi valenčními doplněními slovesa a jména, zjištěné v rozsáhlé anotaci korpusových dat, a ukazujeme, jakou roli sehrává ve formování jejich syntaktické struktury.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we deal with coreference in Czech complex predicates with light verbs. In an extensive data annotation project, we identify,
delimit and thoroughly describe individual types of coreferential relations between valency
complementations of Czech complex predicates.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Obor rozpoznávání pojmenovaných entit v češtině (tj. úkol automaticky identifikovat a klasifikovat významné části textu, jako například jména lidí, míst a organizací) se významně rozvinul po vydání českého korpusu pojmenovaných entit, Czech Named Entity Corpus (CNEC). Tato doktorská práce předkládá autorské výsledky v oblasti rozpoznávání pojmenovaných entit, zejména v češtině. Publikuje práci a výzkum provedený v průběhu přípravy CNEC a později během jeho evaluace. Dále shrnuje autorské výsledky, které představují nejlepší známé výsledky v rozpoznávání českých pojmenovaných entit. Na základě jednoduché neuronové sítě s výstupní funkcí softmax a standardní sadou klasifikačních rysů je popsána metodologie a výsledky, ze kterých později vznikl otevřený software pro rozpoznávání pojmenovaných entit, NameTag. Doktorská práce je zakončena popisem rozpoznávače založeném na rekurentních neuronových sítích s embeddingy slov a embeddingy založenými na znacích, které představují výsledky současného výzkumu v oblasti neuronových sítí. Rozpoznávač nevyžaduje tvorbu klasifikačních rysů a dosahuje v současné době nejlepších známých výsledků v oblasti rozpoznávání pojmenovaných entit v češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Czech named entity recognition (the task of automatic identification and classification of proper names in text, such as names of people, locations and organizations) has become a well-established field since the publication of the Czech Named Entity Corpus (CNEC). This doctoral thesis presents the author's research of named entity recognition, mainly in the Czech language. It presents work and research carried out during CNEC publication and its evaluation. It further envelops the author's research results, which improved Czech state-of-the-art results in named entity recognition in recent years, with special focus on artificial neural network based solutions. Starting with a simple feed-forward neural network with softmax output layer, with a standard set of classification features for the task, the thesis presents methodology and results, which were later used in open-source software solution for named entity recognition, NameTag. The thesis finalizes with a recurrent neural network based recognizer with word embeddings and character-level word embeddings, based on recent advances in neural network research, which requires no classification features engineering and achieves excellent state-of-the-art results in Czech named entity recognition.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme korpus českých vět s ručně anotovanými pojmenovanými entitami, ve kterém byla použita bohatá dvouúrovňová hierarchie typů pojmenovaných entit. Korpus představuje první dostupný českým zdroj pro rozpoznávání pojmenovaných entit a od roku 2007 stimuloval výzkum v tomto oboru. Popisujeme dvouúrovňovou jemnou hierarchii s vnořenými entitami a motivace, které nás vedly k jejímu návrhu. Dále ukazujeme, jak byla tato data prakticky využita při návrhu a trénování rozpoznávače pojmenovaných entit a provádíme velké množství experimentů, abychom kriticky ohodnotili rozhodnutí, která jsme v průběhu návrhu korpusu provedli. Důkladně prodiskutujeme dopad zvoleného výběru vět, velikosti korpusu, způsobu morfologického zpracování, ale i výběr typů pojmenovaných entit a dalších vlastností korpusu na výkon rozpoznávače pojmenovaných entit z hlediska strojového učení s učitelem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a corpus of Czech sentences with manually annotated named entities, in which a rich two-level hierarchy of named entity types was used. The corpus was the first available large Czech named entity resource and since 2007, it has stimulated the research in this field for Czech. We describe the two-level fine-grained hierarchy allowing embedded entities and the motivations leading to its design. We further discuss the data selection and the annotation process. We then show how the data can be used for training a named entity recognizer and we perform a number of experiments to critically evaluate the impact of the decisions made in the process of annotation on the named entity recognizer performance. We thoroughly discuss the effect of sentence selection, corpus size, part-of-speech tagging and lemmatization, representativeness and bias of the named entity distribution, classification granularity and other corpus properties in terms of supervised machine learning.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V PARSEME Shared Task bylo úkolem určit slovesné VV v textu. Slovesné VV obsahují idiomy (let the cat out of the bag), analytické predikáty (make a decision), verb-particle constructions (give up) a inherentně reflexivní slovesa (se suicider 'spáchat sebevraždu' ve francouzštině). SVV byly anotovány podle univerzálních pravidel v 18 jazycích. Corpusy jsou k dispozici ve formátu parsemetsv inspirovaném formátem CONLL-U.

...

Obsahuje trénovací data, testovací data, nástroje a univerzální anotační pravidla.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The PARSEME shared task aims at identifying verbal MWEs in running texts. Verbal MWEs include idioms (let the cat out of the bag), light verb constructions (make a decision), verb-particle constructions (give up), and inherently reflexive verbs (se suicider 'to suicide' in French). VMWEs were annotated according to the universal guidelines in 18 languages. The corpora are provided in the parsemetsv format, inspired by the CONLL-U format.

For most languages, paired files in the CONLL-U format - not necessarily using UD tagsets - containing parts of speech, lemmas, morphological features and/or syntactic dependencies are also provided. Depending on the language, the information comes from treebanks (e.g., Universal Dependencies) or from automatic parsers trained on treebanks (e.g., UDPipe).

This item contains training and test data, tools and the universal guidelines file.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slovní vektory jsou rozhraním mezi světem diskrétníh jednotek slov a spojitým světem neuronových sítí. V této práci prozkoumáme různé druhy jejich inicializace na čtyrech úlohách z oblasti zpracování přirozeného jazyka a dvou různých neuronových architekturách. Potvrdíme, že předtrénované embeddingy rychleji konvergují k řešení a že pro náhodnou inicializaci nezávisí tolik na odchylce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Word embeddings are the interface between
the world of discrete units of text
processing and the continuous, differentiable
world of neural networks. In this
work, we examine various random and
pretrained initialization methods for embeddings
used in deep networks and their
effect on the performance on four NLP
tasks with both recurrent and convolutional
architectures. We confirm that pretrained
embeddings are a little better than
random initialization, especially considering
the speed of learning. On the other
hand, we do not see any significant difference
between various methods of random
initialization, as long as the variance
is kept reasonably low. High-variance initialization
prevents the network to use the
space of embeddings and forces it to use
other free parameters to accomplish the
task. We support this hypothesis by observing
the performance in learning lexical
relations and by the fact that the network
can learn to perform reasonably in its task
even with fixed random embeddings.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prozkoumali jsme vliv řazení vět pro online trénování neuronových sítí pro počítačová překlad. Práce má dvě části: řazení stejných příkladů v rámci minibatche a postupné zvyšování složitosti trénovacích dat (tzv. Curriculum learning). Výsledkem práce je, že homogenita minibatchí nemá na trénování velký vliv zato curriculum lepších větších výsledků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We examine the effects of particular orderings of sentence pairs on the on-line training of neural machine translation (NMT). We focus on two types of such orderings: (1) ensuring that each minibatch contains sentences similar in some aspect and (2) gradual inclusion of some sentence types as the training progresses
(so called “curriculum learning”). In our English-to-Czech experiments, the internal homogeneity of minibatches has no effect on the training but some of our “curricula” achieve a small improvement over the baseline.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V identifikaci jazyka, společný prvním krokem při zpracování přirozeného jazyka, chceme automaticky určit jazyk nějakého vstupního textu. Jednojazyčná identifikace jazyka předpokládá, že daný dokument je napsán v jednom jazyce. Ve vícejazyčné identifikaci jazyka, že dokument je obvykle ve dvou nebo ve třech jazycích a my jen chceme jejich jména. Naš cíl je ještě o krok dále a chceme navrhnout metodu pro identifikaci jazyků, kde se mohou jazyky libovolně měnit v textu a cílem je identifikovat rozpětí každého z jazyků. Naše metoda je založena na obousměrné rekurentních neuronových sítí, která funguje dobře v jednojazyčné a vícejazyčných identifikaci jazyka. Náš nástroj pokrývá 131 jazyků. Tato metoda zachovává přesnost i pro krátké dokumenty a napříč doménami, takže je ideální pro použití bez přípravy tréninkových dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In language identification, a common first step in natural language processing, we want to automatically determine the language of some input text. Monolingual language identification assumes that the given document is written in one language. In multilingual language identification, the document is usually in two or three languages and we just want their names. We aim one step further and propose a method for textual language identification where languages can change arbitrarily and the goal is to identify the spans of each of the languages. Our method is based on Bidirectional Recurrent Neural Networks and it performs well in monolingual and multilingual language identification tasks on six datasets covering 131 languages. The method keeps the accuracy also for short documents and across domains, so it is ideal for off-the-shelf use without preparation of training data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek prezentuje letošní submission CUNI na WAT 2017 Translation Task zaměřující se na japonsko-anglický překlad, jmenovitě překlad vědeckých článků, překlad patentů a novinových článků. Porovnali jsme dvě neuronové architektury, samostatnou standardní sequence-to-sequence s attentionem a architekturu používající konvoluční encoder. Také jsme experimentovali s mnoho druhy předzpracování dat. Navíc přidáváme výsledky našich out-of-domain experimentů získaných kombinací dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents this year's CUNI submissions to the WAT 2017 Translation Task focusing on the Japanese-English translation, namely Scientific papers subtask, Patents subtask and Newswire subtask. We compare two neural network architectures, the standard sequence-to-sequence with attention (Seq2Seq) (Bahdanau et al., 2014) and an architecture using convolutional sentence encoder (FBConv2Seq) described by Gehring et al. (2017), both implemented in the NMT framework Neural Monkey that we currently participate in developing.
We also compare various types of preprocessing of the source Japanese sentences and their impact on the overall results. Furthermore, we include the results of our experiments with out-of-domain data obtained by combining the corpora provided for each subtask.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V prezentácii popisujem nedávno publikovanú kolekciu pre viacjazyčné vyhľadávanie v českých nahrávkach korpusu Malach. Kolekcia obsahuje české nahrávky Archívu vizuálnej histórie, ktoré pozostávajú z rozhovorov s preživšími holokaustu. Archív pozostáva z audio nahrávok, štyroch typov automatických prepisov, manuálnych anotácií vybraných tém a ďalších metadát. Archív spolu obsahuje 353 nahrávok a 592 hodín rozhovorov.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk presents recently published Czech Malach Cross-lingual Speech Retrieval Test Collection.
The collection contains Czech recordings of the Visual History Archive which consists of the interviews with the Holocaust survivors. The archive consists of audio recordings, four types of automatic transcripts, manual annotations of selected topics and interviews' metadata. The archive totally contains 353 recordings and 592 hours of interviews.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V prednáške popisujem základy dolovania informácií z textu. Najprv sa zameriavam na rôzne aplikácie dolovania informácií. Následne popíšem základné a najčastejšie používané metódy dolovania informácií z textu. Nakoniec použijeme niekoľko online nástrojov na spracovanie textu a vytvoríme reprezentáciu textu pomocou word cloudu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the talk, I describe basics of text mining. 
First, I describe various applications of text mining. Then, I focus on basic methods frequently used in text mining applications. Last, we will use online tools to pre-process text and create a word cloud.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článok uvádza prehľad rozličných metód pre spracovanie obrazových dát a porovnáva efektívnosť ich využitia v úlohe vyhľadávania hyperlinkov v archíve videí. Vizuálna informácia, ktorá sa získa pomocou metód Feature Signatures, SIMILE deskriptorov a konvolučných neurónových sietí sa využíva pri výpočte podobnosti snímkov vo videu a umožňuje tak vyhľadávanie podobných tvárí, objektov a pozadia. Vizuálne deskriptory sa tiež používajú pri rozpoznávaní objektov v snímkoch a tento a textový popis je možné ďalej kombinovať s textovým vyhľadávaním na základe automatických prepisov a titulkov. Prezentované experimenty boli robené vrámci benchmarkov MediaEval a TRECVid.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we survey different state-of-the-art visual processing methods and utilize them in hyperlinking. Visual information, calculated using Features Signatures, SIMILE descriptors and convolutional neural networks (CNN), is utilized as similarity between video frames and used to find similar faces, objects and setting. Visual concepts in frames are also automatically recognized and textual output of the recognition is combined with search based on subtitles and transcripts. All presented experiments were performed in the Search and Hyperlinking 2014 MediaEval task and Video Hyperlinking 2015 TRECVid task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Archív obsahuje české nahrávky z Archívu vizuálnej histórie Malach, ktoré pozostávajú z rozhovorov so svedkami holokaustu. Archív pozostáva z audio nahrávok, štyroch typov automatických prepisov, manuálnych anotácií vybraných tém a ďalších metadát. Archív spolu obsahuje 353 nahrávok a 592 hodiín rozhovorov.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The package contains Czech recordings of the Visual History Archive which consists of the interviews with the Holocaust survivors. The archive consists of audio recordings, four types of automatic transcripts, manual annotations of selected topics and interviews' metadata. The archive totally contains 353 recordings and 592 hours of interviews.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem příspěvku je podat přehled anotace elipsy v korpusech Universal Dependencies (UD) 2.0. Z dlouhodobého hlediska je znalost typů a četností eliptických konstrukcí užitečná pro experimenty se syntaktickou analýzou zaměřené na elipsu; to byla také naše původní motivace. Nicméně se ukazuje, že současný stav anotace ještě zdaleka není dokonalý, a tudíž hlavním výstupem naší studie je přehled a popis anotačních chyb či nekonzistencí; doufáme, že tím přispějeme ke zlepšení budoucích verzí korpusů UD.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The goal of this paper is to survey annotation of ellipsis in Universal Dependencies (UD) 2.0 treebanks. In the long term, knowing the types and frequencies of elliptical constructions is important for parsing experiments focused on ellipsis, which was also our original motivation. However, the current state of annotation is still far from perfect, and thus the main outcome of the present study is a description of errors and inconsistencies; we hope that it will help improve the future releases.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Podrobný morfologický popis slovních tvarů v jakémkoli jazyce představuje jednu z podmínek úspěšného automatického zpracování jazykových dat. Cílem této práce je představit projekt nového popisu české morfologie, zejména plánovaných změn v morfologickém značkování. Klíčové změny jsou následující: 1) jednoznačný popis variant; 2) koncept vícenásobného lemmatu; 3) revize definic slovních druhů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A detailed morphological description of word forms in any language represents one of the necessary conditions of a successful automatic processing of linguistic data. 
The aim of this paper is to present the project of a new description of Czech morphology, especially planned changes in the tagset. The key changes are as follows: 1) unambiguous description of variants; 2) concept of a multiple lemma;3) revision of part-of-speech definitions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Text představuje možné analytické přístupy k orálněhistorickému interview s ohledem na jeho situovaný a interakční rozměr.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The oral history (OH) interview is a generally accepted method of obtaining verbal accounts of past events from eyewitnesses. Contemporary OH draws from several scientific disciplines and considers various philosophical and methodological issues. The original approach to OH as the “transparent” locus of information is no longer accepted, and researchers acknowledge that the interview unfolds in a specific time and place, and between particular people. Inevitably, OH interview is a speech exchange nested in situational and interactional context, with participants attempting (among other things) to collaboratively produce a comprehensible account of the past. One of the goals of the interview is to elicit storytelling, often for an imagined audience. Recordings of this specific type of interaction can be subjected to different kinds of analysis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Bezprecedentní možnosti archivace a sdílení informací vedou v naší současné společnosti k tomu, že se obraz minulosti zachycené ve vzpomínkách a vyprávění přímých pamětníků stává důležitým zdrojem našeho (laického i odborného) vědění o této minulosti. S ohledem na rostoucí počet archivovaných orálněhistorických interview (dále OHI), často dostupných na internetu či na specializovaných pracovištích, lze orální historii využít jako specifický zdroj dat, analyzovatelných (mimo jiné) i ze sociologického hlediska. Bližší představení jednoho z takových zdrojů, konkrétně Archivu vizuální historie USC Shoah Foundation, přístupného v Praze prostřednictvím Centra vizuální historie Malach, je hlavním cílem tohoto textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article describes the USC Shoah Foundation's Visual History Archive, and illustrates its utility for doing sociology through two case studies based on student's seminar works.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zabýváme se využitím velkého korpusu (Pražského diskurzního korpusu 2.0) s ručně anotovanými diskurzními vztahy pro vytvoření slovníku českých diskurzních konektorů (CzeDLex). Popisujeme teoretické aspekty projektu a technické řešení založené na Prague Markup Language, které umožňuje efektivní začlenění slovníku do rodiny pražských korpusů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We discuss a process of exploiting a large corpus
manually annotated with discourse relations – the Prague Discourse Treebank 2.0 – to create a lexicon of Czech discourse connectives (CzeDLex). We present theoretical aspects of the project and a technical solution based on the (XML-based) Prague Markup Language that allows for an efficient incorporation of the lexicon into the family of Prague treebanks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje dvě studie zaměřené na softwarové nástroje vyvinuté pro derivační sítě jako např. DeriNet; jde o nástroje určené pro vyhledávání a vizualizaci derivačních stromů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents two  studies on software tools developed for lexical derivationa databases such as  DeriNet; the tools under study serve for querying and visualizing derivational trees contained in the database.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>DeriNet je lexikální síť modelující derivační vztahy v češtině. Uzly odpovídají lexémům, hrany odpovídají slovotvorným derivacím. Současná verze, DeriNet 1.5, byla oproti předcházejícím verzím obohacena například o velké množství předponových i příponových derivací sloveso-sloveso a o vybrané typy příponových derivací substantivum-adjektivum. DeriNet 1.5 je první verzí, která obsahuje anotaci související s dalším slovotvorným typem - kompozicí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>DeriNet is a lexical network which models derivational relations in the lexicon of Czech. Nodes of the network correspond to Czech lexemes, while edges represent derivational relations between a derived word and its base word. The present version, DeriNet 1.5, contains 1,011,965 lexemes (sampled from the MorfFlex dictionary) connected by 785,543 derivational links. Besides several rather conservative updates (such as newly identified prefix and suffix verb-to-verb derivations as well as noun-to-adjective derivations manifested by most frequent adjectival suffixes), DeriNet 1.5 is the first version that contains annotations related to compounding (compound words are distinguished by a special mark in their part-of-speech labels).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje postup transformace latinského závislostního treebanku do dotazovatelné podoby, aby mohl být prohlížen online pomocí dotazovacího nástroje nad závislostními stromy. Nejdříve jsou představeny anotační vrstvy, poté architektura dotazovacího nástroje a nakonec postup konverze do relační databáze.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes how to turn a Latin dependency treebank into queryable information so that it can be browsed online using a tree query engine and its web interface. The annotation layers of the treebank are first introduced, then the query system architecture is detailed, and finally the way the treebank is converted into a relational database architecture is described.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá užíváním předpon v českém sylabickém trocheji.Testujeme hypotézu, kterou nastolili Miroslav Červenka, Květa Sgallová a Petr Kaiser, která uvádí, že někteří autoři v 19. století používali předpony ke zmírnění rytmických nepravidelností. V naší analýze - založené na automatickém rozpoznávání předpony ve velkém souboru poetických textů z Korpusu českého verše - pozorujeme jasnou tendenci v práci některých autorů používat předpony v takových kontextech s četností výrazně vyšší, než by se dalo očekávat pouze náhodou. Dále pozorujeme, že tato technika je velmi běžná v první polovině 19. století, ale v pozdějších pracích postupně vymizela.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article deals with the use of prefixes in the Czech accentual syllabic trochee.
We test a hypothesis raised by Miroslav Červenka, Květa Sgallová, and Petr Kaiser which states
that some authors in the 19 th century used prefixes to moderate rhythmical irregularities. In our
analysis – based on automatic prefix recognition in a large body of poetic texts from the Corpus
of Czech Verse – we observe a clear tendency in the work of some authors to employ prefixes in
such contexts with a frequency significantly higher than would be expected merely by chance.
Furthermore, we observe this technique to be very common in the first half of the 19 th century,
but to gradually disappear in later works.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V teto prednasce provadim srovnani korpusu anotovanych na koreferencni a diskurzni vztahy v ruznych jazycich. Popisuju diachronni a synchronni prechody mezi diskurzem a koreferenci, na prikladech z anglictiny, nemciny a cestiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Coreference and discourse markers are strong cohesive devices providing textual coherence. In my presentation, I discuss the interconnections between them. I present and briefly classify corpora annotated with coreference and discourse relations, with special focus on corpora that have both types of annotation, as well as linguistic research on the topic. Furthermore, I will describe some structural and diachronic coreference-to-discourse transitions and support them with examples from English, German and Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem přednášky je souhrnně představit výzkum výstavby textu, který probíhá v Ústavu formální a aplikované lingvistiky MFF UK.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of the lecture is to present the research in the field of text coherence in the Prague Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Srovnavame nemcinu, cestinu a anglictinu vzhledem k tomu, jak se v cestine a anglictine odrazi lokalni adverbia (typ "damit"). Vyzkum provadime na paralelnich TED-talks.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The purpose of the present study is to analyse the interplay between connectives or
discourse-relational devices (DRDs) and other discourse-related phenomena, i.e. coreference and
bridging in German, English and Czech. DRDs express logico-semantic relations between
propositions, such as contrast, time, addition and others). Coreference serves the task of linking
identical referents and events (i.e. complex anaphors), whereas bridging expresses near identical
relations between referents, linking them with semantic meanings. All these devices contribute to
the construction of meaningful discourse. These phenomena exist in all the three languages under
analysis. However, their realisations depend on the different preferences these languages have (both systemic and context-based). The knowledge of these preferences is important for translation, as
translators have to be aware of the full range of linguistic options that exist in both source and
target language. We aim at describing these preferences for German, English and Czech. For these,
different transfer patterns will be extracted from a parallel corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Srovnavame koreferencni vyrazy v paralelnich textech. Jde nam o vedlejsi vety vs. neosobni klauzy, korelativni vyrazy a pro-drop kvality v cestine, polstine, rustine a anglictine.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The subject of our analysis are cohesive devices important for discourse analysis in three Slavonic languages vs. English based on translated texts, particularly coreferential expressions such as finite and infinite constructs e.g. relative clauses, participial, possessive and correlative constructions with a demonstrative pronoun. A special point of our analysis is comparison of pro-drop qualities of three Slavonic languages in comparison with English.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Analýza jmenných přísudků v predikátech jmenných se sponou v pozici na začátku věty. V naší analýze hledáme pravidelné vzorky v konstrukcích s nezákladním slovosledem, které slouží jako prostředek koherence textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In our contribution we analyse second arguments in sentences with verbonominal predicate in the initial sentence position. Our material
provides evidence that the cohesion in these
cases is not given by semantics of these words only, but it is also determined by syntactic features, such as relational (valency) properties of nouns and adjectives and their initial position in the sentence. Operating together, these two factors represent strong means of text cohesion. In our contribution, we document and analyse this cohesive device in Czech on the data from the Prague Dependency Treebank, a large-scale newspaper corpus, annotated with morphological,
shallow and underlying syntactic information, as well as with coreference, discourse and bridging
relations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popularizační přednáška o výpočetní lingvistice, jejích podoblastech a řešených problémech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A popularizing talk about computational linguistics, its subtasks and problems which are being solved.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Detekce jednotlivých symbolů na stránce hudební notace je jedním ze zbývajících nevyřešených podproblémů rozpoznávání not. Navrhujeme použít plně konvoluční segmentační neuronovou síť, která produkuje vysoce kvalitní pravděpodobnostní masky přes pixely. Pokusy na detekci notových hlaviček nad těmito maskami dosahují f-score 0.98 i při použití pouze elementárních detektorů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Musical symbol detection on the page is an out- standing Optical Music Recognition (OMR) subproblem. We propose using a fully convolutional segmentation network to produce high-quality pixel-wise symbol probability masks. Experiments on notehead detection show a very promising detection f-score of 0.98 with elementary detection methods.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slovník jednoslovných parafrází slovesných idiomatických konstrukcí a víceslovných predikátů s lehkými slovesy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>ParaDi 2.0. is a dictionary of single verb paraphrases of Czech verbal multiword expressions - light verb constructions and idiomatic verb constructions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této práci prezentujeme nový, volně dostupný slovník parafrází českých komplexních predikátů s lehkými slovesy ParaDi. Kandidáti na jednoslovné predikativní parafráze byli vybráni automaticky z velkých jednojazyčných dat pomocí word2vecu. Dále byli manuálně ověřeni. V experimentu s vylepšováním kvality strojového překladu ukazujeme jednu z mnoha praktických aplikací ParaDi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a new freely available dictionary of paraphrases of Czech complex predicates with light verbs, ParaDi. Candidates for single predicative paraphrases of selected complex predicates have been extracted automatically from large mono-lingual data using word2vec. They have been manually verified and further refined. We demonstrate one of many possible applications of ParaDi in an experiment with improving machine translation quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slovník ParaDi poskytuje jednoslovné parafráze komplexních predikátů v češtině, které byly získány automaticky a dále manuálně ověřeny a obohaceny o sémantické a syntaktické rysy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The ParaDi dictionary provides single verb paraphrases of Czech light verb constructions obtained automatically and manually verified, refined and specified with syntactic and semantic properties.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zpráva o účasti týmu Univerzity Karlovy v soutěži vyhledávání zdravotních informací CLEF eHealth Evaluation Lab 2017.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we present our participation as the team of the Charles University at Task3 Patient-Centred Information Retrieval in CLEF eHealth Evaluation lab 2017</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek na interním workshopu DGT "Smart Select" o nástrojích pro neuronový strojový překlad i o potížích při trénování modelů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A talk at an in-house workshop of DGT "Smart Select" on neural MT toolkits and on the difficulties of NMT training.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek představil některé aspekty překotného vývoje na poli neuronového strojového překladu a naznačil, jak v prudce se měnícím prostředí udržet stabilní směr výzkumu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk has highlighted some aspects of the frantic development in the area of neural machine translation and suggested some ways of keeping the research directions stable in the quickly changing environments.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popularizující souhrn nejnovějšího vývoje ve strojovém překladu -- základy neuronového MT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A popularizing summary of the most recent developments of machine translation -- the basics of neural MT.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisující neuronový přístup ke strojovému překladu obecně a první experimenty s neuronovým překladem pro IBM.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A talk describing neural MT in general and our first experiments with neural MT for IBM.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Shrnutí stavu vývoje neuronových překladačů na UK MFF v rámci projektu QT21.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A summary of the development of neural MT at Charles University.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Seznámení zájemců o obor informatiky a počítačové lingvistiky s nejnovějším vývojem na poli strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A quick introduction to prospective students of Computer Science and Computational Linguistics with recent advances in machine translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popularizační přednáška shrnující stručnou historii strojového překladu: frázový, hloubkový a jejich kombinaci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A popularizing talk, a brief summary of the history of MT: phrase-based, deep-syntactic and their combination.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V posledních dvou letech strojový překlad prošel převratnou změnou technologie. Spolehlivý není stále, ale kvalitou se lidem vyrovná. Popíšeme si stav strojového překladu dnes (za půl roku to bude už zas jinak) a naznačíme, co dalšího může tzv. hluboké učení do zpracování textu přinést a jak to může ovlivnit budoucí roli wikipedistů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The last two years of machine translation have seen a tremendous change of the technology. MT is still not reliable but it is close to match human translators' quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce představím tzv. hluboké učení, tj. strojové učení realizované pomocí neuronových sítí. Hluboké učení spustilo revoluci v mnoha oblastech automatického zpracování signálu a zhruba od roku 2016 zcela změnilo techniky užívané ve strojovém překladu. Podrobně si projdeme, jak se dnes strojový překlad pomocí neuronových sítí modeluje, jak dobře překládá a jakých chyb se dopouští.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk presents deep learning, i.e. machine learning carried out by neural networks. Deep learning has started a revolution in many areas of automatic processing of signals, and fully changed the methods used in machine translation in 2016.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje neuronové systémy a systémy založené na frázovém překladu, kterými UK přispělo do anglicko-českého News Translation Task v WMT17. Experimentujeme se syntetickými daty pro trénování a zkoušíme několik technik na kombinaci systémů, jak neuronových, tak frázových. Náš hlavní příspěvek CU-CHIMERA využívá frázových překlad s využitím neuronových a hluboce-syntaktických navrhovaných překladů</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the neural and
phrase-based machine translation systems
submitted by CUNI to English-Czech
News Translation Task of WMT17. We
experiment with synthetic data for training
and try several system combination techniques,
both neural and phrase-based. Our
primary submission CU-CHIMERA ends
up being phrase-based backbone which incorporates
neural and deep-syntactic candidate
translations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek shrnuje výsledky soutěžních úloh konference WMT17: tři překladové úlohy (novinové texty, biomedicínské texty a multimodální překlad), dvě úlohy v automatickém hodnocení (metriky a odhad kvality MT), automatickou korekturu, úlohu v trénování neuronových MT systémů a učení metodou jednorukého bandity.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the
WMT17 shared tasks, which included
three machine translation (MT) tasks
(news, biomedical, and multimodal), two
evaluation tasks (metrics and run-time estimation
of MT quality), an automatic
post-editing task, a neural MT training
task, and a bandit learning task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje výsledky soutěže v automatickém hodnocení kvality překladu WMT17 Metrics Shared Task.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the
WMT17 Metrics Shared Task. We asked
participants of this task to score the outputs
of the MT systems involved in the
WMT17 news translation task and Neural
MT training task. We collected scores
of 14 metrics from 8 research groups. In
addition to that, we computed scores of
7 standard metrics (BLEU, SentBLEU,
NIST, WER, PER, TER and CDER) as
baselines. The collected scores were evaluated
in terms of system-level correlation
(how well each metric’s scores correlate
with WMT17 official manual ranking of
systems) and in terms of segment level
correlation (how often a metric agrees with
humans in judging the quality of a particular
sentence).
This year, we build upon two types of
manual judgements: direct assessment
(DA) and HUME manual semantic judgements.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje výsledky úlohy WMT17 pro trénink neuronového strojového překladu.

Cílem této úlohy je prozkoumat metody výcviku předem zvolené neuronové architektury, zaměřené především na nejlepší kvalitu překladu a jako sekundární cíl kratší čas trénování.

Účastníci měli k dispozici kompletní systém pro neuronový strojový překlad, výchozí parametry trénování a konfiguraci sítě.

Překlad byl proveden v anglicko-českém směru a úkol byl rozdělen na dvě podskupiny s různými konfiguracemi - jedna byla upravena tak, aby se vešla na 4GB a druhá na 8GB GPU kartu.

Obdrželi jsme 3 řešení pro variantu 4 GB a 1 řešení pro variantu 8 GB; také jsme poskytli výsledky nášeho běh pro každou velikost jako baseline.

Přeložili jsme zkušební sadu netrénovanými modely a výsledky vyhodnotili pomocí několika automatických metrik.

Uvádíme také výsledky lidského hodnocení předložených systémů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the WMT17 Neural MT Training Task.

The objective of this task is to explore the methods of training a fixed neural architecture, aiming primarily at the best translation quality and, as a secondary goal, shorter training time.

Task participants were provided with a complete neural machine translation system, fixed training data and the configuration of the network.

The translation was performed in the English-to-Czech direction and the task was divided into two subtasks of different configurations - one scaled to fit on a 4GB and another on an 8GB GPU card.

We received 3 submissions for the 4GB variant and 1 submission for the 8GB variant; we provided also our run for each of the sizes and two baselines.

We translated the test set with the trained models and evaluated the outputs using several automatic metrics.

We also report results of the human evaluation of the submitted systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek přestavil aktuální stav vývoje a experimentů s neuronovými překladači na UK MFF v rámci projektu QT21.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We presented the current state of development and experiments with neural MT at Charles University within the project QT21.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>UFAL Medical Corpus je sbírka paralelních korpusů,
které byly shromážděny pro účely EU projektů KConnect, Khresmoi a HimL s cílem dosáhnout spolehlivějšího strojového překladu medicínských textů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>UFAL Medical Corpus is a collection of parallel corpora assembled for the purposes of the EU projects KConnect, Khresmoi and HimL aiming at more reliable machine translation of medical texts.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>MorphInd je robustní morfologický nástroj, který dané povrchové slovní formě přiřazuje její morfologickou značku a příslušné lemma, a umožňuje tak další automatické zpracování.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>MorphInd is a robust fínite state morphology tool for Indonesian, which handles
both morphological analysis and lemmatization for a given surface word form so that it is
suitable for further language processing.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kapitola popisuje historii, základní východiska a anotační schéma Pražského závislostního korpusu z formálně-lingvistického i technického hlediska.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The chapter describes the history and the linguistic theory behind the Prague Dependency Treebank. It also mentions technical solutions for the creation and use of the treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pro dva valenční slovníky slovanckých jazyků, PDT-Vallex pro češtinu a Walenty pro polštinu článek porovnává jejich frazeologickou část. Oba slovníky jsou založené na korpusu, i když se liší jak způsob propojení, tak technické řešení, ovšem oba jsou dostupné elektronicky ve standardnim formátu. V článku se porovnávají frazeologická hesla, jejich formální popis a možnosti a omezení. V závěru se doporučují rozšíření těchto komponent pro obecnější pokrytí i pro další jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Phraseological components of valency dictionaries for two West Slavic languages are presented, namely, of the PDT-Vallex dictionary for Czech and of the Walenty dictionary for Polish. Both dictionaries are corpus-based, albeit in different ways. Both are machine readable and employable by syntactic parsers and generators. The paper compares the expressive power of the phraseological subformalisms of these dictionaries, discusses their limitations and makes recommendations for their possible extensions, which can be possibly applied also to other valency dictionaries with rich phraseological components.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek obsahuje recenze všech referátů přednesených na konfrenci Komise pro gramatickou stavbu slovanských jazyků konané v listopadu 2011 v Sapporu a vydaných v prestižním nakladatelství Otto Sagner (Muenchen -Berlin - Washington).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this contribution the review of all contributions presented on the conference of The Committee of Grmmatical Structure of Slavonic Languages hold at Sapporo university in Japan and printed by the famous Otto Sagner Publishing House (Muenchen - Berlin - Washington)is included.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve stati je analyzován status některých netypických českých infinitivních konstrukcích (konstrukce s "dvojím" akuzativem), dále se slovesy mít a být (modální existenční konstrukce, tzv. absentiv a kauzální rezultát). Uvedené konstrukce se nepokládají za plně gramatikalizované morfologické kategorie.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the first section of the article a non-standard behaviour of several Czech control verbs is
analyzed and illustrated by examples from the Czech National Corpus. In the second part three
types of Czech syntactic constructions based on the verbs mít [to have] and být [to be] connected
with infinitive are presented and their position in the grammatical description of language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve stati se ověřuje platnost kritérií používaných pro stanovení valence při popisu valence substantiv a adjektiv. Poukazuje se na kritéria použitelná obecně a na specificitu valence u dalších slovních druhů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this article the application of the criteria used for the determination of valency of verbs is presented for the noun valency as well as for the adjective valency. The validity of the basic criteria is confirmed, the specificity of the valency of other parts of speech is exemplified.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Porozumění přirozenému jazyku vyžaduje reprezentaci hloubkové struktury věty. Argumentem může být zachycení aktuálního členění a povrchových elips.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Natural language understanding requires representation of the sentence on the deep syntactic layer.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Analýza aktuálního členění věty z hlediska její sémantické relevance a důležitosti pro strojový překlad.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Analysis of the information structure of the sentence from the point of view of its semantic relevance and importance for machine translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Soubor vybraných statí rozdělený do pěti oddílů, a to 1. Valence, 2. Aktuální členění, negace a presupozice, 3. Zachycení AČV v anotovaném  počítačovém korpusu češtiny, 4. Struktura a analýza diskurzu, 5. Rozbor některých zaharničních přístupů k uvedeným otázkám</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A collection of selected papers divided into five parts: 1. Valency, 2. Topic-Focus Articulation (TFA), Negation and Presupposition, 3. TFA in the annotated treebank of Czech, 4. Discourse structure and analysis, 5. Remarks on some related treatments of these issue in linguistic writings abroad</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Souhrn argumentů pro studium a odpovídající anotaci korpusu z hlediska hloubkové syntaktické struktury založené na závislostní gramatice</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A survey of arguments in favour of the study of the deep sentence structure on the basis of dependency grammar, illustrated by the annotation of the Prague Dependency Treebank</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kapitola o informační struktuře ("tématu") v prestižní encyklopedii jaykovědy vydané Oxford University Press v březnu 2017, pouze online.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the linguistic literature, the term theme has several interpretations, one of which relates to discourse analysis and two others to sentence structure. In a more general (or global) sense, one may speak about the theme or topic (or topics) of a text (or discourse), that is, to analyze relations going beyond the sentence boundary and try to identify some characteristic subject(s) for the text (discourse) as a whole. This analysis is mostly a matter of the domain of information retrieval and only partially takes into account linguistically based considerations. The main linguistically based usage of the term theme concerns relations within the sentence. Theme is understood to be one of the (syntactico-) semantic relations and is used as the label of one of the arguments of the verb; the whole network of these relations is called thematic relations or roles (or, in the terminology of Chomskyan generative theory, theta roles and theta grids). Alternatively, from the point of view of the communicative function of the language reflected in the information structure of the sentence, the theme (or topic) of a sentence is distinguished from the rest of it (rheme, or focus, as the case may be) and attention is paid to the semantic consequences of the dichotomy (especially in relation to presuppositions and negation) and its realization (morphological, syntactic, prosodic) in the surface shape of the sentence.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kritický rozbor studií J.-M.Zemba o sémantice záporu a o informační struktuře věty z pohledu pražské školy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A critical analysis of the studies of J.-M. Zemb on the semantics of negation and on information structure of the sentence from the point of view of Praguian linguistic theory.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace cílů, průběhu a výsledků projektu KConnect na Dnech H2020 na UK.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presentation of the goals, progress, and results of the KConnect project at the Days of H2020 at CU.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Úspěchy v oblasti medicínského strojového překladu v projektu KConnect</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Achievements in Medical-domain Machine Translation within the KConnect project</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Shrnujeme zapojení našeho týmu CEMI do soutěže s úlohou rozpoznávání rodného jazyka autora, tzv. NLI Shared Task~2017, pro kterou byla k dispozici textová a mluvená data.

Představujeme výsledky, kterých jsme dosáhli použitím tří různých architektur, kde každá z nich kombinuje modely natrénované nad různými příznaky.

Jak jsme očekávali, lepších výsledků dosáhly systémy, které kombinují textové a mluvené příznaky. Dokonce bylo dosaženo dramatického zlepšení.

Naš nejlepší systém je založen na feed-forward neural networks, jejichž výstupy skryté vrstvy jsou kombinovány pomocí softmax. Dosáhli jsme úspěšnosti 0.9257 macro-averaged F1 na evaluační testovací sadě a náš tým spolu s dalšími třemi obsadil první místo v hlavní soutěži.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We summarize the involvement of our CEMI team in the Native Language Identification shared task, NLI Shared Task~2017, which deals with both textual and speech input data.

We submitted the results achieved by using three different system architectures; each of them combines multiple supervised learning models trained on various feature sets.

As expected, better results are achieved with the systems that use both the textual data and the spoken responses. Combining the input data of two different modalities led to a rather dramatic improvement in classification performance.

Our best performing method is based on a set of feed-forward neural networks whose hidden-layer outputs are combined together using a softmax layer. We achieved a macro-averaged F1 score of 0.9257 on the evaluation (unseen) test set and our team placed first in the main task together with other three teams.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme náš projekt o synonymii v bilingvním prostředí.Cílem projektu je prozkoumat sémantickou ekvivalenci slovesných významů v česko-anglickém prostředí. Zaměřujeme se na valenční chování sloves ve spojitosti se sémantickými rolemi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce our ongoing project about synonymy in bilingual context. This project aims at exploring semantic ‘equivalence’ of verb senses of generally different verbal lexemes in a bilingual (Czech-English) setting.  Specifically, it focuses on their valency behavior within such equivalence groups. We believe that using bilingual context (translation) as an important factor in the delimitation of classes of synonymous lexical units (verbs, in our case) may help to specify the verb senses, also with regard to the (semantic) roles relation to other verb senses and roles of their arguments more precisely than when using monolingual corpora. In our project, we work “bottom-up”, i.e., from an evidence as recorded in our corpora and not “top-down”, from a predefined set of semantic classes.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek shrnuje první výsledky projektu o kontextové synonymii na základě bilingvního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper summarizes first findings of a three-year study (an ongoing research project) on verb synonymy based on both syntactic and
semantic criteria. Primary language resources used for the study are existing lexical and corpus resources, namely the Prague Dependency
Treebank-style valency lexicons, FrameNet, VerbNet, PropBank and Czech and English WordNets and the parallel Prague Czech-English
Dependency Treebank, which contains deep syntactic and partially semantic annotation of running texts. The resulting lexicon, called
CzEngClass, and all associated resources linked to the existing lexicons and corpora resulting from this project will be made publicly
and freely available. While the project proper assumes manual annotation work, we expect to use the resulting resource (together with
the existing ones) as a necessary resources for developing automatic methods for extending such a lexicon, or creating similar lexicons
for other languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek přináší srovnání valenčního chování českých deverbativních
substantiv zastupujících pět sémantických tříd zahrnutých do valenčního slovníku
NomVallex, konkrétně Mluvení, Výměna, Dotyk, Mentální činnost a Duševní projevy a
stavy. Na datech získaných z českých korpusů představujeme dva způsoby srovnání,
jednak projevy specifického valenčního chování (specifické formy adnominálních
doplnění a redukce počtu valenčních doplnění), jednak kvantitativní analýzu preferencí
v souvýskytu aktantů nebo ve výběru formy některých aktantů. Tato různá hlediska
srovnání umožňují vidět jak některé společné rysy, tak odlišnosti ve valenčním chování
vybraných sémantických tříd, včetně rozlišení centrálních a periferních jevů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper compares valency behavior of Czech deverbal nouns representing
five semantic classes, namely Communication, Exchange, Contact, Mental action and
Psychological nouns, included to a valency lexicon called NomVallex. Using data
extracted from Czech corpora we present two ways of the comparison, first
a manifestation of special valency behavior (special forms of adnominal participants
and reduction of the number of valency slots), second a quantitative analysis focusing
on relative frequencies of combinations of participants and their forms. These different
ways of view enable to see both common and different valency properties of the
semantic classes in question, including differentiation of central and peripheral
phenomena.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku analyzujeme relativní frekvence různých kombinací valenčních doplnění českých deverbativních substantiv v Pražském závislostním korpusu, s cílem optimalizovat korpusové vyhledávky při vytváření valenčního slovníku. Při analýze rozlišujeme produktivně a neproduktivně tvořená substantiva a jejich sémantickou třídu. Zkoumáme také kombinace forem valenčních doplnění a třídíme je s ohledem na jejich relativní frekvence.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In order to optimize corpus searches for valency lexicon production, we analyse the relative frequencies of different combinations of valency complementations of Czech deverbal nouns in the Prague Dependency Treebank, considering differences between productively and non-productively derived nouns and their semantic class. We also classify combinations of forms
of participants according to their frequency.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valenční slovník NomVallex vzniká v teoretickém rámci funkčního generativního popisu, přičemž navazuje na dva valenční slovníky budované v rámci stejné teorie, na Vallex a PDT-Vallex. NomVallex se zaměřuje na deverbativní substantiva zastupující pět sémantických tříd: Mluvení, Kontakt, Výměna, Mentální činnost a Duševní projevy a stavy. V přednášce slovník představíme na materiálu substantiv z třídy Mluvení. U jednotlivých lexémů zpracováváme všechny významy, a kde je to možné, mapujeme je na významy základových sloves ve Vallexu. NomVallex si klade za cíl popsat všechny formy valenčních doplnění u jednotlivých substantiv a také jejich kombinace doložené v Českém národním korpusu (v subkorpusech řady SYN) a v korpusu Araneum Bohemicum Maximum. Přibližný odhad frekvence forem a jejich kombinací získáváme pro jednotlivé sémantické třídy z dat pražských závislostních korpusů (PDT 3.0, PCEDT 2.0 a PDTSC).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presentation focused on the current work on the corpus-based valency lexicon of Czech nouns called NomVallex.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto rozšířeném abstraktu podáváme informaci o vznikajícím valenčním slovníku českých substantiv, nazvaném NomVallex. Analyzujeme rovněž relativní frekvence různých kombinací valenčních doplnění českých deverbativních substantiv v Pražském závislostním korpusu, s cílem optimalizovat korpusové vyhledávky při vytváření valenčního slovníku. Při analýze rozlišujeme produktivně a neproduktivně tvořená substantiva a jejich sémantickou třídu. Zkoumáme také kombinace forem valenčních doplnění a třídíme je s ohledem na jejich relativní frekvence.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The extended abstract summarises the current work on the corpus-based valency lexicon of Czech nouns called NomVallex. In order to optimize corpus searches for valency lexicon production, we analyse the relative frequencies of different combinations of valency complementations of Czech deverbal nouns in the Prague Dependency Treebank, considering differences between productively and non-productively derived nouns and their semantic class. We also classify combinations of forms of participants according to their frequency.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku se zaměřujeme na vyjádření propozice pomocí verbálních substantiv v dějovém užití (např. shánění materiálu, pití čaje) a zkoumáme způsob vyjádření jejich aktantů v psaném a mluveném jazyce. Pro tento účel byla vybrána česká část korpusu Prague Czech-English Dependency Treebank (PCEDT) a mluvený korpus Prague Dependency Treebank of Spoken Czech (PDTSC). Ukazujeme, že v psaném korpusu je nominální skupina syntakticky zhuštěnější a vyjádření adnominálních aktantů je explicitnější, zatímco v mluveném korpusu jsou častější elipsy adnominálních aktantů a častěji se vyskytuje exoforické odkazování. Z kvantitativní analýzy relativních četností kombinací adnominálních aktantů vyplývá, že tyto četnosti jsou sice v psaném korpusu vyšší, jejich pořadí je však stejné v obou korpusech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper extends understanding of differences in expressing actions by verbal
nouns in corpora of written vs. spoken Czech, namely in the Czech part of the Prague Czech-English Dependency Treebank and in the Prague Dependency Treebank of Spoken Czech.
We show that while the written corpus includes more complex noun phrases with more explicit
expression of adnominal participants, noun phrases in the spoken corpus contain more
deletions and more exophoric references. We also carried out a quantitative analysis focusing on relative frequencies of combinations of participants modifying verbal nouns; although the written corpus shows higher relative frequencies, the order of the relative frequencies of particular combinations is the same in both types of communication.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Na příkladu nově vznikajícího slovníku českých diskurzních konektorů CzeDLex představujeme obecné a efektivní technické řešení tvorby takového slovníku, založené na datovém a aplikačním balíku Prague Markup Language a na extrakci základního hrubého obsahu slovníku z velkého anotovaného korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a general and efficient technical solution for building a lexicon of discourse connectives (presented on the case of CzeDLex, a new Lexicon of Czech Discourse Connectives), based on the Prague Markup Language framework and extraction of the raw core version of the lexicon from a large treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jedná se o užitečnou a čtivě napsanou publikaci, která díky svému výrazně praktickému zaměření poslouží jak čtenáři, který chce „pouze“ vyhledávat v již existujících korpusech, tak čtenáři, který si chce vytvořit vlastní, byť třeba jen malý korpus.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>It is a useful and well written book. Thanks to its strong practical focus, it can serve both a user that "only" wants to search in already existing corpora, and also a user that wants to create his own, albeit small corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CzeDLex 0.5 je pilotní verze slovníku českých diskurzních konektorů. Slovník obsahuje konektory částečně automaticky extrahované z Pražského diskurzního korpusu 2.0 (PDiT 2.0), rozsáhlého korpusu s ručně anotovanými diskurzními vztahy. Nejfrekventovanější slovníková hesla (pokrývající více než 2/3 diskurzních vztahů anotovaných v PDiT 2.0) byla ručně zkontrolována, přeložena do angličtiny a doplněna dalšími lingvistickými informacemi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CzeDLex 0.5 is a pilot version of a lexicon of Czech discourse connectives. The lexicon contains connectives partially automatically extracted from the Prague Discourse Treebank 2.0 (PDiT 2.0), a large corpus annotated manually with discourse relations. The most frequent entries in the lexicon (covering more than 2/3 of the discourse relations annotated in the PDiT 2.0) have been manually checked, translated to English and supplemented with additional linguistic information.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CzeDLex je nový elektronický slovník českých diskurzních konektorů. Jeho formát a struktura jsou založeny na studiu obdobných existujících zdrojů a upraveny podle české syntaktické tradice a specifik pražského přístupu k anotaci diskurzních vztahů v textu. Nejprve uvádíme slovník do kontextu podobných zdrojů a probíráme teoretické otázky vytváření slovníku. Poté představujeme technické řešení založené na Prague Markup Language. Následně popisujeme proces získání dat pro slovník z velkého korpusu s ručně anotovanými diskurzními vztahy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CzeDLex is a new electronic lexicon of Czech discourse connectives. Its data format and structure are based on a study of similar existing resources, and adjusted to comply with the Czech syntactic tradition and specifics and with the Prague approach to the annotation of semantic discourse relations in text. We first put the lexicon in context of related resources and discuss theoretical aspects of building the lexicon. Second, we introduce the chosen technical solution based on the Prague Markup Language. Third, we describe the process of getting data for the lexicon by exploiting a large corpus manually annotated with discourse relations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zkoumáme, jakými strategiemi jsou označeny základní aktanty (core arguments) v indoevropských jazycích s pádovou morfologií. Koncept základních aktantů je v Universal Dependencies stěžejní, někdy je však obtížné ho namapovat na terminologie tradičně používané v jednotlivých jazycích. Přezkoumáváme metodologii popsanou Andrewsem (2007) a přidáváme stručné definice některých základních pojmů. Statistiky z 26 treebanků UD ukazují, že ne všichni poskytovatelé dat definují hranici mezi základními (core) a nepřímými (oblique) argumenty stejně. Proto navrhujeme úpravu a upřesnění anotačních pravidel, které zlepší konzistenci napříč treebanky na jedné straně a bude více slučitelné s tradiční gramatikou na straně druhé.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We investigate how core arguments are coded in case-marking Indo-European languages. Core arguments are a central concept in Universal Dependencies, yet it is sometimes difficult to match against terminologies traditionally used for individual languages. We review the methodology described in Andrews (2007), and include brief definitions of some basic terms. Statistics from 26 UD treebanks show that not all treebank providers define the core-oblique boundary the same way. Therefore we propose some refinement and particularization of the guidelines that would improve cross-treebank consistency on the one hand, and be more sensitive to the traditional grammar on the other.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Shrnu soutěž v parsingu, kterou jsme organizovali toto jaro. 11 let po první soutěžní úloze CoNLL v závislostním parsingu byla ta současná jednou z největších soutěží v historii CoNLL, a to jak co do velikosti a rozmanitosti testovacích dat (81 treebanků, 49 jazyků), tak co do počtu účastníků (přes 50 týmů, 32 odevzdaných systémů). Soutěž se vyznačovala několika novinkami: kompletní analýza od prostého textu až po závislostní stromy, jazyky s nedostatkem dat a předem neznámé jazyky, jednotné anotační schéma pro všechny jazyky, evaluace naslepo na vzdáleném serveru. Výsledky představují kvalitativně novou úroveň stavu poznání pro automatickou závislostní analýzu většiny jazyků včetně češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I will summarize the parsing shared task we organized this spring. 11 years after the first CoNLL shared task in dependency parsing, the present one has arguably been one of the largest CoNLL shared tasks ever, both in size and diversity of test data (81 treebanks, 49 languages) and in number of participants (over 50 teams, 32 submissions). There were several novel aspects: end-to-end parsing from raw text, low-resource and surprise languages, unified annotation across languages, blind evaluation on a remote server. The results set the new state of the art for dependency parsing of most languages, including Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Shrnu soutěž v parsingu, kterou jsme organizovali toto jaro. 11 let po první soutěžní úloze CoNLL v závislostním parsingu byla ta současná jednou z největších soutěží v historii CoNLL, a to jak co do velikosti a rozmanitosti testovacích dat (81 treebanků, 49 jazyků), tak co do počtu účastníků (přes 50 týmů, 32 odevzdaných systémů). Soutěž se vyznačovala několika novinkami: kompletní analýza od prostého textu až po závislostní stromy, jazyky s nedostatkem dat a předem neznámé jazyky, jednotné anotační schéma pro všechny jazyky, evaluace naslepo na vzdáleném serveru. Výsledky představují kvalitativně novou úroveň stavu poznání pro automatickou závislostní analýzu většiny jazyků včetně češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I will summarize the parsing shared task we organized this spring. 11 years after the first CoNLL shared task in dependency parsing, the present one has arguably been one of the largest CoNLL shared tasks ever, both in size and diversity of test data (81 treebanks, 49 languages) and in number of participants (over 50 teams, 32 submissions). There were several novel aspects: end-to-end parsing from raw text, low-resource and surprise languages, unified annotation across languages, blind evaluation on a remote server. The results set the new state of the art for dependency parsing of most languages, including Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme převod syntakticky anotované části Slovenského národního korpusu do anotačního schématu známého jako Universal Dependencies. Zatím byla převedena pouze malá část dat, nicméně jde o první slovenský treebank, který je volně přístupný pro výzkumné účely. Uvádíme řadu výzkumných projektů, ve kterých už tato data byla využita, včetně prvních výsledků automatické syntaktické analýzy (parsingu).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe a conversion of the syntactically annotated part of the Slovak National Corpus into the annotation scheme known as Universal Dependencies. Only a small subset of the data has been converted so far; yet it is the first Slovak treebank that is publicly available for research. We list a number of research projects in which the dataset has been used so far, including the first parsing results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme rodinu formátů korpusových formátů zvanou CoNLL, se zvláštním zřetelem na jejího nejnovějšího člena, formát CoNLL-U.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe the family of corpus file formats called CoNLL, with special focus on its newest member, the CoNLL-U format.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme Universal Dependencies (UD), komunitní projekt zaměřený na mezijazykově použitelné anotační schéma pro morfologii a syntaxi přirozených jazyků. Klíčovou myšlenkou UD je, že podobné gramatické konstrukce mají být analyzovány a anotovány podobně; strukturní reprezentace paralelních vět ve dvou jazycích mají být maximálně paralelní. Komunita UD je velmi rozmanitá, stejně jako předpokládané možnosti využití, které se UD snaží podporovat: modely pro počítačové zpracování přirozeného jazyka (zvláště morfologické značkování a syntaktická analýza); jazykovědné bádání a dotazy na korpus; studie z jazykové typologie. Kromě návrhu anotačních pravidel se projekt UD zabývá také sběrem samotných korpusů, jejich převodem do jednotné anotace a jejich zpřístupněním pro výzkum. Vzhledem k tomu, že UD je omezeno dostupností dat, má pochopitelně výrazně větší zastoupení velkých eurasijských jazyků bohatých na digitální zdroje; nicméně, přibývají i vzorky z menšinových jazyků a několika klasických jazyků. První část přednášky představí obecné principy UD. Ve druhé části se podíváme zblízka na treebanky klasických jazyků a probereme obtíže s harmonizací tradiční terminologie ze synchronního i diachronního hlediska. Předvedeme také nástroje, které lze využít k databázovým dotazům nad korpusy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present Universal Dependencies (UD), a community-driven project aimed at defining a cross-linguistically applicable annotation scheme for morphology and syntax of natural languages. The key idea of UD is that comparable constructions should be analyzed and annotated in comparable ways; structural representations of parallel sentences in two languages should be as parallel as possible. The community behind UD is very diverse and so are the use cases that UD tries to support: models for natural language processing (especially tagging and parsing); linguistic research and corpus queries; language-typological studies. Besides defining annotation guidelines, UD also collects actual corpora, converts them to the unified annotation and makes them freely available for research. Being driven by data availability, UD is obviously biased towards resource-rich Eurasian languages; however, there are also samples from minority languages, and several ancient languages, too. The first part of the talk will describe the main principles of UD and present the project in general. In the second part, we will look more closely at the treebanks of classical languages, and discuss some challenges of harmonizing traditional terminology both synchronically and diachronically. We will also demonstrate query tools that can be used to study the data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento balíček obsahuje výstupy 33 automatických syntaktických analyzátorů, které se účastnily společné úlohy (shared task) ve vícejazyčném parsingu z prostého textu do Universal Dependencies, v rámci konference CoNLL 2017.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This package contains the system outputs from the CoNLL 2017 Shared Task in Multilingual Parsing from Raw Text to Universal Dependencies.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Konference o počítačovém učení přirozeného jazyka (CoNLL) zahrnuje soutěž (společnou úlohu, shared task), ve které účastníci trénují a testují své učící systémy na stejných sadách dat. V roce 2017 byla jedna ze dvou soutěží věnována učení závislostních parserů (syntaktických analyzátorů) pro velké množství jazyků, v realistických podmínkách bez jakékoli ruční anotace na vstupu. Všechny testovací sady byly anotovány podle jednotného schématu zvaného Universal Dependencies. V tomto článku definujeme úlohu a vyhodnocovací metodiku, popisujeme přípravu dat, shrnujeme a rozebíráme hlavní výsledky a podáváme stručný přehled jednotlivých přístupů v účastnických systémech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their learning systems on the same data sets. In 2017, one of two tasks was devoted to learning dependency parsers for a large number of languages, in a real-world setting without any gold-standard annotation on input. All test sets followed a unified annotation scheme, namely that of Universal Dependencies. In this paper, we define the task and evaluation methodology, describe data preparation, report and analyze the main results, and provide a brief categorization of the different approaches of the participating systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška se zaměřila na vybrané vlastnosti českých příklonek: především na omezení na jejich umístění ve větě a haplologii reflexivních příklonek.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk evaluated several properties of Czech clitics: mainly constraints on their placement within the sentence and haplology of reflexive clitics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme pilotní studii věnovanou automatické syntaktické analýze žakovského korpusu CzeSL. Provedli jsme experimenty, které naznačily, že základní větné členy subjekt, predikát, objekt, mohou být určeny pomocí parseru natrénovaného na textech od rodilých mluvčích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a pilot study on parsing non-native texts written by learners of Czech. We performed experiments that have shown that at least  high-level syntactic functions, like subject, predicate, and object, can be assigned based on a parser trained on standard native language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>STYX 1.0 je korpus českých vět vybraných z Pražského závislostního korpusu (PDT, https://ufal.mff.cuni.cz/pdt2.0).
Kritériem pro začlenění vět do STYX 1.0 byla jejich vhodnost pro procvičování větných rozborů žáky základních škol.
Věty obsahují anotace z PDT a školní větné rozbory.
Školní rozbory vznikly transformací z anotací PDT pomocí ručně navržených pravidel, dále viz (Kučera, 2006) a (Hladká, Kučera, 2008).
Celkem je v korpusu STYX 1.0 11 655 vět.

Pražský závislostní korpus je pro vývoj a ladění nástrojů rozdělen do tři částí, a sice trénovací, testovací pro vývoj a testovací pro evaluaci (více informací https://ufal.mff.cuni.cz/pdt2.0/doc/pdt-guide/cz/html/ch03.html#a-data-purpose).
STYX 1.0 toto dělení zachovává (viz níže Data).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The STYX 1.0 corpus is a subset of the Prague Dependency Treebank (PDT, https://ufal.mff.cuni.cz/pdt2.0).
The criterion for including sentences into STYX 1.0 was their suitability for practicing Czech morphology and syntax in elementary schools.

The PDT data are divided into three groups: the training data, the development test data and the evaluation test data (see more info https://ufal.mff.cuni.cz/pdt2.0/doc/pdt-guide/cz/html/ch03.html#a-data-purpose).
The STYX 1.0 corpus keeps this division (see Data below). 

The sentences in STYX are annotated according to both the PDT and the Czech school annotation system (sentence diagramming).
The PDT annotation was transformed into the school annotation using manually designed rules, for more info see (Kucera, 2006) and (Hladka, Kucera, 2008).

In total, there are 11,655 sentences in the STYX corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pražský závislostní korpus mluvené češtiny 2.0 (PDTSC 2.0) je korpus mluveného jazyka o objemu 742 316 tokenů, 73 835 vět, což představuje 7 324 minut (více než 120 hodin) spontánních dialogů. Dialogy byly zaznamenány, přepsány a upraveny na několika vzájemně propojených rovinách: zvukový záznam, automatický a ruční přepis a ručně rekonstruovaný text. Tyto vrstvy byly součástí první verze korpusu (PDTSC 1.0). Verze 2.0 je rozšířena o automatickou analýzu závislostí (na analytické rovině) a především o manuální anotaci „hluboké“ syntaxe na tektogramatické rovině, která obsahuje anotaci hloubkových vztahů, valence i anotaci koreference.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Prague Dependency Treebank of Spoken Czech 2.0 (PDTSC 2.0) is a corpus of spoken language, consisting of 742,316 tokens and 73,835 sentences, representing 7,324 minutes (over 120 hours) of spontaneous dialogs. The dialogs have been recorded, transcribed and edited in several interlinked layers: audio recordings, automatic  and manual transcripts and manually reconstructed text. These layers were part of the first version of the corpus (PDTSC 1.0). Version 2.0 is extended by an automatic dependency parser at the analytical and by the manual annotation of “deep” syntax at the tectogrammatical layer, which contains semantic roles and relations as well as annotation of coreference.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme bohatě anotovaný zdroj mluveného jazyka: Pražský závislostní korpus mluvené češtiny, verze 2.0, který je primárně určen pro úlohy NLP orientované na zpracování mluvené řeči, ale najde využití i v nejrůznějších lingvistických studiích. Korpus představuje unikátní anotační schéma: audio, transkript, morfologická, syntaktická a sémantická anotace, které je obdobné jako v ostatních PDT korpusech. Navíc obsahuje novou anotaci: rekonstrukci řeči. Korpus je volně dostupný.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a richly annotated spoken language resource, the Prague Dependency Treebank of Spoken Czech 2.0, the primary purpose of which is to serve for speech-related NLP tasks. The treebank features several novel annotation schemas close to the audio and transcript, and the morphological, syntactic
and semantic annotation corresponds to the family of Prague Dependency Treebanks; it could thus be used also for linguistic studies, including comparative studies regarding text and speech. The most unique and novel feature is our approach to syntactic annotation, which differs from other similar corpora such as Treebank-3 [8] in that it does not attempt to impose syntactic structure over input, but it includes one more layer which edits the literal transcript to fluent Czech while keeping the original transcript explicitly aligned with the edited version. This allows the morphological, syntactic and semantic annotation to be deterministically and fully mapped back to the transcript and audio. It brings new possibilities for modeling morphology, syntax and semantics in spoken language – either at the original transcript with mapped annotation, or at the new layer after (automatic) editing. The corpus is publicly and freely available.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>ForFun je nástroj pro rozmanitý lingvistický výzkum, zejména v oblasti syntaxe pro popis syntaktických funkcí a jejich formálních realizací v českcýh větách. ForFun je založen na datech Pražských závislostních korpusů (PDTs), uspořádává jejich morfologickou a syntaktickou anotaci do nové databáze, ve které je možné rychle a snadno prohledávat autentické příklady užité v PDTs pro jednotlivé funkce (66 položek) a  též opačně lze zkoumat funkce vyjádřené zvolenou formou (téměř 1500 položek).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>ForFun is an interface for various linguistic research, particularly in describing syntactic functions and their formal realizations in Czech sentences. ForFun draws on the complex linguistic annotation of Prague Dependency Treebanks (PDTs) and arranges morphological and syntactical annotation into new tool which gives a possibility to search quickly and in a user-friendly way all forms (almost 1,500 items) used in PDTs for particular function and vice versa to look up all functions (66 items) expressed by the particular forms.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme popis adverbiálních významů v českých větách založený na korpusu (Pražské závislostní korpusy). Na příkladu prostorových určení popisujeme metologii, kterou chceme dosáhnout uceleného a koplexního popisu adverbiálních významů v češtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a corpus based description of selected adverbial meanings in Czech sentences. Its basic repertory is one of a long lasting tradition in both scientific and school grammars. However, before the corpus era, researchers had to rely on their own excerption; but nowadays, current syntax has a vast material basis in the form of electronic corpora available. On the case of spatial adverbials, we describe our methodology which we used to acquire a detailed, comprehensive, well-arranged description of meanings of adverbials including a list of formal realizations with examples. Theoretical knowledge stemming from this work will lead into an improval of the annotation of the meanings in the Prague Dependency Treebanks which serve as the corpus sources for our research. The Prague Dependency Treebanks include data manually annotated on the layer of deep syntax and thus provide a large amount of valuable examples on the basis of which the meanings of adverbials can be defined more accurately and subcategorized more precisely. Both theoretical and practical results will subsequently be used in NLP, such as  machine translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Mezi gramatikou a slovníkem: Rodina elektronických valenčních slovníků VALLEX
Between Lexicon and Grammar: The family of valency distionaries VALLEX</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Between Lexicon and Grammar: The family of valency distionaries VALLEX
Mezi gramatikou a slovníkem: Rodina elektronických valenčních slovníků VALLEX</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Licenční smlouva a smlouva o poskytnutí služeb mezi Univerzitou Karlovou a Datlowe, s.r.o. (1. 1. 2015 -- 1. 1. 2018) obsahuje právo na užití dat PDT 2.5 a software pro textovou analytiku natrénovaného na PDT 2.5, a to pro komerční účely. Jedná se o upravené verze nekomerčního software a dat jinak dostupných z repozitáře LINDAT/CLARIN. Jmenovitě: software pro analýzu tvarosloví a lematizaci, syntaktickou analýzu a analýzu názvů pojmenovanýchch entit.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Licence and service agreement between Charles University and Datlowe Ltd.(January 1, 2015 -- January 1, 2018) covers the right to use the data of PDT 2.5 and the software for text analytics trained on PDT 2.5, for commercial purposes. The data and software (non-adapted) is otherwise available for free through the LINDAT/CLARIN repository for non-commercial use under the appropriate variant of the Creative Commons license. Namely, software for morphological analysis, lemmatization, syntactic analysis and named-entity recognition.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V mnoha jazycích lze některé slova psát více způsoby. Říkáme jim varianty. Hodnoty všech jejich morfologických kategorií jsou totožné, což vede k identické morfologické značce. Spolu s totožným lematem máme dva nebo více slovních tvarů se stejným morfologickým popisem. Tato nejednoznačnost může působit problémy v různých aplikacích automatického zpracování jazyka. Existují dva typy variant - ty, které ovlivňují celé paradigma (globální varianty), a ty, které mají vliv pouze na slovní tvary používající některé kombinace morfologických hodnot (inflexní varianty). V příspěvku navrhujeme prostředky, jak označit všechny slovní tvary, včetně jejich variant, jednoznačně. Tento požadavek nazýváme "Zlaté pravidlo morfologie". Práce se zabývá především češtinou, ale hlavní myšlenky lze uplatnit i v jiných jazycích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In many languages, some words can be written in several ways. We call them variants. Values of all their morphological categories are identical, which leads to an identical morphological tag. Together with the identical lemma, we have two or more wordforms with the same morphological description. This ambiguity may cause problems in various NLP applications. There are two types of variants – those affecting the whole paradigm (global variants) and those affecting only wordforms sharing some combinations of morphological values (inflectional variants). In the paper, we propose means
how to tag all wordforms, including their variants, unambiguously. We call this requirement ”Golden rule of morphology”. The paper deals mainly with Czech, but the ideas can be applied to other languages as well.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zkoumáme lidské úsudky o tom, jak dobře popisují jednotlivé vzory užívání (usage patterns) 29 cílových sloves z modelového slovníku anglických sloves jejich náhodné KWIC. Zaměřujeme se na případy, kdy je pro daný KWIC hodnoceno více než jeden model a snažme se odhadnout vliv účastníků události (argumenty), které jsou denotativně podobné ve dvou vzorcích, s ohledem na všechny kombinace párů v daném lemmatu. Tento efekt porovnáváme s účinkem několika kontextových rysů KWIC, účinkem spárovaných implicit PDEV, které se navzájem implikují, a účinkem příslušnosti k danému lemu. Ukazujeme, že lemmatický efekt je stále silnější než jakákoli vlastnost, která prochází napříč lemma, kterou jsme zatím prozkoumali, takže každé sloveso se zdá být malým vesmírem samo o sobě.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We explore human judgments on how well individual patterns of 29 target verbs from the Pattern Dictionary of English Verbs describe their random KWICs. We focus on cases where more than one pattern is judged as highly appropriate for a given KWIC and seek to estimate the effect of event participants (arguments) being denotatively similar in two patterns, considering all pair combinations in a given lemma. We compare this effect to the effect of several contextual features of the KWICs,  the effect of paired PDEV implicatures implying each other, and the effect of belonging to a given lemma. We show that the lemma effect is still stronger than any feature going across lemmas we have examined so far, so that each verb appears to be a little universe in its own right.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme anotační experiment, který kombinuje témata z lexikografie a disambiguace lexikálních významů. Zahrnuje lexikon (Pattern Dictionary of English Verbs, PDEV), existující datový soubor (VPS-GradeUp) a nepublikovanou sadu dat (RTE v PDEV Implications). Experiment měl dva cíle: pilotní anotace (RTE) na implikaturách PDEV (tj. slovníkových definicích) na jedné straně a na druhé straně analýzu efektu vzájemného textového vyplývání mezi slovníkovými definicemi na anotátorská rozhodování lexikální disambiguace ve srovnání s jinými prediktory, jako je finitnost cílového slovesa, explicitní přítomnost příslušných argumentů a sémantická vzdálenost mezi odpovídajícími syntaktickými argumenty ve dvou různých vzorcích (slovníkové významy).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We  describe  an  annotation  experiment  com-
bining topics from lexicography and Word Sense Disam-
biguation. It involves a lexicon (
Pattern Dictionary of En-
glish Verbs, PDEV
), an existing data set (
VPS-GradeUp
),
and an unpublished data set (
RTE in PDEV Implicatures
).
The aim of the experiment was twofold:  a pilot annota-
tion of Recognizing Textual Entailment (RTE) on PDEV
implicatures  (lexicon  glosses)  on  the  one  hand,  and,  on
the other hand, an analysis of the effect of Textual Entail-
ment between lexicon glosses on annotators’ Word-Sense-
Disambiguation decisions,  compared to other predictors,
such as finiteness of the target verb, the explicit presence
of  its  relevant  arguments,  and  the  semantic  distance  be-
tween corresponding syntactic arguments in two different
patterns (dictionary senses).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Velmi rychla implementace banky filtru pro rozdeleni vstupniho komplexniho signalu do N frekvencne ekvidistantnich kanalu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Very fast implementation of the filter bank for splitting the input complex signal into N equidistant frequency channels.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Soutěž PARSEME Shared Task v identifikaci slovesných víceslovných výrazů požaduje po účastnících vyhledávání výrazů v běžném textu.

V tomto článku ukazujeme, jak je možné česká trénovací data získat nikoli manuální anotací, nýbrž převodem informací z předchozích anotací uložených v Pražském závislostním korpusu na různých úrovních a různým způsobem. Prvním krokem je porovnání anotačních instrukcí a srovnání typologie.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The PARSEME Shared Task on automatic identification of verbal multiword expressions  aims  at  identifying  such  expressions  in  running  texts.   Typology of verbal multiword expressions, very detailed annotation guidelines and gold-standard data for as many languages as possible will be provided. Since the Prague Dependency Treebank includes Czech multiword expression annotation, it was natural to make an attempt to automatically convert the data into the Shared Task format. However, since the Czech treebank predates the Shared Task annotation guidelines, a prior examination was necessary to determine to which extent the conversion can be fully automatic and how much manual work remains.

In this paper, we show that information contained in the Prague Dependency Treebank is sufficient to extract all of the Shared Task categories of verbal multiword expressions relevant for Czech, even if these categories are originally annotated differently; nevertheless, some manual checking and annotation would still be necessary, e.g. for distinguishing borderline cases.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek představuje databázi lingvistických forem a funkcí postavenou nad Pražským závislostním korpusem. Účelem databáze ForFun je usnadnit lingivstům studium vztahu formy a funkce. Ukážeme možnosti využití databáze.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of our contribution is to introduce a database of linguistic forms and their functions built with the use of the multi-layer annotated corpora of Czech, the Prague Dependency Treebanks. The purpose of the Prague Database of Forms and Functions (ForFun) is to help the linguists to study the form-function relation, which we assume to be one of the principal tasks of both theoretical linguistics and natural language processing. We will also demonstrate possibilities of the exploitation of the ForFun database.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku informuji o pokrocích ve vývoji lexikální databáze DeriNet a rozebírám tři témata, kterým je v rámci projektu aktuálně věnována významná pozornost. Databáze DeriNet je jako jazykový zdroj specializovaný na derivační morfologii češtiny budována v Ústavu formální a aplikované lingvistiky MFF UK, mezi více než 1 milionem lexémů se zatím podařilo identifikovat 774 tisíc derivačních vztahů. Prvním z témat probíraných v příspěvku jsou hláskové alternace, které jsou pro poloautomatické metody identifikace derivačních vztahů zásadním problémem. Dále se zaměřím na kategorii slovesného vidu, tato flektivní kategorie českého slovesa je vyjadřována slovotvornými prostředky. Kategorie vidu byla spolu s dalšími rysy použita jako jedno z kritérií při uspořádávání příbuzných sloves do derivačního stromu. V závěru příspěvku - jako téma třetí - bude zachycování sloves odvozených od sloves představeno komplexně. Všechna tři témata jsou vzájemně provázána, ve všech jsou úzce propojeny aspekty teoretické a komputační lingvistiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk summarizes recent steps in development of the derivational network DeriNet. The following topics are taken under scrutiny: vowel and consonant alternations that occur during derivation, the role of the inflectional category of aspect in derivation, and the verb-to-verb derivation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku se zabýváme hranicí mezi přejímáním slov a slovotvorbou (zvláště derivací) na příkladu substantiv s příponou -ismus a -ita. Z hlediska české slovní zásoby jsou zkoumány formální (flektivní i derivační) vlastnosti a význam těchto slov.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the paper, the boundary between borrowing and word formation (particularly, derivation) is studied on the example of the suffixes -ismus and -ita, which occur in nouns in Czech. Formal (both inflectional and derivational) and semantic features of nouns with the suffixes -ismus and -ita are introduced.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se věnuje paradigmatu českého slovesa z hlediska flektivní a derivační morfologie, poukazuje na vzájemnou propojenost obou oblastí. Kategorie vidu je popisována jako flektivní kategorie slovesa, která je formálně vyjadřována derivačními prostředky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper deals with verbal paradigms with respect to inflectional and derivational morphology of Czech, and its aim is to contribute to the discussion on the absence of clear boundaries between inflection and derivation. The aspect is an inflectional category of Czech verbs which is formally expressed by derivation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek představuje poloautomatickou proceduru, jejímž cílem bylo v derivační databázi DeriNet identifikovat vidové dvojice sloves lišící se příponami. Na základě dat z valenčního slovníku Vallex jsme sestavili seznam párů sufixů, kterými se vidové protějšky liší, a tento seznam využili k vyhledání dalších dvojic v rozsáhlých datech sítě DeriNet. Nalezené dvojice byly potvrzeny ruční anotací.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper describes a semi-automatic method of adding derivational links to the lexical database DeriNet by identifying verbs which are derived by suffixation and constitute aspectual pairs. It briefly introduces the notion of aspect in Czech and discusses the account of aspect in the Czech linguistic literature and in existing data resources. As its main focus, it presents an approach to identifying aspectual pairs based on extraction of such pairs from the VALLEX valency dictionary, identification of suffix substitution rules and subsequent manual annotation, which resulted in the addition of almost 6,000 derivational links into the DeriNet database.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vícejazyčný výzkum syntaxe a syntaktické analýzy byl dlouhou dobu brzděn tím, že anotační styly používané v různých jazycích se výrazně lišily a bylo téměř nemožné provést metodologicky čisté srovnání a vyhodnocení vícejazyčných experimentů se strojovým učením.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Multilingual research on syntax and parsing has for a long time been hampered by 
the fact that annotation schemes vary enormously across languages, which has made
it virtually impossible to perform sound comparative evaluations and 
cross-lingual learning experiments. The Universal Dependencies (UD) project 
(www.universaldependencies.org) seeks to tackle this problem by developing 
cross-linguistically consistent treebank annotation for many languages, aiming to
capture similarities as well as idiosyncracies among typologically different
languages (e.g., morphologically rich languages, pro-drop languages, and 
languages featuring clitic doubling). The goal is not only to support comparative
evaluation and cross-lingual learning but also to facilitate multilingual natural
language processing and enable comparative linguistic studies. To serve all these
purposes, the framework needs to have a solid linguistic foundation and at the
same time be transparent and accessible to non-specialists.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je šesté vydání treebanků UD, verze 2.0.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the sixth release of UD Treebanks, Version 2.0.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je doplnění šestého vydání treebanků UD, verze 2.0, o testovací data.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the extension of the sixth release of UD Treebanks, Version 2.0, by test data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je sedmé vydání treebanků UD, verze 2.1.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the seventh release of UD Treebanks, Version 2.1.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek předkládá grafematické a morfologické rysy západoslovanských jazyků. Na základě těchto rysů lze predikovat současné podoby slov v češtině a polštině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article presents graphemic and morphological features of West Slavic languages. Based on these features, Czech and Polish word forms can be predicted.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška o mém výzkumu v oblasti mezijazyčného přenosu závislostních parserů, zahrnuje strojový překlad, segmentaci na podslova, harmonizaci dat, a další.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Cross-lingual transfer of NLP tools is motivated by the fact that sufficient training data and high-performance supervised NLP tools are available only for maybe 1% of world's languages; the remaining 99% of languages are under-resourced and therefore difficult to process automatically. In cross-lingual transfer methods, we try to find effective ways of utilizing supervised training data for resource-rich languages and various automatic transfer methods to be able to process the under-resourced languages (think of e.g. translating the training data with an MT system).
In my talk, I will review my research on cross-lingual transfer of dependency parsers (and taggers to some extent), including both positive and negative results. My work on this problem has involved several subproblems, which I will also address in the talk, including: Universal Dependencies and other annotation harmonization, morphological segmentation and other subword units, Giza++ and other word alignment, Moses and other machine translation systems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>To je něco, co používá každý, přitom není úplně zřejmé, jak to funguje. Pro daný dotaz nejde prostě pro každý dotaz projít všechny existující miliardy webových stránek, a vrátit uživateli několik miliónů z nich, které obsahují hledaná slova. Ukážu, co je reverzní index (to zná každý pod názvem rejstřík), podle čeho se řadí výsledky, a přidám i pokročilejší vylepšení (TF.IDF, vektorový model, lematizace, synonyma, pojmenované entity).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This is something everyone uses, but it is not absolutely clear how it works. For a given query it is not possible to simply traverse all the existing billions of websites, and return the few millions that contain the sought words. I will show what a reverse index is, how results are sorted, and I will also add more advanced techniques (TF.IDF, vector model, lemmatization, synonyms, named entities).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme MonoTrans, systém statistického strojového překladu, který používá pouze jednojazyčná data ve zdrojovém a cílovém jazyce, bez použití paralelních korpů nebo pravidel specifických pro konkrétní jazyk. Systém překládá každé zdrojové slovo cílovém slovem, které je mu nejpodobnější na základě kombinace míry řetězcové podobnosti a podobnosti četností slov. Systém je určen pro překlad mezí blízkými jazyky v situaci kdy není k dispozici dostatek paralelních dat.
Přestože MonoTrans dosahuje nízkých skóre, významně překonává baseline.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present MonoTrans, a statistical machine translation system which only uses monolingual source language and target language data, without using any parallel corpora or language-specific rules. It translates each source word by the most similar target word, according to a combination of a string similarity measure and a word frequency similarity measure. It is designed for translation between very close languages, such as Czech and Slovak or Danish and Norwegian. 
It provides a low-quality translation in resource-poor scenarios where parallel data, required for training a high-quality translation system, may be scarce or unavailable. This is useful e.g. for cross-lingual NLP, where a trained model may be transferred from a resource-rich source language to a resource-poor target language via machine translation. 
We evaluate MonoTrans both intrinsically, using BLEU, and extrinsically, applying it to cross-lingual tagger and parser transfer. Although it achieves low scores, it does surpass the baselines by respectable margins.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Snadný způsob prohlížení souborů ve formátu CoNLL a CoNLLU ve vašem terminálu. Rychlý a textový.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A simple way of browsing CoNLL and CoNLLU format files in your terminal. Fast and text-based.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>SloNLP je workshop speciálně zaměřený na zpracování přirozeného jazyka a počítačovou lingvistiku. Jeho hlavním cílem je podpořit spolupráci mezi výzkumníky v oblasti NLP v Česku a na Slovensku.
Mezi témata workshopu patří automatické rozpoznávání mluvené řeči (ASR), automatická analýza a generování přirozeného jazyka (morfologie, syntax, sémantika...), dialogové systémy (DS), strojový překlad (MT), vyhledávání informací (IR), praktické aplikace NLP technologií, a další témata počítačové lingvistiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>SloNLP is a workshop focused on Natural Language Processing (NLP) and Computational Linguistics. Its primary aim is to promote cooperation among NLP researchers in Slovakia and Czech Republic.
The topics of the workshop include automatic speech recognition, automatic natural language analysis and generation (morphology, syntax, semantics, etc.), dialogue systems, machine translation, information retrieval, practical applications of NLP technologies, and other topics of computational linguistics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V několika rozměrech rozebíráme chyby v kroslingválním přenosu taggeru a parseru z angličtiny do 32 jazyků. Identifikujeme a vysvětlujeme silné a slabé stránky a nepravidelnosti a navrhujeme možná řešení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We thoroughly analyse the performance of cross-lingual tagger and parser transfer from English into 32 languages. We suggest potential remedies for identified issues and evaluate some of them.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>My měli korpus, anebo ten korpus měl nás.

Měl značky pěkný, jednoduchý, harmonický.

Když zvali nás řekli ať použijem všechno, co chcem.

Však k použití nikde nic není, ať hnem se kam hnem.

Tak cizí řeči jsme přeložili, využili.

Dva týdny práce, dřeli jsme fest, a je tu test.

Trénink běžel přes noc do rána, než přišel deadline.

My nespali, čekali, doufali, že dopadnem fajn.

Ráno bylo tu, hleďme na to, my měli zlato.

Tak popíšem papír, co zrodilo ho norský dřevo.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We once had a corp,

or should we say, it once had us

They showed us its tags,

isn’t it great, unified tags

They asked us to parse

and they told us to use everything

So we looked around

and we noticed there was near nothing

We took other langs,

bitext aligned: words one-to-one

We played for two weeks,

and then they said, here is the test

The parser kept training till morning,

just until deadline

So we had to wait and hope what we get

would be just fine

And, when we awoke,

the results were done, we saw we’d won

So, we wrote this paper,

isn’t it good, Norwegian wood.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Natrénované modely pro UDPipe, použité k vytvoření našeho příspěvku zaslaného na sdílenou úlohu VarDial 2017 (https://bitbucket.org/hy-crossNLP/vardial2017) a popsaných v článku stejných autorů s názvem Slavic Forest, Norwegian Wood.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Trained models for UDPipe used to produce our final submission to VarDial 2017 shared task (https://bitbucket.org/hy-crossNLP/vardial2017) and described in a paper by the same authors titled Slavic Forest, Norwegian Wood.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nástroje a skripty užité k vytvoření modelů mezijazyčných parserů, zaslaných na sdílenou úlohu VarDial 2017 (https://bitbucket.org/hy-crossNLP/vardial2017) a popsaných v článku stejných autorů s názvem Slavic Forest, Norwegian Wood.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Tools and scripts used to create the cross-lingual parsing models submitted to VarDial 2017 shared task (https://bitbucket.org/hy-crossNLP/vardial2017) and described in a paper by the same authors titled Slavic Forest, Norwegian Wood.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení Treex CR - systému na rozpoznávání koreference pro angličtinu a češtinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presenting Treex CR - the coreference resolution system for English and Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje Treex CR - systém na rozpoznávání koreference nejenom pro češtinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper introduces Treex CR, a coreference resolution (CR) system not only for Czech. As its name suggests, it has been implemented as an integral part of the Treex NLP framework. The main feature that distinguishes it from other CR systems is that it operates on the tectogrammatical  layer, a representation  of  deep syntax. This feature allows for natural handling of elided expressions, e.g. unexpressed subjects in Czech as well as generally ignored English anaphoric expression – relative pronouns and zeros. The system implements a sequence of mention ranking models specialized at particular types of coreferential expressions (relative, reflexive, personal pronouns etc.). It takes advantage of rich feature set extracted from the data linguistically preprocessed with Treex. We evaluated Treex CR on Czech and English datasets and compared it with other systems as well as with modules used in Treex so far.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška pojednává o experimentech s rozpoznáváním koreference na češtině a angličtině s použitím paralelních korpusů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk describes experiments on coreference resolution in Czech and English using parallel corpora.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Počítačový nástroj Evaluátor diskurzu (EVALD 2.0) slouží k automatickému hodnocení povrchové koherence (koheze) textů v češtině psaných rodilými mluvčími češtiny. Software tedy hodnotí (známkuje) úroveň předloženého textu z hlediska jeho povrchové výstavby (zohledňuje např. frekvenci a rozmanitost spojovacích prostředků, bohatost slovní zásoby, koreferenční vztahy apod.).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Evaluator of Discourse (EVALD 2.0) is a software that serves for automatic evaluation of surface coherence (cohesion) in Czech texts written by native speakers of Czech. Software evaluates the level of the given text from the perspective of its surface coherence (i.e. it takes into consideration, e.g., the frequency and diversity of connectives, richness of vocabulary, coreference realtions etc.).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Počítačový nástroj Evaluátor diskurzu pro cizince (EVALD 2.0 pro cizince) slouží k automatickému hodnocení povrchové koherence (koheze) textů v češtině psaných nerodilými mluvčími češtiny. Software hodnotí úroveň předloženého textu z hlediska jeho povrchové výstavby (zohledňuje např. frekvenci a rozmanitost užitých spojovacích prostředků, bohatost slovní zásoby, koreferenční vztahy apod.).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Evaluator of Discourse for Foreigners (EVALD 2.0 for Foreigners) is a software that serves for automatic evaluation of surface coherence (cohesion) in Czech texts written by non-native speakers of Czech. Software evaluates the level of the given text from the perspective of its surface coherence (i.e. it takes into consideration, e.g., the frequency and diversity of connectives, richness of vocabulary, coreference realtions etc.).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje rozšíření systémů EVALD, jejichž cílem je hodnocení úrovně koherence v slohových pracích, o rysy využívající informace o koreferenci a zájmenách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper contributes to the task of automated evaluation of surface coherence. It introduces a coreference-related extension to the EVALD applications, which aim at evaluating essays produced by native and non-native students learning Czech. Having successfully employed the coreference resolver and coreference-related features, our system outperforms the original EVALD approaches by up to 8 percentage points. The paper also introduces a dataset for non-native speakers' evaluation, which was collected from multiple corpora and the parts with missing annotation of coherence grade were manually judged. The resulting corpora contains sufficient number of examples for each of the grading levels.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje systém pro rozpoznávání koreference  v němčině a ruštině, trénovaný výlučně na koreferenčních vztazích projektovaných skrz paralelní korpus. Rozpoznávač operuje na tektogramatické vrstvě a používá vícero specializovných modelů. Měřeno metrikou CoNLL systém dosahuje 32 bodů pro Ruštinu a 22 bodov pro němčinu. Analýza výsledků ukazuje, že rozpoznávač pro ruštinu je schopen při projekci z angličtiny dosáhnout 66% kvality dosažené na angličtině. Systém byl poslán na CORBON 2017 Shared task.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes the system for coreference resolution in German and Russian, trained exclusively on coreference relations projected through a parallel corpus from English. The resolver operates on the level of deep syntax and makes use of multiple specialized models. It achieves 32 and 22 points in terms of CoNLL score for Russian and German, respectively. Analysis of the evaluation results show that the resolver for Russian is able to preserve 66\% of the English resolver's quality in terms of CoNLL score. The system was submitted to the Closed track of the CORBON 2017 Shared task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Automatická segmentace, tokenizace a morfologická a syntaktická anotace textů ve 45 jazycích, vygenerovaná pomocí UDPipe (http://ufal.mff.cuni.cz/udpipe), spolu se 100rozměrnými slovními embeddingy vypočítanými nad textem převedeným na malá písmena nástrojem word2vec (https://code.google.com/archive/p/word2vec/).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Automatic segmentation, tokenization and morphological and syntactic annotations of raw texts in 45 languages, generated by UDPipe (http://ufal.mff.cuni.cz/udpipe), together with word embeddings of dimension 100 computed from lowercased texts by word2vec (https://code.google.com/archive/p/word2vec/).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek shrnuje výsledky druhého ročníku Biomedical Translation Task konference WMT 2017, tj. strojového překladu biomedicínských textů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Automatic   translation   of   documents   is
an  important  task  in  many  domains,  in-
cluding  the  biological  and  clinical  do-
mains. The second edition of the Biomed-
ical Translation task in the Conference of
Machine  Translation  focused  on  the  au-
tomatic  translation  of  biomedical-related
documents  between  English  and  various
European  languages.   This  year,  we  ad-
dressed  ten  languages:   Czech,  German,
English,  French,  Hungarian,  Polish,  Por-
tuguese, Spanish, Romanian and Swedish.
Test sets included both scientific publica-
tions (from the Scielo and EDP Sciences
databases) and health-related news (from
the Cochrane and UK National Health Ser-
vice web sites).  Seven teams participated
in the task, submitting a total of 82 runs.
Herein we describe the test sets, participat-
ing systems and results of both the auto-
matic and manual evaluation of the trans-
lations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Strojový překlad do tvaroslovně bohatých jazyků představuje složitý problém. Prestože pokrytí na úrovni lemmat může být dostatečné, řada jejich tvaroslovných variant se z trénovacích dat nedá získat. Představujeme statistický překladový systém, který tyto tvaroslovné varinaty dokáže generovat. Na rozdíl od dřívějších prací nerozdělujeme modelování tvarosloví a lexikální volbu na dva navazující kroky. Náš postup je integrován přímo v dekódování a využívá informace z konextu jak na zdrojové, tak na cílové straně.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Translating into morphologically rich languages is  difficult. Although  the  coverage of lemmas may be reasonable,  many morphological variants cannot be learned from the training data. We present a statistical translation system that is able to produce these inflected word forms. Different from most previous work, we do not separate morphological prediction from lexical choice into two consecutive steps. Our approach  is  novel  in  that  it  is  integrated  in decoding  and  takes  advantage  of  context information from both the source language and the target language sides.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje vytváření volně dostupného závislostního korpusu maráthštiny, který odpovídá anotačnímu schématu Universal Dependencies (UD).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the creation of a free and open-source dependency treebank for Marathi,
the first open-source treebank for Marathi following the Universal Dependencies (UD) syntactic annotation scheme. In the paper, we describe some of the syntactic and morphological phenomena in the language that required special analysis, and how they fit into the UD guidelines. We also evaluate the parsing results for three popular dependency parsers on our treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje náš příspěvek do společné úlohy trénování systémů neuronového překladu (WMT2017 Neural MT Training Task).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This  paper  describes  our  submission  to
the WMT 2017 Neural MT Training Task.
We modified the provided NMT system in
order  to  allow  for  interrupting  and  con-
tinuing  the  training  of  models.   This  al-
lowed  mid-training  batch  size  decremen-
tation and incrementation at variable rates.
In  addition  to  the  models  with  variable
batch size,  we tried different setups with
pre-trained word2vec embeddings.  Aside
from batch size incrementation, all our ex-
periments performed below the baseline</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Bylo prokázáno, že zvyšující se hloubka modelu zlepšuje kvalitu neuronového strojového překladu.
Přes mnoho návrhů různých variant arhitektur pro zvýšení hloubky modelu doposud nebyla provedena žádná důkladná srovnávací studie.

V této práci popisujeme a vyhodnocujeme několik stávajících přístupů k zavedení hloubky v neuronovém strojovém překladu.
Navíc prozkoumáváme nové varianty architektur včetně hlubokých přechodových RNN a měníme, jak je hlubokém dekodéru použit mechanismus pozornosti ("attention").
Představujeme novou architekturu "BiDeep" RNN, která kombinuje hluboké přechodové RNN a skládané RNN.

Hodnocení provádíme na anglicko-německém datovém souboru WMT pro překlady novinových článků s využitím stroje s jednou GPU pro trénování i inferenci.
Zjistili jsme, že několik našich navrhovaných architektur zlepšuje stávající přístupy z hlediska rychlosti a kvality překladu.
Nejlepších výsledků jsme získali s BiDeep RNN kombinované hloubky 8, získáním průměrného zlepšení 1,5 BLEU nad silnou baseline.

Náš kód je pro snadný přístup zveřejněn.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>It has been shown that increasing model depth improves the quality of neural machine translation.
However, different architectural variants to increase model depth have been proposed, and so far, there has been no thorough comparative study.

In this work, we describe and evaluate several existing approaches to introduce depth in neural machine translation.
Additionally, we explore novel architectural variants, including deep transition RNNs, and we vary how attention is used in the deep decoder.
We introduce a novel "BiDeep" RNN architecture that combines deep transition RNNs and stacked RNNs.

Our evaluation is carried out on the English to German WMT news translation dataset, using a single-GPU machine for both training and inference.
We find that several of our proposed architectures improve upon existing approaches in terms of speed and translation quality.
We obtain best improvements with a BiDeep RNN of combined depth 8, obtaining an average improvement of 1.5 BLEU over a strong shallow baseline.

We release our code for ease of adoption.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Licenční smlouva pro Google Inc. (Mountain View, California 94043, USA) na užívání Tamilského závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>License agreement for Google Inc. (Mountain View, California 94043, USA) for using the Tamil Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozpoznávání notopisu je specializovaný pod-obor automatického zpracování dokumentů, který se zaměřuje na extrakci hudebního obsahu z obrazové informace (skenu či fotografie) notového zápisu. Vzhledem k tomu, že pravděpodobně existuje více hudebních děl v psané podobě než v podobě nahrávky či jiného záznamu, může automatická transkripce not přinést digitální muzikologii výrazně rozmanitější zdroje. Největší potenciál spočívá v masovém zpřístupnění rozsáhlých notových archivů, jako např. kroměřížská sbírka rodu Lichtenštejn-Kastelkornů, které pak bude možné zkoumat pomocí dalších metod digitální muzikologie, např. automatická detekce duplikátů či partů, vyhledávání pomocí melodií, či sledování genealogie hudebních motivů. 
V rámci zpracování dokumentů se jedná o úlohu výjimečně obtížnou, neboť moderní hudební notace je jedním z nejkomplikovanějších systémů psaní. Obzvlášť pro rukopisy, kde se její složitost násobí s variabilitou realizace zápisu, neexistují v současnosti ani vzdáleně uspokojivá řešení. Na problému také pracuje relativně málo lidí. Příspěvek nabídne vhled do současného stavu poznání a vývoje oboru, s důrazem na možné aplikace v digitální muzikologii.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Optical Music Recognition (OMR) is a sub-field of document analysis and recognition focused on extracting musical content from the image (e.g., scan or photo) of a musical score. Given that more compositions probably exist in written form than have been recorded, automatically transcribing written music can substantially diversify the sources available to digital musicology. The greatest potential of OMR lies in making accessible massive musical archives, such as the Lichtenstein-Castelcorn collection in Kroměříž, to further investigation using methods of digital musicology: automated duplicate or part detection, melody-based search, or tracking the genealogy of musical motifs.
Within document analysis, OMR occupies a particularly difficult niche, because modern music notation is one of the most complicated writing systems overall. Especially for manuscripts, where this difficulty compounds with the variability in handwriting, no remotely satisfactory solutions are available today. The problem also only has a small community dedicated to solving it. The talk will give an overview of the current state of the art and the developments in the field, with focus on potential applications for digital musicology.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Většina hudebních skladeb, které byly kdy vytvořeny, dnes existuje pouze v psané podobě, většinou v rámci archivů; konkrétně v ČR je více než 10 000 takových rukopisů. Pro zachování a šíření této části kulturního dědictví je vhodné jej digitalizovat, a další výhody přináší digitalizace hudebního obsahu těchto dokumentů. Ruční přepisování not v editorech jako Sibelius či MuseScore je však v tomto rozsahu příliš pomalé. Rozpoznávání notopisu (Optical Music Recognition, OMR), ekvivalent OCR pro hudební notaci, může být klíčovým nástrojem pro zpřístupnění obsahu hudebních archivů - pro široký muzikologický výzkum, pro lepší správu (např. detekce opisů), a pro zkrácení cesty, kterou skladby musí ujít od archiválie k živému provedení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Most compositions created throughout history exist today only in written form, usually residing in archive collections; specifically in the Czech Republic, there are many more than 10 000 such manuscripts. To preserve and disseminate this portion of cultural heritage, it is advantageous to digitize it; further usability would be brought by also digitizing the contents of these documents. However, transcribing music with notation editors such as Sibelius or MuseScore is too time-consuming. Optical Music Recognition (OMR), the equivalent of OCR for music notation, can be a key tool for opening the contents of musical archives to large-scale musicological research, better curation (e.g., duplicate search), and for making the way from an archival score to performance easier.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme MUSCIMarker, open-source nástroj pro vývoj systémů rozpoznávání notopisu (Optical Music Recognition, OMR). Nástroj je postavený okolo reprezentace notopisu jakožto grafu, definovaného v datasetu MUSCIMA++. Nástroj je transparentní a interaktivní, umožňuje uživateli vizualizovat, ověřit a upravovat výsledky jednotlivých kroků OMR.
Navíc je díky čistě Pythonové implementaci přenosný mezi operačními systémy, a umožňuje pracovat offline.
Dokládáme hodnotu MUSCIMarkeru skrze prototyp systému na plnohodnotné rozpoznávání notopisu, od předzpracování obrazu po přehrání výstupu a export do formátu MIDI. Publikum prezentace bude mít příležitost MUSCIMarker i rozpoznávací prototyp vyzkoušet.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present MUSCIMarker, an open-source workbench
for developing Optical Music Recognition (OMR) systems from image preprocessing to MIDI export. It is built
around the notation graph data model of the MUSCIMA++
dataset for full-pipeline OMR. The system is transparent
and interactive, enabling the user to visualize, validate
and edit results of individual OMR stages. It is platform-independent, written purely in Python, and can work offline. We demonstrate its value with a prototype OMR system for musical manuscripts that implements the recognition pipeline, up to playing the recognition outputs through
MIDI. The audience will interact with the program and can
test an integrated OMR system prototype.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Notové hlavičky představují rozhraní mezi zápisem hudby a hudbou samotnou. Každá hraná nota je kódována pomocí notové hlavičky, a detekovat hlavičky je tím pádem pro rozpoznávání not nevyhnutelné. V tištěné notaci jsou hlavičky jasně rozlišitelné, avšak různorodost rukopisů činí jejich identifikaci obtížnější. Představujeme jednoduchý detektor notových hlaviček používající konvoluční neuronové sítě pro klasifikaci pixelů a regresi na ohraničení, který dosahuje na detekci f-score 0.97 nad datasetem MUSCIMA++, nepotřebuje odstraňování osnov, a lze jej použít na různorodé rukopisné styly a úrovně složitosti zapsané hudby.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Noteheads are the interface between the written score and music. Each notehead on the page signifies one note to be played, and detecting noteheads is thus an unavoidable step for Optical Music Recognition. In printed notation, noteheads are clearly distinct objects; however, the variety of music notation handwriting makes noteheads harder to identify, and while handwritten music notation symbol classification is a well-studied task, symbol detection has usually been limited to heuristics and rule-based systems instead of machine learning methods better suited to deal with the uncertainties in handwriting. We present ongoing work on a simple notehead detector using convolutional neural networks for pixel classification and bounding box regression that achieves a detection f-score of 0.97 on binary score images in the MUSCIMA++ dataset, does not require staff removal, and is applicable to a variety of handwriting styles and levels of musical complexity.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tvorba datasetů pro ropzoznávání grafických znaků, především ručně psaných, je často drahá a časově náročná záležitost. Nástroj MUSCIMarker, kterým jsme vytvořili dataset MUSCIMA++ pro rozpoznávání not, pomohl omezené zdroje využít efektivně, a je dostatečně flexibilní na to, aby jej bylo možné použít na tvorbu datasetů pro další úlohy s podobnou reprezentací anotované "pravdy". Nejprve popíšeme tuto reprezentaci, aby byla zřejmé, na jaké úlohy je MUSCIMarker aplikovatelný; následně popisujeme samotný nástroj MUSCIMarker, jeho silné a slabé stránky, a praktické zkušenosti s tvorbou datasetu MUSCIMA++.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Dataset creation for graphics recognition, especially for hand-drawn inputs, is often an expensive and time-consuming undertaking. The MUSCIMarker tool used for creating the MUSCIMA++ dataset for Optical Music Recognition (OMR) led to efficient use of annotation resources, and it provides enough flexibility to be applicable to creating datasets for other graphics recognition tasks where the ground truth can be represented similarly. First, we describe the MUSCIMA++ ground truth to define the range of tasks for which using MUSCIMarker to annotate ground truth is applicable. We then describe the MUSCIMarker tool itself, discuss its strong and weak points, and share practical experience with the tool from creating the MUSCIMA++ dataset.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Podstatným problémem pro rozpoznávání not, především ručně psaných, je lokalizace symbolů ve vstupním obrázku. Řešení jsou stavěna jak odspodu, využívajíce nízkoúrovňové vizuální rysy, tak shora, kde se využívá skutečnost, že se hudební notace řídí silnými omezeními na syntakticky správné konfigurace symbolů. Oba přístupy se občas kombinují. V nedávné době se přístup odspodu výrazně zlepšil pomocí konvolučních neuronových sítí. Snížení nejistoty, které může notační syntax poskytnout, však ještě s těmito modely zkombinována nebyla. Tento rozšířený abstrakt diskutuje způsoby, jak neuronové sítě a notační syntax propojit, a analyzuje obtíže, se kterými by se jednotlivé přístupy měly potýkat. Doufáme, že náš příspěvek podnítí další diskusi o těchto možnostech, vyprovokuje výzkumníky v oboru k experimentálnímu prozkoumání navržených přístupů, a podnítí výzkumníky z příbuzných oblastí sdílet své zkušenosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A major roadblock for Optical Music Recognition, especially for handwritten music notation, is symbol detection: recovering the locations of musical symbols from the input page. This has been attempted both with bottom-up approaches exploiting visual features, and top-down approaches based on the strong constraints that music notation syntax imposes on possible symbol configurations; sometimes joined together at appropriate points in the recognition process. The bottom-up approach has recently greatly improved with the boom of neural networks. However, the reduction in uncertainty that music notation syntax can provide has not yet been married to the power of these neural network models. This extended abstract brainstorms ways in which this can be done, and analyzes the difficulties the various combined approaches will have to address. We hope our work will foster further discussion to clarify the issues involed, provoke OMR researchers to try some of these approaches experimentally, and entice researchers from other parts of the graphics recognition community to share relevant experience.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Od rozpoznávání notopisu (Optical Music Recognition, OMR) si lze slibovat zpřístupnění mnoha hudebních dokumentů, které jsou podstatnou součástí kulturního dědictví. OMR však nemá adekvátní data a příslušný formát anotace, který by umožnil porovnávání systémů OMR, což představuje výraznou překážku pro měřitelný pokrok. Řešení OMR využívající strojové učení navíc potřebují trénovací data. Navrhli jsme a sesbírali jsme nový OMR dataset MUSCIMA++. Poskytované anotace tvoří notační graf, jejž naše analýza odhalila jako nutný a postačující popis hudební notace. Stavíme nad daty CVC-MUSCIMA pro odstraňování notových osnov. MUSCIMA++ v1.0 obsahuje 140 stran hudebního rukopisu, s 91245 ručně vyznačenými symboly a 82247 vztahy mezi nimi. Dataset umožňuje trénovat a přímo evaluovat modely pro klasifikaci a lokalizaci symoblů, rekonstrukce logické struktury notace, a extrakce hudebního obsahu. Jsou poskytnuty open-source nástroje pro manipulaci s datasetem, vizualizaci a rozšiřování anotací, a data samotná jsou poskytnuta pod otevřenou licencí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Optical Music Recognition (OMR) promises to make accessible the content of large amounts of musical documents, an important component of cultural heritage. However, the field does not have an adequate dataset and ground truth for benchmarking OMR systems, which has been a major obstacle to measurable progress. Furthermore, machine learn- ing methods for OMR require training data. We design and collect MUSCIMA++, a new dataset for OMR. Ground truth in MUSCIMA++ is a notation graph, which our analysis shows to be a necessary and sufficient representation of music notation. Building on the CVC-MUSCIMA dataset for staffline removal, the MUSCIMA++ dataset v1.0 consists of 140 pages of hand- written music, with 91254 manually annotated notation symbols and 82247 explicitly marked relationships between symbol pairs. The dataset allows training and directly evaluating models for symbol classification, symbol localization, and notation graph assembly, and musical content extraction, both in isolation and jointly. Open-source tools are provided for manipulating the dataset, visualizing the data and annotating more, and the data is made available under an open license.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje přeměnu překladového systému pro příbuzné jazyky Česílko do formátu Open-source. Tento systém byl vytvořen pro účely rychlého a kvalitního překladu z jednoho zdrojového do více příbuzných cílových jazyků. Jeho architektura je založena na mělké analýze a transferu pomocí pravidel. Článek představuje architekturu systému a instalační instrukce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Macine translation system Česílko has been developed as an answer to a growing need of translation and localization from one source language to many target languages. The system
belongs to the shallow parse, shallow transfer RBMT paradigm and it is designed primarily for
translation of related languages. The paper presents the architecture, the development design
and the basic installation instructions of the translation system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku popisujeme vylepšení úlohy pro řízení robota neomezeným přirozeným jazykem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we describe an improvement
on the task of giving instructions to robots
in  a  simulated  block  world  using  unrestricted natural language commands</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>MorphoRuEval-2017 je hodnotící kampaň určená k povzbuzení rozvoje technologií automatického morfologického zpracování pro ruštinu, a to jak pro normativní texty (novinky, beletrie, fakta), tak pro méně formální povahu (blogy a další sociální média). Tento článek porovnává metody, které účastníci použili při řešení úlohy morfologické analýzy. Rovněž se zabývá problémem sjednocení různých stávajících výcvikových sbírek ruského jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>MorphoRuEval-2017 is an evaluation campaign designed to stimulate the development of the automatic morphological processing technologies for Russian, both for normative texts (news, fiction, nonfiction) and those of less formal nature (blogs and other social media). This article compares the methods participants used to solve the task of morphological analysis. It also discusses the problem of unification of various existing training collections for Russian language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Co dál je možné dělat s handlem, kromě "základního" oddělení id od lokace. Metadata handlu. Handle a content negotiation. Template handle.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>What else can be achieved with handles in addition to "basic" separation of resource id from its location. Handle metadata. Handles and content negotiation. Template handles.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Historie vzniku, vývoj a úvod do používání repozitáře clarin-dspace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>History, development and a introduction to usage of clarin-dspace.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku představujeme projekt anotace evaluativního významu v Pražském závislostním korpusu 2.0. Projekt navazuje na sérii anotací malých korpusů prostého textu. V projektu byla použita automatická identifikace potenciálně evaluativních uzlů prostřednictvím českého slovníku hodnotících výrazů Czech SubLex 1.0. V rámci anotačních prací byly odhaleny výhody i nevýhody zvoleného anotačního schématu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the paper, we present our efforts to annotate evaluative language in the Prague Dependency Treebank 2.0. The project is a follow-up of the series of annotations of small plaintext corpora. It uses automatic identification of potentially evaluative nodes through mapping a Czech subjectivity lexicon to syntactically annotated data. The annotations unveiled several advantages and disadvantages of the chosen framework.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku se zabýváme jedním typem strukturní odlišnosti mezi českými a anglickými paralelními větami v dvojjazyčném korpusu PCEDT, konkrétně vzájemným mapováním valenčních doplnění aktorového typu a (nevalenčních) lokačních doplnění. Analýzou korpusových příkladů ukazujeme, do jaké míry se na rozdílech ve valenci sloves v překladově ekvivalentních větách podílejí jazykově specifické syntakticko-sémantické preference konkrétních slov na pozicích subjektu a predikátu a jak je toto mapování ovlivněno vzájemnou souhrou těchto preferencí, větné diateze a sémantického principu potlačení agentu ve větné struktuře.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the paper we address one type of structural difference between Czech and English parallel sentences in a bilingual PCEDT treebank, the mutual mapping of actors and locations. Within the analysis of treebank examples, we show the effect of language-specific syntactic and semantic preferences of individual words in the positions of the subject and predicate on the differences in valency structure, as well as the effect of diathesis and the principal of agent demoting.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek sleduje chování valenčních doplnění sloves vzhledem k pozici evaluačního cíle mezi participanty valenčního rámce. Předkládáme klasifikaci typů evaluativnío významu vyjádřeného slovesy a popisujeme společné charakteristické rysy valenčních rámců evaluačních sloves. V analýze se zabýváme třemi problémy: sémantickou klasifikací evaluačních sloves a jejím vztahem k propagaci evaluačního významu jednotlivým participantům, různým valenčním pozicím evaluačního cíle u vzýjemně překladových sloves, tj. možným posunům evaluativního fokusu a dosahu a ztrátě evaluativního významu během překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper documents the behavior of verb valency complementations regarding the position of the target of evaluation within the valency frame. We classify the types of evaluative meaning expressed by the verbs and identify shared characteristic features considering the valency patterns of the verbs. In the analysis, we comment on three major issues of interest: the semantic classification of evaluative verbs and its relation to the propagation of sentiment value to the participants, the possible non-matching structural positions of the target of evaluation in the valency frame of a verb and its translation, i.e., the possible shift in evaluative focus and scope, and the possible loss of evaluative stance in the process of translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku se zabýváme možnostmi pojetí výuky slovních druhů v  konstruktivistickém  vyučování.  Problematiku  vágnosti  a  vzájemné  prostupnosti  jazykových  kategorií  analyzujeme  z  hlediska  lingvistické  teorie, která usiluje o konzistentnost a přístupnost pro školní praxi, z hlediska lexikografické práce, jejímž cílem je připravit jasnou oporu pro běžného uživatele  i  pedagoga,  i  z  hlediska  moderní pedagogiky,  jejímž  úkolem  je  zpřístupňovat žákům vědecké koncepty a teoretické konstrukty důvěryhodným a smysluplným způsobem.
Důraz  klademe  zejména  na  proměnu  cílů jazykového  vyučování  i  na  proměnu vnímání role učitele. Tvrdíme, že konstruktivistické vyučování je více v souladu s principy současné jazykové deskripce než vyučování transmisivní, neboť umožňuje žákovské budování pojmů vlastní manipulací s materiálem a vlastní logickou úvahou a přechod od nácviku jednoznačné kategorizace k prohlubování obecných analytických schopností.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The  paper  addresses  the  advantages  of teaching  word  class  categorization  in  Czech in  a  constructivist  manner.  The  problem  of  vagueness and blurriness of the linguistic category of word class is analyzed from the linguistic point of view, the lexicographer's point of view and the pedagogical point of view.
We argue that because of the overall change of the aims of the teaching process,  constructivism  is  more  in  compliance  with  the  principles  of modern linguistic approaches than the transmissive teaching. It allows pupils to build their own linguistic concepts by their logical thinking and direct manipulation  of  language,  shifting  the  attention  from  the  word  class  categorization  training to the analytical thinking.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme proces vzniku NUDAR, arabského treebanku ve stylu Universal Dependencies. Představujeme převod z Penn Arabic Treebanku do syntaktické reprezentace Universal Dependencies přes mezilehlou závislostní reprezentaci. Probíráme obtíže, se kterými je převod závislostních stromů spojen, řešení, která jsme použili, a hodnocení námi převedených dat. Dále představujeme prvotní výsledky parsingu na NUDARu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe the process of creating NUDAR, a Universal Dependency treebank for Arabic. We present the conversion from the Penn Arabic Treebank to the Universal Dependency syntactic representation through an intermediate dependency representation. We discuss the challenges faced in the conversion of the trees, the decisions we made to solve them, and the validation of our conversion. We also present initial parsing results on NUDAR.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Víceslovné výrazy představují problém pro všechny aplikace zpracování přirozeného jazyka. V článku představujeme výsledky experimentů s neuronovým strojovým překladem z angličtiny do češtiny a lotyšštiny s důrazem na víceslovné výrazy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Processing of multi-word expressions (MWEs) is a known problem for any natural language
processing task. Even neural machine translation (NMT) struggles to overcome it. This paper
presents results of experiments on investigating NMT attention allocation to the MWEs and
improving automated translation of sentences that contain MWEs in English→Latvian and
English→Czech NMT systems. Two improvement strategies were explored—(1) bilingual
pairs of automatically extracted MWE candidates were added to the parallel corpus used to
train the NMT system, and (2) full sentences containing the automatically extracted MWE
candidates were added to the parallel corpus. Both approaches allowed to increase automated
evaluation results. The best result—0.99 BLEU point increase—has been reached with the first
approach, while with the second approach minimal improvements achieved. We also provide
open-source software and tools used for MWE extraction and alignment inspection.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku popisuje nástroj pro vykreslování výstupu a pozornostních vah neuronového překladu a pro odhad spolehlivosti překladu vypočtený na základě pozornosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this article, we describe a tool for visualizing the output and attention weights of neural
machine translation systems and for estimating confidence about the output based on the
attention.
Our aim is to help researchers and developers better understand the behaviour of their
NMT systems without the need for any reference translations. Our tool includes command
line and web-based interfaces that allow to systematically evaluate translation outputs from
various engines and experiments. We also present a web demo of our tool with examples of
good and bad translations: http://ej.uz/nmt-attention</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá vedlejšími jazykovými signály, které mohou vyjadřovat sémantiku implicitních vztahů. K nim patří např. hodnotící výrazy ve fokusu, za nimiž následuje obvykle argument s významem explikace či specifikace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper deals with secondary language signals that can express the semantics of implicit discourse relations. These include, for example, evaluative expressions in focus, usually followed by an argument with the meaning of explication or specification.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zpráva o vědecké náplni mezinárodního projektu týkajícího se výstavby diskurzu a prostředků jeho konektivity a o workshopu pořádaném v Praze v rámci tohoto projektu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A report on the contents of an international research project on the structure of discourse and on the means of its connectivity and on a workshop organized in Prague.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kapitola se zaměřuje na popis a delimitaci diskurzních konektorů, tj. výrazů přispívajících ke kohereci textu a pomáhajících čtenáři lépe porozumět sémantickým vztahům v textu. V příspěvku je představen etymologický původ diskurzních konektorů z hlediska současné lingvistiky. Cílem je podat definici diskurzních konektorů s ohledem na jejich historický vývoj, díky němuž můžeme lépe nahlížet na  diskurzní vztahy v současném jazyce. V kapitole analyzujeme etymologický původ a vývoj deseti nejfrekventovanějších konektorů v češtině, angličtině a němčině (mezijazykový pohled může napomoci také přesnějšímu překladu konektorů) a poukazujeme na fakt, že tyto konektory prošly ve všech sledovaných jazycích velmi podobným vývojem, než se ustálily v roli současných diskurzních konektorů. Domníváme se proto, že způsob vzniku diskurzních konektorů může být jazykovou univerzálií. V závěru kapitoly ukazujeme, jak naše zjištění mohou pomoci při anotacích diskurzu ve velkých korpusech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper focuses on the description and delimitation of discourse connectives, i.e.
linguistic expressions significantly contributing to text coherence and generally
helping the reader to better understand semantic relations within a text. The paper
discusses the historical origin of discourse connectives viewed from the perspective
of present-day linguistics. Its aim is to define present-day discourse connectives
according to their historical origin through which we see what is happening in discourse in contemporary language. The paper analyzes the historical origin of the most frequent connectives in Czech, English and German (which could be useful for more accurate translations of connectives in these languages) and point out that they underwent a similar process to gain a status of present-day discourse connectives. The paper argues that this historical origin or process of rising discourse connectives might be language universal. Finally, the paper demonstrates how these observations may be helpful for annotations of discourse in large corpora.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Instalace a uživatelské nastavení odborných překladů z češtiny do angličtiny a z angličtiny do češtiny. Překládají se české odborné termíny směrnice INSPIRE a thesaury GEMET a AgroVoc (prostorová data). Pro účely projektu byl připraven balíček obsahující připravené modely v systému Moses a seznam příkazů pro provádění překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Installation and user setting for Czech-English and English-Czech translation of Czech technical terms of the INSPIRE Directive and thesauri GEMET and AgroVoc (spatial data). For the project a package containing trained Moses models and commands for translation was prepared.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme novou verzi UDPipe 1.0, což je trénovatelný nástroj provádějící větnou segmentaci, tokenizaci, morfologické značkování, lemmatizaci a syntaktickou analýzu. Poskytujeme modely pro všech 50 jazyků UD 2.0, a navíc lze jednoduše UDPipe natrénovat pomocí vlastních dat v CoNLL-U formátu.

Pro potřeby CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, upravená verze UDPipe 1.1 byla použita jako základový systém a umístila se na 13. místě z 33 účastníků. Nejnovější verze UDPipe 1.2, která se také účastnila, dosáhla na 8. místo, přičemž potřebuje jen malý čas na běh a středné velké modely.

Nástroj je k dispozici pod open-source licencí MPL a poskytuje rozhraní pro C++, Python (pomocí ufal.udpipe balíčku PyPI), Perl (pomocí UFAL::UDPipe balíčku CPAN), Javu a C#.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an update to UDPipe 1.0, a trainable pipeline which performs sentence segmentation, tokenization, POS tagging, lemmatization and dependency parsing. We provide models for all 50 languages of UD 2.0, and furthermore, the pipeline can be trained easily using data in CoNLL-U format.

For the purpose of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, the updated UDPipe 1.1 was used as one of the baseline systems, finishing as the 13th system of 33 participants. A further improved UDPipe 1.2 participated in the shared task, placing as the 8th best system, while achieving low running times and moderately sized models.

The tool is available under open-source Mozilla Public Licence (MPL) and provides bindings for C++, Python (through ufal.udpipe PyPI package), Perl (through UFAL::UDPipe CPAN package), Java and C#.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme náš příspěvek do First Shared Task on Extrinsic Parser Evaluation (EPE 2017). Náš systém, UDPipe, je trénovatelný nástroj provádějící tokenizaci, morfologickou analýzu, morfologické značkování, lemmatizaci a syntaktickou analýzu. Je nezávislý na jazyku a k dispozici jsou modely pro všech 50 jazyků UD 2.0. Použitím relativně omezeného množství trénovacích dat (200 tisíc tokenů z anglického korpusu UD) a bez nastavení specifického pro angličtinu získal systém celkové hodnocení 56.05 a umístil se mezi soutěžícími systémy jako 7.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our contribution to The First Shared Task on Extrinsic Parser Evaluation (EPE 2017). Our participant system, the UDPipe, is an open-source pipeline performing tokenization, morphological analysis, part-of-speech tagging, lemmatization and dependency parsing. It is trained in a language agnostic manner for 50 languages of the UD version 2. With a relatively limited amount of training data (200k tokens of English UD) and without any English specific tuning, the system achieves overall score 56.05, placing as the 7th participant system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popis architektury projektu clarin-dspace a dodrziavanie standardov a doporuceni RDA-DFT/FAIR/OAIS.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Describing clarin-dspace architecture and the compliance to RDA-DFT/FAIR/OAIS.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje společný systém projektu QT21 pro překlad z angličtiny do lotyšštiny na druhé konferenci WMT 2017.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This  paper  describes  the  joint  submis-
sion    of    the    QT21    projects    for    the
English
→
Latvian  translation  task  of  the
EMNLP 2017 Second Conference on Ma-
chine Translation
(WMT 2017).  The sub-
mission  is  a  system  combination  which
combines  seven  different  statistical  ma-
chine translation systems provided by the
different groups.
The  systems  are  combined  using  either
RWTH’s  system  combination  approach,
or
USFD’s
consensus-based
system-
selection approach.  The final submission
shows   an   improvement   of   0.5   B
LEU
compared  to  the  best  single  system  on
newstest2017.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Navrhujeme propuštěnou hlubokou neuronovou síť (CDNN) hluboký neuronový model pro výběr věty odpovědi v kontextu systémů QA (Question Answering). Pro vytvoření nejlepších předpovědí kombinuje CDNN neurální uvažování s určitým symbolickým omezením. Integruje techniku přizpůsobení vzoru do vektoru vět
učení se. Při výcviku za použití dostatečných vzorků převyšuje CDNN ostatní nejlepší modely pro výběr vět. Ukazujeme, jak se využívají další zdroje
Školení může zvýšit výkon CDNN. V dobře studovaném datovém souboru pro výběr věty odpovědi náš model výrazně zlepšuje nejmodernější technologii.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We propose Constrained Deep Neural Network (CDNN) a deep neural model for answer sentence selection in the context of
Question Answering (QA) systems. To
produce the best predictions, CDNN combines neural reasoning with a kind of
symbolic constraint. It integrates pattern
matching technique into sentence vector
learning. When trained using enough samples, CDNN outperforms the other best
models for sentence selection.
We show how the use of other sources of
training can enhance the performance of
CDNN. In a well-studied dataset for answer sentence selection, our model improves the state-of-the-art significantly</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto krátkém časopise oznamujeme postupnou práci na hybridním hlubokém nervovém systému, který odpovídá na faktoidní a nepravotní otázky otevřené oblasti. Tento systém doplňuje znalostní graf založený na zodpovězení dotazu s vyhledávacími technikami pro volné texty, které řeší problematiku sparsity ve znalostních grafech. Ospravedlňujeme účinnost navrhovaného systému na základě výsledků pilotního experimentu. Rovněž popisujeme nastavení probíhajícího projektu v kontextu tohoto systému.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this short paper, we report a progressing work on a hybrid deep neural system for answering open-domain factoid and non-factoid questions. This system supplements knowledge graph based Question Answering with free-texts searching techniques to address the sparsity issues in knowledge graphs. We justify the efficiency of the proposed system based on the results of a pilot experiment. We also describe the settings of an on-going project in the context of this system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Uvádíme zprávu o pokročilejších úkolech při sestavování datových sad otázka odpovědět Quora. Datová sada Quora se skládá z otázek, které jsou položeny na stránkách Quora answering site. Je to jediná datová sada, která poskytuje odpovědi současně na úrovni vět a slov. Otázky v datové sadě jsou navíc autentické, což je mnohem realističtější pro systémy odpovědí na otázky. Testujeme výkonnost nejmodernějšího záznamníku otázek na datové množině a porovnáváme ji s lidskou výkonností, abychom vytvořili horní hranici datové sady.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We report on a progressing work for compiling Quora question answer dataset. Quora dataset is composed of the questions which are posed in
Quora question answering site. It is the only dataset which provides answers in
sentence level and word level at the same time. Moreover, the questions in the
dataset are authentic which is much more realistic for question answering systems. We test the performance of a state-of-the-art question answering system on
the dataset and compare it with human performance to establish an upper bound
for the dataset</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace nabízí přehled a srovnání současných elektronických lexikonů textových konektorů v různých jazycích se zvláštním zřetelem na nový lexikon českých konektorů - CzeDLex.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presentation offers an overview and comparison of present-day electronic lexicons of discourse connectives in different languages, with a special regard on the recently developed lexicon of Czech connectives – CzeDLex.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek předkládá podrobné srovnání dvou proslulých přístupů k analýze imlicitních diskurzních vztahů, Penn Discourse Treebanku a databáze Rhetorical Structure Theory Signalling Corpus.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Describing implicit phenomena in discourse is known to be a problematic task, both from the
theoretical and from the empirical perspective. The present article contributes to this topic by
a novel comparative analysis of two prominent annotation approaches to discourse (coherence)
relations that were carried out on the same texts. We compare the annotation of implicit relations in the Penn Discourse Treebank 2.0, i.e. discourse relations not signalled by an explicit discourse connective, to the recently released analysis of signals of rhetorical relations in the RST Signalling Corpus. Our data transformation allows for a simultaneous depiction and detailed study of these
two resources.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Neuronový strojový překlad (NMT) se stal v posledních letech široce využívaným přístupem ke strojovému překladu.

V našem tutoriálu začneme úvodem do základů metod hloubkového učení používaných v NMT jako jsou rekurentní neuronové sítě a jejich pokročilé varianty (GRU nebo LSTM sítě) nebo algoritmy pro jejich optimalizaci.

Představujeme modely specifické pro NMT, jako je mechanizmus pozornosti, a popisujeme metody použité pro dekódování cílových vět, včetně ensemblování modelů a paprskového prohledávání.

Projdeme nedávné pokroky v této oblasti a budeme diskutovat o jejich dopadu na nejmodernější metody používané na letošní soutěži WMT (http://www.statmt.org/wmt17/).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Neural machine translation (NMT) has become a widely-adopted approach to machine translation in the past few years.

In our tutorial, we will start with the introduction to the basics of the deep learning methods used in NMT, such as recurrent neural networks and their advanced variants (GRU or LSTM networks), or the algorithms for their optimization.

We introduce the NMT-specific models, such as the attention mechanism, and describe the methods used for decoding the target sentences, including model ensembling and beam search.

We will go through the recent advancements in the field and discuss their impact on the state-of-the-art methods used in this year's WMT competition (http://www.statmt.org/wmt17/).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku popisujeme naše příspěvky do multimodální překladové úlohy na WMT17.
Pro úlohu 1 (multimodální překlad) byl nejlepším systémem čistě textový neuronový překlad titulku zdrojového obrázku do cílového jazyka.
Hlavním rysem našeho systému je využití dalších dat získaných výběrem podobných vět z paralelních korpusů a syntézou dat zpětným překladem.
Pro úlohu 2 (vícejazyčné generování popisu obrázků) náš nejlepší systém generuje anglický popis obrázku, který je poté přeložen podle nejlepšího systému používaného v úloze č. 1.
Také předkládáme negativní výsledky, které jsou založeny na myšlenkách, o kterých se domníváme, že mají potenciál překlad zlepšit, ale v našem konkrétním uspořádání 
 neprokázaly býti prospěšnými.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we describe our submissions to the WMT17 Multimodal
  Translation Task. For Task 1 (multimodal translation), our best scoring
  system is a purely textual neural translation  of the source image caption to
  the target language. The main feature of the system is the use of additional
  data that was acquired by selecting similar sentences from parallel corpora
  and by data synthesis with back-translation. For Task 2 (cross-lingual image
  captioning), our best submitted system generates an English caption which is
  then translated by the best system used in Task 1. We also present negative
  results, which are based on ideas that we believe have potential of making
  improvements, but did not prove to be useful in our particular setup.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku představujeme vývoj Neural Monkey — open source nástroj pro neuronový strojový překlad a sekvenční učení obecně, který je postavený na knihovně TensorFlow. Náš nástroj poskytuje svým uživatelům vysokoúrovňové rozhraní, které umožňuje rychlé vytváření prototypů komplexních modelů s několika enkodéry a dekodéry. Architektura modelů se vytváří pomocí snadno čitelných konfiguračních souborů. Dlouhodobým cílem Neural Monkey je vytvořit a udržovat kolekci moderních metod pro sekvenční učení. Tomu odpovídá i modulární snadno rozšiřitelný design. Natrénované modely je možné použít pro dávkové zpracování dat nebo spustit jako webovou službu. V předkládaném článku popisujeme základní design nástroje a postup, jak spustit trénování jednoduchého překladače.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we announce development of Neural Monkey — an open-source neural machine translation (NMT) and general sequence-to-sequence learning system built over TensorFlow machine learning library.
The system provides a high-level API with support for fast prototyping of complex architectures with multiple sequence encoders and decoders. These models’ overall architecture is specified in easy-to-read configuration files. The long-term goal of Neural Monkey project is to create and maintain a growing collection of implementations of recently proposed components or methods, and therefore it is designed to be easily extensible. The trained models can be deployed either for batch data processing or as a web service. In the presented paper, we describe the design of the system and introduce the reader to running experiments using Neural Monkey.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Účel tohoto labu je seznámit uživatele s toolkitem Neural Monkey, využívaným pro experimenty se sekvenčním učením.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of this lab is to familiarize the users with the Neural Monkey toolkit for sequence learning experiments.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Attention modely ve vícezdrojovém neuronovém  sekvenčním učení zůstávají poměrně neprobádanou oblastí, a to navzdory jeho užitečnosti v úkolech, které které využívají více zdrojových jazyků či modalit. Navrhujeme dvě nové strategie jak kombinovat výstupy attentiion modelu z různých vstupů, plochou a hierarchickou. Navrhované metody porovnáváme se stávajícími a výsledky vyhodnocujeme na datech pro multimodální překlad a automatické post-editování překladu z WMT16. Navrhované metody dosažení konkurenceschopných výsledků na obou úlohách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Modeling attention in neural multi-source sequence-to-sequence learning remains a relatively unexplored area, despite its usefulness in tasks that incorporate multiple source languages or modalities. We propose two novel approaches to combine the outputs of attention mechanisms over each source sequence, flat and hierarchical. We compare the proposed methods with existing techniques and present results of systematic evaluation of those methods on the WMT16 Multimodal Translation and
Automatic Post-editing tasks. We show the proposed methods achieve competitive results on both tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje jeden z možných způsobů, jak zobrazit a prohledávat data s anotací víceslovných jednotek. 
Využíváme mnohojazyčný korpus PARSEME s anotací verbálních víceslovných jednotek v 18 jazycích. Anotované jednotky zahrnují různé typy, jako např. idiomy, konstrukce s lehkými slovesy, inherentně reflexivní slovesa nebo konstrukce se slovesem a částicí. Korpus byl dosud využíván zejména pro trénování prediktivních modelů, ale nikoli k lingvistickému výzkumu per se.
Článek nabízí způsob, jak data zpřístupnit lingvistům skrze jednoduché vyhledávací prostředí a jazyk Corpus Query Language (CQL) známy například z často užívané platformy NoSke.
I přes omezené možnosti k zachycení komplexních jevů jakými jsou nespojité, koordinované nebo vnořené víceslovné predikáty, CQL může postačovat k základním vyhledávkám víceslovných jednotek pro korpusově založený výzkum.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper demonstrates one of the possible ways on how to represent and query corpora with multiword expression (MWE) annotation.
   We exploit the multilingual corpus of 18 languages created under the PARSEME project with verbal multiword expression (VMWE) annotation. VMWEs include categories such as idioms, light verb constructions, verb-particle constructions, inherently reflexive verbs, and others. The corpus was mainly used for the purposes of training predictive models, yet not much linguistic research was conducted based on this data.
We discuss how to allow linguists to query for MWEs in a simple user interface using the Corpus Query Language (CQL) within the NoSke corpus management and concordance system. 
Despite its limited abilities to represent challenging cases such as discontinuous, coordinated or embedded VMWEs, CQL can be sufficient to make basic analysis of the MWE-annotated data in corpus-based studies.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje systém MUMULS, který se účastnil 2017 Shared Task on Automatic Identification of Verbal Multiword expressions (VMWEs). Systém MUMULS byl implementován přístupem učení s učitelem pomocí rekurentních neuronových sítí v open-source knihovně TensorFlow. Model byl trénován na poskytnutých datech s VMWEs a také na morfologických a syntaktických anotacích. MUMULS provádí identifikaci VMWEs v patnácti jazycích, byl to jeden z mála systémů který dokázal kategorizovat VMWEs ve skoro všech jazycích.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we describe the MUMULS system that participated to the 2017 shared task on automatic identification of verbal multiword expressions (VMWEs). The MUMULS system was implemented using a supervised approach based on recurrent neural networks using the open source library TensorFlow. The model was trained on a data set containing annotated VMWEs as well as morphological and syntactic information. The MUMULS system performed the identification of VMWEs in 15 languages, it was one of few systems that could categorize VMWEs type in nearly all languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje novou úlohu na využití dat od uživatelů dialogových systémů v konverzacích člověk-stroj. Tato úloha se zaměřuje na sběr denotací extrahováním z přirozených vět získaných v průběhu dialogu. Motivace spočívá v potřebě velkého monžství trénovacích dat pro vývoj Q&amp;A dialogových systémů, přičemž získání těchto dat je obvykle těžké a nákladné. Získávání denotací při interakcích s uživateli například umožňuje online vylepšování komponent pro porozumění přirozenému jazyku a zjednodušit sběr trénovacích dat. Tento článek také prezentuje výsledky evaluace několika přístupů k extrakci denotací zahrnující modely založené na neuronových sítích s attention architekturou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a novel task using real user data obtained in human-machine conversation. The task concerns with denotation extraction from answer hints collected interactively in a dialogue. The task is motivated by the need for large amounts of training data for question answering dialogue system development, where the data is often expensive and hard to collect. Being able to collect denotation interactively and directly from users, one could improve, for example, natural understanding components online and ease the collection of the training data. This paper also presents introductory results of evaluation of several denotation extraction models including attention-based neural network approaches.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje hybridní dialog state tracker, rozšířený o trénovatelnou jednotku zpracování přirozeného jazyka (SLU). Naše architektura je inspirována belief trackery založenými na neuronových sítích. Tento přístup navíc rozšiřujeme o derivovatelná prvidla, která umožní end-to-end trénink. Tato pravidla umožní našemu trackeru lépe generalizovat v porovnání s trackery založenými pouze na strojovém učení. Pro evaluaci používáme Dialog State Tracking Challenge (DSTC) 2 - populární dataset využívaný pro srovnání výkonnosti belief trackerů. Podle informací, které máme, náš tracker dosahuje state-of-the-art výsledků ve třech ze čtyř kategorií datasetu DSTC2.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a hybrid dialog state tracker enhanced by trainable Spoken Language Understanding (SLU) for slot-filling dialog systems. Our architecture is inspired by previously proposed neural-network-based belief-tracking systems. In addition, we extended some parts of our modular architecture with differentiable rules to allow end-to-end training. We hypothesize that these rules allow our tracker to generalize better than pure machine-learning based systems. For evaluation, we used the Dialog State Tracking Challenge (DSTC) 2 dataset - a popular belief tracking testbed with dialogs from restaurant information system. To our knowledge, our hybrid tracker sets a new state-of-the-art result in three out of four categories within the DSTC2.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Udapi je open-source framework poskytující APO pro zpracovávání dat z projektu Universal Dependencies.
Implementace Udapi je dostupné pro programovací jazyky: Python, Perl a Java. Udapi je vhodné jak pro plnohodnotné aplikace, tak pro rychlé vyváření prototypů: vizualizace stromů, konverze formátu, dotazování, editace, transformace, testy validity, závislostní parsing, vyhodnocování, atd.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Udapi is an open-source framework providing
an application programming interface
(API) for processing Universal Dependencies
data. Udapi is available in
Python, Perl and Java. It is suitable
both for full-fledged applications and fast
prototyping: visualization of dependency
trees, format conversions, querying, editing
and transformations, validity tests, dependency
parsing, evaluation etc.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V návaznosti na loňský systém pro automatickou post-editaci strojového překladu Karlovy Univerzity se soustředíme na využití potenciálu sequence-to-sequence neuronových modelů pro danou úlohu. V článku nejprve porovnáváme několik architektur typu enkodér-dekodér na modelech menšího měřítka a představujeme systém, který byl vybrán na základě těchto předběžných výsledků a odeslán na WMT 2017 Automatic Post-Editing shared task. V článku také ukazujeme jak jednoduchá inkluze umělých dat dokáže vylepšit úspěšnost modelu na základě automatických evaluačních metrik. V závěru uvádíme několik příkladů výstupů vygenerových našim post-editačním systémem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Following upon the last year's CUNI system for automatic post-editing of machine translation output,
we focus on exploiting the potential of sequence-to-sequence neural models for this task. In this system description paper, we compare several encoder-decoder architectures on a smaller-scale models and present the system we submitted to WMT 2017 Automatic Post-Editing shared task based on this preliminary comparison. We also show how simple inclusion of synthetic data can improve the overall performance as measured by an automatic evaluation metric. Lastly, we list few example outputs generated by our post-editing system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje frázové a syntaktické překladové systémy Univerzity v Edinburku účastnící se soutěžní překladového úlohy WMT16.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the University of Edinburgh’s
phrase-based and syntax-based
submissions to the shared translation tasks
of the ACL 2016 First Conference on Machine
Translation (WMT16). We submitted
five phrase-based and five syntaxbased
systems for the news task, plus one
phrase-based system for the biomedical
task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Analýza soutěžních úkolů z Olympiády v českém jazyce, hodnocení jazykových dovedností studentů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Analysis of competition tasks of the Olympiad in Czech language, evaluation of students' language skills.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Počítačový nástroj Evaluátor diskurzu (EVALD 1.0) slouží k automatickému hodnocení povrchové koherence (koheze) textů v češtině psaných rodilými mluvčími češtiny. Software tedy hodnotí (známkuje) úroveň předloženého textu z hlediska jeho povrchové výstavby (zohledňuje např. frekvenci a rozmanitost spojovacích prostředků, bohatost slovní zásoby apod.).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Evaluator of Discourse (EVALD 1.0) is a software that serves for automatic evaluation of surface coherence (cohesion) in Czech texts written by native speakers of Czech. Software evaluates the level of the given text from the perspective of its surface coherence (i.e. it takes into consideration, e.g., the frequency and diversity of connectives, richness of vocabulary etc.).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Počítačový nástroj Evaluátor diskurzu pro cizince (EVALD 1.0 pro cizince) slouží k automatickému hodnocení povrchové koherence (koheze) textů v češtině psaných nerodilými mluvčími češtiny. Software hodnotí úroveň předloženého textu z hlediska jeho povrchové výstavby (zohledňuje např. frekvenci a rozmanitost užitých spojovacích prostředků, bohatost slovní zásoby apod.).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Evaluator of Discourse for Foreigners (EVALD 1.0 for Foreigners) is a software that serves for automatic evaluation of surface coherence (cohesion) in Czech texts written by non-native speakers of Czech. Software evaluates the level of the given text from the perspective of its surface coherence (i.e. it takes into consideration, e.g., the frequency and diversity of connectives, richness of vocabulary etc.).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje, jakou roli hrají elidované větné participanty v koreferenčních řetězcích v textu, tj. zda a do jaké míry se participanty, které jsou v povrchové větné struktuře přítomné pouze implicitně, zapojují do vztahů textové a gramatické koreference. Článek zároveň představuje metody, jimiž je možné zkoumat vzájemné vztahy mezi různými jazykovými jevy, a to na datech Pražského závislostního korpusu, který obsahuje anotaci lingvistických jevů na několika jazykových rovinách.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper focuses on which role is given to elided sentence participants in coreference chains in the text, i.e. whether (and to which degree) the participants that are present only implicitly in the surface layer are involved in relations of textual and grammatical coreference. Generally, the paper introduces  he methods how it is possible to examine the interplays of different language phenomena in corpus data of the Prague Dependency Treebank containing multilayer annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku jsou představeny možnosti automatické evaluace povrchové koherence (koheze) textů psaných nerodilými mluvčími češtiny během certifikovaných zkoušek. Na základě korpusové analýzy jsou vyhledávány a popisovány relevantní rozlišovací rysy (týkající se povrchové koherence textu) pro automatickou detekci úrovní textů nerodilých mluvčích A1–C1 (úrovně jsou ustanoveny Společným evropským referenčním rámcem pro jazyky). Úrovně A1–C1 byly hodnoceny nejprve lidmi (anotátory) – poté byly dělány strojové experimenty s cílem přiblížit se lidskému hodnocení automaticky, a to sledováním vybraných textových rysů, např. frekvence a různorodosti diskurzních konektorů nebo hustoty diskurzních vztahů v daném textu ap. V článku jsou představeny experimenty sledující vždy různé textové rysy při použití dvou algoritmů strojového učení. Úspěšnost automatického měření povrchové koherence (koheze) textu podle Společného evropského referenčního rámce pro jazyky je 73,2 % pro rozpoznávání úrovní A1–C1 a 74,9 % pro rozpoznávání úrovní A2–B2.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce possibilities of automatic evaluation of surface text coherence (cohesion) in texts written by learners of Czech during certified exams for non-native speakers. On
the basis of a corpus analysis, we focus on finding and describing relevant distinctive
features for automatic detection of A1–C1 levels (established by CEFR – the Common
European Framework of Reference for Languages) in terms of surface text coherence.
The CEFR levels are evaluated by human assessors and we try to reach this assessment
automatically by using several discourse features like frequency and diversity of discourse
connectives, density of discourse relations etc. We present experiments with various
features using two machine learning algorithms. Our results of automatic evaluation
of CEFR coherence/cohesion marks (compared to human assessment) achieved 73.2%
success rate for the detection of A1–C1 levels and 74.9% for the detection of A2–B2
levels.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Názvy logických spojek jsou často motivovány názvy spojek z přirozeného jazyka. V případě, že však chceme výroku v přirozeném jazyce (konkrétně češtině) přiřadit jednoznačně logické schéma, zjišt’ujeme, že to není možné dělat automaticky. V tomto článku se zabýváme tím, kdy je možné české spojky „a“ a „nebo“ zapsat pomocí konjunkce a disjunkce a kde je nutné zvolit jinou interpretaci – s ohledem na problematiku množného čísla a vyjádření vztahů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper studies the relationship between
conjunctions in a natural language (Czech) and their logical counterparts. It shows that the process of transformation of a natural language expression into its logical representation is not straightforward. The paper concentrates on the most frequently used logical conjunctions, AND and OR, and it analyzes the natural language phenomena which influence their transformation
into logical conjunction and disjunction.
The phenomena discussed in the paper are temporal
sequence, expressions describing mutual relationship and the consequences of using plural.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Adjektiva velmi často určují polaritu věty. Tento článek studuje roli spojek v analýze polarity adjektiv. Celá studie je prováděna pro češtinu modifikací algoritmu vyvinutého původně pro angličtinu. Modifikace berou v úvahu typologické rozdíly mezi oběma jazyky. Výsledky dosažené pro oba jazyky jsou porovnány a doplněny rozsáhlým rozborem výjimečných případů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Adjectives very often determine the polarity of
an utterance. This paper studies the role of conjunctions in the analysis of adjective polarity. The study is being performed on Czech language on the basis of an existing algorithm for English. The algorithm has been modified in order to reflect the differences between
the two typologically different languages. The results of the original and modified algorithm are being compared and discussed. The paper also contains a thorough discussion of exceptions and special cases supported by a number of examples from a large corpus of Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Značkování slovními druhy (POS tagging) se v počítačovém zpracování přirozeného jazyka někdy považuje za téměř vyřešený problém. Standardní řízené přístupy často dosahují úspěšnosti přes 95 %, pokud je k dispozici dostatek ručně anotovaných trénovacích dat (typicky několik set tisíc tokenů nebo více). My si nicméně myslíme, že je stále užitečné studovat polořízené a neřízené přístupy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Part-of-speech (POS) tagging is sometimes considered an almost solved problem in NLP. Standard supervised approaches often reach accuracy above 95% if sufficiently large hand-labeled training data are available (typically several hundred thousand tokens or more). However, we still believe that it makes sense to study semi-supervised and unsupervised approaches.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje systém hybridního strojového překladu (MT), který byl vytvořen pro překlad z angličtiny do němčiny v oblasti technické dokumentace.
Systém je založen na třech různých systémech MT (frázový, pravidlový a neuronový), které jsou spojeny výběrovým mechanismem, který používá hluboké jazykové rysy v procesu strojového učení.
Součástí je také podrobná manuální analýza chyb, kterou jsme provedli pomocí specializované "zkušební sady", která obsahuje vybrané příklady relevantních jevů.
Zatímco automatické výsledky ukazují obrovské rozdíly mezi systémy, celkový průměrný počet chyb, které (ne) dělají, je pro všechny systémy velmi podobný.
Podrobné rozdělení chyb však ukazuje, že systémy se chovají velmi odlišně, pokud jde o různé jevy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes a hybrid Machine Translation (MT) system built for translating from English
to German in the domain of technical documentation. The system is based on three different
MT engines (phrase-based SMT, RBMT, neural) that are joined by a selection mechanism
that uses deep linguistic features within a machine learning process. It also presents a detailed
source-driven manual error analysis we have performed using a dedicated “test suite” that contains
selected examples of relevant phenomena. While automatic scores show huge differences
between the engines, the overall average number or errors they (do not) make is very similar for
all systems. However, the detailed error breakdown shows that the systems behave very differently
concerning the various phenomena.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje VPS-GradeUp, sadu 11 400 manuálně anotovaných ohodnocení vzorů užití 29 anglických sloves z Pattern Dictionary of English Verbs Patricka Hankse.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present VPS-GradeUp ― a set of 11,400 graded human decisions on usage patterns of 29 English lexical verbs from the Pattern Dictionary of English Verbs by Patrick Hanks. The annotation contains, for each verb lemma, a batch of 50 concordances with the given lemma as KWIC, and for each of these concordances we provide a graded human decision on how well the individual PDEV patterns for this particular lemma illustrate the given concordance, indicated on a 7-point Likert scale for each PDEV pattern. With our annotation, we were pursuing a pilot investigation of the foundations of human clustering and disambiguation decisions with respect to usage patterns of verbs in context. The data set is publicly available at http://hdl.handle.net/11234/1-1585.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje metodu automatické identifikace deverbativ na základě slovníků a manuálně a automaticky anotovaných korpusů. Metoda je evaluována na novém testovacím korpusu, který je rovněž zveřejněn.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present an attempt to automatically identify Czech deverbative nouns using several methods that use large corpora as well as existing lexical resources. The motivation for the task is to extend a verbal valency (i.e., predicate-argument) lexicon by adding nouns that share the valency properties with the base verb, assuming their properties can be derived (even if not trivially) from the underlying verb by deterministic grammatical rules. At the same time, even in inflective languages, not all deverbatives are simply created from their underlying base verb by regular lexical derivation processes. We have thus developed hybrid techniques that use
both large parallel corpora and several standard lexical resources. Thanks to the use of parallel
corpora, the resulting sets contain also synonyms, which the lexical derivation rules cannot get. For evaluation, we have manually created a gold dataset of deverbative nouns linked to 100 frequent Czech verbs since no such dataset was initially available for Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje vyhledávací systém pro spojené vyhledávání ve dvojjazyčném slovníku CzEngVallex a v paralelním anglicko-českém korpusu PCEDT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper and the associated system demo, we present an advanced search system that allows
to perform a joint search over a (bilingual) valency lexicon and a correspondingly annotated
linked parallel corpus. This search tool has been developed on the basis of the Prague Czech-English Dependency Treebank, but its ideas are applicable in principle to any bilingual
parallel corpus that is annotated for ependencies and valency (i.e., predicate-argument structure), and where verbs are linked to appropriate entries in an associated valency lexicon. Our online search tool consolidates more search interfaces into one, providing expanded
structured search capability and a more efficient advanced way to search, allowing users to
search for verb pairs, verbal argument pairs, their surface realization as recorded in the lexicon, or for their surface form actually appearing in the linked parallel corpus. The search system is currently under development, and is replacing our current search tool available at
http://lindat.mff.cuni.cz/services/CzEngVallex, which could search the lexicon but the queries cannot take advantage of the underlying corpus nor use the additional surface form information from the lexicon(s). The system is available as open source.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Recognizing textual entailment is typically considered as a binary decision task – whether a text T entails a hypothesis H. Thus, in case of a negative answer, it is not possible to express that H is almost entailed by T. Partial textual entailment provides one possible approach to this issue. This paper presents an attempt to use word2vec model for recognizing partial (faceted) textual entailment. The proposed approach does not rely on language dependent NLP tools and other linguistic resources, therefore it can be easily implemented in different language environments where word2vec models are available.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Recognizing textual entailment is typically considered as a binary decision task – whether a text T entails a hypothesis H. Thus, in case of a negative answer, it is not possible to express that H is “almost entailed” by T. Partial textual entailment provides one possible approach to this issue. This paper presents an attempt to use word2vec model for recognizing partial (faceted) textual entailment. The proposed approach does not rely on language dependent NLP tools and other linguistic resources, therefore it can be easily implemented in different language environments where word2vec models are available.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Sentiment analysis neboli automatická extrakce subjektivních názorů z textu je v současnosti jedním z hlavních témat počítačového zpracovávání přirozeného jazyka. V příspěvku bude shrnut současný stav bádání v této oblasti na českých datech. Obšírněji představím základní metody vytěžování emocí z textu, datové zdroje pro češtinu i specifika českých dat a přiblížím konkrétní aplikace a use cases např. z oblasti internetového marketingu, monitoringu sociálních sítí apod.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Sentiment analysis or automatic extraction of subjective opinions from a written texts is currently one of the major topics in NLP. In this talk, I will introduce the state-of-the-art methods and approaches to sentiment analysis on Czech data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ignite talk na téma automatické detekce emocí, základní přístupy a datové zdroje.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Ignite talk on automatic detection of emotions, basic approaches, data resources overview.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pokud mají dostatek trénovacích dat, umějí stroje rozpoznat emoce v psaném textu. Je tomu skutečně tak? A jak se řeší ironie?</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>If well trained, computers can count emotions in texts. Is that so? And what about irony?</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Současná počítačová lingvistika zažívá masový zájem o postojovou analýzu jakožto nástroj zjišťování veřejného mínění. Je ale možné rozlišit dobro a zlo pomocí statistických metod? Jak zacházet s ironií, idiomy či vulgarismy? Jsou emoce jazykově nezávislé? V příspěvku zodpovím tyto otázky a popíšu současné přístupy k postojové analýze.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Current computational linguistics witnesses a massive increase of interest in sentiment analysis, as it is a powerful means of public opinion mining. However, is it even possible to distinguish between good and evil using statistical methods? How would you treat irony, idioms, innovative vulgarisms and other inherent elements of natural language? Are emotions language-independent? I will share my thoughts and opinions towards these issues, describing the state-of-the art approaches employed in sentiment analysis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Objem textových dat stále roste, rozvoj Webu 2.0 přináší množství textů generovaných samotnými uživateli Internetu. Jejich příspěvky nezřídka obsahují subjektivní názory, emoce, hodnocení… K čemu a jak můžeme tato data použít? Je možné emoce v textu spolehlivě strojově třídit? 
V přednášce si představíme základní prostředky emocionálního vyjadřování v češtině na všech rovinách popisu jazyka a popíšeme metody automatického zpracovávání emocionálně laděných textů i jejich praktické aplikace z oblasti internetového marketingu, monitoringu sociálních sítí apod.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The volume of text data is increasing constantly, the rise of Web 2.0 brings lots of data created by the Internet users themselves. How do we categorize these data automatically with respect to emotions they express? In this talk, I will introduce basic means of emotional language and methods we use to process it automatically.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce si představíme základní prostředky emocionálního vyjadřování v češtině na všech rovinách lingvistického popisu: zaměříme se na stránku lexikální (slovník emocionálních výrazů pro češtinu, evaluativní idiomy, vulgarismy), gramatickou (význam jednotlivých slovních druhů, typické syntaktické vzorce hodnotících vět) a především sémantickou a pragmatickou (ironie, sarkasmus). Popíšeme také základní metody automatického zpracovávání emocionálně laděných textů a jejich praktické aplikace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this talk, I will introduce basic means of Czech emotional language on all the levels of linguistic description. I will focus on both grammar and lexicon and semantics and pragmatics of evaluative utterances.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Může mít umělá inteligence vlastní úsudek? A jak dokážeme počítačově odhalit objektivní či subjektivní názor? Přednáška shrne základní přístupy k objektivitě a subjektivitě v textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Can artificial intelligence have an opinion? And how do we distinguish between subjective and objective stances using computer linguistic? This talk introduces basic approaches to these problems.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek představuje projekt Universal Dependencie a konkrétní případ jeho využití v oblasti sentiment analysis.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper is to present a unique project of parallel treebank annotation and a case study of its practical use. We describe the project of Universal Dependencies, explain the methodological approach, illustrate the basic principles of parallel annotation and introduce the universal annotation scheme. Also, we provide a contrastive syntactic analysis of evaluative sentences in typologically different languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce si představíme základní prostředky emocionálního vyjadřování v češtině na všech rovinách lingvistického popisu: zaměříme se na stránku lexikální (slovník emocionálních výrazů pro češtinu, evaluativní idiomy, vulgarismy), gramatickou (význam jednotlivých slovních druhů, typické syntaktické vzorce hodnotících vět) a především sémantickou a pragmatickou (ironie, sarkasmus). Popíšeme také základní metody automatického zpracovávání emocionálně laděných textů a jejich praktické aplikace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this talk, we introduce the basic means of expressing emotions in Czech on all levels of the linguistic description.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Poster představuje tvorbu pravidel pro určení cílů hodnocení založenou na evaluativních strukturách automaticky vyhledaných v Pražském závislostním korpusu. Tato pravidla jsme zkombinovali s metodami strojového učení, konkrétně s conditional random fields.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This poster shows how we created the rules for  opinion  target  identification  based  on  evaluative  structures automatically found in Prague Dependency Treebank and  combined  them  with  machine  learning  methods, namely linear-chain conditional random fields.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Česko-anglické ruční zarovnání slov. Česko-anglické ruční zarovnání slov.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Czech-English Manual Word Alignment. Czech-English Manual Word Alignment.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku budeme porovnávat delexicalized přenos a minimálně pod dohledem rozebrat techniky na 32 různých jazyků od Universal závislostí korpusu sbírky. Minimální dohled při přidávání pouštět univerzální gramatická pravidla pro POS tagy. Pravidla jsou začleněny do nekontrolované závislost parseru ve formách externích dřívějších pravděpodobnostech. Také jsme experimentovat s učit se toto pravděpodobností z jiných stromových korpusů. Průměrná připevnění skóre našeho parseru je o něco nižší než v delexicalized přenosu analyzátor, nicméně, to funguje lépe pro jazyky z méně zdroji jazykových rodin (non-Indo-Evropan), a proto je vhodný pro ty, pro které stromových korpusů často neexistují.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we compare delexicalized transfer and minimally supervised parsing techniques on 32 different languages from Universal Dependencies treebank collection. The minimal supervision is in adding handcrafted universal grammatical rules for POS tags. The rules are incorporated into the unsupervised dependency parser in forms of external prior probabilities. We also experiment with learning this probabilities from other treebanks. The average attachment score of our parser is slightly lower then the delexicalized transfer parser, however, it performs better for languages from less resourced language families (non-Indo-European) and is therefore suitable for those, for which the treebanks often do not exist.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku představujeme náš nový experimentální systém sloučení závislost
reprezentace dvou rovnoběžných vět
do jednoho strom závislostí. Všechny vnitřní uzly v závislosti stromu představují zdroj-cílové páry slovy, další slova jsou ve formě koncové uzly. Používáme Univerzální Závislosti anotace styl, ve kterém funkční slova, jejichž použití se často liší mezi jazyky, jsou zaznamenány jako listy.
Paralelní korpus je analyzován v minimálně dohlíží způsobem. Nezarovnaný slova jsou zde automaticky tlačil na povrch listů. Představujeme jednoduchý systém překladu vyškoleného na takových sloučených stromech a vyhodnocovat jej WMT 2016 anglicko-to-český a česko-to-anglický překlad úloh. I přesto, že model je doposud velmi jednoduché a byl používán žádný jazykový model a model word-li řazení varianta Český k angličtině dosáhl podobného Bleu skóre jako další zavedeného systému stromu bázi.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In  this  paper,  we  present  our  new  experimental system of merging dependency
representations  of  two  parallel  sentences
into  one  dependency  tree. All  the  inner  nodes  in  dependency  tree  represent source-target  pairs  of  words,   the  extra words are in form of leaf nodes.  We use Universal Dependencies annotation style, in  which  the  function  words,  whose  usage  often  differs  between  languages,  are annotated  as  leaves.
The  parallel  treebank  is  parsed  in minimally  supervised way. Unaligned  words  are  there  automatically  pushed  to  leaves. We  present a  simple  translation  system  trained  on such   merged   trees   and   evaluate   it   in WMT 2016 English-to-Czech and Czech-to-English  translation  task. Even though the model is so far very simple and no language  model  and  word-reordering  model were  used,  the  Czech-to-English  variant reached similar BLEU score as another established tree-based system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V posledních 12 letech došlo k velkým pokroku v oblasti bez dozoru závislostní parsování. Různé přístupy však někdy liší v motivaci a ve vymezení problému. Některé z nich umožňují využití zdrojů, které jsou zakázány jinými, neboť se s nimi zachází jako druh dohledu. Cílem tohoto příspěvku je definovat všechny varianty bez dozoru závislost rozebrat problém a ukázat jejich motivace, pokrok, a nejlepší výsledky. Také jsme diskutovali o užitečnosti látky jako součásti bez dozoru analýze obecně, a to jak pro formální lingvistiky a pro aplikace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the last 12 years, there has been a big progress in the field of unsupervised dependency parsing. Different approaches however sometimes differ in motivation and definition of the problem. Some of them allow using resources that are forbidden by others, since they are treated
as a kind of supervision. The goal of this paper is to define all the variants of unsupervised dependency parsing problem and show their motivation, progress, and the best results. We also discuss the usefulness of the unsupervised
parsing generally, both for the formal linguistics and for the applications.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Texty ve 107 jazycích z korpusu W2C (http://hdl.handle.net/11858/00-097C-0000-0022-6133-9), první 1000000 tokenů pro každý jazyk, označkované delexikalizovaným taggerem popsaným v Yu et al. (2016, LREC, Portorož, Slovenia).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Texts in 107 languages from the W2C corpus (http://hdl.handle.net/11858/00-097C-0000-0022-6133-9), first 1,000,000 tokens per language, tagged by the delexicalized tagger described in Yu et al. (2016, LREC, Portorož, Slovenia).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nedokončené zaměřená na těžbu překladové páry zdrojové a cílové závislost treelets které mají být použity v systému strojového překládání závislostí na bázi. Představíme nový voze metodu pro paralelní segmentaci stromu na základě vzorků Gibbs. S použitím dat z české, anglické paralelním korpusu, ukážeme, že postup konverguje ke slovníku, který obsahuje poměrně velké treelets; V některých případech se zdá, že segmentace mít zajímavé lingvistické výklady.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a work in progress aimed at extracting translation pairs of source and target dependency treelets to be used in a dependency-based machine translation system. We introduce a novel unsupervised method for parallel tree segmentation based on Gibbs sampling. Using the data from a Czech-English parallel treebank, we show that the procedure converges to a dictionary containing reasonably sized treelets; in some cases, the segmentation seems to have interesting linguistic interpretations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme novou datovou sadu pro generování jazyka v hlasových dialogových systémech, která spolu s každou odpovědí systému k vygenerování (pár zdrojová sémantická reprezentace – cílová věta v přirozeném jazyce) uvádí i předcházející kontext (uživatelský dotaz). Očekáváme, že tento kontext dovolí generátorům jazyka adaptovat se na způsob vyjadřování uživatele a tím docílit přirozenějších a potenciálně úspěšnějších odpovědí. Datová sada byla vytvořena za pomoci crowdsourcingu v několika fázích, aby bylo možno získat přirozené uživatelské dotazy a odpovídající přirozené, relevantní a kontextově zapojené odpovědi systému. Datová sada je dostupná online pod otevřenou licencí Creative Commons 4.0 BY-SA.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a novel dataset for natural language generation (NLG) in spoken dialogue systems which includes preceding context (user utterance) along with each system response to be generated, i.e., each pair of source meaning representation and target natural language paraphrase. We expect this to allow an NLG system to adapt (entrain) to the user’s way of speaking, thus creating more natural and potentially more successful responses. The dataset has been collected using crowdsourcing, with several stages to obtain natural user utterances and corresponding relevant, natural, and contextually bound system responses. The dataset is available for download under the Creative Commons 4.0 BY-SA license.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nový generátor přirozeného jazyka pro hlasové dialogové systémy, který je schopný přizpůsobit se způsobu, jakým mluví uživatel, a poskytovat odpovědi přiměřené kontextu dialogu. Generátor je založen na neuronových sítích a přístupu sequence-to-sequence. Je plně trénovatelný z dat, která spolu s trénovacími výstupy generátoru obsahují také předchozí kontext. Ukazujeme, že kontextový generátor přináší signifikantní zlepšení oproti základnímu generátoru, a to jak z pohledu automatických metrik, tak v preferenčním testu lidského hodnocení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a novel natural language generation system for spoken dialogue systems capable of entraining (adapting) to users' way of speaking, providing contextually appropriate responses. The generator is based on recurrent neural networks and the sequence-to-sequence approach. It is fully trainable from data which include preceding context along with responses to be generated. We show that the context-aware generator yields significant improvements over the baseline in both automatic metrics and a human pairwise preference test.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Datová sada pro plně trénovatelné generátory jazyka v hlasových dialogových systémech, která pokrývá doménu anglických informací o veřejné dopravě. 
Spolu s každou datovou položkou (pár zdrojové reprezentace významu a věty v přirozeném jazyce jako cíl generování) obsahuje i předcházející kontext (uživatelův dotaz, který má systém egnerovanou větou zodpovědět). Zohlednění formy předchozího dotazu pro generování umožní generátorům natrénovaným na této datové sadě adaptovat se na předchozí dotazy, tj. používat stejné shodné výrazy a syntaktické konstrukce jako uživatel dialogového systému. Předpokládáme, vygenerované věty tak budou vnímány jako přirozenější, což může vést i k úspěšnějším dialogům.
Pro získání přirozených uživatelských dotazů i odpovědí systému byla použita metoda crowdsourcingu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A dataset intended for fully trainable natural language generation (NLG) systems in task-oriented spoken dialogue systems (SDS), covering the English public transport information domain. 
It includes preceding context (user utterance) along with each data instance (pair of source meaning representation and target natural language paraphrase to be generated). Taking the form of the previous user utterance into account for generating the system response allows NLG systems trained on this dataset to entrain (adapt) to the preceding utterance, i.e., reuse wording and syntactic structure. This should presumably improve the perceived naturalness of the output, and may even lead to a higher task success rate.
Crowdsourcing has been used to obtain natural context user utterances as well as natural system responses to be generated.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme generátor přirozeného jazyka založený na metodě sequence-to-sequence, který lze natrénovat tak, aby z dialogových aktů produkoval buď věty v přirozeném jazyce, nebo hloubkově syntaktické závislostní stromy. Na generátoru potom přímo porovnáváme dvoufázové generování, které používá oddělené větné plánování a povrchovou realizaci, s jednofázovým přístupem.

Obě nastavení generátoru jsme byli schopni natrénovat s použitím velmi malého množství trénovacích dat. Jednofázové generování dosahuje lepších výsledků; překonává nejlepší předchozí výsledek podle n-gramových automatických metrik a zároveň nabízí relevantnější výstupy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a natural language generator based on the sequence-to-sequence approach that can be trained to produce natural language strings as well as deep syntax dependency trees from input dialogue acts, and we use it to directly compare two-step generation with separate sentence planning and surface realization stages to a joint, one-step approach.

We were able to train both setups successfully using very little training data. The joint setup offers better performance, surpassing state-of-the-art with regards to n-gram-based scores while providing more relevant outputs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Diateze představují vztahy mezi různými povrchověsyntaktickými strukturami sloves. Jsou podmíněny změnou morfologické charakteristiky slovesného rodu a jsou spojeny se změnami v povrchověsyntaktickém vyjádření valenčních doplnění sloves. V příspěvku jsme se zaměřili na specifické změny, kterým podléhají valenční doplnění českých funkčních sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Diatheses represent relations between different surface syntactic structures of verbs. They are conditioned by changes in morphological meaning of a verb and they are associated with specific changes in its valency structure. In this contribution, I discussed changes in the surface syntactic expressions of Czech complex predicates.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá distribucí Konatele v konstrukcích komplexních predikátů s kategoriálními slovesy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The contribution discusses the distribution of ACTors in light verb constructions in Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kapitola popisuje syntaktickou strukturu českých komplexních predikátů s funkčním slovesem v aktivu. Vymezuje čtyři typy komplexních predikátů. Navrhuje pravidlový popis daných jazykových jevů a jejich zachycení ve slovníku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This chapter describes the syntactic structure of Czech complex predicates with light verbs in active voice. Four types of Czech complex predicates are distinguished. Their rules based description and representation in the valency lexicon are proposed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku navrhujeme lexikografickou reprezentaci komplexivních predikátů v češtině. Zvláštní oddíl je věnován výběru kolokací funkčních sloves a predikativních jmen a anotaci jejich syntaktické struktury.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this contribution, we propose a lexicographic representation of Czech complex predicates. Special attention is devoted to the selection of collocations of function verbs and predicative nouns and to the annotation of their syntactic structure.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku se zaměřujeme na české složené predikáty, které jsou tvořené lehkým slovesem a predikativním jménem vyjádřeným jako přímý objekt. Ačkoli čeština -- jakožto inflekční jazyk -- poskytuje výbornou příležitost pro studium distribuce valenčních doplnění složených predikátů, tato distribuce dosud nebyla poplsána. Na základě manuální analýzy bohatě anotovaných dat PDT formulujeme poučky řídicí tuto distribuci. V automatickém experimentu tyto poučky ověřujeme na korektních syntaktických strukturách PDT a PCEDT s velmi uspokojivými výsledky: distribuce 97% valenčních doplnění se řídí navrženými poučkami. Tyto výsledky dokládají, že vytváření povrchové struktury složených predikátů je pravidelný proces.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we focus on Czech complex predicates formed by a light verb and a predicative noun expressed as the direct object. Although Czech – as an inflectional language encoding syntactic relations via morphological cases – provides an excellent opportunity to study the distribution of valency complements in the syntactic structure with complex predicates, this distribution has not been described so far. On the basis of a manual analysis of the richly annotated data from the Prague Dependency Treebank, we thus formulate principles governing this distribution. In an automatic experiment, we verify these principles on well-formed syntactic structures from the Prague Dependency Treebank and the Prague Czech-English Dependency Treebank with very satisfactory results: the distribution of 97% of valency complements in the surface structure is governed by the proposed principles. These results corroborate that the surface structure formation of complex predicates is a regular process.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zvýšený zájem o ‚porozumění‘ přirozeným jazykům způsobil, že do centra pozornosti současného výzkumu se dostávají různé postupy, často popisované jako ‚sémantická analýza‘.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Increased interest in natural language ‘understanding’ has brought into the focus of much current work a variety of techniques often described as ‘semantic parsing’.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Předkládáme systém pro rozpoznávání pojmenovaných entit, který je jazykově nezávislý a nepotřebuje klasifikační rysy pro strojové učení. Systém využívá současných výsledků v oblasti umělých neuronových sítí, jako jsou parametric rectified linear units (PReLU), embeddingy slov a embeddingy charakterů ve slovech založené na gated linear units (GRU). Systém nepotřebuje vyhledávání vhodné sady klasifikačních rysů (feature engineering) a pouze s využitích povrchových forem, lemmat a slovních druhů na vstupu dosahuje vynikajících výsledků v rozpoznávání pojmenovaných entit v češtině a překonává stávající výsledky dříve publikovaných prací, které využívají ručně vytvořené klasifikační rysy založené na ortografické podobnosti slov. Navíc tato síť podává robustní výkon i v případě, kdy jsou na vstupu pouze povrchové formy. Síť dovede využít navíc i kombinaci ručně vytvořených klasifikačních rysů a v tom případě překonává stávající výsledky s markantním rozdílem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a completely featureless, language agnostic named entity recognition system. Following recent advances in artificial neural network research, the recognizer employs parametric rectified linear units (PReLU), word embeddings and character-level embeddings based on
gated linear units (GRU). Without any feature engineering, only with surface forms, lemmas and tags as input, the network achieves excellent results in Czech NER and surpasses the current state of the art of previously published Czech NER systems, which use manually designed rule-based orthographic classification features. Furthermore, the neural network achieves robust results even when only surface forms are available as input. In addition, the proposed neural network can use the manually designed rule-based orthographic classification features and in such combination, it exceeds the current state of the art by a wide margin.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Závislostní parsery jsou obvykle vyhodnocovány pomocí podílu správně zavěšených uzlů, toto skóre ale neříká nic o užitečnosti daného parseru pro koncové aplikace, v nichž slouží jako komponenta. V této kapitole se zabývá tím, jak rozdílné parsery a jejich různé kombinace přispívají ke kvalitě strojového překladu založeného na syntaxi. Překládáme výsledky pro několik základních typů parserů a pro různé metody jejich kombinace. Ukazujeme korelace se standardními metrikami pro vyhodnocování kvality strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Dependency parsers are almost ubiquitously evaluated on their accuracy
scores, these scores say nothing of the complexity and usefulness of the
resulting structures. As dependency parses are basic structures in which
other systems are built upon, it would seem more reasonable to judge
these parsers down the NLP pipeline. In this chapter, we will discuss
how different forms and different hybrid combinations of dependency
parses effect the overall output of Syntax-Based machine translation both
through automatic and manual evaluation. We show results from a variety
of individual parsers, including dependency and constituent parsers, and
describe multiple ensemble parsing techniques with their overall effect
on the Machine Translation system. We show that parsers’ UAS scores
are more correlated to the NIST evaluation metric than to the BLEU
Metric, however we see increases in both metrics. To truly see the effect
of hybrid dependency parsers on machine translation, we will describe
and evaluate a combined resource we have released, that contains gold
standard dependency trees along with gold standard translations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme SubGram, rozšíření Skip-gram modelu, které používá podřetězce slov během trénování reprezentace slov.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Skip-gram (word2vec) is a recent method for creating vector representations of words (“distributed word representations”) using a neural network. The representation gained popularity in various areas of natural language processing, because it seems to capture syntactic and semantic information about words without any explicit supervision in this respect.
We propose SubGram, a refinement of the Skip-gram model to consider also the word structure during the training process, achieving large gains on the Skip-gram original test set.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nové anotační schéma závislostní syntaxe pro ruštinu, založené na formalismu Universal Dependencies a aplikované na korpus SynTagRus.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a new annotation schema of dependency syntax for Russian. The schema is based on the Universal Dependencies formalism. It has been applied to the SynTagRus corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku popisujeme náš postup aplikovaný v benchmarku TRECVID 2015 v úlohe Video Hyperlinking. Náš prístup kombinuje podobnosť textov vypočítanú z titulkov, vizuálnu podobnosť medzi zábermi vypočítanú pomocou Feature Signatures a informáciou o tom, či query segment a vyhľadaný segment pochádzajú z toho istého televízneho programu. Všetky experimenty boli odladené a otestované na kolekcii 2500 hodín televíznych programov poskytnutých BBC.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present our approach used in the TRECVID 2015 Video Hyperlinking Task. Our approach combines text-based similarity calculated on subtitles, visual similarity between keyframes calculated using Feature Signatures, and preference whether the query
and retrieved answer come from the same TV series. All experiments were tuned and tested on about 2500 hours of BBC TV programmes.
Our Baseline run exploits fixed-length segmentation, text-based retrieval of subtitles, and query expansion which utilizes metadata, context, in-formation about music and artist contained in the query segment and visual concepts. The Series run combines the Baseline run with weighting based on information whether the query and data segment come from the same TV series. The FS run combines the Baseline run with the similarity between query and data keyframes calculated using Feature Signatures. The FSSeriesRerank run is based on the FS run on which we applied reranking which, again, uses information about the TV series. The Series run significantly outperforms the FSSeriesRerank run. Both these runs are significantly inferior to our Baseline run in terms of all our reported measures. The FS run outperforms the Baseline run in terms of all measures but it is significantly better than the Baseline run only in terms of the MAP score. Our test results confirm that employment of visual similarity can improve video retrieval based on information contained in subtitles but information about TV series which was most helpful in our training experiments did not lead to further improvements.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článok popisuje systém SHAMUS určený na jednoduché vyhľadávanie a navigáciu v multimediálnych archívoch. Systém pozostáva z troch komponentov. Search poskytuje textové vyhľadávanie v mutlimediálnej kolekcii, Anchoring automaticky detekuje najvýznamnejšie segmenty videa a sémanticky súvisiace segmenty sú vyhľadané v komponente Hyperlinking. V článku popisujeme jednotlivé komponenty systému, ako aj online demo rozhranie, ktoré pracuje s kolekciou videí z konferencie TED.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we describe SHAMUS, our system for an easy search and navigation in multimedia archives. The system consists of three components. The Search component provides a text-based search in a multimedia collection, the Anchoring component determines the most important segments of videos, and segments topically related to the anchoring ones are retrieved by the Hyperlinking component. In the paper, we describe each component of the system as well as the online demo interface which currently works with a collection of TED talks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje práci na strojovém překladu provedenou v rámci projektu KConnect, financovaném v rámci programu H2020 a zaměřeném na vývoj a komercializaci cloudových služeb pro vícejazyčnou sémantickou anotaci, sémantické vyhledávání a strojový překlad elektronických zdravotních záznamů a medicínských publikací. Nejprve prezentujeme hlavní cíl a úlohu strojového překladu v projektu, následně stručně popisujeme hlavní metody a komponenty vyvinuté v rámci projektu, mj. získání dat, metody doménové adaptace, nasazení MT jako cloudové webové služby a nástroj pro trénování překladových systémů, který umožňuje snadnou adaptaci překladové služby na nové jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the work on Machine Translation (MT) that has been conducted within the KConnect project, funded under the H2020 programme and focused on development and commercialization of cloud-based services for multilingual Semantic Annotation, Semantic Search and Machine Translation of electronic health records and medical publications. We first present the main goal and role of MT in the project and then briefly describe the main methods and components developed in the project, including training data acquisition, methods of domain adaptation, deployment of MT as cloud-based web-service, and the training toolkit allowing easy adaptation of the MT service to new languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Na základě objednávky komerčního partnera ACREA CR byl implementován software pro binární klasifikaci dokumentů v ruštině na dokumenty relevantní nebo nerelevantní vůči tématu zadanému příklady relevantních dokumentů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Given the specification of ACREA CR, a commercial partner of UFAL, a software for binary classification of documents written in the Russian language was implemented. New documents are classified as relevant or non-relevant with respect to a given topic, while the topic is specified using a training dataset of documents that are relevant for the topic.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>The aim of our talk is to present results of quantitative analysis of dependency characteristics of particular analytical functions. For each word in a syntactically annotated corpus (Bejček et al. 2013), a dependency frame is derived first. The dependency frame consists of all analytical functions assigned to its directly dependent words.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Cílem prezentace je kvantitativní analýza syntaktických závislostí v češtině. Pro každé slovo syntakticky anotovaného korpusu (Bejček et al. 2013) odvozujeme závislostní rámec , který je tvořen analytickými funkcemi přiřazenými každému bezprostředně závislému slovu.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem je podat snadno použitelné webové aplikace speciálně vyvinuté pro anotaci ruských textů s morfologickou a syntaktickou informaci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of this poster is to present an easy-to-use web application specifically developed for annotating Russian texts with morphological and syntactic information.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato zpráva představuje ruský korpus anotovaný podle standardu Universal Dependencies a popisuje postup, kterým byl existující ruský závislostní korpus SynTagRus transformován do stylu UD.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This report presents the Universal Dependencies (UD) annotated corpus for Russian and a conversion process which was developed to transform the SynTagRus dependency treebank of Russion into a UD-style annotated corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zaměřuje na generování pádových příznaků ve strojovém překladu do jazyků s volným slovosledem. Navrhujeme několik pravidel identifikujících vhodná místa pro takové příznaky při překladu z angličtiny do urdštiny. Výsledky ukazují zlepšení až 1 bod BLEU.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper focuses on the generation of case markers for free word order languages that use case markers as phrasal clitics for marking the relationship between the dependent- noun and its head. The generation of such clitics becomes essential task especially when translating from fixed word order languages where syntactic relations are identified by the positions of the dependent-nouns. To address the problem of missing markers on source-side, artificial markers are added in source to improve alignments with its target counterparts. Up to 1 BLEU point increase is observed over the baseline on different test sets for English-to-Urdu.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek shrnuje výsledky soutěže v optimalizaci parametrů pevně daného překladového systému. V letošní variantě soutěže jde o systém opírající se o velká trénovací data.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the
WMT16 Tuning Shared Task. We provided
the participants of this task with a
complete machine translation system and
asked them to tune its internal parameters
(feature weights). The tuned systems were
used to translate the test set and the outputs
were manually ranked for translation
quality. We received 4 submissions in the
Czech-English and 8 in the English-Czech
translation direction. In addition, we ran
2 baseline setups, tuning the parameters
with standard optimizers for BLEU score.
In contrast to previous years, the tuned
systems in 2016 rely on large data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Hlavní témata přednášky: (1) užití orální historie jako zdroje a tématu výzkumu; (2) rozmanité bariéry v tomto kontextu (epistemologické, metodologické, technické); (3) možnosti a budoucí příležitosti, nové výzkumné směřování.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Key themes of the talk were: (1) using oral history as topic and resource; (2) different barriers in doing so (epistemological, methodological, technical); (3) imagined possibilities, future opportunities and 
new research directions. Some of the current barriers seem to result from the fragmentary nature of social/human sciences, which is hard to overcome. At the same time, facilitating the use of ICT provides enviroment for interdisciplinary cooperation, which often leads to interesting and ground-breaking research.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kapitola popisuje Archiv vizuální historie USC Shoah Foundation, jeho obsah relevantní s ohledem na tematické zaměření knihy, a některé metodologické souvislosti sekundární analýzy orálněhistorických nahrávek.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The chapter describes USC Shoah Foundation's Visual History Archive, its relevant content for the thematic focus of the book, and some methodological issues of secondary analysis of oral history recordings.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V práci popisujeme syntaktický přístup zpracovaní asociační anafory. Představujeme nově vytvořený korpus RuGenBridge  s anotací tzv. genitivní asociační anafory v ruštině. Srovnáváme výsledky anotace se statistyky a typy anotovaných vztahů v PDT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present a syntactic approach to the annotation of bridging relations, so-called genitive bridging. We introduce the RuGenBridge corpus for Russian annotated with genitive bridging and compare it to the semantic approach that was applied in the Prague Dependency Treebank for Czech. We discuss some special aspects of bridging resolution for Russian and specifics of bridging annotation for languages where definite nominal groups are not as frequent as e.g. in Romance and Germanic languages. To verify the consistency of our method, we carry out two
comparative experiments: the annotation of a small portion of our corpus with bridging relations according to both approaches and finding for all relations from the RuGenBridge their semantic interpretation that would be annotated for Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jedna z našich čtyř anglických reprezentací (DM) a kompletní česká data (v reprezentaci PSD) nejsou odvozené od anotací licencovaných LDC, a tudíž mohou být poskytnuta k přímému stažení (Open SDP; verze 1.1; duben 2016) pod svobodnější licencí, konkrétně Creative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA). Tento balíček také obsahuje některé bohatší reprezentace významu, ze kterých jsou anglické bilexikální DM grafy odvozeny, totiž logické formy a abstraktnější nelexikalizované sémantické sítě. Ty jsou formálně (když už ne lingvisticky) podobné abstraktní reprezentaci významu (AMR) a jsou k dispozici v různých formátech, včetně syntaxe podobné AMR.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>One of the four English target representations (viz. DM) and the entire Czech data (in the PSD target representation) are not derivative of LDC-licensed annotations and, thus, can be made available for direct download (Open SDP; version 1.1; April 2016) under a more permissive licensing scheme, viz. the Creative Common Attribution-NonCommercial-ShareAlike scheme (CC BY-NC-SA 2.0). This package also includes some ‘richer’ meaning representations from which the English bi-lexical DM graphs derive, viz. scope-underspecified logical forms and more abstract, non-lexicalized ‘semantic networks’. The latter of these are formally (if not linguistically) similar to Abstract Meaning Representation (AMR) and are available in a range of serializations, including in AMR-like syntax.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>SDP 2014 &amp; 2015: Sémantická závislostní analýza se širokým pokrytím sestává z dat, nástrojů, systémových výstupů a publikací spojených se soutěžemi v sémantické závislostní analýze (SDP) v letech 2014 a 2015, které byly pořádány spolu s mezinárodním workshopem SemEval. Tento balíček dat a softwarových nástrojů byl připraven organizátory soutěží SDP.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>SDP 2014 &amp; 2015: Broad Coverage Semantic Dependency Parsing consists of data, tools, system results, and publications associated with the 2014 and 2015 tasks on Broad-Coverage Semantic Dependency Parsing (SDP) conducted in conjunction with the International Workshop on Semantic Evaluation (SemEval) and was developed by the SDP task organizers.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek zkoumá podobnost aplikace atributových gramatik ve dvou zdánlivě odlišných vědeckých oblastech, jmenovitě ve formálním popisu pracovních postupů a v kontrole syntaktické správnosti přirozených jazyků. Jsou použity existující modely a formalismy a hledá se společný jmenovatel, který by umožnil využít zkušenosti nabyté v obou oblastech. Zároveň také ukazuje nutnost mírné adaptace formalismu pro kontrolu gramatiky pro jazyky s vysokým stupněm volnosti slovosledu, která by mohla vést k vytvoření nástroje použitelného pro kontrolu pracovních postupů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper investigates the similarities in the application of attribute grammars to two seemingly different research areas, namely the area of formal description of workflows and the area of checking the syntactic correctness
of natural languages. It uses existing models
and formalisms and tries to find a common ground
which would enable to exploit mutually the experience gained in both individual fields. It shows how a slight adaptation of a grammar formalism used for grammar checking of languages with a high degree of word-order freedom may lead to a tool useful for a workflow verification.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nedávné výsledky ukázaly, že vhodným nástrojem popisu procesních toků jsou atributové gramatiky. Ty v minulosti posloužily také jako vhodný prostředek implementace kontroly gramatické správnosti přirozených jazyků. Článek popisuje způsob, jak by bylo možné validovat, zda daný procesní tok je správně sestaven podobně jako se to dělá pro věty přirozeného jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Grammar checking has been used for a long time to validate if a given sentence - a sequence of words - complies with grammar rules. Recently attribute grammars have been proposed as a formal model to describe workflows. A workflow specifies valid processes, which are sequences of actions. In this paper we show how a grammar checker developed for natural languages can be used to validate whether or not a given process complies with the workflow model expressed using an attribute grammar. The checker can also suggest possible corrections of the process to become a valid process.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve světě akademických federací identit je častým problémem neuvolňování atributů potřebných pro chod služeb.
Naše služba umožňuje sbírat statistiky o uvolňovaných atributech mezi různými poskytovateli služeb (SP) a poskytovateli identit (IDP).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the world of academic identity (inter-)federations, the release attribute problem is often mentioned. The problem is that Service Providers (SPs) do not get the attributes from Identity Providers (IdPs) they need. This service makes it possible to gather statistics about released attributes names between various SPs and IDPs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Služba pro snadnou a trvalou citaci dat vzešlých z výzkumu. Tuto službu poskytujeme formou zkracovače URL. Reprodukovatelnost výsledků, důležitá součást vědeckého výzkumu, přímo závisí na dostupnosti dat, nad kterými výzkum probíhal. Rozvoj webových technologií velmi usnadnil sdílení dat. Kvůli dynamické povaze webu se ale obsah často přesouvá z jednoho místa na druhé. Běžné URL, které může výzkumník použít k citaci svých dat, nemá prostředky, kterými by se s tímto přesouváním dalo vyrovnat. Čtenář/uživatel tak často zjistí, že data se na dané URL již nenalézají, nebo dostane k dispozici novější verzi. Námi navrhované řešení, ve kterém je zkrácené URL perzistentním identifikátorem, poskytuje spolehlivý mechanismus pro stálou dostupnost dat a může tak zlepšit dopad výzkumu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Easy-to-cite and persistent infrastructure (shortref.org) for research and data citation in the form a URL shortener service. Reproducibility of results is very important for the extension of research and is directly depends on the availability of the research data. The advancements in the web technologies made redistribution of the data much more easy nowadays, however, due to the dynamic nature of the web, the content is consistently on the move from one destination to another. The URLs researchers use for the citation of their contents do not directly account for these changes and many times when the users try to access the cited URLs, the data is either not available or moved to a newer version. In our proposed solution, the shortened URLs are not simple URLs but use persistent identifiers and provide a reliable mechanism to make the data always accessible that can directly improve the impact of research.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CLARIN centrum sa môže nazývať CLARIN-B centrum iba ak spĺňa špecifické kritériá. Digitálny repozitár LINDAT/CLARIN postavený na DSpace spĺňa väčšinu týchto požiadaviek.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This talk presents the steps required for becoming a CLARIN-B centre using LINDAT/CLARIN digital library based on DSpace.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>ConFarm je webová služba, věnovaná extrakci povrchových reprezentaci slovesnych a jmennych tvaru ze závislostních anotovaných korpusů ruských textů. V současné době je  k dispozici extrakce konstrukci s konkrétním lemmatem z korpusu SynTagRus a ruského národního korpusu. Nas systém poskytuje flexibilní rozhraní, které umožňuje uživatelům vyladit výstup. Extrahované konstrukce jsou seskupeny podle jejich obsahu, aby byla možna kompaktní reprezentace, a skupiny jsou zobrazeny ve formě grafu. ConFarm se liší od podobných existujících nástrojů pro rustinu v tom, že nabízí kompletní konstrukce, v protikladu k samostatni extrakci zavislostnich clenu nebo práci s slovních spojení, a umožňuje uživatelům objevit nečekané konstrukce, na rozdíl od vyhledavani příkladu podle formy zadane uživatelem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>ConFarm is a web service dedicated to extraction of surface representations of verb and noun constructions from dependency annotated corpora of Russian texts. Currently, the extraction of constructions with a specific lemma from SynTagRus and Russian National Corpus is available. The system provides flexible interface that allows users to fine-tune the output. Extracted constructions are grouped by their contents to allow for compact representation, and the groups are visualized as a graph in order to help navigating the extraction results. ConFarm differs from similar existing tools for Russian language in that it offers full constructions, as opposed to extracting separate dependents of search word or working with collocations, and allows users to discover unexpected constructions as opposed to searching for examples of a user-defined construction.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato práce se zabývá otázkou budování svobodného NLP potrubí pro zpracování ruské texty z prostého textu na morfologicky a syntakticky anotovaný struktury ve formátu CONLL. Potrubí je napsán v python3. Segmentace je zajišťována vlastní modul. Mystem s četnými postprocesních oprav se používá pro lemmatizace a morfologie značkování.
A konečně, syntaktická anotace se získá MaltParser využitím naší vlastní model vyškolený na SynTagRus, který byl převeden do formátu CONLL pro tento účel, s jeho morfologické tagset převádí do Mystem / ruského národního korpusu tagset</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This work addresses the issue of building a free NLP pipeline for processing Russian texts from plain text to morphologically and syntactically annotated structures in CONLL format. The pipeline is written in python3. Segmentation is provided by our own module. Mystem with numerous postprocessing fixes is used for lemmatization and morphology tagging.
Finally, syntactical annotation is obtained with MaltParser utilizing our own model trained on SynTagRus, which was converted into CONLL format for this purpose, with its morphological tagset being converted into Mystem/Russian National Corpus tagset</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Lexikální síť DeriNet verze 1.2 obsahuje 1.003.590 lexémů (z morfologického slovníku MorfFlex), mezi nimiž bylo poloautomatickými metodami vytvořeno 740.750 hran odpovídajících vztahu odvození (mezi slovem odvozeným a slovem základovým). V této verzi dat jsou nově se základovým slovem spojena i slova, při jejichž odvozování dochází k hláskovým alternacím (př. boží - bůh).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Lexical network DeriNet version 1.2 contains 1,003,590 lexemes (from the morphological dictionary MorfFlex) among which 740.750 edges were created according to derivational relations (between a derived word and its base word). In the current version, words containg consonant and/or vowel alternations were connected to their base word.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá klasifikací českých sloves s genitivním doplněním z hlediska tvoření pasivní a deagentní diateze. Slovesa jsou klasifikována na ta, která se chovají podobně jako slovesa s akuzativním doplněním,,
slovesa, u nichž je genitivní vazba v příznakových členech diateze zachována, a
slovesa, jejichž genitivní doplnění může být v diatezi vyjádřeno nominativem i genitivem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This contribution dealt with passive and deagentive diatheses of Czech verbs with genitive complementation. According to their changes in valency structure, these verbs are classified into three groups: (1) verbs that behave in the same way as verbs with accusative complementation, (2) verbs that preserve genitive complementations and (3) verbs complementations of which can be expressed in the nominative, or in the genitive.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje konverzi katalánského a španělského treebanku AnCora do formalismu Universal Dependencies (univerzální závislosti). Popisujeme proces konverze a odhadujeme kvalitu výsledného treebanku nepřímo pomocí automatické syntaktické analýzy v jednojazykovém, mezijazykovém a mezidoménovém testu. Převedené treebanky vykazují interní konzistenci srovnatelnou s původní distribucí AnCora pro CoNLL09. Od jiného, dříve vydaného španělského UD treebanku se liší zejména v repertoáru vyznačených víceslovných výrazů. Tyto dva nově převedené treebanky budou vydány v Universal Dependencies verzi 1.3.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article describes the conversion of the Catalan and Spanish AnCora treebanks to the Universal Dependencies formalism. We describe the conversion process and assess the quality of the resulting treebank in terms of parsing accuracy by means of monolingual, cross-lingual and cross-domain parsing evaluation. The converted treebanks show an internal consistency comparable to the one shown by the original CoNLL09 distribution of AnCora, and indicate some differences in terms of multiword expression inventory with regards to the already existing UD Spanish treebank. The two new converted treebanks will be released in version 1.3 of Universal Dependencies.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Provedli jsme přehled VV v existujících treebancích pomocí online průzkumu. Výsledky ukazují významné odlišnosti. Srovnání se zaměřuje na anotaci analytických predikátů a slovesných idiomů.
Na základě průzkumu navrhujeme všeobecné směrnice pro anotaci VV v treebancích. Doporučení se týkají následujících potřeb: rozlišovat VV od podobných, avšak kompozicionálních konstrukcí; vyhledávat rozličné druhy VV v treebanku; rozlišení doslovného a přeneseného významu a normalizace reprezentace VV. Průzkum vedený napříč jazyky a teoriemi je míněn jako pomůcka pro zpracování a další práci s anotovanými treebanky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>By means of an online survey, we have investigated ways in which various types of multiword expressions are annotated in existing treebanks. The results indicate that there is considerable variation in treatments across treebanks and thereby also, to some extent, across languages and across theoretical frameworks. The comparison is focused on the annotation of light verb constructions and verbal idioms. The survey shows that the light verb constructions either get special annotations as such, or are treated as ordinary verbs, while VP idioms are handled through different strategies. Based on insights from our investigation, we propose some general guidelines for annotating multiword expressions in treebanks. The recommendations address the following application-based needs: distinguishing MWEs from similar but compositional constructions; searching distinct types of MWEs in treebanks; awareness of literal and nonliteral meanings; and normalization of the MWE representation. The cross-lingually and cross-theoretically focused survey is intended as an aid to accessing treebanks and an aid for further work in treebank annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Práce se zabývá automatickou kontrolou pravopisu arabštiny a ukazuje, jak vylepšení jednotlivých komponent (slovníku, jazykového modelu, chybového modelu) vede ke kumulativnímu zlepšení celého systému.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A spelling error detection and correction application is typically based on three main components: a dictionary (or reference word list), an error model and a language model. While most of the attention in the literature has been directed to the language model, we show how improvements in any of the three components can lead to significant cumulative improvements in the overall performance of the system. We develop our dictionary of 9.2 million fully-inflected Arabic words (types) from a morphological transducer and a large corpus, validated and manually revised. We improve the error model by analyzing error types and creating an edit distance re-ranker. We also improve the language model by analyzing the level of noise in different data sources and selecting an optimal subset to train the system on. Testing and evaluation experiments show that our system significantly outperforms Microsoft Word 2013, OpenOffice Ayaspell 3.4 and Google Docs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kontrastivní popis “svůj” v češtině a “svoj” v ruštině. Korpusová data ve třijazyčném paralelním korpusu jsou analyzována se zřetelem k existujícím popisům reflexiv v každém jazyce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents the contrastive description of reflexive possessive pronouns “svůj” in Czech and “svoj” in Russian. The research concerns syntactic, semantic and pragmatic aspects. With our analysis, we shed a new light on the already investigated issue, which comes from the detailed comparison of the phenomenon of possessive reflexivization in two typologically and genetically similar languages. We show that whereas in Czech, the possessive reflexivization is mostly limited to syntactic functions and does not go beyond the grammar, in Russian it gets additional semantic meanings and moves substantially towards the lexicon. The obtained knowledge lets us explain heretofore unclear marginal uses of
reflexives in each language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáska je zaměřena na anotační nástroj, které se používají pro zaznamenávání diskurzních jevů (TrEd, PDTB, Brat, PDTB-tool, MMAX, etc.)</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This session focuses on discussing tools applied to the annotation of discourse phenomena in different languages. Following the general session on “Annotation theories and tools”, trainees will be invited to use several annotation tools, such as  TrEd, PDTB, Brat, PDTB-tool and MMAX.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Shrnutí čeho jsme dosáhli v rozborech, anotacích a analýze češtiny (PDT) a němčiny v rámci korpusu GECCo.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the talk, I summariye common and different features in the annotation approaches to Czech (PDT-like) and German (applied in GECCo).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V rámci přednásky byly představeny existující teorie anotace diskurních jevů a nástroje, které se pro tento typ anotací používají.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Discourse coherence is a complex natural language phenomenon which is achieved by different linguistic means (e.g., anaphoricity, information structure, discourse markers and connectives, rhetorical structure of text, etc.). Many approaches in computational linguistics are used to capture discourse relations and Tind practical applications. This session focuses on discussing theories and approaches applied to the annotation of discourse phenomena in different languages, such as Rhetorical Structure Theory, Penn Discourse Treebank, Segmented Discourse Representation Theory and so on. Participants will have the opportunity to compare these approaches to see which phenomena are central to them and which ones are less prominent. We will also introduce the tools of discourse annotation and demonstrate esp. TrEd, PDTB and MMAX.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>K zajímavým lingvistickým poznatkům lze dojít různými cestami: analýzou korpusových dat, srovnáním typologicky blízkých a vzdálenějších jazyků mezi sebou, použitím dotazníků nebo podnětů z odborné literatury. V plánovaném příspěvku chceme kombinací těchto přístupů  dospět  k důkladnějšímu popisu chování posesívních reflexivních zájmen v češtině a ruštině.

Rozbor anglo-česko-ruského paralelního korpusu (PCEDT-R) poukázal na značné statistické rozdíly v použití posesiv v těchto třech jazycích. Zatímco v angličtině se často používají ve funkci determinátoru při jmenné frázi, v češtině a ruštině je jejich frekvence výrazně nižší. Avšak i mezi češtinou a ruštinou se najdou značné – jak frekvenční tak funkční – rozdíly. Chování osobních a reflexivních přivlastňovacích zájmen a jejich konkurence byly dosud popisovány pro každý jazyk zvlášť. Pro češtinu jsou normativní pravidla použití reflexivního posesiva „svůj“ formulována již v Trávníčkově mluvnici (Trávníček, 1951). Pravidlům jejich skutečné distribuce v současné češtině se podrobně věnovaly např. práce F. Daneše a K. Hausenblase (1962), J. Panevové (1986) a S. Čmejrkové (1998, 2011). Konkurenci „svoj“ a „jego“ v ruštině se věnovala hlavně E. Padučevová (1983, 1985), která nabídla formálně syntaktická, sémantická a pragmatická pravidla distribuce těchto zájmen. Každá z těchto prací se však soustředí na analýzu tohoto jevu v jednom jazyce. Zde se zaměříme na porovnání výsledků, k nimž jmenovaní autoři došli a budeme hledat inspiraci pro lepší pochopení použití těchto zájmen v češtině a ruštině zvlášť.

Na první pohled se zdá, že se pravidla distribuce osobních posesiv a „svůj“ se v ruštině a češtině do značné míry shodují. Avšak dotazníky vytvořené na základě použité literatury tuto podobnost neprokazují. Zjišťujeme například, že Padučevovou navržený distributivní význam reflexivního posesiva, který je pro ruštinu úplně běžný, je v češtině zcela marginální (U každogo učenogho jesť svoja biblioteka – Každý vědec má *svou/vlastní knihovnu). Podobné je to u významu, který je nazýván 'ovladání'. Ten je pro užití ruského „svoj“ natolik produktivní, že se již vymyká pojetí frazému (Svoja kvartira lučše čem sjemnaja - *Svůj (=vlastní) byt je lepší než pronajatý.) Ruské „svoj“ má také řadu sekundárních významů, které v češtině chybí.

Analýza paralelních korpusových dat (Intercorp, PCEDT-R) ukazuje, že česká posesiva se dají častěji vynechat, jejich výskyt je tedy často fakultativní, tím se čeština od ruštiny liší. Zjistili jsme to na textech, které byly přeloženy z angličtiny, ale ověřili jsme to statisticky i na originálních textech přiměřené velikosti a tematiky (z PDT a RTB).

V referátu sjednotíme teoretické poznatky s výsledky korpusové analýzy a vymezíme společné a odlišné rysy použití osobních a reflexivních přivlastňovacích zájmen v češtině a ruštině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Comparative analysis of personal and possessive reflexives in Czech and Russian. Analysis of secondary literature.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme anotaci koreference na paralelních česko-anglických textech Pražského česko-anglického závislostního treebanku (PCEDT).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present coreference annotation on parallel Czech-English texts of the Prague Czech-English Dependency Treebank (PCEDT). The paper describes innovations made to PCEDT 2.0 concerning coreference, as well as coreference information already present there. We characterize the coreference annotation scheme, give the statistics and compare our annotation with the coreference annotation in Ontonotes and Prague Dependency Treebank for Czech. We also present the experiments made using this corpus to improve the alignment of coreferential expressions, which helps us to collect better statistics of correspondences between types of coreferential relations in Czech and English. The corpus released as PCEDT 2.0 Coref is publicly available.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vydání Pražského česko-anglického závislostního treebanku 2.0 (PCEDT), obsahující všechny anotace koreference (původní z PCEDT 2.0 i nové) a vylepšené mezijazyčné zarovnání koreferenčních výrazǔ.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present an extended version of the Prague Czech-English Dependency Treebank 2.0 (PCEDT 2.0). It includes all annotation of coreference (the original one from PCEDT 2.0 as well as the new one) and improved cross-lingual alignment of coreferential expressions. The corpus released as PCEDT 2.0 Coref is publicly available.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Srovnávácí analýzy koreferenčních výrazů poukazujících k abstraktním entitám a propozicím v češtině a němčině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper aims at a cross-lingual analysis of coreference to abstract entities in Czech and German, two languages that are typologically not very close, since they belong to two
different language groups – Slavic and Germanic. We will specifically focus on coreference chains to abstract entities, i.e. verbal phrases, clauses, sentences or even longer text
passages. To our knowledge, this type of relation is underinvestigated in the current stateof-the-art literature.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V clanku venujeme koreferencnim retezcum v ceskych a nemeckych textech. Za zaklad bereme jiz anotovane originalni texty v techto jazycich, aplikujeme spolecne prunikove schema a analyzujeme vysledky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we analyse coreference patterns in Czech and German. We specifically
focus on different types of coreference chains and their properties,
for instance, their length, number and functional subtypes of the elements
inside these chains. We use two datasets annotated within different annotation
frameworks, showing that this approach is possible if an interoperable
analysis scheme is applied.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Korpusová analýza použití posesivních zájmen v nově vytvořeném anglicko-česko-ruském paralelním korpusu PCEDT-R. V článku představujeme korpus, popisujeme jeho anotaci a prezentujeme statistiky na něm spočtené.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a corpus-based analysis of the use of possessive and reflexive possessive pronouns in a newly created English-Czech-Russian parallel corpus (PCEDT-R). Automatic word-alignment was applied to the texts, which were subsequently manually corrected. In the word-aligned data, we have manually annotated all correspondences of possessive and possessive reflexive pronouns from the perspective of each analysed language. The collected statistics and the analysis of the annotated data allowed us to formulate assumptions about language differences. Our data confirm the relative frequency of possessive pronouns in English as compared to Czech and Russian, and we explain it by the category of definiteness in English. To confirm some of our hypotheses, we used other corpora and questionnaires. We compared the translated texts in Czech and Russian from our corpus to the original texts from other corpora, in order to find out to what degree the translation factor might influence the frequency of possessives.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Titul předkládané publikace napovídá, že navazuje na Encyklopedický slovník češtiny (2002) a cíle, které si kladl. Rozsahem, obsahem a formou jednotlivých hesel i slovníku jako celku jej však přesahuje.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The title of the present publication suggests that it follows up the Encyclopedic Dictionary of Czech (2002) and its goals. However, it exceeds the previous dictionary in the extent, contents and form of the individual entries as well as the dictionary as a whole.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek se zabývá metodou překladu vyhledávacích dotazů, která přeuspořádávává překladové hypotézy produkované překladovým systémem, a to s ohledem na kvalitu výsledného vyhledávání. V článku je prezentován způsob adaptace této metody na nové zdrojové jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We investigate adaptation of a supervised  machine learning model for reranking of query translations to new languages in the context of cross-lingual information retrieval. The model is trained to rerank multiple translations produced by a statistical machine translation system and optimize retrieval quality. The model features do not depend on the source language and thus allow the model to be trained on query translations coming from multiple languages. In this paper, we explore how this affects the final retrieval quality. The experiments are conducted on medical-domain test collection in English and multilingual queries (in Czech, German, French) from the CLEF eHealth Lab series 2013--2015.
We adapt our method to allow reranking of query translations for four new languages (Spanish, Hungarian, Polish, Swedish). The baseline approach, where a single model is trained for each source language on query translations from that language, is compared with a model co-trained on translations from the three original languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Strojový překlad dotazů pro vícejazyčné vyhledávání informací je obvykle cílen na maximální překladovou kvalitu. To ovšem nemusí být optimální s ohledem na kvalitu vyhledávání a jiné překladové hypotézy mohou vést k lepším výsledkům vyhledávání. V tomto článku zkoumáme metodu využívající více možných překladů, které jsou přeuspořádávány pomocí metody založené na strojovém učení, která je optimalizována přímo na kvalitu překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Machine Translation (MT) systems employed to translate queries for Cross-Lingual Information Retrieval typically produce single translation with maximum translation quality. This, however, might not be optimal with respect to retrieval quality and other translation variants might lead to better retrieval results. In this paper, we explore a method exploiting multiple translations produced by an MT system, which are reranked using a supervised machine-learning method trained to directly optimize the retrieval quality. We experiment with various types of features and the results obtained on the medical-domain test collection from the CLEF eHealth Lab series show significant improvement of retrieval quality compared to a system using single translation provided by MT.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zpráva o účasti týmu Univerzity Karlovy v soutěži vyhledávání zdravotních informací CLEF eHealth Evaluation Lab 2016.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we present our participation as the team of
the Charles University at Task3 Patient-Centred Information Retrieval. In the monolingual task and its subtasks, we submitted two runs: one is based on language model approach and the second one is based on vector space model. For the multilingual task, Khresmoi translator, a Statistical Machine Translation (SMT) system, is used to translate the queries into English and get the n-best-list. For the baseline system, we take 1-best-list translation and use it for the retrieval, while for other runs, we use a machine learning model to rerank the n-best-list translations and predict the translation that gives the best CLIR performance in terms of P@10. We present set of features to train the model, these features are generated from the SMT verbose output, different resources like UMLS Metathesaurus, MetaMap, document collection and from the Wikipedia
articles. Experiments on previous CLEF eHealth IR tasks test set show significant improvement brought by the reranker over the baseline system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vystadial 2016 je databáze telefonních hovorů v češtině, vyvinuté pro trénování akustických modelů pro automatické rozpoznávání řeči v dialogových systémech. Data obsahují více než 75 hodin v českém jazyce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Vystadial 2016 is a dataset of telephone conversations in Czech, developed for training acoustic models for automatic speech recognition in spoken dialogue systems. The data comprise over over 75 hours in Czech, plus orthographic transcriptions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek se zabývá modely pro sledování stavu dialogu pomocí rekurentních neuronových sítí (RNN). Představujeme pokusy na datové sadě, DSTC2. Na jedné straně, RNN modely dosahují vynikajících výsledků. Na druhou stranu většina state-of-the-art modelů jsou "turn-based" a vyžadují specifické předzpracování (např pro data z DSTC2) k dosažení vynikajících výsledků. Představili jsme dvě architektury, které mohou být použity v inkrementálních nastavení a nevyžadují téměř žádnou předzpracování. V članku porovnáváme jejich výkonnost na referenčních hodnotách pro DSTC2 a diskutujeme jejich vlastnosti. S pouze triviální předzpracováním se výkon našich modelů blíží k výsledkům state-of-the-art.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper discusses models for dialogue state tracking using recurrent neural networks (RNN). We present experiments on the standard dialogue state tracking (DST) dataset, DSTC2. On the one hand, RNN models became the state of the art models in DST, on the other hand, most state-of-the-art models are only turn-based and require dataset-specific preprocessing (e.g. DSTC2-specific) in order to achieve such results. We implemented two architectures which can be used in incremental settings and require almost no preprocessing. We compare their performance to the benchmarks on DSTC2 and discuss their properties. With only trivial preprocessing, the performance of our models is close to the state-of- the-art results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek prezentuje novou datovou sadu pro výcvik end-to-end úkol orientovaně konverzační agentů. Obsahuje rozhovory mezi operátorem - odborníkem na danou doménu, a klientem, který hledá informace o úloze. Spolu s konverzační přepisy zaznamenáme databázová volání prováděné operátorem, které zachycují význam dotazu uživatele. Očekáváme, že se snadno získatelné databázová volání nám umožní trénovat end-to-end dialog agenty se s výrazně méně tréninkových dat. Datová sada je sbírána pomocí crowdsourcing a rozhovory pokrývají dobře známé restaurace doménu. Kvalita dat je vynucováno vzájemné kontroly mezi přispěvateli. Datový soubor je k dispozici ke stažení pod licencí Creative Commons 4.0 BY-SA licencí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a novel dataset for training end-to-end task oriented conversational agents. The dataset contains conversations between an operator – a task expert, and a client who seeks information about the task. Along with the conversation transcriptions, we record database API calls performed by the operator, which capture a distilled meaning of the user query. We expect that the easy-to-get supervision of database calls will allow us to train end-to-end dialogue agents with significantly less training data. The dataset is collected using crowdsourcing and the conversations cover the well-known restaurant domain. Quality of the data is enforced by mutual control among contributors. The dataset is available for download under the Creative Commons 4.0 BY-SA license.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Navrhujeme dva příspěvky k diskriminativnímu výběru pravidel v hierarchickém strojovém překladu. Ověřujeme předchozí přístupy na dvou úlohách francouzsko-anglického překladu na doménách s omezenými zdroji a ukazujeme, že nedokážou zlepšit překladovou kvalitu. Navrhujeme model pro výběr pravidel, který je (i) globální a využívá bohatou sadu rysů a (ii) je trénován s využitím všech dostupných negativních příkladů. Náš globální model přináší významné zlepšení až 1 bod BLEU oproti předchozím modelům pro výběr pravidel.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We propose two contributions to discriminative rule selection in hierarchical machine translation. First, we test  previous  approaches on two French-English translation tasks in domains for which only limited resources are  available  and  show  that  they  fail  to improve  translation  quality. To improve on  such  tasks,  we  propose  a  rule  selection  model  that  is  (i)  global  with  rich label-dependent  features  (ii)  trained  with all available negative samples.  Our global model yields significant improvements, up to 1  BLEU  point,  over  previously  proposed rule selection models.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení technik strojového překladu pro pracovníky státní správy a veřejných institucí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An introduction to machine translation for employees of governmental and other public institutions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přehled o metodách získávání paralelních dat a jejich přípravě pro trénování modelů strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An overview of method for acquiring parallel data and for their preparation for the purposes of MT training.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek shrnul deset let konání soutěže v automatickém hodnocení kvality překladu: představil úspěšné minulé metriky, současné experimenty a poukázal na potřebné další směry vývoje.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A summary of ten years of automatic MT evaluation, highlighting past successful metrics, current evaluation challenges and topics that need to be tackled in the future.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nové vydání česko-anglického paralelního korpusu CzEng. CzEng 1.6 obsahuje kolem půl miliardy slov v každém z jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A new release of the Czech-English parallel corpus CzEng. CzEng 1.6 consists of about 0.5 billion words (“gigaword”) in each language. The corpus is equipped with automatic annotation at a deep syntactic level of representation and alternatively in Universal Dependencies.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Lingvistická anotace angličtiny a češtiny použitá pro CzEng 1.6 je nyní k dispozici pro jakýkoliv text ve formě Docker kontejneru.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A pipeline of linguistic analysis used for CzEng 1.6 packaged in a Docker container. Any English or Czech text can be processed by this pipeline. The processing pipeline includes part-of-speech tagging, parsing, named entity recognition, semantic role labelling, coreference resolution etc.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nové vydání česko-anglického paralelního korpusu CzEng. CzEng 1.6 obsahuje kolem půl miliardy slov v každém z jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a new release of the Czech-English parallel corpus CzEng. CzEng 1.6 consists of about 0.5 billion words (“gigaword”) in each language. The corpus is equipped with automatic annotation at a deep syntactic level of representation and alternatively in Universal Dependencies. Additionally, we release the complete annotation pipeline as a virtual machine in the Docker virtualization toolkit.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje výsledky společných úloh WMT16, které zahrnovaly 5 úloh strojového překladu (novinové texty, IT doména, biomedicína, multimodální překlad a překlad zájmen), 3 úlohy vyhodnocovací (metriky, tuning, odhadu kvality překladu) a automatické posteditace a bilingvní párování dokumentů. Letos se WMT zůčastnilo 102 překladových systémů z 24 institucí (plus 36 anonymizovaných online systémů).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the
WMT16 shared tasks, which included five
machine translation (MT) tasks (standard
news, IT-domain, biomedical, multimodal,
pronoun), three evaluation tasks (metrics,
tuning, run-time estimation of MT quality),
and an automatic post-editing task
and bilingual document alignment task.
This year, 102 MT systems from 24 institutions
(plus 36 anonymized online systems)
were submitted to the 12 translation
directions in the news translation task. The
IT-domain task received 31 submissions
from 12 institutions in 7 directions and the
Biomedical task received 15 submissions
systems from 5 institutions. Evaluation
was both automatic and manual (relative
ranking and 100-point scale assessments).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek shrnuje výsledky soutěže v hodnocení kvality strojového překladu. Letos je soutěž rozšířena o několik novinek: větší počet jazykových párů, data z více domén, tři způsoby ručního vyhodnocení, jemuž se mají automatické metody v soutěži přiblížit.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the
WMT16 Metrics Shared Task. We asked
participants of this task to score the outputs
of the MT systems involved in the
WMT16 Shared Translation Task. We
collected scores of 16 metrics from 9 research
groups. In addition to that, we computed
scores of 9 standard metrics (BLEU,
SentBLEU, NIST, WER, PER, TER and
CDER) as baselines. The collected scores
were evaluated in terms of system-level
correlation (how well each metric’s scores
correlate with WMT16 official manual
ranking of systems) and in terms of segment
level correlation (how often a metric
agrees with humans in comparing two
translations of a particular sentence).
This year there are several additions to
the setup: large number of language pairs
(18 in total), datasets from different domains
(news, IT and medical), and different
kinds of judgments: relative ranking
(RR), direct assessment (DA) and HUME
manual semantic judgments. Finally, generation
of large number of hybrid systems
was trialed for provision of more conclusive
system-level metric rankings.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme naše systémy v překladové soutěži IWSLT 2016. Jedná se o naše první systémy pro překlad filmových titulků a současně o jedny z prvních experimentů s neuronovým překladem, které provádíme.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our submissions to the IWSLT 2016 machine
translation task, as our first attempt to translate subtitles and
one of our early experiments with neural machine translation
(NMT). We focus primarily on English→Czech translation
direction but perform also basic adaptation experiments for
NMT with German and also the reverse direction. Three MT
systems are tested: (1) our Chimera, a tight combination of
phrase-based MT and deep linguistic processing, (2) Neural
Monkey, our implementation of a NMT system in TensorFlow
and (3) Nematus, an established NMT system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek shrnuje deset ročníků soutěží ve strojovém překladu a ve vyhodnocování jeho kvality: WMT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The WMT evaluation campaign (http://www.statmt.org/wmt16) has been run annually since 2006. It is a collection of shared
tasks related to machine translation, in which researchers compare their techniques against those of others in the field. The longest
running task in the campaign is the translation task, where participants translate a common test set with their MT systems. In addition
to the translation task, we have also included shared tasks on evaluation: both on automatic metrics (since 2008), which compare the
reference to the MT system output, and on quality estimation (since 2012), where system output is evaluated without a reference. An
important component of WMT has always been the manual evaluation, wherein human annotators are used to produce the official ranking
of the systems in each translation task. This reflects the belief of theWMTorganizers that human judgement should be the ultimate arbiter
of MT quality. Over the years, we have experimented with different methods of improving the reliability, efficiency and discriminatory
power of these judgements. In this paper we report on our experiences in running this evaluation campaign, the current state of the art in
MT evaluation (both human and automatic), and our plans for future editions of WMT.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Soutěž ve strojovém překladu WMT používá od roku 2007 ruční hodnocení, kdy anotátor vidí najednou pět kandidátských překladů. Přes relativně dlouhou dobu používání nemáme dosud jasnou představu, co přesně anotátoři dělají. Tento článek představuje pilotní studii s osmi anotátory, jejichž oční pohyby jsme během hodnocení zaznamenávali na eyetrackeru a následně vyhodnotili.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The shared translation task of the Workshop of Statistical Machine Translation (WMT) is one of the key annual events of the field. Participating
machine translation systems in WMT translation task are manually evaluated by relatively ranking five candidate translations
of a given sentence. This style of evaluation has been used since 2007 with some discussion on interpreting the collected judgements but
virtually no insight into what the annotators are actually doing. The scoring task is relatively cognitively demanding and many scoring
strategies are possible, influencing the reliability of the final judgements. In this paper, we describe our first steps towards explaining the
scoring task: we run the scoring under an eye-tracker and monitor what the annotators do. At the current stage, our results are more of a
proof-of-concept, testing the feasibility of eye tracking for the analysis of such a complex MT evaluation setup.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace vítězného systému ze soutěže (Shared Task) WMT 2013-2015. Byla prezentována architektura, zásadní moduly a principy, a statistické komponenty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The TectoMT system is a result of long-term development which began in the pre-statistical era at Charles University in Prague and continued to include state-of-the-art tools for POS tagging, morphological feature disambiguation, lemmatization parsing, and some aspects of semantic analysis. It follows the usual Analysis – Transfer – Generation workflow, with transfer trained on a large parallel corpus using Hidden Markov Tree Model. Generation is partly rule-based (at the syntax level) and partly statistical (at the inflection/morphology level). Chimera is a hybrid system that uses a specific combination of TectoMT and a standard Phrase-based SMT (Moses), complemented by a “Depfix” automatic post-editing system, which as a whole improves on the individual systems, as documented in the results of the recent WMT Shared tasks. The system has been originally developed for English-Czech and recently transferred to several other languages within the EU QTLeap project (qtleap.eu), where it has been successfully used in the IT domain for both question and answer translation in a Q&amp;A context. Both the TectoMT and Chimera systems will be presented together with a discussion about language (in)dependence of such a hybrid solution.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku popisujeme CzEngVallex, bilingvní česko-anglický valenční slovník, zachycující propojení slovesné valence na základě překladových textů. Předkládáme zde základní statistiky, které podchycují mapování argumentů propojených sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe CzEngVallex, a bilingual Czech-English valency lexicon which aligns verbal valency frames and their arguments. It is based on a parallel Czech-English corpus, the Prague Czech-English Dependency Treebank, where for each occurrence of a verb a reference to the underlying Czech and English valency lexicons is explicitly recorded. CzEngVallex lexicon pairs the entries (verb senses) of these two lexicons, and allows for detailed studies of verb valency and argument structure in translation. While some related studies have already been published on certain phenomena, we concentrate here on basic statistics, showing that the variability of verb argument mapping between verbs in the two languages is richer than it might seem and than the perception from the studies published so far might have been.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Případová studie založená na zkušenosti autorů s lingvistickým výzkumem opírajícím se o anotovaný monolingvální I paralelní korpus. Příspěvek zahrnuje popis jevů patřících k různým jazykovým rovinám (morfologie, syntax povrchová I hloubková a diskurs). Podkladem je  Pražský závislostní korpus , soubor českých textů komplexně anotovaných na hloubkové syntaktické rovině včetně diskurzních vztahů a informační struktury věty. Autoři ukazují, že anotování (značkování) textových korpusů, pokud je založeno na spolehlivé lingvistické teorii,  není  účelem samo o sobě, ale že může především sloužit jako dobrý test pro danou teorii i pro jazykovědný výzkum obecně.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A case study based on experience in linguistic investigations using annotated monolingual and multilingual text corpora; the “cases” include a description of language phenomena belonging to different layers of the language system: morphology, surface and underlying syntax, and discourse; the analysis is based on a complex annotation of syntax, semantic functions, information structure and discourse relations of the Prague Dependency Treebank, a collection of annotated Czech texts. We want to demonstrate that annotation of corpus is not a self-contained goal: in order to be consistent, it should be based on some linguistic theory, and, at the same time, it should serve as a test bed for the given linguistic theory in particular and for linguistic research in general.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pro dva valenční slovníky slovanckých jazyků, PDT-Vallex pro češtinu a Walenty pro polštinu článek porovnává jejich frazeologickou část. Oba slovníky jsou založené na korpusu, i když se liší jak způsob propojení, tak technické řešení, ovšem oba jsou dostupné elektronicky ve standardnim formátu. V článku se porovnávají frazeologická hesla, jejich formální popis a možnosti a omezení. V závěru se doporučují rozšíření těchto komponent pro obecnější pokrytí i pro další jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Phraseological components of valency dictionaries for two West Slavic languages are presented, namely, of the PDT-Vallex dictionary for Czech and of the Walenty dictionary for Polish. Both dictionaries are corpus-based, albeit in different ways. Both are machine readable and employable by syntactic parsers and generators. The paper compares the expressive power of the phraseological subformalisms of these dictionaries, discusses  their limitations and makes recommendations for their possible extensions, which can be possibly applied also to other valency dictionaries with rich phraseological components.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku jsou analyzovány výhrady vůči dichotomii arguemtu (valenčního členu) a volného doplnění, které jsou předloženy ve stati A. Przepiórkovského ve stejné publikaci. Obhajují se kritéria opakovatelnosti, specifičnosti a podmínky užití dialogového testu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The arguments against the Argument-Adjunct distinction presented by A. Przepiórkowski in this volume are analyzed. The criteria iterability, specificity and dialogue test conditions are analyzed here and their usefulness for the framework of FGD is demonstrated.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve stati jsou charakterizovány přínosy skladby Vladimíra Šmilauera v kontextu jejího vzniku (1947) a její trvalé přínosy pro současnou českou syntax. Analyzují se její základní rysy odrážející se v nových syntaxích do dnešních dnů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this article the contribution of Šmialuer´s syntax in the period when it was published (1947) as well as its permanent influence on the contemporary books and studies on the syntax of contemporary Czech are analyzed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V kapitole jsou rozebrána kritéria používaná pro stanovení valenčních členů a způsob jejich zacycení ve slovníkové hesle. Zvláštní pozornost je věnována analýze možností vypuštění valenčního členu v povrchové struktuře.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this chapter the criteria used for the determination of valency members and the shape of the valency frames in lexicon are presented. The possibilities of the surface deletion of obligatory valency member are analyzed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce se budeme zabývat stromovými strukturami, které jsou využívány v počítačovém zpracování přirozeného jazyka.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We will present tree-shaped structures that are used in Natural Languages Processing for representing sentence syntactical structure, and discuss the benefits and drawbacks of such representations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá propojením dvou existujících, vzájemně se doplňujících datových zdrojů pro morfologii češtiny, a to flektivního slovníku MorfFlex CZ a derinační sítě DeriNet. MorfFlex CZ pokrývá několik milionů slovních forem v češtině. DeriNet obsahuje několik set tisíc českých lemmat, která jsou propojena explicitními slovotvornými relacemi odpovídajícímu. Výsledný zdroj je zpřístupněn pod licencí CC-BY-NC-SA a lze k němu rovněž přistupovat pomocí několika webových uživatelských rozhraní.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper deals with merging two complementary resources of morphological data previously existing for Czech, namely the inflectional dictionary MorfFlex CZ and the recently developed lexical network DeriNet. The MorfFlex CZ dictionary has been used by a morphological analyzer capable of analyzing/generating several million Czech word forms according to the rules of Czech inflection. The DeriNet network contains several hundred thousand Czech lemmas interconnected with links corresponding to derivational relations (relations between base words and words derived from them). After summarizing basic characteristics of both resources, the process of merging is described, focusing on both rather technical aspects (growth of the data, measuring the quality of newly added derivational relations) and linguistic issues (treating lexical homonymy and vowel/consonant alternations). The resulting resource contains 970 thousand lemmas connected with 715 thousand derivational relations and is publicly available on the web under the CC-BY-NC-SA license. The data were incorporated in the MorphoDiTa library version 2.0 (which provides morphological analysis, generation, tagging and lemmatization for Czech) and can be browsed and searched by two web tools (DeriNet Viewer and DeriNet Search tool).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vedle diskurzních vztahů vyjádřených primárně tzv. diskurzními konektory (nebo jsou případně dány implicitně) je třeba při analýze diskurzu brát v úvahu též další vztahy. To se týká především aktuálního členění větného a vztahů koreference. Všechny tři tyto aspekty jsou začleněny do anotačního scénáře Pražského závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Apart to discourse relations expressed primarily by discourse connectives (and partly also being implicit), there are other relations in the discourse that contribute to the text connectedness and have to be taken into consideration. This concerns first of all the information  structure of the sentence and the relations of coreference. All these three aspects are included in the annotation scheme of the Prague Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zaměřuje na popis neprojektivních konstrukcí z hlediska valence sloves. Pro tuto studii jsme použili data PDT a PCEDT a zameřili jsme se na kombinaci povrchové hloubkové syntaxe. V článku navrhujeme novou definici projektivity a klasifikujeme neprojektivni konstrukce z hlediska predikatove struktury. 
Porovnáváme češtinu a angličinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe results of investigation of a
specific type of discontinuous constructions,
namely non-projective constructions concerning
verbs and their arguments. This topic is
especially important for languages with a relatively
free word order, such as Czech, which is
the language we have primarily worked with.
For comparison, we have included some results
for English. The corpora used for both
languages are the Prague Czech-English Dependency
Treebank and the Prague Dependency
Treebank, which are both annotated at
a dependency syntax level as well as a deep
(semantic) level, including verbs and their valency
(arguments). We are using traditionally
defined non-projectivity on trees with full linear
ordering, but the two levels of annotation
are innovatively combined to determine if a
particular (deep) verb -argument structure is
non-projective. As a result, we have identi-
fied several types of discontinuities, which we
classify either by the verb class or structurally
in terms of the verb, its arguments and their
dependents. In addition, we have quantitatively
compared selected phenomena found in
Czech translated texts (in the PCEDT) to the
native Czech as found in the original Prague
Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje nový dvojjazyčný česko-anglický valenční slovník nazvaný CzEng-Vallex. Elektronická podoba slovníku je  umístěna v repozitáři Centra jazykové výzkumné infrastruktury LINDAT/CLARIN v XML formátu a je zde k dispozici také v prohledávatelné podobě. Slovník je propojen s českým valenčním slovníkem PDT-Vallex, s anglickýmm valenčním slovníkem EngVallex a rovněž je propojen s příklady z PCEDT korpusu (PCEDT 2.0). CzEngVallex  obsahuje 20835 propojených párů valenčních rámců (slovesných významů) překladových ekvivalentů a obsahuje i propojení jejich argumentů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper introduces a new bilingual Czech-English verbal valency lexicon (called CzEng-Vallex) representing a relatively large empirical database. It includes 20,835 aligned valency frame pairs (i.e., verb senses which are translations of each other) and their aligned arguments.
This new lexicon uses data from the Prague Czech-English Dependency Treebank and also takes advantage of the existing valency lexicons for both languages: the PDT-Vallex for Czech and the EngVallex for English. The CzEngVallex is available for viewing in LINDAT/CLARIN repository.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>PARSEME Shared Task (PST) se zabývá automatickou identifikací víceslovných výrazů (VV) v textu.
Jeho organizátoři připravili základní anotační pokyny se čtyřmi základními skupinami slovesných VV.
Jedním z dvaceti vybraných jazyků je i čeština.
Článek popisuje konversi dat Pražského závislostního korpusu (PDT), prozatím výhradně inherentně zájmenných sloves (IPronV) -- současnou anotaci v PDT, porovnává ji s anotačními pokyny PST. Závěrem je, že PDT a přidružený slovník obsahuje pro konversi dostatek údajů (ačkoli specifické jevy budou muset být kontrolovány ručně). Vedlejším efektem je, že jsme odhalili některé drobné chyby v anotaci PDT, které teď mohou být opraveny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes results of a study related to the PARSEME Shared Task on automatic detection of verbal Multi-Word Expressions (MWEs) which focuses on their identification in running texts in many languages. The Shared Task’s organizers have provided basic annotation guidelines where four basic types of verbal MWEs are defined including some specific subtypes. Czech is among the twenty languages selected for the task. We will contribute to the Shared Task dataset, a multilingual open resource, by converting data from the Prague Dependency Treebank (PDT) to the Shared Task format. The question to answer is to which extent this can be done automatically. In this paper, we concentrate on one of the relevant MWE categories, namely on the quasi-universal category called “Inherently Pronominal Verbs” (IPronV) and describe its annotation in the Prague Dependency Treebank. After comparing it to the Shared Task guidelines, we can conclude that the PDT and the associated valency lexicon, PDT-Vallex, contain sufficient information for the conversion, even if some specific instances will have to be checked. As a side effect, we have identified certain errors in PDT annotation which can now be automatically corrected.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme návrh nového slovníku českých diskurzních konektorů. Formát dat i anotační schéma vycházejí z podobných existujících elektronických zdrojů; zabýváme se důvody pro volbu struktury dat a pro výběr vlasností jednotlivých záznamů ve slovníku. Pozornost věnujeme obzvláště konzistentnímu zachycení primárních i sekundárních konektorů. Samotná data pocházejí z Pražského závislostního korpusu, rozsáhlého korpusu s manuálně anotovanými diskurzními vztahy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a design for a new electronic lexicon of Czech discourse connectives. The data format and the annotation scheme are based on a study of similar existing resources, and we discuss arguments for choosing the data structure and
selecting features of the lexicon entries. A special attention is paid to a consistent encoding
of both primary and secondary connectives. The data itself comes from exploiting the Prague Dependency Treebank, a large treebank manually annotated with discourse relations.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>PML-Tree Query je mocný a uživatelsky přítulný vyhledávací nástroj pro vyhledávání v bohatě lingvisticky anotovaných datech. Článek ukazuje, jak může být PML-TQ využito k vyhledávání diskurzních vztahů Penn Discourse Treebanku 2.0 namapovaných na syntaktické stromy Penn Treebanku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The PML-Tree Query is a general, powerful and user-friendly system for querying richly linguistically annotated treebanks. The paper shows how the PML-Tree Query can be used for searching for discourse relations in the Penn Discourse Treebank 2.0 mapped onto the syntactic annotation of the Penn Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies (UD) je projekt, který hledá morfologické a syntaktické anotační schéma aplikovatelné na mnoho jazyků, a vydává data anotovaná podle tohoto schématu. Stručně motivujeme a uvedeme tento projekt, jeho historii a principy, na kterých stojí. Poté popíšeme návrh nové (druhé) verze anotačních pravidel UD. Nakonec, pokud zbude čas, popíšeme několik zajímavých problémů, které se týkají aplikace UD na slovanské jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies (UD) is a project that seeks to find a cross-linguistically applicable morphological and syntactic annotation scheme, and to publish data conforming to that scheme in a wide range of languages. We will briefly motivate and introduce the project, its history and underlying principles. Then we will present the new proposal of version 2 UD guidelines. Finally, time permitting, we will describe several interesting problems related to the application of UD to Slavic languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek je pokusem navrhnout aplikaci podmnožiny standardu Universal Dependencies (UD) na skupinu slovanských jazyků. Dotyčnou podmnožinou jsou morfosyntaktické rysy jednotlivých slovesných tvarů. Systematicky dokumentujeme kategorie, které lze pozorovat u slovanských sloves, a přinášíme množství příkladů z 10 jazyků. Ukazujeme, že terminologie známá z literatury se často liší, i když podstata zkoumaných jevů je stejná. Náš cíl je praktický. Rozhodně nemáme v úmyslu přehodnocovat výsledky mnohaletého bádání slovanské srovnávací jazykovědy. Spíše chceme zasadit vlastnosti slovanských sloves do kontextu UD a navrhnout jednotný (všeslovanský) způsob, jak mají být prostředky UD aplikovány na tyto jazyky. Věříme, že náš návrh je kompromisem, který bude přijatelný pro korpusové lingvisty pracující se všemi slovanskými jazyky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This article is an attempt to propose application of a subset of the Universal Dependencies (UD) standard to the group of Slavic languages. The subset in question comprises morphosyntactic features of various verb forms. We systematically document the inventory of features observable with Slavic verbs, giving numerous examples from 10 languages. We demonstrate that terminology in literature may differ, yet the substance remains the same. Our goal is practical. We definitely do not intend to overturn the many decades of research in Slavic comparative linguistics. Instead, we want to put the properties of Slavic verbs in the context of UD, and to propose a unified (Slavic-wide) application of UD features and values to them. We believe that our proposal is a compromise that could be accepted by corpus linguists working on all Slavic languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies (UD) je mezinárodní projekt, který se snaží definovat pravidla morfologické a syntaktické anotace, aplikovatelná na všechny přirozené jazyky. Motivací projektu je najít společnou lingua franku pro lidi a nástroje, které pracují s lingvisticky anotovanými daty. Kromě pravidel UD také vydává závislostní korpusy převedené do anotačního stylu UD; z těchto dat se stává významný zdroj pro výzkum a aplikace ve vícejazyčném zpracování přirozených jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies (UD) is an international project that seeks to define guidelines for annotation of morphology and syntax, applicable to all natural languages. Its motivation is to find a common lingua franca for people and tools that deal with annotated linguistic data. Besides guidelines, UD also releases dependency treebanks converted to the UD annotation style; this data is becoming an important resource for multilingual NLP research and applications. In my talk, I will give a brief introduction to UD in general, and then I will focus on phenomena specific to Slavic languages. The UD release 1.2 includes six Slavic languages and others are being worked on. This gives us plenty of material for comparative studies, that in turn can (and should) further contribute to improved cross-linguistic consistency and fine-grained UD guidelines for Slavic languages. I will present observations from the UD data, as well as some results of dependency parsers trained on the data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Pro morfologické značkování a syntaktickou analýzu neznámých jazyků byla navržena řada metod. My zkoumáme delexikalizovaný parsing, navržený Zemanem a Resnikem (2008), a delexikalizované značkování, navržené Yu et al. (2016). V obou případech předkládáme podrobné vyhodnocení na datech z Universal Dependencies (Nivre et al., 2016), de-facto standardu pro vícejazyčné morfosyntaktické zpracování (předchozí práce pracovaly s jinými daty). Naše výsledky potvrzují, že každá z uvedených delexikalizovaných metod samostatně má určitý omezený potenciál v případech, kdy není k dispozici žádná ruční anotace cílového jazyka. Nicméně, pokud obě metody zkombinujeme, jejich chyby se vzájemně zmnožují nad přijatelnou mez. Ukazujeme, že i sebemenší střípek expertní anotace cílového jazyka může významně zvýšit úspěšnost a měl by být použit, jestliže ho lze získat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Various unsupervised and semi-supervised
methods have been proposed to tag and parse
an unseen language. We explore delexicalized
parsing, proposed by (Zeman and Resnik,
2008), and delexicalized tagging, proposed
by (Yu et al., 2016). For both approaches
we provide a detailed evaluation on Universal
Dependencies data (Nivre et al., 2016), a de-facto standard for multi-lingual morphosyntactic processing (while the previous work used other datasets). Our results confirm that in separation, each of the two delexicalized techniques has some limited potential when no annotation of the target language is available. However, if used in combination, their errors multiply beyond acceptable limits. We demonstrate that even the tiniest bit of expert annotation in the target language may contain significant potential and should be used if available.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku na příkladu reciprocity představujeme popis gramatikalizovaných alternací v češtině, který je založen na rozdělení informace do slovníku a gramatiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we introduce the representation of grammaticalized alternations in Czech based on the division of the information between grammar and lexicon. This representation is exemplified by reciprocity.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek představil rodinu elektronických valenčních slovníků, které jsou rozvíjeny v Ústavu formální a aplikované lingvistiky, VALLEX, PDT-Vallex, EngVallex a CzEngVallex, jejichž teoretické zázemí tvoří funkční generativní popis.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this contribution, the family of valency lexicons whose theoretical background represents the Functional Generative Description is introduced, VALLEX, PDT-Vallex, EngVallex and CzEngVallex.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto příspěvku jsme představili valenční slovník českých sloves, VALLEX, a jeho nejnovější verzi 3.0, která je založena na dvou komponentech: na slovníkové a gramatické komponentě.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this talk, a new version of the valency lexicon of Czech verbs, VALLEX, was introduced. This version is based on the division of the lexicon between two components: a data component and a grammar component.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valenční slovník českých sloves (dále též VALLEX 3.0) poskytuje informace o valenční struktuře českých sloves v jejich jednotlivých významech, které charakterizuje pomocí glos a příkladů; tyto základní údaje jsou doplněny o některé další syntaktické a sémantické charakteristiky. VALLEX 3.0 zachycuje 4586 českých sloves náležejících 2722 lexémům, které odpovídají 6710 lexikálním jednotkám, tedy daným slovesům v daném významu (počítáme-li vidové protějšky zvlášť, jde -- bez iterativ, jež nejsou detailně zpracována -- o 10816 lexikálních jednotek).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The Valency Lexicon of Czech Verbs (henceforth VALLEX 3.0) provides information on the valency structure of Czech verbs in their respective senses. Each sense is characterized by a gloss and an example; further syntactic and semantic characteristics are provided as relevant. VALLEX 3.0 covers 4586 Czech verbs in 2722 lexemes with a total of 6710 lexical units (i.e. a given verb in a given meaning; if aspectual counterparts are counted separately, we get a total of 10816 lexical units, excluding iteratives).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Valenční slovník českých sloves (též VALLEX 3.0) poskytuje informace o valenční struktuře českých sloves v jejich jednotlivých významech,
které charakterizuje pomocí glos a příkladů;
tyto základní údaje jsou doplněny o některé další syntaktické a sémantické charakteristiky.
VALLEX 3.0 zachycuje téměř 4 600 českých sloves, která odpovídají více než 10 700 lexikálním jednotkám, tedy vždy danému slovesu v daném  významu.

VALLEX je  budován jako odpověď na potřebu rozsáhlého a kvalitního veřejně dostupného slovníku, který by obsahoval
syntaktické a sémantické charakteristiky českých sloves a byl přitom široce uplatnitelný v aplikacích pro zpracování
přirozeného jazyka. Při navrhování koncepce slovníku byl proto kladen důraz
na všestranné využití jak pro lidského uživatele, tak pro automatické zpracování češtiny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>VALLEX 3.0 provides information on the valency structure (combinatorial potential) of verbs in their particular senses, which are characterized by glosses and examples. VALLEX 3.0 describes almost 4 600 Czech verbs in more than 10 700 lexical units, i.e., given verbs in the given senses.

VALLEX 3.0 is a is a collection of linguistically annotated data and documentation, resulting from an attempt at formal description of valency frames of Czech verbs.  In order to satisfy different needs of different potential users, the lexicon is distributed (i) in a HTML version (the data allows for an easy and fast navigation through the lexicon) and (ii) in a machine-tractable form as a single XML file, so that the VALLEX data can be used in NLP applications.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této kapitole je popsáno lexikografické zpracování diatezí ve valenčním slovníku VALLEX. Dále je představen poloautomatický postup při identifikaci diatezí lexikálních jednotek sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this chapter, the representation of Czech diatheses in the valency lexicon VALLEX is described. A semiautomatic method identifying the possibility of lexical units of verbs to create diatheses is introduced.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Licenční smlouva a smlouva o poskytnutí služeb mezi Univerzitou Karlovou a Datlowe, s.r.o. (1. 1. 2015 -- 1. 1. 2018) obsahuje právo na užití dat PDT 2.5 a software pro textovou analytiku natrénovaného na PDT 2.5, a to pro komerční účely. Jedná se o upravené verze nekomerčního software a dat jinak dostupných z repozitáře LINDAT/CLARIN. Jmenovitě: software pro analýzu tvarosloví a lematizaci, syntaktickou analýzu a analýzu názvů pojmenovanýchch entit.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Licence and service agreement between Charles University and Datlowe Ltd.(January 1, 2015 -- January 1, 2018) covers the right to use the data of PDT 2.5 and the software for text analytics trained on PDT 2.5, for commercial purposes. The data and software (non-adapted) is otherwise available for free through the LINDAT/CLARIN repository for non-commercial use under the appropriate variant of the Creative Commons license. Namely, software for morphological analysis, lemmatization, syntactic analysis and named-entity recognition.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Slovník valence substantiv založený na korpusu je začínající projekt navazující na předchozí výzkumy valence substantiv v češtině. Tento příspěvek se zabývá zachycením valence substantiv v moderním slovníku založeném na korpusu, dostupném jak pro lidské uživatele, tak ve formě strojově čitelných dat. Referujeme také o omezeních, která pro výzkum jmenné valence představují v současnosti nejčastěji užívané korpusové vyhledávače.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Corpus-based Valency Lexicon of Czech Nouns is a starting project picking up the threads of our previous work on nominal valency. It builds upon solid theoretical foundations of the theory of valency developed within the Functional Generative Description. In this paper, we describe the ways of treating valency of nouns in a modern corpus-based lexicon, available as machine readable data in a format suitable for NLP applications, and report on the limitations that the most commonly used corpus interfaces provide to the research of nominal valency. The linguistic material is extracted from the Prague Dependency Treebank, the synchronic written part of the Czech National Corpus, and Araneum Bohemicum. We will utilize lexicographic software and partially also data developed for the valency lexicon PDT-Vallex but the treatment of entries will be more exhaustive, for example, in the coverage of senses and in the semantic classification added to selected lexical units (meanings). The main criteria for including nouns in the lexicon will be semantic class membership and the complexity of valency patterns. Valency of nouns will be captured in the form of valency frames, enumeration of all possible combinations of adnominal participants, and corpus examples.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Na příkladu českého morfologického slovníku MorfFlex CZ upozorňujeme na problém homonymie a polysémie. V morfologickém slovníku není nutné rozlišovat význam slov, pokud takové rozlišení nemá důsledky ve slovotvorbě nebo v syntaxi. Příspěvek navrhuje několik důležitých pravidel a principů pro dosažení konzistence.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We focus on a problem of homonymy and polysemy in morphological dictionaries on the example of the Czech morphological dictionary MorfFlex CZ. It is not necessary to distinguish meanings in morphological dictionaries unless the distinction has consequencies in word formation or syntax. The contribution proposes several  important rules and principles for achieving consistency.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje vznik nového lexikálního zdroje pro češtinu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Human judgments of lexical similarity/relatedness are used as evaluation data for Vector Space Models, helping to judge how the distributional similarity captured by a given Vecotr Space Model correlates with human intuitions. A well establishde data set for the evaluation of lexical similarity/relatedness id WordSim353, alnog with its translations into sefveral other languages. Thiospaper presents its Czech translation and annotation, which is publicly available via the Lindat-Clarin repository.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme pilotní analýzu nového lingvistického zdroje, VPS-GradeUp, který je dostupný z http://hdl.handle.net/11234/1-1585.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a pilot analysis of a new linguistic resource, VPS-GradeUp (available at http://hdl.handle.net/11234/1-1585). The resource contains 11,400 graded human decisions on usage patterns of 29 English lexical verbs, randomly selected from the Pattern Dictionary of English Verbs (Hanks, 2000 2014) based on their frequency and the number of senses their lemmas have in PDEV. This data set has been created to observe the interannotator agreement on PDEV patterns produced using the Corpus Pattern Analysis (Hanks, 2013). Apart from the graded decisions, the data set also contains traditional Word-Sense-Disambiguation (WSD) labels. We analyze the associations between the graded annotation and WSD annotation. The results of the respective annotations do not correlate with the size of the usage pattern inventory for the respective verbs lemmas, which makes the data set worth further linguistic analysis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek pojednává o škálovaných rozhodnutích anotátorů v CPA analýze anglických sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We work with 1450 concordances of 29 English verbs (50 concordances per lemma) and their
corresponding entries in the Pattern Dictionary of English Verbs (PDEV). Three human annotators
working independently but in parallel judged how well each lexical unit of the corresponding PDEV
entry illustrates the given concordance. Thereafter they selected one best-fitting lexical unit for each concordance – while the former setup allowed for ties (equally good matches), the latter did not. We measure the interannotator agreement/correlation in both setups and show that our results are not worse (in fact, slightly better) than in an already published graded-decision annotation performed on a traditional dictionary. We also manually examine the cases where several PDEV lexical units were classified as good matches and how this fact affected the interannotator agreement in the bestfit
setup. The main causes of overlap between lexical units include semantic coercion and regular
polysemy, as well as occasionally insufficient abstraction from regular syntactic alternations, and eventually also arguments defined as optional and scattered across different lexical units despite not being mutually exclusive.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Toto je dataset k evaluaci vzájemných sémantických vztahů mezi slovy (podobnost a souvislost). Vytvořeno podle již klasického anglického zdroje WordSim353</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Human judgments of lexical similarity/relatedness are used as evaluation data for Vector Space Models, helping to judge how the distributional similarity captured by a given Vector Space Model correlates with human intuitions. A well established data set for the evaluation of lexical similarity/relatedness is WordSim353, along with its translations into several other languages. This paper presents its Czech translation and annotation, which is publicly available via the LINDAT-CLARIN repository at hdl.​handle.​net/​11234/​1-1713.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Poster představuje přehled výsledků automatické analýzy slovosledu ve 23 treebancích. Tyto treebanky byly vytvořeny v rámci projektu HamleDT, jehož cílem je poskytnout univerzální anotaci závislostních korpusů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This poster gives an overview of the results of automatic analysis of word order in 23 dependency treebanks. These treebanks have been collected in the frame of in the HamleDT project, whose main goal is to provide universal annotation for dependency corpora.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek porovnává několik způsobů měření stupně volnosti slovosledu na datech 23 jazyků, získaných z jejich syntakticky anotovaných korpusů. Tyto korpusy jsou součástí projektu HamleDT. Měření bere v úvahu vzájemné postavení podmětu, přísudku a předmětu v hlavních a vedlejších větách a měří jej pomocí euklidovské vzdálenosti, vzdálenosti max-min, entropie a kosinové podobnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper compares various means of measur-
ing of word order freedom applied to data from syntactically annotated corpora for 23 languages. The corpora are part of the HamleDT project, the word order statistics are relative frequencies of all word order combinations of subject, predicate and object both in main and subordinated clauses. The measures include Euclidean distance, max-min distance, entropy and cosine similarity. The dif-
ferences among the measures are discussed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení mých projektů týkajících se víceslovných výrazů na PARSEME Training school v La Rochelle.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Overview of my project concerning MWEs at Second PARSEME Training School in La Rochelle.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Analyzujeme vnitřní strukturu komplexních adresních datových údajů v textu, kategorizujeme je, anotujeme a vyhodnocujeme výsledky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We analyze inner structure of complex address data
given in text, categorize it, annotate
it and evaluate the annotation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ukážeme, že slovesné víceslovné výrazy (VV) specifikované v Shared Tasku PARSEME jsou anotovány v PDT a jednotlivé kategorie VV je možné z něj extrahovat a v Shared Tasku použít.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We show that verbal MWEs similar to those specified in PARSEME Shared Task have already been annotated in Prague Dependency Treebank and all their individual categories can be extracted and used within the Shared Task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek prezentuje dva problémy, při jejichž řešení bylo využíváno vztahů mezi derivačními a flektivními paradigmaty. Za prvé, tyto vztahy byly využity při identifikaci nových vztahů mezi odvozenými slovy a slovy základovými, a to především v případech, kdy během odovzení dochází k hláskové alternaci. Za druhé, flektivní rysy je možné využívat rovněž jako důležitou vstupní informaci při poloautomatickém značkování významových vztahů mezi deriváty a slovy základovými.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper  deals  with  two  main  issues,  each  approaching  the  relation between  derivational  (sub-)paradigms (as discussed by Štekauer 2014) and inflectional paradigms  from  a  different perspective.  First,  a  recent  enrichment  of  the  DeriNet network  is  described  which  focused  on  words  the  derivation  of  which  is  accompanied by consonant and/or vowel alternations. Second,  inflectional  features  have  been  used  as  an  important  input  for  the  semi-automatic task of semantic labelling of derivational relations in the DeriNet network.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje lexikální databázi DeriNet, která obsahuje téměř 1 milion českých slov propojených více než 700 tisíci hranami odpovídajícími vztahu mezi slovem odvozeným a základovým. V textu je popsán proces budování této databáze, lingvistická rozhodnutí, která bylo nutné během tohoto procesu učinit, i možné využití a výhledy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the present paper, the lexical database DeriNet is introduced which includes more than 969 thousand Czech word interconnected with 718 thousand links corresponding to derivational relations (relations between a base word and a word derived from it). Derivational relations were identified by semi-automatic procedures and manual annotation. As the DeriNet network is fully compatible with a big inflectional dictionary of Czech (MorfFlex CZ), it can be used as a resource for an integrating approach to derivational and inflectional morphology of Czech both in linguistic research and in natural language processing.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje zdroje dosud vytvořené v projektu Universal Dependencies (UD), který se snaží vypořádat s chybějící závislostní reprezentací pro zpracování přirozených jazyků aplikovatelnou na více jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the resources so far created in the Universal Dependencies (UD) project, which attempts to address a lack of cross-linguistically adequate dependency representations for natural language processing.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je páté vydání treebanků UD, verze 1.4.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the fifth release of UD Treebanks, Version 1.4.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Universal Dependencies je projekt, který vyvíjí mezijazykově konzistentní morfologicko-syntaktickou anotaci mnoha jazyků s cílem usnadnit vývoj jazykově nezávislých parserů, mezijazykové strojové učení a výzkum parsingu z perspektivy jazykové typologie. Anotační schéma je založeno na univerzálních Stanfordských závislostech (de Marneffe et al., 2006, 2008, 2014), univerzálních značkách Google (Petrov et al., 2012) a na Intersetu, tj. interlingvě pro sady morfologických značek (Zeman, 2008). Toto je čtvrté vydání treebanků UD, verze 1.3.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). This is the fourth release of UD Treebanks, Version 1.3.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Toto je dokument-alignovaný paralelní korpus anglických a českých abstraktů vědeckých článků, které publikovali autoři z Institutu formální a aplikované lingvistiky Univerzity Karlovy v Praze, jak byly reportovány v systému institutu Biblio. Autoři každé publikace jsou povinni poskytnout jak originální abstrakt v češtině nebo angličtině, tak jeho překlad do angličtiny  respektive češtiny. Žádné filterování nebylo provedeno, kromě odstranění záznamů, kterým chybí český nebo anglický abstrakt a nahrazení nových řádků a tabulátorů mezerami.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This is a document-aligned parallel corpus of English and Czech abstracts of scientific papers published by authors from the Institute of Formal and Applied Linguistics, Charles University in Prague, as reported in the institute's system Biblio. For each publication, the authors are obliged to provide both the original abstract in Czech or English, and its translation into English or Czech, respectively. No filtering was performed, except for removing entries missing the Czech or English abstract, and replacing newline and tabulator characters by spaces.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme lexikon-lesový rule-bazovaný systém machín translace od Engliše Čecha, který bazoval na verově limitované amountu rulů transformace. Jeho kor je novelový modul translace, implementovalo jako komponent systému translace tektomtu a dependuje masivně na extensivní pipelínu lingvistické preprocesování a postprocesovat v Tektomtu. Jeho skop je naturálně limitovaná, ale pro specifické texty z například scientifické nebo marketování doménu okasionálně producuje sensibilní resulty.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a lexicon-less rule-based machine translation system from English to Czech, based on a very limited amount of transformation rules. Its core is a novel translation module, implemented as a component of the TectoMT translation system, and depends massively on the extensive pipeline of linguistic preprocessing and postprocessing within TectoMT. Its scope is naturally limited, but for specific texts, e.g. from the scientific or marketing domain, it occasionally produces sensible results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme naši práci v oblasti částečně řízené syntaktické analýzy vět přirozeného jazyka, se zaměřením na mezijazyčný přenos delexikalizovaných závislostních parserů s více zdroji. Představujeme KLcpos3, empirickou míru podobnosti jazyků, navrženou a vyladěnou pro vážení zdrojových parserů při přenosu delexicalizovaných parserů s více zdroji. Nakonec představíme novou metodu kombinace zdrojů, založenou na interpolaci natrénovaných modelů parserů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our work on semi-supervised parsing of natural language sentences, focusing on multi-source crosslingual transfer of delexicalized dependency parsers. We present KLcpos3, an empirical language similarity measure, designed and tuned for source parser weighting in multi-source delexicalized parser transfer. And finally, we introduce a novel resource combination method, based on interpolation of trained parser models.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>To je něco, co používá každý, přitom není úplně zřejmé, jak to funguje. Pro daný dotaz nejde prostě pro každý dotaz projít všechny existující miliardy webových stránek, a vrátit uživateli několik miliónů z nich, které obsahují hledaná slova. Ukážu, co je reverzní index (to zná každý pod názvem rejstřík), podle čeho se řadí výsledky, a přidám i pokročilejší vylepšení (TF.IDF, lematizace, synonyma, pojmenované entity).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This is something everyone uses, but it is not absolutely clear how it works. For a given query it is not possible to simply traverse all the existing billions of websites, and return the few millions that contain the sought words. I will show what a reverse index is, how results are sorted, and I will also add more advanced techniques (TF.IDF, lemmatization, synonyms, named entities).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>SloNLP je workshop speciálně zaměřený na zpracování přirozeného jazyka a počítačovou lingvistiku. Jeho hlavním cílem je podpořit spolupráci mezi výzkumníky v oblasti NLP v Česku a na Slovensku.
Mezi témata workshopu patří automatické rozpoznávání mluvené řeči (ASR), automatická analýza a generování přirozeného jazyka (morfologie, syntax, sémantika...), dialogové systémy (DS), strojový překlad (MT), vyhledávání informací (IR), praktické aplikace NLP technologií, a další témata počítačové lingvistiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>SloNLP is a workshop focused on Natural Language Processing (NLP) and Computational Linguistics. Its primary aim is to promote cooperation among NLP researchers in Slovakia and Czech Republic.
The topics of the workshop include automatic speech recognition, automatic natural language analysis and generation (morphology, syntax, semantics, etc.), dialogue systems, machine translation, information retrieval, practical applications of NLP technologies, and other topics of computational linguistics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisuje náš příspěvek do úlohy překladu v doméně IT na WMT16.
Provádíme doménovou adaptaci již natrénovaných překladových systémů pomocí slovníkových dat bez dalšího trénování.
Náš postup aplikujeme na dva konceptuálně odlišné překladače, vyvíjené v rámci projektu QTLeap -- TectoMT a Moses -- a na jejich kombinaci -- Chiméru.
Všechny naše metody zlepšují ve všech experimentech kvalitu překladu.
Základní varianta naší metody je navíc použitelná na libovolný překladový systém, včetně takového, který je pro uživatele černou skříňkou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe our submission to the IT-domain translation task of WMT 2016.
We perform domain adaptation with dictionary data on already trained MT systems with no further retraining.
We apply our approach to two conceptually different systems developed within the QTLeap project: TectoMT and Moses, as well as Chimera, their combination.
In all settings, our method improves the translation quality.
Moreover, the basic variant of our approach is applicable to any MT system, including a black-box one.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Moses je dobře známý reprezentant rodiny frázových systémů statistického strojového překladu, který je známý tím, že je extrémně chudý na explicitní lingvistické znalosti, a operuje na plochých reprezentacích jazyka, složených jen z tokenů a frází. Na druhé straně, Treex NLP toolkit, který je vysoce lingvisticky motivovaný, operuje na několika vrstvách reprezentace jazyka, bohatých na lingvistické anotace. Jeho hlavní aplikace je TectoMT, hybridní systém strojového překladu s hloubkovým transfrem syntaxe. Nabízíme přehled velkého počtu systémů strojového překladu, sestavených v minulých letech, které různými způsoby kombinují Mosese a Treex/TectoMT.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Moses is a well-known representative of the phrase-based statistical machine translation systems family, which are known to be extremely poor in explicit linguistic knowledge, operating on flat language representations, consisting only of tokens and phrases. Treex, on the other hand, is a highly linguistically motivated NLP toolkit, operating on several layers of language representation, rich in linguistic annotations. Its main application is TectoMT, a hybrid machine translation system with deep syntax transfer. We review a large number of machine translation systems that have been built over the past years by combining Moses and Treex/TectoMT in various ways.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentujeme dialogový systém Politik. Politik je navržen jako chatovací robot, který imituje politika. Odpovídá na otázky týkající se politických témat. Otázky jsou zpracovány nástroji počítačového zpracování jazyka bez použití externí znalostní báze. Jazykem dialogu je čeština.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a question-answering system
Politician designed as a chatbot imitating a
politician. It answers questions on political
issues. The questions are analyzed using
natural language processing techniques and
no complex knowledge base is involved.
The language used for the interviews is
Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme systém na mezijazyčnou predikci zájmen, konkrétně mezi angličtinou a němčinou.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a system submitted to the WMT16 shared task in cross-lingual pronoun prediction, in particular, to the English-to-German and German-to-English sub-tasks. The system is based on a linear classifier making use of features both from the target language model and from linguistically analyzed source and target texts. Furthermore, we apply example weighing in classifier learning, which proved to be beneficial for recall in less frequent pronoun classes. Compared to other shared task participants, our best English-to-German system is able to rank just below the top performing submissions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento projekt rozšiřuje část PCEDT 2.0 o ruské překlady a anotaci zaměřenou na koreferenci a mezijazyčné zarovnání koreferenčních výrazǔ. Ve verzi 0.5 korpus obsahuje překlad více než 1000 anglických vět s automatickými anotacemi na úrovni morfologie a ručním mezijazyčným zarovnáním českých, anglických a ruských posesivních zájmen.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Prague Czech-English Dependency Treebank - Russian translation (PCEDT-R) is a project of translating a subset of Prague Czech-English Dependency Treebank 2.0 (PCEDT 2.0) to Russian and  linguistically annotating the Russian translations with emphasis on coreference and cross-lingual alignment of coreferential expressions. Cross-lingual comparison of coreference means is currently the purpose that drives development of this corpus. In version 0.5, it contains Russian translations of more than 1000 English sentences with automatic morphological analysis and manual cross-lingual alignment of Czech, English and Russian possessive pronouns.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Rozšířená a přepracovaná verze Lapshinova et al. (2016). V článku provádíme srovnávací analýzu diskurzních jevů v němčině a češtině na základě dvou korpusových zdrojů pro tyto jazyky. Korpusy jsou anotovány v rámci různých teoretických přístupů: FGP (Sgall et al. 1986) pro češtinu a teorie textové koheze (Halliday &amp; Hasan 1976) pro němčinu. Hledáme společné rysy, automaticky je identifikujeme v textech a tím se dostáváme k srovnatelným korpusům anotovaných stejnými typy vztahů, které dále rozebíráme. (viz hlavně angl. abstrakt a text)</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the present paper, we analyse variation of discourse phenomena in two typologically different languages, i.e. in German and Czech.
The novelty of our approach lies in the nature of the resources we are using. Advantage is taken of existing resources, which are,
however, annotated on the basis of two different frameworks. We use an interoperable scheme unifying discourse phenomena in both
frameworks into more abstract categories and considering only those phenomena that have a direct match in German and Czech. The
discourse properties we focus on are relations of identity, semantic similarity, ellipsis and discourse relations. Our study shows that the
application of interoperable schemes allows an exploitation of discourse-related phenomena analysed in different projects and on the
basis of different frameworks. As corpus compilation and annotation is a time-consuming task, positive results of this experiment open
up new paths for contrastive linguistics, translation studies and NLP, including machine translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku provádíme srovnávací analýzu diskurzních jevů v němčině a češtině na základě dvou korpusových zdrojů pro tyto jazyky. Korpusy jsou anotovány v rámci různých teoretických přístupů: FGP (Sgall et al. 1986) pro češtinu a teorie textové koheze (Halliday &amp; Hasan 1976) pro němčinu. Hledáme společné rysy, automaticky je identifikujeme v textech a tím se dostáváme k srovnatelným korpusům anotovaných stejnými typy vztahů, které dále rozebíráme. (viz hlavně angl. abstrakt a text)</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>in this paper, we perform a cross-lingual analysis of discourse phenomena in German and Czech, using two corpus resources annotated monolingually within two different frameworks: functional Generative description (Sgall et al. 1986) for Czech, and textual cohesion (Halliday &amp; Hasan 1976) for German.
We take advantage of the existing resources reflecting systemic peculiarities and realisational options of the languages under analysis. in our previous work (Lapshinova et al. 2015), we have shown that the annotations of the involved resources are comparable if abstract categories are used and only the phenomena with a direct match in German and Czech are taken into consideration. Our
analysis is a first step towards unifying separate analyses of discourse relations in Germanic and Slavic languages. at the same time, it demonstrates that the application of ’theoretically’ different resources is possible in one contrastive analysis. This is especially valuable for nlp, which uses annotated resources to train language models for various tools.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje metodu hledání dvojic dokumentů, které jsou si překladem, na základě podobného umístění klíčových slov v kandidátech a na základě podobnosti měřené n-gramovým jazykovým modelem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The WMT Bilingual Document Alignment
Task requires systems to assign
source pages to their “translations”, in a
big space of possible pairs. We present
four methods: The first one uses the term
position similarity between candidate document
pairs. The second method requires
automatically translated versions of the
target text, and matches them with the candidates.
The third and fourth methods try
to overcome some of the challenges presented
by the nature of the corpus, by
considering the string similarity of source
URL and candidate URL, and combining
the first two approaches.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>MUSCIMarker 1.1 je open-source nástroj pro anotaci vizuálních objektů a jejich vztahů v obrázkách. Byl navržen pro anotování hudební notace, avšak uživatel může definovat libovolnou sadu objektů. Poskytuje specializované funkce pro zrychlení anotace binárních obrázků (tj. obrázků, kde je jsou jasně vymezené oblasti pozadí, ve kterých se žádný anotovaný objekt nevyskytuje). MUSCIMarker je samostatná aplikace s grafickým rozhraním pro osobní počítače a nevyžaduje připojení na internet; je implementována v jazyce Python a otestována na všech třech hlavních operačních systémech (Windows, Linux a OS X).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>MUSCIMarker 1.1 is an open-source tool for annotating visual objects and their relationships in images. It was designed with music notation in mind, but it can annotate any user-defined object set.
Specialized functionality is provided to speed up annotation of binary images (where the background, defined as an area where no object of interest resides, is known). MUSCIMarker is an offline standalone graphical user interface desktop application implemented in Python and known to run on all major desktop operating systems (Windows, Linux and OS X).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Evaluace rozpoznávání notopisu (Optical Music Recognition, OMR) je notoricky obtížná a automatické metriky pro evaluaci konečného výstupu OMR nejsou k dispozici. V "Towards a Standard Testbed for Optical Music Recognition: Definitions, Metris and Page Images", Byrd a Simonsen nedávno zdůraznili, že benchmarkovací standard je v komunitě OMR nutný: jak evaluační data, tak metriky. Navazujeme na jejich analýzu a předkládáme prototyp benchmarku pro OMR. Náš příspěvek není úplné řešení komplexního problému evaluace OMR; je to (a) snaha definovat víceúrovňový testovací dataset pro OMR a implementace jeho prototypu pro tištěná a rukopisná data, (b) na korpusu založená metodologie pro vyhodnocování automatických evaluačních metrik, a příslušný korpus více než 1000 expertních posouzení relativní obtížnosti opravy různých druhů chyb. Na (b) pak navazujeme vyhodnocením několika přímočarých evaluačních metrik pro MusicXML a nastavujeme tak baseline, kterou další metriky budou zlepšovat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Evaluating Optical Music Recognition (OMR) is notoriously difficult and automated end-to-end OMR evaluation metrics are not available to guide development. In “Towards a Standard Testbed for Optical Music Recognition: Definitions, Metrics, and Page Images”, Byrd and Simonsen recently stress that a benchmarking standard is needed in the OMR community, both with regards to data and evaluation metrics. We build on their analysis and definitions and present a prototype of an OMR benchmark. We do not, however, presume to present a complete solution to the complex problem of OMR benchmarking. Our contributions are: (a) an attempt to define a multi- level OMR benchmark dataset and a practical prototype implementation for both printed and handwritten scores, (b) a corpus-based methodology for assessing automated evaluation metrics, and an underlying corpus of over 1000 qualified relative cost-to-correct judgments. We then assess several straightforward automated MusicXML evaluation metrics against this corpus to establish a baseline over which further metrics can improve.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje pokus o automatizaci všech procesů vytváření dat u systému strojového překladu založeného na pravidlovém přístupu s mělkým transferem. Uváděné metody byly vyzkoušeny na čtyřech plně funkčních překladových systémech pokrývajících tyto jazykové páry: slovinštinu do srbštiny, češtiny, angličtiny a estonštiny. Aplikovatelnost použitých metod byla testována rozsáhlou řadou evalučních testů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents an attempt to automate all data creation processes of a rule-based shallow-transfer machine translation system. The presented methods were tested on four fully functional translation systems covering language pairs: Slovenian paired with Serbian, Czech, English and Estonian language. An extensive range of evaluation tests was performed to assess the applicability of the methods.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Medical and healthcare study programmes are quite complicated in terms of branched structure and heterogeneous content. In logical sequence a lot of requirements and demands placed on students appear there. This paper focuses on an innovative way how to discover and understand complex curricula using modern information and communication technologies. We introduce an algorithm for curriculum metadata automatic processing - automatic keyword extraction based on unsupervised approaches, and we demonstrate a real application during a process of innovation and optimization of medical education. The outputs of our pilot analysis represent systematic description of medical curriculum by three different approaches (centrality measures) used for relevant keywords extraction. Further evaluation by senior curriculum designers and guarantors is required to obtain an objective benchmark.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Medical and healthcare study programmes are quite complicated in terms of branched structure and heterogeneous content. In logical sequence a lot of requirements and demands placed on students appear there. This paper focuses on an innovative way how to discover and understand complex curricula using modern information and communication technologies. We introduce an algorithm for curriculum metadata automatic processing -- automatic keyword extraction based on unsupervised approaches, and we demonstrate a real application during a process of innovation and optimization of medical education. The outputs of our pilot analysis represent systematic description of medical curriculum by three different approaches (centrality measures) used for relevant keywords extraction. Further evaluation by senior curriculum designers and guarantors is required to obtain an objective benchmark.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Navrhujeme metodu, která zlepšuje závislostní parsing složených vět. Metoda předpokládá segmentaci věty do klauzí a nevyžaduje přetrénování parseru. Klauzální strukturu věty reprezentujeme pomocí klauzálních grafů, které poskytují informaci o vnoření každé klauze. Navrhujeme postup, ve kterém parsujeme klauze nezávisle a vzniklé závislostní stromy vkládáme jako podstromy do finálního stromu pro celou větu. Metodu aplikujeme na češtinu a experimentujeme s MST parserem natrénovaným na PDT 2.0. Dosahujeme zvýšení UAS o 0.97%.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We propose a method for improving the dependency parsing of complex sentences. This method assumes segmentation of input sentences into clauses and does not require to re-train a parser of one's choice. We represent a sentence clause structure using clause charts that provide a layer of embedding for each clause in the sentence. Then we formulate a parsing strategy as a two-stage process where (i) coordinaed and subordinated clauses of the sentence are parsed separately with respect to the sentence clause chart and (ii) their dependency trees become subtrees of the final tree of the sentence. The object language is Czech and the parser used is a maximum spanning tree parser trained on the Prague Dependency Treebank. We have achieved an average 0.97% improvement in the unlabeled attachment score. Although the method has been designed for the dependency parsing of Czech, it is useful for other parsing techniques and languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>RExtractor je systém pro extrakci informací. Vstupní dokumenty jsou zpracovány NLP nástroji. Extrakce následně probíhá pomocí dotazů nad závislostními stromy. Výsledkem je znalostní báze dokumentu, definována jako množina entit a vztahů mezi nima. Dotazy nad stromy jsou definovány manuálně. Architektura systému je navržena doménově a jazykově nezávisle. Systém demonstrujeme na českých a anglických právních dokumentech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The RExtractor system is an information extractor that processes input documents by natural language processing tools and consequently queries the parsed sentences to extract a knowledge base of entities and their relations. The extraction queries are designed manually using a tool that enables natural graphical representation of queries over dependency trees. A workflow of the system is designed to be language and domain independent. We demonstrate RExtractor on Czech and English legal documents.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>RExtractor je systém pro extrakci informací. Vstupní dokumenty jsou zpracovány NLP nástroji. Extrakce následně probíhá pomocí dotazů nad závislostními stromy. Výsledkem je znalostní báze dokumentu, definována jako množina entit a vztahů mezi nima. Dotazy nad stromy jsou definovány manuálně. Architektura systému je navržena doménově a jazykově nezávisle. Systém demonstrujeme na českých a anglických právních dokumentech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The RExtractor system is an information extractor that processes input documents by natural language processing tools and consequently queries the parsed sentences to extract a knowledge base of entities and their relations. The extraction queries are designed manually using a tool that enables natural graphical representation of queries over dependency trees. A workflow of the system is designed to be language and domain independent. We demonstrate RExtractor on Czech and English legal documents.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Navrhujeme metodu, která zlepšuje závislostní parsing složených vět. Metoda předpokládá segmentaci věty do klauzí a nevyžaduje přetrénování parseru. Klauzální strukturu věty reprezentujeme pomocí klauzálních grafů, které poskytují informaci o vnoření každé klauze. Navrhujeme postup, ve kterém parsujeme klauze nezávisle a vzniklé závislostní stromy vkládáme jako podstromy do finálního stromu pro celou větu. Metodu aplikujeme na češtinu a experimentujeme s MST parserem natrénovaným na PDT 2.0. Dosahujeme zvýšení UAS o 0.97%.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We propose a method for improving the dependency parsing of complex sentences. This method assumes segmentation of input sentences into clauses and does not require to re-train a parser of one's choice. We represent a sentence clause structure using clause charts that provide a layer of embedding for each clause in the sentence. Then we formulate a parsing strategy as a two-stage process where (i)  coordinaed and subordinated clauses of the sentence are parsed separately with respect to the sentence clause chart and (ii) their dependency trees become subtrees of the final tree of the sentence. The object language is Czech and the parser used is a maximum spanning tree parser trained on the Prague Dependency Treebank. We have achieved an average 0.97% improvement in the unlabeled attachment score. Although the method has been designed for the dependency parsing of Czech, it is useful for other parsing techniques and languages.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nového člena rodiny pražských závislostních korpusů. Český korpus právních textů je morfologicky a syntakticky anotovaný korpus 1128 vět, který obsahuje texty z právní domény, konkrétně dokumenty ze Sbírky zákonů České republiky. Právní texty se odlišují od jiných domén v několika jazykových jevech vyplývajících z vysoké četnosti velmi dlouhých vět. Manuální anotace takových vět představuje novou výzvu. Popisujeme strategii a nástroje pro tento úkol. Korpus je dostupný několika způsoby, a sice z repozitáře LINDAT/CLARIN a on-line pomocí aplikací KonText a TreeQuery.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a new member of the family of Prague dependency treebanks. The Czech Legal Text Treebank 1.0 is a morphologically and syntactically annotated corpus of 1,128 sentences. The treebank contains texts from the legal domain, namely the documents from the Collection of Laws of the Czech Republic. Legal texts differ from other domains in several language phenomena influenced by rather high frequency of very long sentences. A manual annotation of such sentences presents a new challenge. We describe a strategy and tools for this task. The resulting treebank can be explored in various ways. It can be downloaded from the LINDAT/CLARIN repository and viewed locally using the TrEd editor or it can be accessed on-line using the KonText and TreeQuery tools.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>VIADAT-REPO je součást projektu VIADAT, tj. část "virtuálního asistenta" pro zpracování, anotaci, obohacení a přístup k audio a video nahrávkám. Základem tohoto softwaru je repozitář lindat-dspace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>VIDATA-REPO is a modification to lindat-dspace platform; it's a part of the VIADAT project and as such will be a part of a "virtual assistant" for processing, annotation, enrichment and accessing of audio and video recordings.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Souhrnný článek o přípravě, průbehu a výsledcích soutěže vyhledávání zdravotních informací CLEF eHealth Evaluation Lab 2016.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper details the collection, systems and evaluation methods used in the IR Task of the CLEF 2016 eHealth Evaluation Lab. This task investigates the effectiveness of web search engines in providing access to medical information for common people that have no or little medical knowledge. The task aims to foster advances in the development of search technologies for consumer health search by providing resources and evaluation methods to test and validate search systems. The problem considered in this year’s task was to retrieve web pages to support the information needs of health consumers that are faced by a medical condition and that want to seek relevant health information online through a search engine. As part of the evaluation exercise, we gathered 300 queries users posed with respect to 50 search task scenarios. The scenarios were developed from real cases of people seeking health information through posting requests of help on a web forum. The presence of query variations for a single scenario helped us capturing the variable quality at which queries are posed. Queries were created in English and then translated into other languages. A total of 49 runs by 10 different teams were submitted for the English query topics; 2 teams submitted 29 runs for the multilingual topics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>nástroj, který pomůže uživateli vybrat vhodnou veřejnou licenci jeo jeho data nebo software</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Customizable tool that will help user select the right public license for his data or software</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CLARIN Concept Registry (CCR, Rejstřík konceptů CLARIN) je součást Infrastruktury CLARIN a místo, kde se definuje společná a sdílená sémantika. To je důležité pro dosažení sémantické interoperability a pro překonání různosti datových struktur v metadatech a lingvistických zdrojích, se kterou se v rámci infrastruktury setkáváme.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The CLARIN Concept Registry (CCR) is the place in the CLARIN Infrastructure where common
and shared semantics are defined. This is important to achieve semantic interoperability, and to overcome to a degree the diversity in data structures, either in metadata or linguistic resources, encountered within the infrastructure. Whereas in the past, CLARIN has been using the ISOcat registry for these purposes, nowadays this new CCR registry is being used, as ISOcat turned out to have some serious drawbacks as far as its use in the CLARIN community is concerned. The main difference between both semantic registries is that the new CCR registry is a concept
registry whereas ISOcat is a data category registry. In this paper we describe why this choice has been made. We also describe the most important other characteristics of the new (Open)SKOSbased registry, as well as the management procedures used to prevent a recurrent proliferation of entries, as was the case with ISOcat.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku se zabýváme jedním typem strukturní odlišnosti mezi českými a anglickými paralelními větami v dvojjazyčném korpusu PCEDT, konkrétně vzájemným mapováním valenčních doplnění aktorového typu a (nevalenčních) lokačních doplnění. Analýzou korpusových příkladů ukazujeme, do jaké míry se na rozdílech ve valenci sloves v překladově ekvivalentních větách podílejí jazykově specifické- syntakticko-sémantické preference konkrétních slov na pozicích subjektu a predikátu</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the talk, we discuss one particular type of structural difference between Czech and English parallel sentences in a bilingual dependency treebank PCEDT, i.e., the mapping of valency complementation of teh ACT type and the free modification of the LOC type. In the analysis of corpus sentences we comment on the impact of the language specific syntacto-semantic preferences of concrete words in the position of subject and predicate on the differences in valency patterns of verbs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>PDiT 2.0 je novou verzí Pražského diskurzního korpusu. Přináší komplexní anotaci diskurzních jevů, obohacenou o anotaci sekundárních konektorů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>PDiT 2.0 is a new version of the Prague Discourse Treebank. It contains a complex annotation of discourse phenomena enriched by the annotation of secondary connectives.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Popisujeme experimenty s použitím rozlišení významu slov (WSD) ve strojovém překladu. Zaměřujeme se na WSD u sloves a používáme dva různé přístupy -- slovesné vzorce založené na metodě corpus pattern analysis a významy sloves z valenčního slovníku. Vyhodnocujeme několik způsobů, jak použít významy sloves jako dodatečný faktor ve strojovém překladači Moses. Výsledky ukazují statisticky signifikantní zlepšení kvality překladu z pohledu metriky BLEU pro přístup s valenčním slovníkem, ale v ruční evaluaci vycházejí obě metody WSD jako přínos.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe experiments in Machine Translation using word sense disambiguation (WSD) information. This work focuses on WSD in verbs, based on two different approaches -- verbal patterns based on corpus pattern analysis and verbal word senses from valency frames. We evaluate several options of using verb senses in the source-language sentences as an additional factor for the Moses statistical machine translation system.
Our results show a statistically significant translation quality improvement in terms of the BLEU metric for the valency frames approach, but in manual evaluation, both WSD methods bring improvements.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento představuje nástroj MT-ComparEval pro kvalitativní vyhodnocení strojového překladu. MT-ComparEval je opensourcový nástroj, který byl navržen tak, aby pomáhal vývojářům strojových překladačů díky grafickému rozhraní, které umožňuje porovnávat a vyhodnocovat různé překladače či experimenty a nastavení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper showcases the MT-ComparEval tool for qualitative evaluation of machine translation (MT). MT-ComparEval is an opensource
tool that has been designed in order to help MT developers by providing a graphical user interface that allows the comparison
and evaluation of different MT engines/experiments and settings.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek reaguje na potřebu podpory strojového překladu pomocí vývojových cyklů s integrovanými evaluačními metodami. Naším cílem je zhodnotit, porovnat a vylepšit různé varianty strojových překladačů. Článek pojednává o nových nástrojích a praktikách, které podporují informovaný přístup k vývoji strojových překladačů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This work addresses the need to aid Machine Translation (MT) development cycles with a complete workflow of MT evaluation methods. Our aim is to assess, compare and improve MT system variants. We hereby report on novel tools and practices that support various measures, developed in order to support a principled and informed approach of MT development</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje náš systém pro aspektovou analýzu postojů (ABSA). Účastníme se podúkolu 1 (ABSA na úrovni vět), v jeho rámci se pak zaměřujeme na detekci kategorií aspektů. Trénujeme binární klasifikátory pro každou kategorii. Letošní rozšíření o další jazyky motivuje jazykově nezávislé přístupy. Navrhujeme využít neuronové sítě, které by měly automaticky detekovat v datech jazykové konstrukce, a tak omezit nutnost využívat jazykově závislé nástroje a ruční návrh rysů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This  paper  describes  our  system  for  aspect-based sentiment analysis (ABSA). We participate in Subtask 1 (sentence-level ABSA), focusing specifically on aspect category detection. We  train  a  binary  classifier  for  each category. This  year’s  addition  of  multiple languages  makes  language-independent  approaches  attractive. We  propose  to  utilize neural  networks  which  should  be  capable  of
discovering linguistic patterns in the data automatically,   thereby  reducing  the  need  for language-specific tools and feature engineering.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Diskriminativní překladové modely, které využívají zdrojový kontext, zlepšují kvalitu statistického strojového překladu. V tomto článku navrhujeme nové rozšíření, které navíc využívá i informace z cílového kontextu. Ukazujeme, že i takový model lze efektivně integrovat přímo do procesu dekódování. Náš přístup lze uplatnit i na velká trénovací data a jeho využití konzistentně zlepšuje kvalitu překladu u čtyř jazykových párů. Analyzujeme také zvlášť přínos zdrojového a cílového kontextu a ukazujeme, že toto rozšíření lépe zachycuje morfologickou shodu. Model je volně dostupný v rámci softwaru Moses.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Discriminative translation models utilizing source context have been shown to help statistical machine translation performance. We propose a novel extension of this work using target context information. Surprisingly, we show that this model can be efficiently integrated directly in the decoding process. Our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs. We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and target-context model and we show that our extension allows us to better capture morphological coherence. Our work is freely available as part of Moses.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Již v minulosti se ukázalo, že parafrázování referenčních překladů zlepšuje korelaci s lidským hodnocením při automatickém vyhodnocování strojového překladu. V této práci představujeme novou sadu dat k vyhodnocování anglicko-českého překladu založenou na automatických parafrázích. Tuto sadu porovnáváme s existující sadou ručně tvořených parafrází a zjišťujeme, že i automatické parafráze mohou hodnocení překladu zlepšit. Navrhujeme a vyhodnocujeme také několik kritérií pro výběr vhodných referenčních překladů z větší sady.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Paraphrasing of reference translations has been shown to improve the correlation with human judgements in automatic evaluation of
machine translation (MT) outputs. In this work, we present a new dataset for evaluating English-Czech translation based on automatic
paraphrases. We compare this dataset with an existing set of manually created paraphrases and find that even automatic paraphrases can
improve MT evaluation. We have also propose and evaluate several criteria for selecting suitable reference translations from a larger set.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje společnou praci UK a LMU nad anglické-českého a anglický-rumunského systémy frázových překladu pro WMT16. V porovnání s minulými pokusy, jsme přísně omezili trénovací daty na constrained, aby bylo možné spolehlivé porovnat naší systémy s jiné výzkumné systémy. My jsme zkoušeli využiti několika dalších modelů v naších systémech, včetně diskriminačního modelu frázových překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the phrase-based systems
jointly submitted by CUNI and LMU
to English-Czech and English-Romanian
News translation tasks of WMT16. In contrast
to previous years, we strictly limited
our training data to the constraint datasets,
to allow for a reliable comparison with
other research systems. We experiment
with using several additional models in our
system, including a feature-rich discriminative
model of phrasal translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>UDPipe je trénovatelný nástroj pro tokenizaci, tagging, lemmatizaci a závislostní parsing CoNLL-U souborů. UDPipe je jazykově nezávislý a pro natrénování jazykového modelu stačí označkovaná data v CoNLL-U formátu. Předtrénované jazykové modely jsou k dispozici pro téměř všechny UD korpusy. UDPipe je k dispozici jako spustitelný soubor, jako knihovna pro C++, Python, Perl, Java, C#, a také jako webová služba.

UDPipe je svobodný software licencovaný pod Mozilla Public License 2.0 a jazykové modely jsou k dispozici pro nekomerční použití pod licencí CC BY-NC-SA, nicméně původní data použitá k vytvoření modelů mohou v některých případech ukládat další licenční omezení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>UDPipe is an trainable pipeline for tokenization, tagging, lemmatization and dependency parsing of CoNLL-U files. UDPipe is language-agnostic and can be trained given only annotated data in CoNLL-U format. Trained models are provided for nearly all UD treebanks. UDPipe is available as a binary, as a library for C++, Python, Perl, Java, C#, and as a web service.

UDPipe is a free software under Mozilla Public License 2.0 and the linguistic models are free for non-commercial use and distributed under CC BY-NC-SA license, although for some models the original data used to create the model may impose additional licensing conditions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme naše softwarové nástroje pro NLP: UDPipe, open-source nástroj pro zpracování CoNLL-U souborů, který provádí tokenizaci, morfologickou analýzu, určování slovních druhů a závislostní parsing pro 32 jazyků; a NameTag, rozpoznávač pojmenovaých entit pro češtinu a angličtinu. Oba nástroje dosahují vynikajících výsledků, jsou dostupné s předtrénovanými modely a mají minimální nároky na čas a paměť počítače. UDPipe a NameTag lze také trénovat s použitím vlastních dat. Oba nástroje jsou open-source a jsou dostupné v licenci Mozilla Public License 2.0 (software) a CC BY-NC-SA (data). Program, C++ knihovna s vazbami na Python, Perl, Java a C# a také online web service a demo jsou dostupné na odkazech http://ufal.mff.cuni.cz/udpipe a http://ufal.mff.cuni.cz/nametag.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present our in-house software tools for NLP: UDPipe, an open-source tool for processing CoNLL-U files which performs tokenization, morphological analysis, POS tagging and dependency parsing for 32 languages; and NameTag, a named entity recognizer for Czech and English. Both tools achieve excellent performance and are distributed with pretrained models, while running with minimal time and memory requirements. UDPipe and NameTag are also trainable with your own data. Both tools are open-source and distributed under Mozilla Public License 2.0 (software) and CC BY-NC-SA (data). The binary, C++ library with Python, Perl, Java and C# bindings along with online web service and demo are available at http://ufal.mff.cuni.cz/udpipe and http://ufal.mff.cuni.cz/nametag.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Při automatickém zpracování rozsáhlých textů v přirozeném jazyce se často opakují podobné úkoly v několika jazycích: i při zpracování obtížných úloh jsou texty vždy zpracovávány obvyklými základními kroky od tokenizace k parsingu. Představujeme mimořádně jednoduchý a použitelný nástroj pro základní zpracování přirozeného jazyka, který sestává pouze z jednoho programu a jednoho modelu (pro každý jazyk). Tento nástroj provádí tyto úkoly pro mnoho jazyků, aniž by vyžadoval dodatečná data. UDPipe je tedy nástroj, který zpracovává soubory ve formátu CoNLL-U a provádí tokenizaci, morfologickou analýzu, rozpoznávání slovních druhů, lematizaci a závislostní parsing pro téměř všechny jazyky korpusu Universtal Dependencies 1.2 (konkrétně je nástroj dostupný pro 32 jazyků). Navíc je celý nástroj snadno trénovatelný při použití vlastních trénovacích dat v CoNLL-U formátu a vyžaduje minimální znalost lingvistiky. Kód pro trénovaní nástroje je také dostupný.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Automatic natural language processing of large texts often presents recurring challenges in multiple languages: even for most advanced tasks, the texts are first processed by basic processing steps – from tokenization to parsing. We present an extremely simple-to-use tool consisting of one binary and one model (per language), which performs these tasks for multiple languages without the need for any other external data. UDPipe, a pipeline processing CoNLL-U-formatted files, performs tokenization, morphological analysis, part-of-speech tagging, lemmatization and dependency parsing for nearly all treebanks of Universal Dependencies 1.2 (namely, the whole pipeline is currently available for 32 out of 37 treebanks). In addition, the pipeline is easily trainable with training data in CoNLL-U format (and in some cases also with additional raw corpora) and requires minimal linguistic knowledge on the users’ part. The training code is also released.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jedním z cílů infrastruktury LINDAT/CLARIN je poskytovat technické prostředky pro sdílení nástrojů a dat pro výzkumné účely a v souladu s technickými požadavky CLARIN ERIC. Proto byl vybudován a je rozvíjen tento silně adaptovaný repozitářový systém založený na platformě DSpace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>One of the goals of LINDAT/CLARIN Centre for Language Research Infrastructure is to provide technical background to institutions or researchers who wants to share their tools and data used for research in linguistics or related research fields. The digital repository is built on a highly customised DSpace platform.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CLARIN SPF není jediná inter-federace, ale je první, která se systematicky zabývá problémy, které působí neuvolnění povinných atributů poskytovateli identit. Například, jsou-li data zveřejněná pod restriktivní licencí, musí být uživatel jednoznačně identifikovatelný v průběhu času. Pokud poskytovatel identity nezpřístupní potřebnou informaci, služba nemůže uživateli dovolit, aby data získal.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CLARIN Service Provider Federation is not the only inter-federation but it is the first one to systematically address the issue when Identity Providers do not release mandatory attributes to a service. For instance, if a data set is licensed under a restrictive license the user must be uniquely identifiable over time. However, if the Identity Provider does not release such information, the service cannot let the user download the data.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku představujeme shortref.org - službu pro snadnou a trvalou citaci
dat vzešlých z výzkumu. Tuto službu poskytujeme formou zkracovače URL.
Reprodukovatelnost výsledků, důležitá součást vědeckého výzkumu, přímo závisí
na dostupnosti dat, nad kterými výzkum probíhal.
Rozvoj webových technologií velmi usnadnil sdílení dat. Kvůli
dynamické povaze webu se ale obsah často přesouvá z jednoho místa na druhé.
Běžné URL, které může výzkumník použít k citaci svých dat, nemá prostředky,
kterými by se s tímto přesouváním dalo vyrovnat. Čtenář/uživatel tak často
zjistí, že data se na dané URL již nenalézají, nebo dostane k dispozici novější
verzi.
Námi navrhované řešení, ve kterém je zkrácené URL perzistentním
identifikátorem, poskytuje spolehlivý mechanismus pro stálou dostupnost dat a
může tak zlepšit dopad výzkumu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present a easy-to-cite and persistent infrastructure (shortref.org) for research and data citation in the form a URL shortener service. Reproducibility of results is very important for the extension of research and is directly depends on the availability of the research data. The advancements in the web technologies made redistribution of the data much more easy nowadays, however, due to the dynamic nature of the web, the content is consistently on the move from one destination to another. The URLs researchers use for the citation of their contents do not directly account for these changes and many times when the users try to access the cited URLs, the data is either not available or moved to a newer version. In our proposed solution, the shortened URLs are not simple URLs but use persistent identifiers and provide a reliable mechanism to make the data always accessible that can directly improve the impact of research.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje společný systém projektů QT21 a HimL pro překlad z angličtiny do rumunštiny. Systém je automatickou kombinací 12 základních systémů připravených několika skupinami.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the joint submission
of the QT21 and HimL projects for
the English→Romanian translation task of
the ACL 2016 First Conference on Machine
Translation (WMT 2016). The submission
is a system combination which
combines twelve different statistical machine
translation systems provided by the
different groups (RWTH Aachen University,
LMU Munich, Charles University in
Prague, University of Edinburgh, University
of Sheffield, Karlsruhe Institute of
Technology, LIMSI, University of Amsterdam,
Tilde). The systems are combined
using RWTH’s system combination
approach. The final submission shows an
improvement of 1.0 BLEU compared to the
best single system on newstest2016.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme vysoce škálovatelný přístup k otevřenému question answeringu,
který nezávisí na datasetu mapujícím z logické formy na povrchovou, ani
na jakýchkoliv lingvistických nástrojích pro analýzu jako např. POS tagger nebo
rozpoznavání pojmenovaných entit. Popisovaný přístup je definován v rámci
frameworku Constrained Conditional Models, který umožnuje škálovat
znalostní grafy bez jakéhokoliv omezení velikosti. Ve standardním benchmarku jsme dosáhli
výsledky srovnatelné se state-of-the-art v otevřeném question answeringu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a highly scalable approach for open-domain question answering with no dependence on any logical form to surface form mapping data set or any linguistic analytic tool such as POS tagger or named entity recognizer. We define our approach under the Constrained Conditional Models framework which lets us scale to a full knowledge graph with no limitation on the size. On a standard benchmark, we obtained competitive results to state-of-the-art in open-domain question answering task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme vysoce škálovatelný přístup k otevřenému question answeringu,
který nezávisí na datasetu mapujícím z logické formy na povrchovou, ani
na jakýchkoliv lingvistických nástrojích pro analýzu, jako např. POS tagger nebo
rozpoznavání pojmenovaných entit. Popisovaný přístup je definován v rámci
frameworku Constrained Conditional Models, který umožnuje škálovat
znalostní grafy bez omezení velikosti. Ve standardním benchmarku jsme dosáhli
4% zlepšení výsledků oproti state-of-the-art v otevřeném question answeringu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We introduce a highly scalable approach for open-domain question answering with no dependence on any data set for surface form to logical form mapping or any linguistic analytic tool such as POS tagger or named entity recognizer. We define our approach under the Constrained Conditional Models framework which lets us scale up to a full knowledge graph with no limitation on the size. On a standard benchmark, we obtained near 4 percent improvement over the state-of-the-art in open-domain question answering task.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje 12 systémů zaslaných do úlohy strojového překladu textů z IT domény v rámci WMT16. Systémy překládají z angličtiny do 6 jazyků: baskičtiny, bulharštiny, holandštiny, češtiny, portugalštiny a španělštiny. Všechny systémy byly vyvinuty v rámci projektu QTLeap. Pro každý jazykový pár byly zaslány dva systémy: frázový (Moses) a hloubkově-syntaktický (TectoMT, krom bulharštiny). Pro 4 z 6 jazyků dosáhlo TectoMT lepších výsledků než Moses.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the description of 12
systems submitted to the WMT16 IT-task,
covering six different languages, namely
Basque, Bulgarian, Dutch, Czech, Portuguese
and Spanish. All these systems
were developed under the scope of the
QTLeap project, presenting a common
strategy. For each language two different
systems were submitted, namely a phrase-based
MT system built using Moses, and
a system exploiting deep language engineering
approaches, that in all the languages
but Bulgarian was implemented
using TectoMT. For 4 of the 6 languages,
the TectoMT-based system performs better
than the Moses-based one.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje náš systém UFAL_MULTIVEC v soutěži WMT16 Quality Estimation. Systém odhaduje kvalitu strojového překladu z angličtiny do němčiny. Náš přínos spočívá v rozšíření zavedené metody o dvojjazyčné vektorové reprezentace slov a o slovní zarovnání.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes our submission
UFAL MULTIVEC to the WMT16 Quality
Estimation Shared Task, for EnglishGerman
sentence-level post-editing effort
prediction and ranking. Our approach exploits
the power of bilingual distributed
representations, word alignments and also
manual post-edits to boost the performance
of the baseline QuEst++ set of
features. Our model outperforms the
baseline, as well as the winning system
in WMT15, Referential Translation Machines
(RTM), in both scoring and ranking
sub-tasks.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek se zabývá studiem diskurzních konektorů umístěných mimo oba argumenty, které jsou jimi spojeny.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>During annotation of discourse connectives in Czech written texts, externally placed connectives were found (outside both their arguments and non-adjacent to them). These cases occur in vast majority in connection with attribution (reporting).
In these structures, a connective expressing contrast between two reported contents moves left in order to stand closer to its left (first in the linear order) argument. We call this phenomenon connective movement.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V rámci přednásky byly představeny existující teorie anotace diskurních jevů a nástroje, které se pro tento typ anotací používají.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Discourse coherence is a complex natural language phenomenon which is achieved by different linguistic means (e.g., anaphoricity, information structure, discourse markers and connectives, rhetorical structure of text, etc.). Many approaches in computational linguistics are used to capture discourse relations and Tind practical applications. This session focuses on discussing theories and approaches applied to the annotation of discourse phenomena in different languages, such as Rhetorical Structure Theory, Penn Discourse Treebank, Segmented Discourse Representation Theory and so on. Participants will have the opportunity to compare these approaches to see which phenomena are central to them and which ones are less prominent. We will also introduce the tools of discourse annotation and demonstrate esp. TrEd, PDTB and MMAX.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace software, který pomáhá autorovi vybrat pro jeho dílo nejvhodnější (nejsvobodnější) veřejnou licenci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presenting a software solution that helps authors choose the best public license for their works.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek prezentuje nový online interaktivní nástroj vydaná pod OpenSource licensí MIT, který usnadňuje uživateli pomocí sekvence otázek a jejich vysvětlení výběr tzv. veřejné licence, pokud je ji možno jeho výsledku (datům nebo software) přiřadit. Cílem nástroje je podpořit nejotevřenější licencování vědeckých výsledků, které je v souladu se zákony možné, a vůbec podpora autorů v tom, aby své výsledky zvěřejňovali a dokázali jim přiřadit vhodnou licenci.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Researchers  in  Natural  Language  Processing rely  on  availability  of  data  and  software, ideally under open licenses, but little is done to actively encourage it. In fact, the current Copyright framework grants exclusive rights to authors to copy their works, make them available to the public and make derivative works (such as annotated language corpora). Moreover, in the EU databases are protected against unauthorized extraction and re-utilization of their contents. Therefore,  proper  public  licensing  plays  a  crucial  role  in providing access to research data.  A public license is a license that grants certain rights not to one particular user, but to the general public (everybody). Our article presents a tool that we developed and whose purpose is to assist the user in the licensing process. As software and data should be licensed under different licenses, the tool is composed of two separate parts: Data and Software. The underlying logic as well as elements of the graphic interface are presented.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Spojité distribuované slovní reprezentace prokázaly svoji užitečnost v mnoha úlohách zpracování přirozeného jazyka. V tomto článku používáme vektorové reprezentace slov jako další vstupy pro MST parser. Použití distribuovaných reprezentací umožňuje snížit dimenzionalitu lexikálních rysů a neuronovou sítí odpadá nutnost ručního ladění kombinací rysů. Přestože jeho úspěšnost je nižší, než u klasického MST Parseru, výsledný model je výrazně menší než modely využívající klasickou sadu rysů. Navíc funguje velice dobře pro jazyky, pro které je dostupný pouze relativně malý treebank a výsledky vypadají velice slibně i pro delexikalizovaný parsing.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Continuous word representations appeared to be a useful feature in many natural language processing tasks.  Using fixed-dimension
pre-trained word embeddings allows avoiding sparse bag-of-words representation and to train models with fewer parameters.  In this
paper,  we  use  fixed  pre-trained  word  embeddings  as  additional  features  for  a  neural  scoring  function  in  the  MST  parser.   With  the
multi-layer architecture of the scoring function we can avoid handcrafting feature conjunctions. The continuous word representations on
the input also allow us to reduce the number of lexical features, make the parser more robust to out-of-vocabulary words, and reduce the
total number of parameters of the model.  Although its accuracy stays below the state of the art, the model size is substantially smaller
than with the standard features set.  Moreover, it performs well for languages where only a smaller treebank is available and the results
promise to be useful in cross-lingual parsing.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku se zabýváme získáváním textových informací z fotografií. Dosud se v této oblasti pracovalo pouze s rozpoznáváním izolovaných slov a krátká slova se pro zjednodušení vynechávala. Tento přístup není vhodný pro další lingvistické zpracování rozpoznaného textu. Proto se pokoušíme a lepší definice úlohy, která zahrnuje i souvislost rozpoznaného textu. Rozšířili jsme anotaci stávajících datastetů a vyvinuli evaluační metriku, které bude sloužit k hodnocení rozpoznávání souvislého textu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we deal with extraction of textual information from scene images.  So far, the task of Scene Text Recognition (STR) has only been focusing on recognition of isolated words and, for simplicity, it omits words which are too short.  Such an approach is not suitable for further processing of the extracted text.  We define a new task which aims at extracting coherent blocks of text from scene images with regards to their future use in natural language processing tasks, mainly machine translation.  For this task, we enriched the annotation of existing STR benchmarks in English and Czech and propose a string-based evaluation measure that highly correlates with human judgment.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Neuronové sekvenční modely jsou velmi slibným novým paradigmatem ve strojovém překladu, které dosahuje srovnatelných výsledků s frázovým strojovým překladem. Tento článek popisuje systém, se kterým se Univerzita Karlova účastnila soutěže při WMT16 v úkolech automatické post-editace strojového překladu a multimodálního strojového překladu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Neural sequence to sequence learning recently became a very promising paradigm in machine translation, achieving competitive results with statistical phrase-based systems. In this system description paper, we attempt to utilize several recently published methods used for neural sequential learning in order to build systems for WMT 2016 shared tasks of Automatic Post-Editing and Multimodal Machine Translation.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V prezentaci jsem ukázala jak se dá vyhledávat v korpusech z repozitáře Lindat v KonTextu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I described describe how KonText – a corpus query interface for Czech National Corpus is adopted for handling various corpora from Lindat. The repository contains corpora with different types of annotation – like syntactic, shallow semantic, sentiment and other types. I showed how to search for this information within the KonText environment using CQL (Corpus Query Language) on the example of two corpora. First, I focused on the Universal Dependencies – syntactically annotated treebanks for several languages. Secondly, I demonstrated the queries over the Prague Dependency Treebank that are related both to syntactic (analytical) and deep syntactic (tectogrammatical) layers. I also showed several query examples from other Lindat corpora.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V posteru jsem popsala jak víceslovné výrazy se dají reprezentovat a vyhledávít v systému KonText.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the poster, I demonstrated how multiword expressions can be represented and quired in a corpus query system KonText</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku popisujeme jak se dá vyhledávat v syntakticky anotovaných korpusech v KonTextu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we describe an addition to the corpus query system Kontext that enables to enhance the search using syntactic attributes in addition to the existing features, mainly lemmas and morphological categories. We present the enhancements of the corpus query system itself, the attributes we use to represent syntactic structures in data, and some examples of querying 
the syntactically annotated corpora, such as treebanks in various languages as well as an automatically parsed large corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Vztah mwe se v současném vydání Universal Dependencies nepoužívá konzistentně. To může být zčásti způsobeno rozdíly mezi jazyky, nicméně v našem příspěvku ukazujeme na blízce příbuzných jazycích, že tomu tak není vždy; i doslovně ekvivalentní výrazy jsou v některých případech anotovány rozdílně.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The UD relation mwe is not used consistently in the current release of Universal Dependencies. While part of the issue may be caused by true linguistic differences, we demonstrate on closely related languages that it is not always the case; even literally equivalent expressions do not always receive the same analysis.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se soustřeďuje na včlenění valenčního slovníku do systému TectoMT pro jazykový pár čeština-ruština. Ukazuje na chyby ve valenci na výstupu ze systému a popisuje, jak začlenění slovníku ovlivnilo výsledky. Ačkoli podle BLEU skóre nebylo zaznamenáno zlepšení, ruční inspekce prokázala zlepšení v některých konkrétních případech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we focus on the incorporation of a valency lexicon into TectoMT system for Czech-Russian language pair.  We demonstrate valency errors in MT output and describe how the introduction of a lexicon influenced the translation results.  Though there was no impact on BLEU score, the manual inspection of concrete cases showed some improvement.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek popisuje paralelní korpusy s automatickou anotací pomocí několika nástrojů pro zpracování přirozeného jazyka, včetně lemmatizace, taggingu, rozpoznání a klasifikace pojmenovaných entit a jejich disambiguace, word-sense disambiguation  a koreference.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This work presents parallel corpora automatically annotated with several NLP tools, including lemma and part-of-speech tagging,
named-entity recognition and classification, named-entity disambiguation, word-sense disambiguation, and coreference.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tento článek představuje datovou sadu vytvořenou z přirozených dialogů, která umožňuje testovat schopnost dialogových systémů učit se nová fakta od uživatelů v průběhu dialogu. Toto interaktivní učení pomůže s jedním z nejpodstatnějších problémů dialogových systémů v otevřených doménách, kterým  je omezenost dat, se kterými je dialogový systém schopen pracovat. Představovaná datová sada, skládající se z 1900 dialogů, umožňuje simulaci interaktivního získávání rad od uživatelů v podobě odpovědí a podrobných vysvětlení otázek, které mohou být použity pro interaktivní učení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a dataset collected from natural dialogs which enables to test the ability of dialog systems to learn new facts from user utterances throughout the dialog. This interactive learning will help with one of the most prevailing problems of open domain dialog system, which is the sparsity of facts a dialog system can reason about. The proposed dataset, consisting of 1900 collected dialogs, allows simulation of an interactive gaining of denotations and questions explanations from users which can be used for the interactive learning.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dataset sesbíraný z přirozených dialogů umožňující testovat schopnost dialogových systémů učit se nová fakta z uživatelských výpovědí v průběhu dialogu. Dataset, skládající se z 1900 dialogů umožňuje simulovat interaktivní získávání denotací a vysvětlení k otázkám od uživatelů ve formě, která je vhodná pro interaktivní učení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Dataset collected from natural dialogs which enable to test the ability of dialog systems to interactively learn new facts from user utterances throughout the dialog. The dataset, consisting of 1900 dialogs, allows simulation of an interactive gaining of denotations and questions explanations from users which can be used for the interactive learning.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Chimera systém strojovéo překladu, který kombinuje hluboce lingvistické jádro TectoMT s frázovým strojovým překladačem Moses. Pro anglicko-český překlad také používá post-editovací systém Depfix. Všechny komponenty běží na platformě Unix/Linux a jsou open-source (dostupné z Perlového repozitáře CPAN a repozitáře LINDAT/CLARIN). Hlavní webová stránka je https://ufal.mff.cuni.cz/tectomt. Vývoj je momentálně podporován projektem QTLeap ze 7th FP (http://qtleap.eu).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Chimera is a machine translation system that combines the TectoMT deep-linguistic core with phrase-based MT system Moses. For English–Czech pair it also uses the Depfix post-correction system. All the components run on Unix/Linux platform and are open source (available from Perl repository CPAN and the LINDAT/CLARIN repository). The main website is https://ufal.mff.cuni.cz/tectomt. The development is currently supported by the QTLeap 7th FP project (http://qtleap.eu).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme MLFix, systém pro automatickou statistickou post-editaci, který je duchovním
následníkem pravidlového systému, Depfixu. Cílem této práce bylo prozkoumat možné postupy
automatické identifikace nejčastějších morfologických chyb tvořených současnými systémy pro strojový
překlad a natrénovat vhodné statistické modely, které by byly postaveny na získaných znalostech.
Provedli jsme automatickou i ruční evaluaci našeho systému a výsledky porovnali s Depfixem. Systém
byl vyvíjen především na výstupech anglicko-českého strojového překladu, cílem
ale bylo zobecnit post-editační proces tak, aby byl aplikovatelný na další jazykové páry. Upravili jsme
původní pipeline, aby post-editovala výstupy anglicko-německého strojového překladu, a provedli
dodatečnou evaluaci této modifikace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present MLFix, an automatic statistical post-editing system, which is a spiritual successor to the rulebased
system, Depfix. The aim of this
thesis was to investigate the possible approaches to automatic identification of the most common
morphological errors produced by the state-of-the-art machine translation systems and to train sufficient
statistical models built on the acquired knowledge. We performed both automatic and manual evaluation
of the system and compared the results with Depfix. The system was mainly developed on the English-toCzech
machine translation output, however, the aim was to generalize the post-editing process so it can
be applied to other language pairs. We modified the original pipeline to post-edit English-German
machine translation output and performed additional evaluation of this modification.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje náš příspěvek do optimalizační soutěže WMT16 Tuning Task. Grid search ve standardní implementaci MERT nahrazujeme optimalizací pomocí metody reje částic.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes our submission to the
Tuning Task of WMT16. We replace the
grid search implemented as part of standard
minimum-error rate training (MERT)
in the Moses toolkit with a search based
on particle swarm optimization (PSO). An
older variant of PSO has been previously
successfully applied and we now test it
in optimizing the Tuning Task model for
English-to-Czech translation. We also
adapt the method in some aspects to allow
for even easier parallelization of the
search.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek analyzuje úkoly 41. ročníku lingvistické soutěže pro žáky ZŠ a studenty SŠ.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper analyzes the tasks of the 41st year of the linguistic competition for primary and secondary school pupils.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem této práce je ukázat na datech Pražského závislostního korpusu, že domnělý volný slovosled v češtině ve skutečnosti volný není, a že je řízen jistými pravidly, odlišnými od pravidel gramatiky, které určují slovosled v jiných evropských jazycích, jako např. v angličtině, němčině nebo francouzštině. Na nově anotovaných datech je rovněž testována implementace algoritmu pro rozdělení věty na základ a ohnisko na základě kontextové zapojenosti uzlů na tektogramatické rovině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of the present contribution is to document, on the material of the Prague Dependency Treebank, that the assumed freedom of Czech word order is not really a freedom but that it is guided by certain principles, different from the grammatically given principles determining the word order in some other European languages such as English, German or French. On newly annotated data, an implementation of the algorithm for the division of the sentence into topic and focus based on contextual boundness of nodes at the tectogrammatical layer is tested.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Výzkum korpusových dat se zabývá interakcí vztahů asociativní anafory a aktuálního členění větného.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Corpus-based research demonstrates an existence of a mutual interaction of bridging anaphoric relations in the text and sentence information
structure.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Ve zprávě přestavujeme koncept aktuálního členění větného na základě funkčního generativního popisu. V první části zprávy prezentujeme základní pojmy spojené s aktuálním členěním větným – zejména kontextovou zapojenost a výpovědní dynamičnost a popisujeme operativní kritéria na rozlišení základu a ohniska: tzv. otázkový test a test s negací. V další části představujeme principy pro anotaci aktuálního členění větného v Pražském česko-anglickém závislostním korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the report, we introduce the concept of topic–focus articulation on the basis of Functional Generative Description. Firstly, we present the crucial terms connected with topic–focus articulation – mainly contextual boundness and communicative dynamism and we describe operational criteria how to detect topic and focus: the so-called question test and test by negation. In the next part, we present the annotation principles for annotation of topic–focus articulation in the Prague Czech-English Dependency Treebank.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Výroba a evaluace subjectivity lexikonu pro indonéštinu. Výsledný slovník byl pořízen za využití automatického strojového překladu a jeho spolehlivost byla ověřena v rámci trénování pravděpodobnostního klasifikátoru na ručně označkovaných datech.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present subjectivity lexicon of positive and negative words for Indonesian language created by automatically translating English lexicons. We compare the lexicons in the task of predicting sentence polarity.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje přehled moderních trendů ve studiu jazykové morfologie z neurolingvistické perspektivy. Neurologický výzkum morfologie započal diskusí, zda jsou morfologicky komplexní slova reprezentována v mozku rozloženým způsobem, nebo uložena jako celé jednotky. Ani jeden z těchto pohledů neodpovídá zcela exterimentálním zjištěním, proto byl přijat kompromisní dvoucestný přístup. V článku je diskutovány faktory související s reprezentací slova jako transparence, četnost konstituentů nebo typ afixace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper presents an overview of recent trends in studying morphology
from the neurolinguistic perspective. Neurolinguistic investigations of morphology
started by discussing whether morphologically complex words are represented in the
brain in a decomposed manner or stored as whole forms. None of these could fully
account for available experimental findings, thus a compromise dual-route approach
was adopted. While nowadays there is a wide agreement that representations
of complex words have at least partial morphological structure, it is still not clear
what precisely determines whether such structure emerges for a particular word
or not; commonly mentioned factors are transparency, constituent frequency or
type of affixation. These factors are discussed in the paper, and an experiment is
proposed that can contribute to the current debate about the topic.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato studie popisuje první úkol v rámci SemEvalu, který je zaměřen na užití NLP systémů pro automatické generování slovníkových hesel podle metody Corpus Pattern Analysis.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes the first SemEval task to
explore the use of Natural Language Processing
systems for building dictionary entries,
in the framework of Corpus Pattern Analysis.
CPA is a corpus-driven technique which provides
tools and resources to identify and represent
unambiguously the main semantic patterns
in which words are used. Task 15 draws
on the Pattern Dictionary of English Verbs
(www.pdev.org.uk), for the targeted lexical
entries, and on the British National Corpus
for the input text.
Dictionary entry building is split into three
subtasks which all start from the same concordance
sample: 1) CPA parsing, where arguments
and their syntactic and semantic categories
have to be identified, 2) CPA clustering,
in which sentences with similar patterns have
to be clustered and 3) CPA automatic lexicography
where the structure of patterns have to
be constructed automatically.
Subtask 1 attracted 3 teams, though none
could beat the baseline (rule-based system).
Subtask 2 attracted 2 teams, one of which beat
the baseline (majority-class classifier). Subtask
3 did not attract any participant.
The task has produced a major semantic multidataset
resource which includes data for 121
verbs and about 17,000 annotated sentences,
and which is freely accessible.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>VPS-GradeUp je kolekce trojnásobných manuálních anotací na 29 slovesných lemmatech popsaných v Pattern Dictionary of English Verbs (PDEV): abolish, act, adjust, advance, answer, approve, bid, cancel, conceive, cultivate, cure, distinguish, embrace, execute, hire, last, manage, murder, need, pack, plan, point, praise, prescribe, sail, seal, see, talk, urge. 
Anotace obsahuje dvě úlohy:
1) odstupňované hodnocení vhodnosti každého slovníkového významu (přesněji syntakticko-sémantického vzorce užívání) definovaného PDEV na každé z padesáti vět náhodně vybraných z BNC pro každé zkoumané sloveso
2) klasickou značkovací úlohu, kdy anotátor pro každou větu vybere nejvhodnější slovníkový význam. Tato úloha je obohacena o detaily tzv. exploitations -- čím se slovesné užití v daném kontextu odchyluje od přiřazeného vzorce užití.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>VPS-GradeUp is a collection of triple manual annotations of 29 English verbs based on the Pattern Dictionary of English Verbs (PDEV) and comprising the following lemmas: abolish, act, adjust, advance, answer, approve, bid, cancel, conceive, cultivate, cure, distinguish, embrace, execute, hire, last, manage, murder, need, pack, plan, point, praise, prescribe, sail, seal, see, talk, urge . It contains results from two different tasks:

    Graded decisions
    Best-fit pattern (WSD) .

In both tasks, the annotators were matching verb senses defined by the PDEV patterns with 50 actual uses of each verb (using concordances from the BNC). The verbs were randomly selected from a list of completed PDEV lemmas with at least 3 patterns and at least 100 BNC concordances not previously annotated by PDEV’s own annotators. Also, the selection excluded verbs contained in VPS-30-En, a data set we developed earlier. This data set was built within the project Reviving Zellig S. Harris: more linguistic information for distributional lexical analysis of English and Czech and in connection with SEMEVAL 2015.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku je popsán CzEngVallex, dvojjazyčný valenční slovník, který páruje valenční rámce a jejich členy. Je založen na parelelním česko-anglickém korpusu, konkrétně na Pražském česko-anglickém závislostním korpusu. V tomto korpusu je pro každý výskyt slovesa uveden odkaz na rámec v příslušném českém nebo anglickém valenčním slovníku. CzEngVallex pak přidává párování slovesných rámců a argumentů v nich. Umožňuje tak studium valence v kombinaci s překladem mezi češtinou a angličtinou. CzEngVallex je k dispozici veřejně online, a umožňuje prohlížení a základní prohledávání a zobrazení párovaných rámců a zároveň i konrétních příkladů z korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We describe CzEngVallex, a bilingual English-Czech valency lexicon which aligns verbal valency frames and their arguments. It is based on a parallel Czech-English corpus, the Prague Czech-English Dependency Treebank, where for each occurrence of a verb, a reference to the underlying Czech and English valency lexicons is recorded. CzEngVallex then pairs the entries (verb senses) of the two lexicons, and allows for detailed studies of verb valency and argument structure in translation.  The CzEngVallex lexicon is now accessible online, and the demo will show all the possibilities for browsing and searching the lexicon and their examples from the PCEDT corpus.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Objem nestrukturovaných dat stále roste, rozvoj Webu 2.0 přináší množství textů generovaných samotnými uživateli Internetu. Jejich příspěvky nezřídka obsahují subjektivní názory, emoce, hodnocení… K čemu a jak můžeme tato data použít? Je možné emoce v textu spolehlivě strojově třídit? Příspěvek z oblasti sentiment analysis představí metody a úspěchy automatické extrakce emocí z textu s důrazem na česká data a aplikace pro byznys.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The volume of unstructured data is constantly increasing, the rise of Web 2.0 brings lots of texts created by the Internet users. Their comments are usually full of subjective opinions, emotions, evaluation... Why and how to use this data? Is it possible to categorize emotions automatically? A contribution from the area of sentiment analysis introduce methods and outputs of automatic emotion extraction with the emphasis on Czech data and business applications.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dizertační práce má dva hlavní cíle. Za prvé přináší analýzu jazykových prostředků,
které společně formují emocionální význam psaných výpovědí v češtině. Za druhé
využívá zjištění týkající se emocionálního jazyka v komputačních aplikacích.
Podáváme systematický přehled lexikálních, morfosyntaktických, sémantických
a pragmatických aspektů emocionálního významu v českých výpovědích
a navrhujeme formální reprezentaci emocionálních struktur v rámci Pražského
závislostního korpusu a konstrukční gramatiky.
V oblasti komputačních aplikací se zaměřujeme na témata postojové analýzy,
tedy automatické extrakce emocí z textu. Popisujeme tvorbu ručně anotovaných
emocionálních zdrojů dat a řešíme dvě základní úlohy postojové analýzy, klasifikaci
polarity a identifikaci cíle hodnocení. V obou těchto úlohách dosahujeme
uspokojivých výsledků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This thesis has two main goals. First, we provide an analysis of language means
which together form an emotional meaning of written utterances in Czech. Second,
we employ the findings concerning emotional language in computational
applications.
We provide a systematic overview of lexical, morphosyntactic, semantic and
pragmatic aspects of emotional meaning in Czech utterances. Also, we propose
two formal representations of emotional structures within the framework of the
Prague Dependency Treebank and Construction Grammar.
Regarding the computational applications, we focus on sentiment analysis, i.e.
automatic extraction of emotions from text. We describe a creation of manually
annotated emotional data resources in Czech and perform two main sentiment
analysis tasks, polarity classification and opinion target identification on Czech
data. In both of these tasks, we reach the state-of-the-art results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška má dva hlavní cíle. Za prvé předložíme analýzu lexikálních, morfosyntaktických, sémantických a pragmatických jazykových prostředků, které společně formují emocionální význam psaných výpovědí v češtině, a podáme návrh formální reprezentace emocionálních struktur. Za druhé se zaměříme na komputační aplikace, které zjištění týkající se emocionálního jazyka využívají. Budeme se věnovat tématům z oblasti postojové analýzy, tedy automatické extrakce emocí z textu. Popíšeme tvorbu ručně anotovaných emocionálních zdrojů dat a řešení dvou základních úloh postojové analýzy, klasifikace polarity a identifikace cíle hodnocení.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk has two main goals. First, we provide an analysis of language means which together form an emotional meaning of written utterances in Czech. Second, we employ the findings concerning emotional language in computational
applications.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku popisujeme experimenty s neřízeným závislostním parsingem bez použití jakéhokoliv slovnědruhových kategorií naučených z manuálně označkovaných korpusů. Používáme pouze neřízeně naučené slovní třídy a prezentujeme tedy plně neřízenou syntaktickou analýzu věty z prostého textu Ukazuje se, že výsledky nejsou o mnoho horší, než ty, které využívají anotovaná data.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present experiments with unsupervised dependency parser without using any part-of-speech tags learned from manually annotated data. We use only unsupervised word-classes and therefore propose fully unsupervised approach of sentence structure induction from a raw text. We show that the results are not much worse than the results with supervised part-of-speech tags.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>LiStr - Sada nástrojů pro odvození lingvistické struktury. Obsahuje nástroje a podpůrné skripty pro
indukci závislostních stromů z textu s přiřazenými tagy. V této verzi je navíc automatický odhadovač slovních druhů. Nástroj UDPC byl vylepšen, nyní umí pracovat i s neřízenými slovními třídami a s apriorními pravděpodobnostmi z kolekce korpusů "universal dependencies".</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>LiStr - a toolkit for induction of linguistic structures without annotated corpora. This is the second version of the toolkit, released at December 2015. It comprises the tools and supporting scripts for induction of dependency trees from texts with already assigned part-of-speech tags. In this version, the delexicalized part-of-speech tag guesser is comprised. The UDPC was improved, now it may work with unsupervised word-classes and with universal-dependencies prior probabilities.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje systém rozlišování významu sloves na základě paralelního korpusu a paralelního valenčího slovníku, který rovněž obsahuje informaci o párování významu sloves a jejich argumentů. Jedná se o rozšíření předchozí metody publikované v roce 2014. Metoda byla testována na angličtině i češtině a v obou případech dala signifikantně lepší výsledky než metoda předchozí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a system for verbal Word Sense Disambiguation (WSD) that is able to exploit additional information from parallel texts and lexicons. It is an extension of our previous WSD method, which gave promising results but used only monolingual features. In the follow-up work described here, we have explored two additional ideas: using English-Czech bilingual resources (as features only - the task itself remains a monolingual WSD task), and using a 'hybrid' approach, adding features extracted both from a parallel corpus and from manually aligned bilingual valency lexicon entries, which contain subcategorization information. Albeit not all types of features proved useful, both ideas and additions have led to significant improvements for both languages explored.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme nový systém pro generování přirozeného jazyka založený na sytaxi, který je možné trénovat z nezarovnaných párů vstupních reprezentací významu a výstupních vět. Dělí se na větný plánovač, který inkrementálně staví hloubkově syntaktické závislostní stromy, a povrchový realizátor. Větný plánovač je založen na A* vyhledávání s perceptronovým rankerem, který používá nové updaty na základě odlišných podstromů a jednoduchý odhad budoucího potenciálu stromů; povrchová realizace je zajištěna pravidlovým systémem z prostředí Treex.
První výsledky ukazují, že trénování z nezarovnaných dat je možné, výstupy našeho generátoru jsou většinou plynulé a relevantní.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present a novel syntax-based natural language generation system that is trainable from unaligned pairs of input meaning representations and output sentences. It is divided into sentence planning, which incrementally builds deep-syntactic dependency trees, and surface realization. Sentence planner is based on A* search with a perceptron ranker that uses novel differing subtree updates and a simple future promise estimation; surface realization uses a rule-based pipeline from the Treex NLP toolkit.
Our first results show that training from unaligned data is feasible, the outputs of our generator are mostly fluent and relevant.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Strojový překladač TectoMT byl během posledního roku vylepšen tak, aby umožňoval snadnější přidávání nových překladových jazykových párů.
K tomu se používají multilinguální standardy pro morfologickou a syntaktickou anotaci a také jazykově nezávislé moduly s pravidly.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The TectoMT tree-to-tree machine translation system has been updated this year to support easier retraining for more translation directions. We use multilingual standards for morphology and syntax annotation and language-independent base rules. We include a simple, non-parametric way of combining TectoMT’s transfer model outputs.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto přípsěvku jsme se zabývali distribucí slovesných a jmenných valenčních doplnění v konstrucích s tzv. funkčními slovesy. Dále jsme vymezili principy, podle kterých je utvářena povrchověsyntaktická struktura věty s těmito slovesy.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this talk, the distribution of verbal and nominal valency complementations in the syntactic structure of light verb constructions is debated. Esp. principles governing the surface form of these constructions are proposed.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Komplexní predikáty s funkčními slovesy představují z hlediska syntaktického popisu problematický jev. Příspevěk se zabývá především distribucí valenčních doplnění funkčního slovesa a predikativního jména v syntaktické struktuře věty. Na základě syntaktické analýzy pak navrhuje teoreticky adekvátní a přitom ekonomické zachycení komplexních predikátů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Complex predicates with light verbs have
proven to be very challenging for syntactic
theories, particularly due to the tricky distribution of valency complementations of
light verbs and predicative nouns in their syntactic structure. We propose a theoretically adequate
and economical representation of complex
predicates  with  Czech  light  verbs  based
on a division of their description between
the lexicon and the grammar.  We demonstrate that a close interplay between these
two components makes the analysis of the
deep  and  surface  syntactic  structures  of
complex predicates reliable and efficient.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku se zabýváme vlastní (syntaktickou) reflexivitou – tedy případy, kdy reflexiva se/si jsou klitickými tvary reflexivního zájmena, vyjadřujícího referenční totožnost aktantu v subjektové pozici (typicky Konatele děje) a aktantu vyjádřeného daným zájmenem. K diskusím o tom, zda reflexivní klitika se/si a po řadě jejich dlouhé tvary sebe/sobě u tzv. vlastních reflexiv plní ve větě stejnou funkci, chceme přispět analýzou jejich vlivu na kongruenci s povrchověsyntaktickým doplňkem uvozeným (zejména) výrazem jako. Předmětem naší argumentace jsou významové distinkce mezi reflexivními konstrukcemi vyplývající z rozdílů ve shodě doplňku a dále ověření, či vyvrácení obvyklého popisu, podle kterého rozdíl klitické a dlouhé formy reflexiva má vliv na udělování pádu doplňku.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we deal with syntactic reflexivity. In this case, the reflexives represent the clitic forms of the reflexive pronoun expressing referential identity of the valency complementation realized in subject (usually of ACTor) and of the valency complementation lexically expressed by the given pronoun. This linguistic phenomenon is studied here especially in relation to the agreement of a surface syntactic complement (with the function of EFF or COMPL). We aim to demonstrate that the case of complement is rather determined by the meaning of the reflexive construction (as in non-reflexive structures) than by the distinction of the clitic vs. long form of the reflexives.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Sdílení vědeckých dat se stalo pro data driven science základní službou a může vědecký výzkum zasadně zlepšit tím, že zpřístupní spolehlivá a důvěryhodná data. Aby byly takovéto služby pro sdílení vědeckých dat ve vědeckých procech použitelné a užitečné, musejí splnit řadu požadavků na vyhledatelnost a přístupnost dat. Služba B2SHARE vyvinutá v projektu EUDAT tyto požadavky plní pro řadu vědeckých oborů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Scientific data sharing is becoming an essential service for data driven science and can significantly improve the scientific process by making reliable, and trustworthy data available. Thereby reducing redundant work, and providing insights on related research and recent advancements. For data sharing services to be useful in the scientific process, they need to fulfill a number of requirements that cover not only discovery, and access to data. But to ensure the integrity, and reliability of published data as well. B2SHARE, developed by the EUDAT project, provides such a data sharing service to scientific communities. For communities that wish to download, install and maintain their own service, it is also available as software. B2SHARE is developed with a focus on user-friendliness, reliability, and trustworthiness, and can be customized for different organizations and use-cases. In this paper we discuss the design, architecture, and implementation of B2SHARE. We show its usefulness in the scientific process with some case studies in the biodiversity field.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Úloha 18 na SemEval 2015 definuje sémantickou závislostní analýzu (SDP) se širokým pokrytím jako problém nalezení vztahů mezi predikátem a jeho argumenty ve větě pro všechna plnovýznamová slova, tj. sémantické struktury, která představuje relační jádro významu věty. V tomto článku, který úlohu 18 představuje, zasazujeme tento problém do rámce dalších podúloh analýzy jazyka, představujeme a srovnáváme použité cílové reprezentace sémantických závislostí a shrnujeme zadání úlohy, informace o zúčastněných systémech, jakož i hlavní výsledky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Task 18 at SemEval 2015 defines Broad-Coverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicate-argument relationships for all content words, i.e. the semantic structure constituting the relational core of sentence meaning. In this task description, we position the problem in comparison to other language analysis sub-tasks, introduce and compare the semantic dependency target representations used, and summarize the task setup, participating systems, and main results.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>SHAMUS (ÚFAL Multimediálny vyhľadávací systém) je systém na jednoduché vyhľadávania a navigáciu v multimediálnych archívoch. Systém pozostáva z textového vyhľadávania, automatického výberu dôležitých segmentov a prepájania súvisiacich video segmentov.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>SHAMUS (UFAL Search and Hyperlinking Multimedia System) is a system for an easy search and navigation in multimedia archives. The system consists of a text-based Search, an automatic selection of Anchoring segments and video-based retrieval of Hyperlinks.

The segments of the videos in a collection relevant to the user-typed query are provided by the Search component. The Anchoring component determines the most important segments of the videos. The beginnings of the most informative segments are marked in each video and the transcription of the beginning of each Anchoring segment is displayed. The segments topically related to each Anchoring segments (the Hyperlinks) are then retrieved. The list of the segments is generated on the fly for each Anchoring segment.  All three system components use Terrier IR Platform.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku skúmame využitie zvukovej informácie pri vyhľadávaní multimediálneho obsahu. Konkrétne sa zaoberáme prepájaním príbuzných segmentov v kolekcii 4000 hodín televíznych programov BBC. V článku popisujeme náš systém použitý v úlohe Search and Hyperlinking v rámci Benchmarku MediaEval 2014, v ktorom dosiahol najlepšie výsledky. V článku ďalej skúmame tri automatické prepisy, porovnávame ich s titulkami a potvrdzujeme vzťah medzi kvalitou prepisov a kvalitou vyhľadávania. Kvalita vyhľadávania je ďalej vylepšená pomocou rozšírenia prepisov o metadáta a kontext, kombináciou rôznych prepisov, využitím najspoľahlivejších slov z prepisov a využitím akustickej podobnosti.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we explore the use of audio information in the retrieval of multimedia content. Specifically, we focus on linking similar segments in a collection consisting of 4,000 hours of BBC TV programmes. We provide a description of our system submitted to the Hyperlinking Sub-task of the Search and Hyperlinking Task in the MediaEval 2014
Benchmark, in which it scored best. We explore three automatic transcripts and compare them to available subtitles. We confirm the relationship between retrieval performance and transcript quality. The performance of the retrieval
is further improved by extending transcripts by metadata and context, by combining different transcripts, using the highest confident words of the transcripts, and by utilizing acoustic similarity.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku sa venujeme automatickej detekcii anchoring segmentov v kolekcii televíznych programov. Nájdené anchoring segmenty by mali byť ďalej použité pri následnom vyhľadávaní príbuzných segmentov v kolekcii. Anchoring segmenty by mali byť preto pútavé pre používateľov video kolekcie. S využitím linkov odkazujúcich z anchoring segmentov môžu užívatelia prechádzať kolekciu a nájsť ďalšie informácie, ktoré ich zaujímajú. V článku popisujeme dva prístupy - jeden založený na metadatách, druhý na frekvencii vlastných mien a čísloviek nachádzajúcich sa v segmente.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the paper we deal with automatic detection of anchoring segments in a collection of TV programmes. The anchoring segments are intended to be further used as a basis for subsequent hyperlinking to another related video segments.
The anchoring segments are therefore supposed to be fetching for the users of the collection.  Using the hyperlinks, the users can easily navigate through the collection and find
more information about the topic of their interest.
We present two approaches, one based on metadata, the second one based on frequencies of proper names and numbers contained in the segments.  Both approaches proved to be helpful for different aspects of anchoring problem: the
segments which contain a large number of proper names and numbers are interesting for the users, while the segments most similar to the video description are highly informative.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V článku je zkoumán vztah mezi
dvěma důležitými semantickými vlastnostmi jazyka (polysémií a synonymií) a jednou ze základních vlastností syntaktických sítí (stupňem uzlu). Na základě synergické teorie jazyka je předpokládáno, že slovo, které se objevuje ve více syntaktických kontextech, tzn. má větší stupeň,  by mělo být více polysémní a mít více synonym než slovo, které se objevuje v méně kontextech. Pro testování hypotézy je použito šest jazyků. Analýza syntakticky-závislostních sítí předložená v této studii přináší novou interpretaci známého vztahu mezi frekvencí a polysémii (nebo synonymií).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The relationship between two important semantic properties (polysemy and synonymy) of language and one of the most fundamental syntactic network properties (a degree of the node) is observed. Based on the synergetic theory of language, it is hypothesized that a word which occurs in more syntactic contexts, i.e. it has a higher degree, should be more polysemous and have more synonyms than a word which occurs in less syntactic contexts, i.e. it has a lesser degree. Six languages are used for hypotheses testing and, tentatively, the hypotheses are corroborated. The analysis of syntactic dependency networks presented in this study brings a new interpretation of the well-known relationship between frequency and polysemy (or synonymy).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje sérii experimentů na vybudování závislostního parsovácího
modelu s použitím MaltParser, korpusu ruštiny SynTagRus a morfologického
Tagger Mystem. Experimenty mají dva účely. První je natrénovat model s rozumnou rovnováhu mezi kvalitou a parsovácího času. Druhý účel je vytvořit uživatelsky přívětivý software, který by byl praktický
pro získání rychlych výsledku bez nutnosti technických znalostí (programovácí jazyky, jazykové nástroje, atd.)</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The paper describes a series of experiments on building a dependency parsing
model using MaltParser, the SynTagRus treebank of Russian, and the morphological
tagger Mystem. The experiments have two purposes. The first
one is to train a model with a reasonable balance of quality and parsing time.
The second one is to produce user-friendly software which would be practical
for obtaining quick results without any technical knowledge (programming
languages, linguistic tools, etc.).</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Poster představuje CVH Malach a dostupné archivy orální historie, stejně jako některé výzkumné a vzdělávací aktivity realizované s využitím dostupných dat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Malach Center for Visual History (Malach CVH) at the Faculty of Mathematics and Physics of the Charles University in Prague provides local access to the extensive digital archives of the USC Shoah Foundation – the Institute for Visual history and Education, the Refugee Voices archive of the Association of Jewish Refugees, and other archives of oral histories. The Visual History Archive of USC Shoah Foundation contains over 53 000 witness testimonies covering the history of entire 20th century. The Refugee Voices archive complements this collection with additional 150 interviews, conducted in English language by Association of Jewish Refugees in United Kingdom. Generally speaking, oral history interviews are valuable information resource for students and researchers in many disciplines. Secondary analysis of the testimonies is also applicable to many different scientific fields besides historiography and genocide studies. This paper provides a brief overview and description of available data, and it presents some of the more recent research and educational activities facilitated by the existence of Malach CVH in the Czech Republic.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje digitální orálněhistorické sbírky dostupné uživatelům CVH Malach a příklady badatelského a vzdělávacího využití těchto materiálů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Malach Center for Visual History (Malach CVH) at the Faculty of Mathematics and Physics of the Charles University in Prague provides local access to the extensive digital archives of the USC Shoah Foundation – the Institute for Visual history and Education, the Refugee Voices archive of the Association of Jewish Refugees, and other archives of oral histories. The Visual History Archive of USC Shoah Foundation contains over 53 000 witness testimonies covering the history of entire 20th century. The Refugee Voices archive complements this collection with additional 150 interviews, conducted in English language by Association of Jewish Refugees in United Kingdom. Generally speaking, oral history interviews are valuable information resource for students and researchers in many disciplines. Secondary analysis of the testimonies is also applicable to many different scientific fields besides historiography and genocide studies. This paper provides a brief overview and description of available data, and it presents some of the more recent research and educational activities facilitated by the existence of Malach CVH in the Czech Republic.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zpráva popisuje poslední přírůstek v Archivu vizuální historie USC Shoah Foundation, kolekci rozhovorů se svědky a přeživšími genocidy Arménů, která je dostupná od dubna 2015 i v CVH Malach. Pojednáno je o charakteristikách kolekce i o jejích specificích v rámci celého Archivu USC SF.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The article describes the latest addition into USC Shoah Foundation's Visual History Archive, which is the collection of interviews with witnesses and survivors of Armenian genocide. The text deals with the characteristics of the collection as well as its specific features in the context of VHA.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>DeriNet je databáze lexikálních derivátů vyvíjená v Ústavu formální a aplikované lingvistiky. Takovou síť vztahů s velkým počtem derivačních stromů je těžké vizualizovat. V tomto článku popisujeme softwarový nástroj, který byl vyvinut za účelem zobrazování a prohledávání derivačních stromů. Nástroj má formu webové aplikace a využívá nově vyvinutý dotazovací jazyk podobný jazyku CQL, který se používá k prohledávání Českého národního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>DeriNet, a new database of lexical derivates,
is being developed at the Institute of Formal and Applied Linguistics.  Since it is a wordnet containing large amounts of trees, it is hard to visualize, search and extend without specialized tools. This paper describes a program which has been developed to display the trees and enable finding certain types of errors, and its query language. The application is web-based for ease of use and access. The language has been inspired by CQL, the language used for searching the Czech National Corpus, and uses similar key-words and syntax extended to facilitate searching in tree structures.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>DeriNet 1.0 je lexikální síť zahrnující informace o lemmatech obsažených v morfologickém slovníku Morflex.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The lexical network DeriNet 1.0 captures core word-formation relations on the set of lemmas stored in the morphological lexicon Morflex.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme metologii a výsledky průzkumu anotace víceslovných výrazů v treebancích. Průzkum probíhal pomocí webu na principu wiki, kam přispívali lidé obeznámení s treebanky. Výsledky průzkumu jsme studovali zejména z hlediska porovnání přístupů k předložkovým víceslovným výrazům, slovesným konstrukcím s předložkou a víceslovným pojmenovaným entitám.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We present the methodology and results of a survey on the annotation of multiword expressions in treebanks. The survey was conducted using a wiki-like website filled out by people knowledgeable about various treebanks. The survey results were studied with a comparative focus on prepositional MWEs, verb-particle constructions and multiword named entities.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace popisuje textove jevy zachycene v tektogramaticke anotace PDT - koreference, elipsy, aktualni cleneni a diskurz.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the presentation, the general overview of textual phenomena annotated in the Prague Dependency treebank is presented. We discuss coreference,  reconstruction of ellises, topic-focus articulation and discourse structure.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V teto prednasce se zamerime na detailnejsi popis koreference v cestine. predstavim proces anotace, vysledky automatickeho rozreseni koreference a srovname to s vysledky mezianotatorske shody. Zamerime se taky na oreferencni retezce a jejich reprezentace v cestine a jinych jazycich.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk presents manual annotation of coreference relations in more detail: coreference resolution experiments, manual annotation of English (Ontonotes vs. PCEDT, problematic cases (generics, premodifiers, etc.)), (dis)agreements, coreference chains in Czech, English and Russian.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce jsme představili anotaci koreference a diskurzu v PDT s přihlednutím ke konkrétním problémům, jako je mezianotátorská shoda a příčiny neshod, koreference generických jmenných frází atd.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk presents manual annotation of coreference and discourse relations in PDT in more detail: coreference resolution experiments, manual annotation (generics, premodifiers, etc.)), (dis)agreements, etc.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V prezengtaci představujeme několik přístupu k analýze a anotace textových jevů, např. ke koreference, elipsám, aktualnímu cleneni a diskurzním konektorům a argumentům.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the presentation, the general overview of textual phenomena annotated is presented. We discuss coreference,  reconstruction of ellises, topic-focus articulation and discourse structure.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V prezentaci jsme analyzovali výskyty kontextově zapojených výrazů ve větě, které nejsou propojeny žádným z anaforických vztahů (koreference, asociační anafora) anotatovaném v Pražském závislostním korpusu. Ukázalo se, že takových případů je v PDT cca. 30%. Vyčlenili jsme tři základní příčiny této absence: (i) kontextově zapojeny je sémanticky či pragmaticky spojen s s předchozím textem nebo s extralingvisticou situací; (ii) sekundární okolnosti, kulisy a (iii) výrazy s nízkým referenčním potenciálem.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the presentation, we analyzed contextually bound nominal expressions explicitly expressed in the sentence, that lack an anaphoric (bridging, coreference or segment) link to a previous context. The statistics collected from the PDT annotated data has shown that in almost one third of contextually bound expressions there is no anaphoric link to the previous context. 
Disregarding different types of more or less technical reasons evoked by the tectogrammatical structure of the PDT sentences and annotation errors, we can claim that there are three groups of reasons why contextually bound nominal groups are not linked by any anaphoric link. These are (i) contextually bound nominal groups semantically or pragmatically related to previous textual or extralinguistic context but not specified as bridging relations within the PDT; (ii) secondary circumstances (temporal, local, etc.) and (iii) nominal groups with low referential potential.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Daný článek popisuje pokus o vytvoření obecného anotačního schématu pomocí existujících anotací textových vztahů v češtině, němčině a angličtině.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The present paper describes an attempt to create
an interoperable scheme using existing annotations
of textual phenomena across languages
and genres including non-canonical
ones. Such a kind of analysis requires annotated
multilingual resources which are costly.
Therefore, we make use of annotations already
available in the resources for English,
German and Czech. As the annotations in
these corpora are based on different conceptual
and methodological backgrounds, we
need an interoperable scheme that covers existing
categories and at the same time allows a
comparison of the resources. In this paper, we
describe how this interoperable scheme was
created and which problematic cases we had
to consider. The resulting scheme is supposed
to be applied in the future to explore contrasts
between the three languages under analysis,
for which we expect the greatest differences in
the degree of variation between non-canonical
and canonical language.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem výzkumu je statistické srovnání použití diskurzních konektorů v mluvené češtině, angličtině a němčině. Analýza se opírá na srovnatelné texty ve třech zkoumaných jazycích, na které byla promítnuta automatická anotace diskurzu vytvořena na základě srovnání anotačních schémat pro tyto jazyky, primárně provedena ručně v rámci různých vědeckých paradigmat.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of the present paper is to analyse contrasts in Czech, English and German in terms of discourse-relational devices (DRDs). The novelty of our approach lies in the nature of the resources we are using. Advantage is taken of existing resources, which are, however, annotated on the basis of two different frameworks. We use an interoperable scheme unifying DRDs of both frameworks in more abstract categories and considering only those phenomena that have a direct match in German, English and Czech.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této prezentaci jsme představili první výsledky srovnání dvou anotačních schémat, PDiT a GECCo v oblasti zpracování diskurzních konektorů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In the presentation, the two approaches to discourse-structuring devices have been compared and analysed: PDiT and GECCo</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>This paper is a pilot comparative study on coreference chaining in three languages,
namely, Czech, English and Russian. We have analyzed 16 parallel English-Czech newspaper texts and 16 texts in Russian (similar to the English-Czech ones in length and topics). Our motivation was to find out what the linguistic structure of coreference chains in different languages is and what types of distinctions we should take into account for advancing the development of systems for coreference resolution. Taking into account
theoretical approaches to the phenomenon of coreference we based our research on the following assumption: the recognition of coreference links for different structural types of noun phrases is regulated by different language mechanisms. The other starting point was that different languages
allow pronominal chaining of different length and that coreference chains properties differ for the languages with different strategies for zero
anaphora and different systems for definiteness marking. This work reports our first findings within the task of the structural NP types’ distribution comparison in three languages under analysis.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Tento článek je pilotní srovnavací výzkum koreferenčních řetězců v češtině, angličtině a ruštině. Podrobili jsme analýze 16 srovnatelných textů ve třech jazycích. Naší motivací bylo zjistit lingvistickou strukturu koreferenčních řetězců v těchto jazycích a určit, které faktory ovlivňují tuto strukturu.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>SloNLP je workshop speciálně zaměřený na zpracování přirozeného jazyka a počítačovou lingvistiku. Jeho hlavním cílem je podpořit spolupráci mezi výzkumníky v oblasti NLP v Česku a na Slovensku.
Mezi témata workshopu patří automatické rozpoznávání mluvené řeči (ASR), automatická analýza a generování přirozeného jazyka (morfologie, syntax, sémantika...), dialogové systémy (DS), strojový překlad (MT), vyhledávání informací (IR), praktické aplikace NLP technologií, a další témata počítačové lingvistiky.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>SloNLP is a workshop focused on Natural Language Processing (NLP) and Computational Linguistics. Its primary aim is to promote cooperation among NLP researchers in Slovakia and Czech Republic.
The topics of the workshop include automatic speech recognition, automatic natural language analysis and generation (morphology, syntax, semantics, etc.), dialogue systems, machine translation, information retrieval, practical applications of NLP technologies, and other topics of computational linguistics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představujeme vylepšení evaluace strojového překladu do češtiny pomocí cíleného parafrázování referenčních vět na rovině hloubkové syntaxe.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we present a method of improving
quality of machine translation
(MT) evaluation of Czech sentences via
targeted paraphrasing of reference sentences
on a deep syntactic layer. For
this purpose, we employ NLP framework
Treex and extend it with modules
for targeted paraphrasing and word order
changes. Automatic scores computed using
these paraphrased reference sentences
show higher correlation with human judgment
than scores computed on the original
reference sentences.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek se zabývá různými přístupy ke získávání informací napříč jazyky, zejména slovníkovými a korpusovými přístupy. Zaměříme se také na systémy, které používají statistický strojový překlad pro překládání dotazů nad kolekcí jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper we will explore different approaches used in cross languages
information retrieval (CLIR) systems, mainly dictionary-based and corpus based
systems. Then we will focus on CLIR systems which use statistical machine
translation (SMT) systems to translate queries into collection language. Different
approaches which use SMT are studied in this paper including the using of SMT
as black box and looking inside the box of the SMT system to tune it to get better
performance in IR’s point of views rather than using SMT system to give better
results for human’s point of views.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Tato zpráva popisuje účast týmu Univerzity Karlovy v Praze na CLEF eHealth 2015 Task 2.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This report describes the participation of the team of Charles University in Prague at the CLEF eHealth 2015 Task 2.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení problému rozpoznávání řeči a dialogových systémů. Prezentace dialogového systému Alex.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Introducing the task of speech recognition, problems related with spoken dialogue systems. Presentation of Alex spoken dialogue system.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Dialogovový agent musí být uživateli velmi důkladně představen, aby se zabránilo četným nedorozumněním. Přesto však se uživatelé často ptají na funkcionalitu, které systém nerozumí a neumí jí poskytnout. V naší pokračující práci adresujeme tento problém tak, že agent umí popsat své možnosti a zároveň vysvětluje proč zvolil své akce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Having set up several dialogue systems for
multiple domains we realized that we need
to introduce each system thoroughly to
the users in order to avoid misunderstanding.
Even with well-described systems,
users still tend to request out-of-domain
information and are often confused by the
system response. In our ongoing work,
we try to address these issues by allowing
our conversational agent to speak about its
abilities. Our agent is able to simply describe
what it understands and why it decided
to perform one of its summary actions.
In this short paper, we present our
current system architecture.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje probíhající výzkum zaměřený na vývoj konverzačního agenta, který je schopen diskutovat o svých schopnostech. Náš implementovaný agent je schopen jednoduše popsat, čemu rozumí a proč se rozhodl pro kterou akci. Zaměřujeme se na sebeznalost agenta, protože to pomůže popsat uživatelům agentovu doménu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In our ongoing work, we try to build an conversational agent which is
able to chat about its abilities. Our implemented agent is able to simply describe
what it understands and why it decided to perform one of its summary actions.
We focus on self-awareness of our agent because it helps defining the agent’s
domain, and its architecture to its users. In this paper we introduce our current
implementation, outline our goals and describe our future intentions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace představila Archiv vizuální historie USC Shoah Foundation a související vzdělávací aktivity a projekty. Dva krátké sestřihy z úryvků vzpomínek tří pamětnic přiblížily z různých úhlů dějiny Podkarpatské Rusi jako někdejší součásti Československa.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The presentation provided an overview of USC Shoah Foundation's Visual History Archive and related educational activities and projects. Two short videoclips from three witnesses' life story narratives captured different aspects of the history of Carpathian Ruthenia as a former part of Czechoslovakia.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Představení technik strojového překladu pro pracovníky státní správy a veřejných institucí.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>An introduction to machine translation for employees of governmental and other public institutions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Kapitola v příručce o tvarosloví Oxford Handbook of Inflection se věnuje problematice bohatosti slovních forem v úloze strojového překladu. Po úvodu popisujícím několik klasických typů strojového překladu se podrobně zabývá jednotlivými fázemi či kroky strojového překladu a potížím, které v nich pestrost tvarosloví přináší.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The chapter on Machine translation in Oxford Handbook of Inflection starts by describing the few key approaches to machine translation and proceeds over individual stages of the process, highlighting the problems caused by the richness of word forms.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek představuje výsledky společných úloh WMT15 -- překladu novinových textů, odhadu kvality překladu a automatické posteditace. Do standardní překladové úlohy v 10 překladových směrech se letos zapojilo 68 systémů strojového překladu z 24 institucí. Zároveň bylo vyhodnoceno 7 anonymizovaných systémů. Úloha odhadu kvality překladu měla 3 podúlohy, kterých se zúčastnilo 10 týmů a celkem 34 systémů. Pilotní úloha automatické posteditace měla 4 týmy, které odeslaly 7 systémů</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents the results of the
WMT15 shared tasks, which included a
standard news translation task, a metrics
task, a tuning task, a task for run-time
estimation of machine translation quality,
and an automatic post-editing task. This
year, 68 machine translation systems from
24 institutions were submitted to the ten
translation directions in the standard translation
task. An additional 7 anonymized
systems were included, and were then
evaluated both automatically and manually.
The quality estimation task had three
subtasks, with a total of 10 teams, submitting
34 entries. The pilot automatic postediting
task had a total of 4 teams, submitting
7 entries.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek představil problematiku strojového překladu při přenosu technologie z výzkumné sféry do rutinního komerčního využití a způsob hodnocení takového překladu v kontextu lokalizace dokumentace v oblasti ifnormačních technologií.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk presented solutions for successful transfer of machine translation technology from research to commercial use and its evaluation in the context of translation and localization of documentation in the area of information technology.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Článek popisuje náš systém pro Translation Task WMT15. Jedná se o hybridní systém pro anglicko-český překlad. Využíváme úspěšnou konfiguraci z předchozích dvou let.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper describes our WMT15 system submission for the translation task, a hybrid system for English-to-Czech translation. We repeat the successful setup from the previous two years.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nástroje a modely pro dva překladové systémy: anglicko-český překlad (především souvislých textů) a česko-anglický překlad (krátkých dotazů  i textů). V prvním případě jde o statistickou komponentu překladového systému Chiméra. Modely byly zmenšeny a uloženy ve formátu, který umožňuje velmi rychlý překlad, za cenu možné drobné ztráty kvality výstupu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Tools and models for two machine translation systems: English->Czech translation (mainly longer text) and Czech->English translation (short queries as well as text). In the first case, the models are the statistical component of the Chimera system. Models were pruned and stored in a format which allows for very fast translation, at the expense of possible slight reduction of output quality.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Shrnutí aktivit v oblasti strojového překladu v EU  včetně příspěvků několika projektů v této oblasti podporovaných Evropskou komisí, a směřování této oblasti směrem k jednotnému digitálnímu trhu v EU.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A summary of research activities in the area of machine translation in the EU (Technologies – Demands – Gaps – Roadmaps) has been presented, including contributions from multiple other EU-funded projects.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Abstraktní reprezentace významu, vyvinutá konsorciem amerických univerzit, slibuje vyšší úroveň reprezentace významu a jeho přiblížení i mezi vzdálenými jazyky. Přednáška představila analýzu různých problémů, které přitom vznikají.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Abstract Meaning Representation is a newly developed formalism for representing meaning, which abstracts from syntax and some other phenomena but is (still) language-dependent. It has been developed by a consortium of mostly U.S. universities, with team members including Martha Palmer, Kevin Knight, Philipp Koehn, Ulf Hermjakob, Kathy McKeown, Nianwen Xue and others. In the talk, the basic facts about the AMR will be presented, and then comparison will be made for Czech and English as carried out in detail on a small 100-sentence corpus; some examples from Chinese-English comparison will also be shown. In addition, AMR will be compared to the deep syntactic representation used in the set of Prague Dependency Treebanks (again, for Czech And English), and observations will be made about the level of abstraction used in these two formalisms. Plans for future studies and possible corpus annotation work in the nearest future will also be mentioned. The work reported has been done primarily by the CLAMR (Cross-Lingual AMR) team led by Martha Palmer of UCB at the Johns Hopkins Summer workshop in 2014.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Abstraktní reprezentace významu se snazží podchytit formálně význam vět v přirozeném jazyce. V příspěvku se porovnává s existujícími reprezentacemi a rovněž i napříč jazyky. Hlavní porovnání vylo provedeno s formalismem Pražského závislostního korpusu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Abstract Meaning Representation is a newly developed formalism for representing meaning, which abstracts from syntax and some other phenomena but is (still) language-dependent. It has been developed by a consortium of mostly U.S. universities, with team members including Martha Palmer, Kevin Knight, Philipp Koehn, Ulf Hermjakob, Kathy McKeown, Nianwen Xue and others. In the talk, the basic facts about the AMR will be presented, and then comparison will be made for Czech and English as carried out in detail on a small 100-sentence corpus; some examples from Chinese-English comparison will also be shown. In addition, AMR will be compared to the deep syntactic representation used in the set of Prague Dependency Treebanks, and observations will be made about the level of abstraction used in these two formalisms. There is an ongoing work on possible conversion between the Prague deep syntactic representation and the AMR representation, and the main issues of such a conversion will also be described, as well as plans for future studies and possible corpus annotation work in the nearest future.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Byly prezentovány základy strojového překladu s použitím hloubkové jazykové analýzy podle tradičního systému "Vauquois trojúhelník", s využitím moderních statistických metod a metod strojového učení. Rovněž byly ukázány poslední výsledky, kdy hybridní systém Chiméra vytvořený na ÚFAL MFF UK v rámci různých projektů zvítězil v soutěži en-cs překladačá na WMT 2015 Shared Task.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Fundamentals of Machine Translation technology using Deep language analysis using the traditional "Vauquois triangle" have been presented, which now use advanced statistical and machine learning techniques. In addition, latest WMT 2015 Shared Task competition results for the en-cs pair have been shown in which the Chimera system created at UFAL MFF UK won.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Technologická agentura České republiky již šest let poskytuje podporu v oblasti aplikovaného výzkumu. Je odpovědná za tvorbu programů v oblasti aplikovaného výzkumu. V prezentaci je představeno hodnocení programu Center kompetence TAČR, která byla hodnocena v polovině jejich existence.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Technology Agency of the Czech Republic is less than six-years-old funding agency charged with promoting applied research and innovation. Its responsibility is to create programs, to be approved by the government, and then execute the whole lifecycle of each program -- from calls, ex-ante evaluation or proposals, handling the awards organizationally and financially throughout projects' execution, and performing ex-post evaluation of projects and the whole programs. While the ex-ante evaluation system has been described at AEA 2014, this year we plan to present results of a mid-term evaluation of a program for applied research in the area of social sciences and of the top program of the agency, the Centers of Competence program.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku byly prezentovány zušenosti Ústavu formáln a aplikované lingvistiky MFF UK s projekty v rámcových programech EU a zejména v programu Horizon 2020.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>EU Framework and Horizon 2020 projects carried out in the Institute of Formal and Applied Linguistics of the Faculty of Mathematics and Physics, Charles University in Prague, have been presented.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Přednáška představila projekty a další aktivity v oblasti jazykových technologií a speciálně strojového překladu pro tématickou oblast medicíny a lékařství.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The talk presented activities in the area of language technology and specifically machine translation in the medical domain in the context of EC-funded projects.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Projekt výzkumné infrastruktury LINDAT/CLARIN (“Vybudování a provoz českého uzlu panevropské infrastruktury pro výzkum”) je koncipován jako český „uzel“ mezinárodní sítě Clarin (Common Language Resources and Technology Infrastructure), která je zřízena ve formě ERIC (Evropské konsorcium pro výzkumnou  infrastrukturu) se sídlem v Utrechtu v Holandsku. Cílem Clarin ERIC je otevřený přístup k jazykovým datům a technologiím zejména pro humanitní a společenské obory. LINDAT/CLARIN je rovněž kompatibilní s technologickou sítí META-SHARE (http://www.meta-net.eu), která je zaměřená na jazykové technologie a aplikace, čímž tyto dvě oblasti propojuje. LINDAT/CLARIN shromažďuje,
zpracovává, anotuje (manuálně a automaticky) a uchovává jazyková data v českém jazykovém prostředí a poskytuje softwarové nástroje pro vyhledávání, analýzu a syntézu přirozeného jazyka. Použitá technologie a její kvalita a rozšiřitelnost je přímo aplikovatelná jak v humanitních a společenských vědách a výzkumu (jazykověda a interdisciplinární výzkum s jazykovou složkou, jako například formální a počítačová lingvistika, translatologie, lexikografie, sociolingvistika, psychologie, sociologie, historie, literární vědy, neurolingvistika, kognitivní vědy, umělá inteligence a strojové učení), tak ve výzkumu a
vývoji jazykových technologií založených na statistických metodách (zpracování přirozeného jazyka, rozpoznávání a syntéza řeči a kombinovaná analýza obrazu, textu a multimédií obecně, vč. text and data mining“, strojového překladu a extrakce informace).</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The project LINDAT/CLARIN (Building and operation of a Czech node of the pan-European infrastructure for research) is a Czech node of the international network Clarin ERIC (Common Language Resources and Technology Infrastructure), headquartered in Utrecht, Netherlands. The goal of Clarin is to provide open access to research data, technology and services.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Strojový překlad přirozených jazyků je problém stejně starý jako počítačová lingvistika samotná, a v jejím rámci je to problém, který byl mnohokrát podceněn jak z hlediska metodologického, tak z hlediska doby potřebné k jeho vyřešení – a ani dnes vyřešen není. Pohled na překlad prošel vývojovou spirálou od primitivních pokusů o jednoduché statistické řešení přes nesmírně komplikované algoritmy popisující všechny detaily a výjimky v přirozeném jazyce až k dnešním přístupům, které kombinují statistický a lingvistický přístup.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Machine Translation is as old as the field of Computational Linguistics itself. It is also a problem that has been predicted to „be solved in the next five years“ many times, but in fact it is not yet solved today. Machine translation has been naively considered a simple problem solvable by simple statistical means, then studied in depth by complicated but unsuccessful detailed sets of rules trying to describe all details of natural language use, only to return to statistical approach on a completely different level, using a combination of linguistic analysis and powerful machine learning algorithms.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce byl prezentován LINDAT/CLARIN jako uzel mezinárodní infrastrukturní sítě Clarin ERIC. Zejména byl prezentován repozitář LINDAT/CLARIN a jeho možnosti ukládání a archivace jazykových a jiných dat, a s ním spojené služby.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>LINDAT/CLARIN, as a node of the pan-European research infrastructure Clarin ERIC, has been presented. Its repository has been featured together with data archivation techniques and related web services and applications.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V přednášce byly prezentovány známé a používané korpusy se zachycenou syntaktickou analýzou (Penn Treebank, Pražské závislostní korpusy) a jejich vztah k analýze váceslovných výrazů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presented were existing syntactically analyzed treebanks, such as the Penn Treebank and the Prague family of dependency Treebanks, and their relation to Multiword Expressions.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Licence mezi UK v Praze a Google Inc., platná od 2. 12. 2014 na dobu neurčitou, PO 270122, fakturováno 2. 1. 2015. Obsahuje právo na užití dat PDT 2.5 a asociovaného software "MorphoDiTa" pro morfologické značkování na základě PDT 2.5, a to pro komerční účely. Jedná se o identické verze uložené v repozitáři LINDAT/CLARIN s nekomerční licencí typu Creative Commons.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Licence agreement between Charles University in Prague and Google Inc., effective Dec. 2, 2014, duration unless terminated, PO No. 270122, invoiced Jan. 2, 2015. The license covers the right to use the data of PDT 2.5 and the associated software "MorphoDiTa" for morphological tagging, for commercial purposes. The same data and software is otherwise available for free through the LINDAT/CLARIN repository for non-commercial use under the appropriate variant of the Creative COmmons license.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Cílem tohoto příspěvku je podrobně prozkoumat, jak se v syntakticky anotovaných korpusech zachází s prvky věty, které jsou na povrchu vypuštěné, a pokusit se o kategorizaci výpustek v rámci víceúrovňového anotačního schématu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>The aim of the present contribution is to put under scrutiny the ways in which the so-called deletions of elements in the surface shape of the sentence are treated in syntactically annotated corpora and to attempt at a categorization of deletions within a multilevel annotation scheme.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Projekt Amalach vytvořil software pro prohledávání nahrávek archivu VHA v češtině a angličtině. Součástí projektu byl rovněž překlad tezauru použitého pro indexaci archivu. Přednáška uvedla celou problematiku vyhledávání a audioarchivu VHA.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Project Amalach has resulted in software for cross=language search in the VHA archive. It also resulted in the Czech version of the VHA thesaurus. The talk has presented the topic of audio search in the VHA archive.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Nekrolog významného českého lingvisty prof. dr. Františka Daneše, DrSc. Zamyšlení nad rozpětím jeho zájmů a nad jeho přínosy české a světové lingvistice.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Obituary of prof. dr. František Daneš,DrSc. A survay of his wide scale of linguistic interests and of his important contributions to the Czech and word linguistics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Srovnávací struktury uvozené v češtině výrazy jako a než se reprezentují jako gramatikalizované elipsy, jejichž zdrojem jsou rekonstruované predikační struktury s opakováním slov z řídící predikace nebo zastoupené umělým uzlem s obecnou sémantickou platností. Dále se analyzuje srovnání uvozeneé sekundárními předložkami.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Czech adverbial constructions introduced by the expressions jako and než are understood as grmmaticalized ellipsis. Their undelying structures are proposed in the shape of reconstructed constructions containing either the repeated items from the governing predication or by the new artfitial nodes with general semantics.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V příspěvku se prezentuje způsob zachycení vybraných typů elips v češtině v rámci FGP. Jde zejména o vypouštění zájmenného subjektu v 1. a 2. osobě, o nepřítomnost subjektu v kontrolovaných infinitivních konstrukcích,o konstrukce uvozené "kromě" a "místo" a o srovnávací konstrukce.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this contribution the treatment of selected types of deletions in Czech are presented. The pro-dropped constructions,absent subjects in controlled infinitive constructions, selected types of Czech "small clauses" and comparison constructions are analyzed and exemplified.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Prezentace pokrývá formát PML (Prague Markup Language) pro reprezentaci liongvistických dat, nástroje pracující s tímto formátem (Treex, TrEd, PML-TQ) a na příkladech anotovaných dat ukazuje využití PML a nástrojů pro reprezentaci a vyhledávání lingvistických jevů.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>Presentation covers PML format and tools using it (Treex, PML-TQ, TrEd). Using real annotated data as examples representation of constructions and searching for them using the tools was demonstrated.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V tomto článku se zabýváme problémem doménové adaptace statistického strojového překladu (SMT) s využitím doménově specifických dat získaných cíleným prohledáváním (crawling) v Internetu. Navrhujeme a empiricky vyhodnocujeme proces automatického získávání jednojazyčných a paralelních textů, stejně jako jejich využití v trénování, optimalizaci i testování v rámci frázového SMT. Navrhujeme strategii, jak tyto zdroje využít v závislosti na jejich dostupnosti a množství, která ja podpořena výsledky rozsáhlé evaluace. Ta byla provedena v doménách legislativy pro životní prostředí a práci, ve dvou jazykových párech (angličtina-francouzština a angličtina-řečtina) a v obou směrem: z angličtiny i do angličtiny. Obecně lze tvrdit, že systémy SMT trénované a optimalizované na datech z obecné domény na specifických doménách dosahují špatných výsledků. Ukazujeme, že takové systémy lze úspěšně adaptovat optimalizací parametrů modelu s využitím malého množství paralelních dat z cílové domény, a dále je lze zlepšit přidáním jednojazyčných i paralelních dat pro adaptaci jazykového, resp. překladového modelu. Průměrné pozorované zlepšení v BLEU o 15,3 absolutních bodů je významné.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this paper, we tackle the problem of domain adaptation of statistical machine
translation (SMT) by exploiting domain-specific data acquired by domain-focused crawling
of text from the World Wide Web. We design and empirically evaluate a procedure for auto-
matic acquisition of monolingual and parallel text and their exploitation for system training,
tuning, and testing in a phrase-based SMT framework. We present a strategy for using such
resources depending on their availability and quantity supported by results of a large-scale
evaluation carried out for the domains of environment and labour legislation, two language
pairs (English–French and English–Greek) and in both directions: into and from English.
In general, MT systems trained and tuned on a general domain perform poorly on specific
domains and we show that such systems can be adapted successfully by retuning model
parameters using small amounts of parallel in-domain data, and may be further improved
by using additional monolingual and parallel training data for adaptation of language and
translation models. The average observed improvement in BLEU achieved is substantial at
15.30 points absolute.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Webová služba pro podporu indexování naskenovaných obsahů knih. Pro Národní technickou knihovnu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>A webservice supporting indexation of book table of contents. For the National Technical Library.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>CzEngVallex je dvojjazyčný valenční slovník, který obsahuje provázané dvojice českých a anglických sloves. Zahrnuje 20835 odpovídajících slovesných dvojic (překladových ekvivalentů) valenčních rámců (významů slovesa) a zachycuje také propojení jejich argumentů. Tato databáze dvojic rámců a jejich argumentů je založena na reálných textech a může být využita například v aplikacích pro strojový překlad.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>CzEngVallex is a bilingual valency lexicon of corresponding Czech and English verbs. It connects 20835 aligned valency frame pairs (verb senses) which are translations of each other, aligning their arguments as well. The CzEngVallex serves as a powerful, real-text-based database of frame-to-frame and subsequently argument-to-argument pairs and can be used for example for machine translation applications. It uses the data from the Prague Czech-English Dependency Treebank project.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Technická zprava shrnuje pravidla pro tvorbu valenčního slovníku (CzEngVallexu) českých a anglických sloves,  budovaného na základě paralelního závislostního korpusu (PCEDT) a dvou existujících valenčních slovníků: PDT-Vallexu a EngVallexu.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This report presents a guideline for building a valency lexicon (CzEngVallex) of Czech and English verbs  based on the parallel dependency corpus (PCEDT) and two existing valency lexicons: PDT-Vallex a EngVallex.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Příspěvek popisuje budování dvojjazyčného valenčního anglicko-českého slovníku CzEngVallex, budovaného na základě paralelního překladového Pražského česko-anglického závislostního korpusu. Díky propojení s korpusovými daty bude možné slovník využívat jak pro lingvistický výzkum, tak pro NLP aplikace.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>This paper presents a resource and the associated annotation process used in a project of interlinking Czech and English verbal translational equivalents based on a parallel, richly annotated dependency treebank containing also valency and semantic roles, namely the Prague Czech-English Dependency Treebank. One of the main aims of this project is to create a high-quality and relatively large empirical base which could be used both for linguistic comparative research as well as for natural language processing applications, such as machine translation or cross-language sense disambiguation. This paper describes the resulting lexicon, CzEngVallex, and the process of building it, as well some interesting observations and statistics already obtained.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Sketch Engine byl využit zejména pro vytipování dvojic substantiv tvořících nominalizace verbonominálních predikátů s kategoriálním slovesem. V prezentaci byly rovněž představeny odlišné syntaktické vlastnosti konstrukcí s plnovýznamovými substantivy a konstrukcí s týmiž substantivy, užitými ve vyprázdněném významu jako nominalizace kategoriálních sloves.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>I use the Sketch Engine in order to match up collocations standing for particular nominalizations of SVCs. I also search for valency properties of nouns in question using CQL query type which, to some extent, allows searching for syntactic relations in linear corpora. Nominalizations of support verbs display different valency behaviour when compared to notional nouns, avoiding deverbal compounds in English, cf. selling the books / book(-/ )selling / sale of books / books sale vs. selling the idea / *sale of the idea / *idea sale, or loss of memory / memory loss vs. loss of confidence / *confidence loss. In Czech, semantic impoverishing of nominalizations of support verbs has an impact on ways of expressing their Agent, allowing double post-nominal genitives (e.g. sbírání zkušeností nejmladších závodníků ‘gaining experience-GEN.PL [by] youngest-GEN.PL racer-GEN.PL’) which are ungrammatical when the nouns have full semantic meaning. Some nominalized SVCs also allow so-called crossing of nominal valency relations which is distinctive esp. in Czech but also possible in English, cf. giving (opportunity) to the inhabitants vs. opportunity to provide in giving opportunity to the inhabitants to provide for themselves.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>V této přednášce popíšu svou práci na univerzálním zachycení morfologie a závislostní syntaxe v korpusech různých jazyků. Taková harmonizace je nejen výhodná pro lingvisty-uživatele korpusů, ale je také nezbytným předpokladem pro techniky mezijazykové adaptace parserů, např. delexikalizovaného parsingu. Představím Interset, nástroj podobný interlingvě pro překlad morfosyntaktických reprezentací mezi sadami značek; ukážu také, jak se rysy z Intersetu používají v novém formalismu nazvaném Universal Dependencies. Nakonec proberu důležitost jednotlivých morfologických rysů s ohledem na závislostní parsing.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>In this talk I will describe my work towards universal representation of morphology and dependency syntax in treebanks of various languages. Not only is such harmonization advantageous for linguists-users of corpora, it is also a prerequisite for cross-language parser adaptation techniques such as delexicalized parsing. I will present Interset, an interlingua-like tool to translate morphosyntactic representations between tagsets; I will also show how the features from Interset are used in a recent framework called Universal Dependencies. Some experiments with delexicalized parsing on harmonized data will be presented. Finally, I will discuss the extent to which various morphological features are important in the context of statistical dependency parsing.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Zabýváme se aplikací UD na slovanské jazyky. Nejvíce prostoru věnujeme zájmenům, determinátorům, číslovkám a kvantifikátorům. Kromě toho diskutujeme i další jazykové jevy jako způsobová slovesa, elipsu, jmenné přísudky a zvratná zájmena. Většina našich příkladů pochází z češtiny, ale jazykové jevy, které na příkladech ukazujeme, jsou obvykle přenositelné na další slovanské jazyky. Tam, kde je to vhodné, uvádíme příklady i z jiných jazyků.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>We address the application of UD to Slavic languages. We devote the most space to peculiarities of pronouns, determiners, numerals and quantifiers. Other language features that are discussed include modal verbs, ellipsis, nominal predicates, and reflexive pronouns. Most of our examples are from Czech but the language features demonstrated are usually portable to other Slavic languages. We include examples from the other languages where appropriate.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>HamleDT (HArmonized Multi-LanguagE Dependency Treebank) je sbírka existujících závislostních korpusů (nebo do závislostí převedených jiných syntaktických korpusů), transformovaných do jednotného anotačního stylu. Tato verze používá Universal Dependencies jako svůj ústřední anotační styl.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>HamleDT (HArmonized Multi-LanguagE Dependency Treebank) is a compilation of existing dependency treebanks (or dependency conversions of other treebanks), transformed so that they all conform to the same annotation style. This version uses Universal Dependencies as the common annotation style.</seg>
            </tuv>
        </tu>
        
        <tu>
            <tuv xml:lang="cs">
                <seg>Jako výzkumníci jsme přistoupili k vývoji systémů extrakce informací z textů legislativní a enviromentální domény. V prezentaci systémy představíme a podělíme se o zkušenosti z jejich vývoje. Zaměříme se na alternativy strojové učení vs. pravidla a surové texty vs. texty obohacené o informace. Aspekty správy systémů a způsobu hodnocení jejich kvality budou rovněž diskutovány.</seg>
            </tuv>
            <tuv xml:lang="en">
                <seg>As researchers, we decided to develop information extraction systems focused on texts from the legislative and environmental domains. We will present them and we will share the experience on their development. We will focus on the alternatives machine learning vs. rules and raw texts vs. enriched texts. Aspects of their management and evaluation will be discussed as well.</seg>
            </tuv>
        </tu>
      
  </body>
</tmx>
